<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>SpringBoot自动装配</title>
      <link href="/2023/07/08/2023-07-08-springboot-zi-dong-zhuang-pei/"/>
      <url>/2023/07/08/2023-07-08-springboot-zi-dong-zhuang-pei/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《SpringBoot 源码解读与原理分析》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="SpringFramework的手动装配"><a href="#SpringFramework的手动装配" class="headerlink" title="SpringFramework的手动装配"></a>SpringFramework的手动装配</h2><p>在原生的 SpringFramework 中，装配组件有三种方式：</p><ul><li>使用模式注解 @Component 等（Spring2.5+）</li><li>使用配置类 @Configuration 与 @Bean （Spring3.0+）</li><li>使用模块装配 @EnableXXX 与 @Import （Spring3.1+）</li></ul><p>SpringFramework 提供了模块装配功能，通过给配置类标注 @EnableXXX 注解，再在注解上标注 @Import 注解，即可完成组件装配的效果。</p><h3 id="EnableXXX与-Import的使用"><a href="#EnableXXX与-Import的使用" class="headerlink" title="@EnableXXX与@Import的使用"></a>@EnableXXX与@Import的使用</h3><h4 id="Import"><a href="#Import" class="headerlink" title="@Import"></a>@Import</h4><pre><code>public @interface Import {    /**     * {@link Configuration @Configuration}, {@link ImportSelector},     * {@link ImportBeanDefinitionRegistrar}, or regular component classes to import.     */    Class&lt;?&gt;[] value();}</code></pre><p>@Import value中写的很明白了，可以导入配置类、ImportSelector 的实现类，ImportBeanDefinitionRegistrar 的实现类，或者普通类。</p><h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><p>创建几个颜色的实体类，如Red，Yellow，Blue，Green，Black等。</p><p>新建 @EnableColor 注解：</p><pre><code>@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)public @interface EnableColor {}</code></pre><h4 id="导入普通类"><a href="#导入普通类" class="headerlink" title="导入普通类"></a>导入普通类</h4><p>直接在 @Import 注解中标注Red类：</p><pre><code>@Import({Red.class})public @interface EnableColor {}</code></pre><p>之后启动类标注 @EnableColor，引导启动IOC容器：</p><pre><code>@EnableColor@Configurationpublic class ColorConfiguration {}public class App {    public static void main(String[] args) throws Exception {        AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext(ColorConfiguration.class);        String[] beanDefinitionNames = ctx.getBeanDefinitionNames();        Stream.of(beanDefinitionNames).forEach(System.out::println);    }}</code></pre><p>控制台打印：</p><pre><code>org.springframework.context.annotation.internalConfigurationAnnotationProcessororg.springframework.context.annotation.internalAutowiredAnnotationProcessororg.springframework.context.annotation.internalCommonAnnotationProcessororg.springframework.context.event.internalEventListenerProcessororg.springframework.context.event.internalEventListenerFactorycolorConfigurationcom.example.demo.enablexxx.Red</code></pre><p>可见Red类已经被注册。</p><h4 id="导入配置类"><a href="#导入配置类" class="headerlink" title="导入配置类"></a>导入配置类</h4><p>新建 ColorRegistrarConfiguration，并标注 @Configuration ：</p><pre><code>@Configurationpublic class ColorRegistrarConfiguration {    @Bean    public Yellow yellow() {        return new Yellow();    }}</code></pre><p>之后在 @EnableColor 的 @Import 注解中加入 ColorRegistrarConfiguration：</p><pre><code>@Import({Red.class, ColorRegistrarConfiguration.class})public @interface EnableColor {}</code></pre><p>控制台打印：</p><pre><code>org.springframework.context.annotation.internalConfigurationAnnotationProcessororg.springframework.context.annotation.internalAutowiredAnnotationProcessororg.springframework.context.annotation.internalCommonAnnotationProcessororg.springframework.context.event.internalEventListenerProcessororg.springframework.context.event.internalEventListenerFactorycolorConfigurationcom.example.demo.enablexxx.Redcom.example.demo.enablexxx.ColorRegistrarConfigurationyellow</code></pre><p>可见配置类 ColorRegistrarConfiguration 和 Yellow 都已注册到IOC容器中。</p><h4 id="导入ImportSelector"><a href="#导入ImportSelector" class="headerlink" title="导入ImportSelector"></a>导入ImportSelector</h4><p>新建 ColorImportSelector，实现 ImportSelector 接口：</p><pre><code>public class ColorImportSelector implements ImportSelector {    @Override    public String[] selectImports(AnnotationMetadata importingClassMetadata) {        return new String[] {Blue.class.getName(), Green.class.getName()};    }}</code></pre><p>之后在 @EnableColor 的 @Import 注解中加入 ColorImportSelector：</p><pre><code>@Import({Red.class, ColorRegistrarConfiguration.class, ColorImportSelector.class})public @interface EnableColor {}</code></pre><p>控制台打印：</p><pre><code>org.springframework.context.annotation.internalConfigurationAnnotationProcessororg.springframework.context.annotation.internalAutowiredAnnotationProcessororg.springframework.context.annotation.internalCommonAnnotationProcessororg.springframework.context.event.internalEventListenerProcessororg.springframework.context.event.internalEventListenerFactorycolorConfigurationcom.example.demo.enablexxx.Redcom.example.demo.enablexxx.ColorRegistrarConfigurationyellowcom.example.demo.enablexxx.Bluecom.example.demo.enablexxx.Green</code></pre><p>ColorImportSelector 没有注册到IOC容器中，两个新的颜色类被注册。</p><h4 id="导入ImportBeanDefinitionRegistrar"><a href="#导入ImportBeanDefinitionRegistrar" class="headerlink" title="导入ImportBeanDefinitionRegistrar"></a>导入ImportBeanDefinitionRegistrar</h4><p>新建 ColorImportBeanDefinitionRegistrar，实现 ImportBeanDefinitionRegistrar 接口：</p><pre><code>public class ColorImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar {    @Override    public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) {        registry.registerBeanDefinition(&quot;black&quot;, new RootBeanDefinition(Black.class));    }}</code></pre><p>之后在 @EnableColor 的 @Import 注解中加入 ColorImportBeanDefinitionRegistrar：</p><pre><code>@Import({Red.class, ColorRegistrarConfiguration.class, ColorImportSelector.class, ColorImportBeanDefinitionRegistrar.class})public @interface EnableColor {}</code></pre><p>控制台打印：</p><pre><code>org.springframework.context.annotation.internalConfigurationAnnotationProcessororg.springframework.context.annotation.internalAutowiredAnnotationProcessororg.springframework.context.annotation.internalCommonAnnotationProcessororg.springframework.context.event.internalEventListenerProcessororg.springframework.context.event.internalEventListenerFactorycolorConfigurationcom.example.demo.enablexxx.Redcom.example.demo.enablexxx.ColorRegistrarConfigurationyellowcom.example.demo.enablexxx.Bluecom.example.demo.enablexxx.Greenblack</code></pre><p>由于在注册Black的时候要指定Bean的id，而上面已经标明了使用 “black” 作为id，故打印的 beanDefinitionName 就是black。</p><h2 id="SpringBoot的自动装配"><a href="#SpringBoot的自动装配" class="headerlink" title="SpringBoot的自动装配"></a>SpringBoot的自动装配</h2><p>SpringBoot的自动配置完全由 @EnableAutoConfiguration 开启。</p><p>@EnableAutoConfiguration 的内容：</p><pre><code>@AutoConfigurationPackage@Import(AutoConfigurationImportSelector.class)public @interface EnableAutoConfiguration</code></pre><p>文档注释原文翻译：（文档注释很长，但句句精华）</p><blockquote><p>启用Spring-ApplicationContext的自动配置，并且会尝试猜测和配置您可能需要的Bean。通常根据您的类路径和定义的Bean来应用自动配置类。例如，如果您的类路径上有 tomcat-embedded.jar，则可能需要 TomcatServletWebServerFactory （除非自己已经定义了 ServletWebServerFactory 的Bean）。<br>使用 @SpringBootApplication 时，将自动启用上下文的自动配置，因此再添加该注解不会产生任何其他影响。<br>自动配置会尝试尽可能地智能化，并且在您定义更多自定义配置时会自动退出（被覆盖）。您始终可以手动排除掉任何您不想应用的配置（如果您无法访问它们，请使用 excludeName() 方法），您也可以通过 spring.autoconfigure.exclude 属性排除它们。自动配置始终在注册用户自定义的Bean之后应用。<br>通常被 @EnableAutoConfiguration 标注的类（如 @SpringBootApplication）的包具有特定的意义，通常被用作“默认值”。例如，在扫描@Entity类时将使用它。通常建议您将 @EnableAutoConfiguration（如果您未使用 @SpringBootApplication）放在根包中，以便可以搜索所有包及子包下的类。<br>自动配置类也是常规的Spring配置类。它们使用 SpringFactoriesLoader 机制定位（针对此类）。通常自动配置类也是 @Conditional Bean（最经常的情况下是使用 @ConditionalOnClass 和 @ConditionalOnMissingBean 标注）。</p></blockquote><p>@EnableAutoConfiguration 是一个组合注解，分别来看：</p><h3 id="AutoConfigurationPackage"><a href="#AutoConfigurationPackage" class="headerlink" title="@AutoConfigurationPackage"></a>@AutoConfigurationPackage</h3><pre><code>@Import(AutoConfigurationPackages.Registrar.class)public @interface AutoConfigurationPackage</code></pre><p>文档注释原文翻译：</p><blockquote><p>Indicates that the package containing the annotated class should be registered with AutoConfigurationPackages.<br>表示包含该注解的类所在的包应该在 AutoConfigurationPackages 中注册。</p></blockquote><p>我们从一开始学 SpringBoot 就知道一件事：主启动类必须放在所有自定义组件的包的最外层，以保证Spring能扫描到它们。由此可知是它起的作用。</p><p>它的实现原理是在注解上标注了 @Import，导入了一个 AutoConfigurationPackages.Registrar 。</p><h4 id="AutoConfigurationPackages-Registrar"><a href="#AutoConfigurationPackages-Registrar" class="headerlink" title="AutoConfigurationPackages.Registrar"></a>AutoConfigurationPackages.Registrar</h4><pre><code>/** * {@link ImportBeanDefinitionRegistrar} to store the base package from the importing * configuration. */static class Registrar implements ImportBeanDefinitionRegistrar, DeterminableImports {    @Override    public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) {        register(registry, new PackageImport(metadata).getPackageName());    }    @Override    public Set&lt;Object&gt; determineImports(AnnotationMetadata metadata) {        return Collections.singleton(new PackageImport(metadata));    }}</code></pre><p>文档注释原文翻译：</p><blockquote><p>ImportBeanDefinitionRegistrar to store the base package from the importing configuration.<br>用于保存导入的配置类所在的根包。</p></blockquote><p>很明显，它就是实现把主配置所在根包保存起来以便后期扫描用的。分析源码：</p><p>Registrar 实现了 ImportBeanDefinitionRegistrar 接口，它向IOC容器中要手动注册组件。</p><p>在重写的 registerBeanDefinitions 方法中，它要调用外部类 AutoConfigurationPackages 的register方法。</p><p>实例化的 PackageImport 对象的构造方法：</p><pre><code>PackageImport(AnnotationMetadata metadata) {    this.packageName = ClassUtils.getPackageName(metadata.getClassName());}</code></pre><p>它取了一个 metadata 的所在包名。那 metadata 又是什么呢？</p><p><img src="/2023/07/08/2023-07-08-springboot-zi-dong-zhuang-pei/1688874776482.jpg" alt></p><p>翻看 ImportBeanDefinitionRegistrar 的文档注释：</p><pre><code>public interface ImportBeanDefinitionRegistrar {    /**     * ......     * @param importingClassMetadata annotation metadata of the importing class     * @param registry current bean definition registry     */    void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry);}</code></pre><p>注意 importingClassMetadata 的参数说明：导入类的注解元数据。</p><p>它实际代表的是被 @Import 标记的类的信息。</p><p>那在 SpringBoot 的主启动类中，被标记的肯定就是最开始案例里的 DemoApplication。</p><p>也就是说它是 DemoApplication 的类信息，那获取它的包名就是获取主启动类的所在包。</p><h4 id="register方法"><a href="#register方法" class="headerlink" title="register方法"></a>register方法</h4><pre><code>private static final String BEAN = AutoConfigurationPackages.class.getName();public static void register(BeanDefinitionRegistry registry, String... packageNames) {    // 判断 BeanFactory 中是否包含 AutoConfigurationPackages    if (registry.containsBeanDefinition(BEAN)) {        BeanDefinition beanDefinition = registry.getBeanDefinition(BEAN);        ConstructorArgumentValues constructorArguments = beanDefinition.getConstructorArgumentValues();        // addBasePackages：添加根包扫描包        constructorArguments.addIndexedArgumentValue(0, addBasePackages(constructorArguments, packageNames));    }    else {        GenericBeanDefinition beanDefinition = new GenericBeanDefinition();        beanDefinition.setBeanClass(BasePackages.class);        beanDefinition.getConstructorArgumentValues().addIndexedArgumentValue(0, packageNames);        beanDefinition.setRole(BeanDefinition.ROLE_INFRASTRUCTURE);        registry.registerBeanDefinition(BEAN, beanDefinition);    }}</code></pre><p>划重点：它要判断当前IOC容器中是否包含 AutoConfigurationPackages 。如果有，就会拿到刚才传入的包名，设置到一个 basePackage 里面！basePackage 的意义很明显是根包。</p><p>换句话说，它要取主启动类所在包及子包下的组件。</p><p><img src="/2023/07/08/2023-07-08-springboot-zi-dong-zhuang-pei/1688874833769.jpg" alt></p><p>不过，在实际Debug时，并不是走的上面流程，因为 AutoConfigurationPackages 对应的 Bean 还没有创建，所以走的下面的 else 部分，直接把主启动类所在包放入 BasePackages 中，与上面 if 结构中最后一句一样，都是调用 addIndexedArgumentValue 方法。那这个 BasePackages 中设置了构造器参数，一定会有对应的成员：</p><pre><code>static final class BasePackages {    private final List&lt;String&gt; packages;    BasePackages(String... names) {        List&lt;String&gt; packages = new ArrayList&lt;&gt;();        for (String name : names) {            if (StringUtils.hasText(name)) {                packages.add(name);            }        }        this.packages = packages;    }</code></pre><p><img src="/2023/07/08/2023-07-08-springboot-zi-dong-zhuang-pei/1688874932157.jpg" alt></p><h4 id="basePackage的作用"><a href="#basePackage的作用" class="headerlink" title="basePackage的作用"></a>basePackage的作用</h4><p>如果这个 basePackage 的作用仅仅是提供给 SpringFramework 和 SpringBoot 的内部使用，那这个设计似乎有一点多余。回想一下，SpringBoot 的强大之处，有一点就是整合第三方技术可以非常的容易。以咱最熟悉的 MyBatis 为例，咱看看 basePackage 如何在整合第三方技术时被利用。</p><p>引入 mybatis-spring-boot-starter 依赖后，可以在 IDEA 中打开 MyBatisAutoConfiguration 类。在这个配置类中，咱可以找到这样一个组件：AutoConfiguredMapperScannerRegistrar</p><pre><code>public static class AutoConfiguredMapperScannerRegistrar implements BeanFactoryAware, ImportBeanDefinitionRegistrar {    private BeanFactory beanFactory;    @Override    public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) {        if (!AutoConfigurationPackages.has(this.beanFactory)) {            logger.debug(&quot;Could not determine auto-configuration package, automatic mapper scanning disabled.&quot;);            return;        }        logger.debug(&quot;Searching for mappers annotated with @Mapper&quot;);        List&lt;String&gt; packages = AutoConfigurationPackages.get(this.beanFactory);        // logger ......        // 注册Mapper ......    }</code></pre><p>看类名也能看的出来，它是扫描 Mapper 并注册到 IOC 容器的 ImportBeanDefinitionRegistrar ！那这里头，取扫描根包的动作就是 AutoConfigurationPackages.get(this.beanFactory) ，由此就可以把事先准备好的 basePackages 都拿出来，之后进行扫描。</p><p>也解释了为什么 SpringBoot 的启动器一定要在所有类的最外层。</p><h3 id="Import-AutoConfigurationImportSelector-class"><a href="#Import-AutoConfigurationImportSelector-class" class="headerlink" title="@Import(AutoConfigurationImportSelector.class)"></a>@Import(AutoConfigurationImportSelector.class)</h3><p>它导入了一个 ImportSelector，来向容器中导入组件。导入的组件是：AutoConfigurationImportSelector</p><h4 id="AutoConfigurationImportSelector"><a href="#AutoConfigurationImportSelector" class="headerlink" title="AutoConfigurationImportSelector"></a>AutoConfigurationImportSelector</h4><pre><code>public class AutoConfigurationImportSelector implements DeferredImportSelector, BeanClassLoaderAware,        ResourceLoaderAware, BeanFactoryAware, EnvironmentAware, Ordered</code></pre><p>文档注释原文翻译：</p><blockquote><p>DeferredImportSelector to handle auto-configuration. This class can also be subclassed if a custom variant of @EnableAutoConfiguration is needed.<br>DeferredImportSelector 处理自动配置。如果需要自定义扩展 @EnableAutoConfiguration，则也可以编写该类的子类。</p></blockquote><h4 id="DeferredImportSelector"><a href="#DeferredImportSelector" class="headerlink" title="DeferredImportSelector"></a>DeferredImportSelector</h4><pre><code>public interface DeferredImportSelector extends ImportSelector</code></pre><p>它是 ImportSelector 的子接口，它的文档注释原文和翻译：</p><blockquote><p>ImportSelector 的一种扩展，在处理完所有 @Configuration 类型的Bean之后运行。当所选导入为 @Conditional 时，这种类型的选择器特别有用。<br>实现类还可以扩展 Ordered 接口，或使用 @Order 注解来指示相对于其他 DeferredImportSelector 的优先级。<br>实现类也可以提供导入组，该导入组可以提供跨不同选择器的其他排序和筛选逻辑。</p></blockquote><p>DeferredImportSelector 的执行时机，是在 @Configuration 注解中的其他逻辑被处理完毕之后（包括对 @ImportResource、@Bean 这些注解的处理）再执行，换句话说，DeferredImportSelector 的执行时机比 ImportSelector 更晚。</p><p>AutoConfigurationImportSelector，它的核心部分，就是 ImportSelector 的 selectImport 方法：</p><pre><code>@Overridepublic String[] selectImports(AnnotationMetadata annotationMetadata) {    if (!isEnabled(annotationMetadata)) {        return NO_IMPORTS;    }    AutoConfigurationMetadata autoConfigurationMetadata = AutoConfigurationMetadataLoader            .loadMetadata(this.beanClassLoader);    // 加载自动配置类    AutoConfigurationEntry autoConfigurationEntry = getAutoConfigurationEntry(autoConfigurationMetadata,             annotationMetadata);    return StringUtils.toStringArray(autoConfigurationEntry.getConfigurations());}</code></pre><p>关键的源码在 getAutoConfigurationEntry(autoConfigurationMetadata, annotationMetadata) ：</p><h4 id="getAutoConfigurationEntry-autoConfigurationMetadata-annotationMetadata"><a href="#getAutoConfigurationEntry-autoConfigurationMetadata-annotationMetadata" class="headerlink" title="getAutoConfigurationEntry(autoConfigurationMetadata, annotationMetadata)"></a>getAutoConfigurationEntry(autoConfigurationMetadata, annotationMetadata)</h4><pre><code>/** * Return the {@link AutoConfigurationEntry} based on the {@link AnnotationMetadata} * of the importing {@link Configuration @Configuration} class. *  * 根据导入的@Configuration类的AnnotationMetadata返回AutoConfigurationImportSelector.AutoConfigurationEntry。 */protected AutoConfigurationEntry getAutoConfigurationEntry(AutoConfigurationMetadata autoConfigurationMetadata,         AnnotationMetadata annotationMetadata) {    if (!isEnabled(annotationMetadata)) {        return EMPTY_ENTRY;    }    AnnotationAttributes attributes = getAttributes(annotationMetadata);    // 【核心】加载候选的自动配置类    List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes);    configurations = removeDuplicates(configurations);    Set&lt;String&gt; exclusions = getExclusions(annotationMetadata, attributes);    checkExcludedClasses(configurations, exclusions);    configurations.removeAll(exclusions);    configurations = filter(configurations, autoConfigurationMetadata);    fireAutoConfigurationImportEvents(configurations, exclusions);    return new AutoConfigurationEntry(configurations, exclusions);}</code></pre><p>这个方法里有一个非常关键的集合：configurations（最后直接拿他来返回出去了，给 selectImports 方法转成 String[]）。</p><p>这个 configurations 集合的数据，都是通过 getCandidateConfigurations 方法来获取：</p><pre><code>protected Class&lt;?&gt; getSpringFactoriesLoaderFactoryClass() {    return EnableAutoConfiguration.class;}protected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) {    // SPI机制加载自动配置类    List&lt;String&gt; configurations = SpringFactoriesLoader.loadFactoryNames(getSpringFactoriesLoaderFactoryClass(),             getBeanClassLoader());    Assert.notEmpty(configurations, &quot;No auto configuration classes found in META-INF/spring.factories. If you &quot;             + &quot;are using a custom packaging, make sure that file is correct.&quot;);    return configurations;}</code></pre><p>这个方法又调用了 SpringFactoriesLoader.loadFactoryNames 方法，传入的Class就是 @EnableAutoConfiguration</p><h4 id="SpringFactoriesLoader-loadFactoryNames"><a href="#SpringFactoriesLoader-loadFactoryNames" class="headerlink" title="SpringFactoriesLoader.loadFactoryNames"></a>SpringFactoriesLoader.loadFactoryNames</h4><pre><code>public static final String FACTORIES_RESOURCE_LOCATION = &quot;META-INF/spring.factories&quot;;public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryClass, @Nullable ClassLoader classLoader) {    String factoryClassName = factoryClass.getName();    //     ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓    return loadSpringFactories(classLoader).getOrDefault(factoryClassName, Collections.emptyList());}private static Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(@Nullable ClassLoader classLoader) {    MultiValueMap&lt;String, String&gt; result = cache.get(classLoader);    if (result != null) {        return result;    }    try {        // ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓        Enumeration&lt;URL&gt; urls = (classLoader != null ?                 classLoader.getResources(FACTORIES_RESOURCE_LOCATION) :                 ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION));        result = new LinkedMultiValueMap&lt;&gt;();        while (urls.hasMoreElements()) {            URL url = urls.nextElement();            UrlResource resource = new UrlResource(url);            Properties properties = PropertiesLoaderUtils.loadProperties(resource);            for (Map.Entry&lt;?, ?&gt; entry : properties.entrySet()) {                String factoryClassName = ((String) entry.getKey()).trim();                for (String factoryName : StringUtils.commaDelimitedListToStringArray((String) entry.getValue())) {                    result.add(factoryClassName, factoryName.trim());                }            }        }        cache.put(classLoader, result);        return result;    }    catch (IOException ex) {        throw new IllegalArgumentException(&quot;Unable to load factories from location [&quot; +                                       FACTORIES_RESOURCE_LOCATION + &quot;]&quot;, ex);    }}</code></pre><p>源码中使用 classLoader 去加载了指定常量路径下的资源： FACTORIES_RESOURCE_LOCATION ，而这个常量指定的路径实际是：META-INF/spring.factories 。</p><p>这个文件在 spring-boot-autoconfiguration 包下可以找到。</p><p>spring-boot-autoconfiguration 包下 META-INF/spring.factories 节选：</p><pre><code># Initializersorg.springframework.context.ApplicationContextInitializer=\org.springframework.boot.autoconfigure.SharedMetadataReaderFactoryContextInitializer,\org.springframework.boot.autoconfigure.logging.ConditionEvaluationReportLoggingListener# Application Listenersorg.springframework.context.ApplicationListener=\org.springframework.boot.autoconfigure.BackgroundPreinitializer# Auto Configuration Import Listenersorg.springframework.boot.autoconfigure.AutoConfigurationImportListener=\org.springframework.boot.autoconfigure.condition.ConditionEvaluationReportAutoConfigurationImportListener# Auto Configuration Import Filtersorg.springframework.boot.autoconfigure.AutoConfigurationImportFilter=\org.springframework.boot.autoconfigure.condition.OnBeanCondition,\org.springframework.boot.autoconfigure.condition.OnClassCondition,\org.springframework.boot.autoconfigure.condition.OnWebApplicationCondition# Auto Configureorg.springframework.boot.autoconfigure.EnableAutoConfiguration=\org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\org.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration,\......</code></pre><p>之后拿到这个资源文件，以 Properties 的形式加载，并取出 org.springframework.boot.autoconfigure.EnableAutoConfiguration 指定的所有自动配置类（是一个很大的字符串，里面都是自动配置类的全限定类名），装配到IOC容器中，之后自动配置类就会通过 ImportSelector 和 @Import 的机制被创建出来，之后就生效了。</p><p>这也就解释了为什么 即便没有任何配置文件，SpringBoot的Web应用都能正常运行。</p><p>从上面的 Properties 中发现，所有配置的 EnableAutoConfiguration 的自动配置类，都以 AutoConfiguration 结尾！由此规律，以后我们要了解一个 SpringBoot 的模块或者第三方集成的模块时，就可以大胆猜测基本上一定会有 XXXAutoConfiguration 类出现！</p><h3 id="SpringBoot使用的工厂机制"><a href="#SpringBoot使用的工厂机制" class="headerlink" title="SpringBoot使用的工厂机制"></a>SpringBoot使用的工厂机制</h3><p>SpringBoot 在非常多的位置都利用类似于上面 “通过读取 spring.factories 加载一组预先配置的类” 的机制，而这个机制的核心源码来自 SpringFactoriesLoader 。</p><pre><code>package org.springframework.core.io.support;/** * ...... * * @since 3.2 */public final class SpringFactoriesLoader</code></pre><p>我们发现它不是来自 SpringBoot，而是在 SpringFramework3.2 就已经有了的类。它的文档注释原文翻译：</p><blockquote><p>它是一个框架内部内部使用的通用工厂加载机制。<br>SpringFactoriesLoader 从 META-INF/spring.factories 文件中加载并实例化给定类型的工厂，这些文件可能存在于类路径中的多个jar包中。spring.factories 文件必须采用 properties 格式，其中key是接口或抽象类的全限定名，而value是用逗号分隔的实现类的全限定类名列表。<br>例如：example.MyService=example.MyServiceImpl1,example.MyServiceImpl2<br>其中 example.MyService 是接口的名称，而 MyServiceImpl1 和 MyServiceImpl2 是两个该接口的实现类。</p></blockquote><p>到这里已经能够发现，这个思路跟Java原生的SPI非常类似。</p><h4 id="Java的SPI"><a href="#Java的SPI" class="headerlink" title="Java的SPI"></a>Java的SPI</h4><p>SPI全称为 Service Provider Interface，是jdk内置的一种服务提供发现机制。简单来说，它就是一种动态替换发现的机制。</p><p>SPI规定，所有要预先声明的类都应该放在 META-INF/services 中。配置的文件名是接口/抽象类的全限定名，文件内容是抽象类的子类或接口的实现类的全限定类名，如果有多个，借助换行符，一行一个。</p><p>举个例子：</p><p>在 META-INF/services 中声明一个文件名为 com.linkedbear.boot.demo.SpiDemoInterface 的文件，文件内容为：</p><pre><code>com.linkedbear.boot.demo.SpiDemoInterfaceImpl</code></pre><p>在 com.linkedbear.boot.demo 包下新建一个接口，类名必须跟上面配置的文件名一样：SpiDemoInterface。在接口中声明一个 test() 方法：</p><pre><code>public interface SpiDemoInterface {    void test();}</code></pre><p>接下来再新建一个类 SpiDemoInterfaceImpl，并实现 SpiDemoInterface：</p><pre><code>public class SpiDemoInterfaceImpl implements SpiDemoInterface {    @Override    public void test() {        System.out.println(&quot;SpiDemoInterfaceImpl#test() run...&quot;);    }}</code></pre><p>编写主运行类，测试效果：</p><pre><code>public class App {    public static void main(String[] args) {        ServiceLoader&lt;SpiDemoInterface&gt; loaders = ServiceLoader.load(SpiDemoInterface.class);        loaders.foreach(SpiDemoInterface::test);    }}</code></pre><p>运行结果：</p><pre><code>SpiDemoInterfaceImpl#test() run...</code></pre><h4 id="SpringFramework的SpringFactoriesLoader"><a href="#SpringFramework的SpringFactoriesLoader" class="headerlink" title="SpringFramework的SpringFactoriesLoader"></a>SpringFramework的SpringFactoriesLoader</h4><p>SpringFramework 利用 SpringFactoriesLoader 都是调用 loadFactoryNames 方法：</p><pre><code>public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryClass, @Nullable ClassLoader classLoader) {    String factoryClassName = factoryClass.getName();    return loadSpringFactories(classLoader).getOrDefault(factoryClassName, Collections.emptyList());}</code></pre><p>文档注释原文翻译：</p><blockquote><p>使用给定的类加载器从 META-INF/spring.factories 中加载给定类型的工厂实现的全限定类名。</p></blockquote><h4 id="loadSpringFactories"><a href="#loadSpringFactories" class="headerlink" title="loadSpringFactories"></a>loadSpringFactories</h4><pre><code>public static final String FACTORIES_RESOURCE_LOCATION = &quot;META-INF/spring.factories&quot;;private static final Map&lt;ClassLoader, MultiValueMap&lt;String, String&gt;&gt; cache = new ConcurrentReferenceHashMap&lt;&gt;();// 这个方法仅接收了一个类加载器private static Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(@Nullable ClassLoader classLoader) {    MultiValueMap&lt;String, String&gt; result = cache.get(classLoader);    if (result != null) {        return result;    }    try {        Enumeration&lt;URL&gt; urls = (classLoader != null ?                 classLoader.getResources(FACTORIES_RESOURCE_LOCATION) :                 ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION));        result = new LinkedMultiValueMap&lt;&gt;();        while (urls.hasMoreElements()) {            URL url = urls.nextElement();            UrlResource resource = new UrlResource(url);            Properties properties = PropertiesLoaderUtils.loadProperties(resource);            for (Map.Entry&lt;?, ?&gt; entry : properties.entrySet()) {                String factoryClassName = ((String) entry.getKey()).trim();                for (String factoryName : StringUtils.commaDelimitedListToStringArray((String) entry.getValue())) {                    result.add(factoryClassName, factoryName.trim());                }            }        }        cache.put(classLoader, result);        return result;    }    catch (IOException ex) {        throw new IllegalArgumentException(&quot;Unable to load factories from location [&quot; +                                       FACTORIES_RESOURCE_LOCATION + &quot;]&quot;, ex);    }}</code></pre><h4 id="加载spring-factories"><a href="#加载spring-factories" class="headerlink" title="加载spring.factories"></a>加载spring.factories</h4><pre><code>        Enumeration&lt;URL&gt; urls = (classLoader != null ?                 classLoader.getResources(FACTORIES_RESOURCE_LOCATION) :                 ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION));        result = new LinkedMultiValueMap&lt;&gt;();</code></pre><p>这部分动作就是获取当前 classpath 下所有jar包中有的 spring.factories 文件，并将它们加载到内存中。</p><p><a href="https://adbo.gitee.io/javaify/#/docs/spring/2021-10-15-springboot%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D">参考Debug</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>SpringFramework 提供了模式注解、@EnableXXX + @Import 的组合手动装配。</li><li>@SpringBootApplication 标注的主启动类所在包会被视为扫描包的根包。</li><li>AutoConfigurationImportSelector 配合 SpringFactoriesLoader 可加载 “META-INF/spring.factories” 中配置的 @EnableAutoConfiguration 对应的自动配置类。</li><li>DeferredImportSelector 的执行时机比 ImportSelector 更晚。</li><li>SpringFramework 实现了自己的SPI技术，相比较于Java原生的SPI更灵活。</li></ol>]]></content>
      
      
      <categories>
          
          <category> SpringBoot </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpringBootApplication注解</title>
      <link href="/2023/07/07/2023-07-07-springbootapplication-zhu-jie/"/>
      <url>/2023/07/07/2023-07-07-springbootapplication-zhu-jie/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《SpringBoot 源码解读与原理分析》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="SpringBootApplication"><a href="#SpringBootApplication" class="headerlink" title="@SpringBootApplication"></a>@SpringBootApplication</h2><p>启动类上@SpringBootApplication注解源码</p><pre><code>/** * ...... * @since 1.2.0 */@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class),        @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) })public @interface SpringBootApplication</code></pre><p>文档注释原文翻译：</p><blockquote><p>Indicates a configuration class that declares one or more @Bean methods and also triggers auto-configuration and component scanning. This is a convenience annotation that is equivalent to declaring<br>@Configuration, @EnableAutoConfiguration and @ComponentScan.<br>标识了一个配置类，这个配置类上声明了一个或多个 @Bean 的方法，并且它会触发自动配置和组件扫描。<br>这是一个很方便的注解，它等价于同时标注 @Configuration + @EnableAutoConfiguration + @ComponentScan 。</p></blockquote><p>![](16e3b73d238f1154~tplv-t2oaga2asx-zoom-in-crop-mark 3024 0 0 0.webp)</p><h2 id="ComponentScan"><a href="#ComponentScan" class="headerlink" title="@ComponentScan"></a>@ComponentScan</h2><p>在SpringFramework中@ComponentScan可以指定包扫描的根路径，让 SpringFramework 来扫描指定包及子包下的组件，也可以不指定路径，默认扫描当前配置类所在包及子包里的所有组件<strong>（其实这就解释了为什么 SpringBoot 的启动类要放到所有类所在包的最外层）</strong>。</p><pre><code>@ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class),        @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) })</code></pre><p>声明中有显式的指定了两个过滤条件</p><h3 id="TypeExcludeFilter"><a href="#TypeExcludeFilter" class="headerlink" title="TypeExcludeFilter"></a>TypeExcludeFilter</h3><p>向IOC容器中注册一些自定义的组件过滤器，以在包扫描的过程中过滤它们。</p><p>这种Filter的核心方法是 match 方法，它实现了过滤的判断逻辑：</p><pre><code>public boolean match(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory)        throws IOException {    if (this.beanFactory instanceof ListableBeanFactory &amp;&amp; getClass() == TypeExcludeFilter.class) {        Collection&lt;TypeExcludeFilter&gt; delegates = ((ListableBeanFactory) this.beanFactory)                .getBeansOfType(TypeExcludeFilter.class).values();        for (TypeExcludeFilter delegate : delegates) {            if (delegate.match(metadataReader, metadataReaderFactory)) {                return true;            }        }    }    return false;}</code></pre><p>从 BeanFactory （可以暂时理解成IOC容器）中获取所有类型为 TypeExcludeFilter 的组件，去执行自定义的过滤方法。</p><p>由此可见，TypeExcludeFilter 的作用是做扩展的组件过滤。</p><h3 id="AutoConfigurationExcludeFilter"><a href="#AutoConfigurationExcludeFilter" class="headerlink" title="AutoConfigurationExcludeFilter"></a>AutoConfigurationExcludeFilter</h3><p>注解源码：</p><pre><code>public boolean match(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory)        throws IOException {    return isConfiguration(metadataReader) &amp;&amp; isAutoConfiguration(metadataReader);}private boolean isConfiguration(MetadataReader metadataReader) {    return metadataReader.getAnnotationMetadata().isAnnotated(Configuration.class.getName());}private boolean isAutoConfiguration(MetadataReader metadataReader) {    return getAutoConfigurations().contains(metadataReader.getClassMetadata().getClassName());}protected List&lt;String&gt; getAutoConfigurations() {    if (this.autoConfigurations == null) {        this.autoConfigurations = SpringFactoriesLoader.loadFactoryNames(EnableAutoConfiguration.class,                this.beanClassLoader);    }    return this.autoConfigurations;}</code></pre><p>match 方法要判断两个部分：是否是一个配置类，是否是一个自动配置类。</p><h2 id="SpringBootConfiguration"><a href="#SpringBootConfiguration" class="headerlink" title="@SpringBootConfiguration"></a>@SpringBootConfiguration</h2><pre><code>@Configurationpublic @interface SpringBootConfiguration</code></pre><p>文档注释原文翻译：</p><blockquote><p>Indicates that a class provides Spring Boot application @Configuration . Can be used as an alternative to the Spring’s standard @Configuration annotation so that configuration can be found automatically (for example in tests).<br>Application should only ever include one @SpringBootConfiguration and most idiomatic Spring Boot applications will inherit it from @SpringBootApplication.<br>标识一个类作为 SpringBoot 的配置类，它可以是Spring原生的 @Configuration 的一种替换方案，目的是这个配置可以被自动发现。<br>应用应当只在主启动类上标注 @SpringBootConfiguration，大多数情况下都是直接使用 @SpringBootApplication。</p></blockquote><p>它被 @Configuration 标注，说明它实际上是标注配置类的，而且是标注主启动类的。</p><h3 id="Configuration的作用"><a href="#Configuration的作用" class="headerlink" title="@Configuration的作用"></a>@Configuration的作用</h3><p>被 @Configuration 标注的类，会被 Spring 的IOC容器认定为配置类。</p><p>一个被 @Configuration 标注的类，相当于一个 applicationContext.xml 的配置文件。</p><p>例如：声明一个类，并标注 @Configuration 注解：</p><pre><code>@Configurationpublic class ConfigurationDemo {    @Bean    public Date currentDate() {        return new Date();    }}</code></pre><p>上述注册Bean的方式类比于xml：</p><pre><code>&lt;bean id=&quot;currentDate&quot; class=&quot;java.util.Date&quot;/&gt;</code></pre><p>之后使用注解启动方式，初始化一个IOC容器，并打印IOC容器中的所有bean的name：</p><pre><code>public class MainApp {    public static void main(String[] args) throws Exception {        AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext(ConfigurationDemo.class);        String[] beanDefinitionNames = ctx.getBeanDefinitionNames();        Stream.of(beanDefinitionNames).forEach(System.out::println);    }}</code></pre><p>输出结果：</p><pre><code>org.springframework.context.annotation.internalConfigurationAnnotationProcessororg.springframework.context.annotation.internalAutowiredAnnotationProcessororg.springframework.context.annotation.internalCommonAnnotationProcessororg.springframework.context.event.internalEventListenerProcessororg.springframework.context.event.internalEventListenerFactoryconfigurationDemocurrentDate</code></pre><p>可以发现组件，以及配置类本身被成功加载。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>@SpringBootApplication 是组合注解。@SpringBootConfiguration + @EnableAutoConfiguration + @ComponentScan</li><li>@ComponentScan 默认扫描当前配置类所在包及子包下的所有组件，提供TypeExcludeFilter和AutoConfigurationExcludeFilter。</li><li>@SpringBootConfiguration 可标注配置类，@SpringBootConfiguration 并没有对@Configuration做实质性扩展。</li></ol>]]></content>
      
      
      <categories>
          
          <category> SpringBoot </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git常用命令</title>
      <link href="/2023/05/16/2023-05-16-git-chang-yong-ming-ling/"/>
      <url>/2023/05/16/2023-05-16-git-chang-yong-ming-ling/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本篇主要应前同事需要，整理工作中常用的Git操作</p></blockquote><h2 id="安装配置"><a href="#安装配置" class="headerlink" title="安装配置"></a>安装配置</h2><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><p>官网下载：<a href="https://git-scm.com/downloads" target="_blank" rel="noopener">Git-Downloads</a></p><h3 id="生成秘钥"><a href="#生成秘钥" class="headerlink" title="生成秘钥"></a>生成秘钥</h3><ol><li>右击打开git bash</li><li>执行命令，按3个回车，密码为空</li></ol><pre><code>ssh-keygen -t rsa -C &quot;个人邮箱地址&quot;</code></pre><ol start="3"><li>在C:\Users{windows用户名}.ssh目录下找到id_rsa.pub文件</li><li>复制id_rsa.pub添加到仓库SSH秘钥配置中。</li></ol><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><ul><li>设置自己的姓名和邮箱</li></ul><pre><code>git config --global user.name &quot;Your Name&quot;git config --global user.email &quot;email@example.com&quot;</code></pre><ul><li>设置git仓库能正常显示中文</li></ul><pre><code>git config --global core.quotepath false</code></pre><h2 id="概念理解"><a href="#概念理解" class="headerlink" title="概念理解"></a>概念理解</h2><p><img src="/2023/05/16/2023-05-16-git-chang-yong-ming-ling/8205a12c11fc45ba4ee13a5af081577c.jpg" alt></p><ul><li>workspace：工作区</li><li>staging area：暂存区/缓存区</li><li>local repository：本地仓库</li><li>remote repository：远程仓库</li></ul><h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>新项目执行Git初始化，并进行第一次提交</p><pre><code>git initgit add .git commit -m &quot;first commit&quot;git remote add origin 仓库地址路径git push origin master</code></pre><h3 id="克隆"><a href="#克隆" class="headerlink" title="克隆"></a>克隆</h3><p>对github、gitlab远程仓库，已有项目进行克隆</p><pre><code>git clone 仓库地址路径</code></pre><h3 id="分支"><a href="#分支" class="headerlink" title="分支"></a>分支</h3><ul><li>查看所有分支</li></ul><pre><code>git branch -a</code></pre><ul><li>切换分支</li></ul><pre><code>git checkout 需要切换的分支名</code></pre><ul><li>创建新分支</li></ul><pre><code>git checkout -b 分支名</code></pre><p>-b表示创建并切换分支</p><h3 id="拉取"><a href="#拉取" class="headerlink" title="拉取"></a>拉取</h3><pre><code>git pull</code></pre><p>会从远程仓库拉取最新代码合并到当前分支</p><h3 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h3><h4 id="add"><a href="#add" class="headerlink" title="add"></a>add</h4><p>将文件添加到仓库</p><ul><li>将工作区的某个文件添加到暂存区</li></ul><pre><code>git add 文件名</code></pre><ul><li>将当前工作区的所有文件都加入暂存区</li></ul><pre><code>git add .</code></pre><h4 id="commit"><a href="#commit" class="headerlink" title="commit"></a>commit</h4><p>将暂存区文件提交到本地仓库</p><ul><li>将暂存区内容提交到本地仓库</li></ul><pre><code>git commit -m &quot;提交说明&quot;</code></pre><ul><li>跳过缓存区操作，直接把工作区内容提交到本地仓库</li></ul><pre><code>git commit -am &quot;提交说明&quot;</code></pre><p>这个命令等于git add . 和git commit -m “提交说明”组合</p><h4 id="push"><a href="#push" class="headerlink" title="push"></a>push</h4><p>将本地仓库分支推送同步到远程仓库分支</p><pre><code>git push</code></pre><h3 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h3><p>查看当前分支状态，可以看到修改、删除、还没提交的文件状态</p><pre><code>git status</code></pre><h3 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h3><ul><li>开发分支（dev）上的代码达到上线的标准后，要合并到 master 分支</li></ul><pre><code>git checkout devgit pullgit checkout mastergit merge devgit push origin master</code></pre><ul><li>当master代码有更新，需要更新开发分支（dev）上的代码</li></ul><pre><code>git checkout master git pull git checkout devgit merge master git push origin dev</code></pre><h3 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h3><p>到一个重要的阶段，并希望永远记住那个特别的提交快照，你可以使用 git tag 给它打上标签</p><ul><li>查看所有标签</li></ul><pre><code>git tag</code></pre><ul><li>创建并推送标签</li></ul><pre><code>git tag v1.0 -am &quot;release 1.0 version&quot;git push origin v1.0</code></pre><h3 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h3><p>查看历史记录</p><ul><li>查看所有commit记录(SHA-A校验和，作者名称，邮箱，提交时间，提交说明)</li></ul><pre><code>git log</code></pre><ul><li>查看最近多少次的提交记录</li></ul><pre><code>git log -p -次数</code></pre><ul><li>让提交记录以精简的一行输出</li></ul><pre><code>git log --oneline</code></pre><ul><li>图形展示分支的合并历史</li></ul><pre><code>git log --graph</code></pre><ul><li>查询作者的提交记录</li></ul><pre><code>git log --author=作者</code></pre><ul><li>列出提交信息中包含过滤信息的提交记录</li></ul><pre><code>git log --grep=过滤信息</code></pre><h3 id="回滚（谨慎）"><a href="#回滚（谨慎）" class="headerlink" title="回滚（谨慎）"></a>回滚（谨慎）</h3><p>可以通过 git reset 命令将代码回滚到之前某次提交的状态，但一定要将现有的代码做好备份，否则回滚之后这些变动都会消失</p><ul><li>回退成上次提交的版本</li></ul><pre><code>git reset HEAD^</code></pre><ul><li>回退到指定版本</li></ul><p>查看提交日志，回退到指定commit_id</p><pre><code>git loggit reset --hard commit_id</code></pre><ul><li>将本地的状态回退到和远程的一样</li></ul><pre><code>git reset --hard origin/master</code></pre><ul><li>回退 xxx.java 文件的版本到上一个版本</li></ul><pre><code>git reset HEAD^ xxx.java</code></pre>]]></content>
      
      
      <categories>
          
          <category> Work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis数据结构</title>
      <link href="/2022/10/25/2022-10-25-redis-shu-ju-jie-gou/"/>
      <url>/2022/10/25/2022-10-25-redis-shu-ju-jie-gou/</url>
      
        <content type="html"><![CDATA[<p>Redis之所以这么快，除了它是内存数据库，使得所有的操作都在内存上进行之外，还有一个重要因素，它实现的数据结构，使得我们对数据进行增删查改操作时，Redis 能高效的处理。</p><p><img src="/2022/10/25/2022-10-25-redis-shu-ju-jie-gou/9fa26a74965efbf0f56b707a03bb9b7f.png" alt></p><blockquote><p>String类型底层是简单动态字符串</p></blockquote><h2 id="SDS-简单动态字符串"><a href="#SDS-简单动态字符串" class="headerlink" title="SDS(简单动态字符串)"></a>SDS(简单动态字符串)</h2><p><img src="/2022/10/25/2022-10-25-redis-shu-ju-jie-gou/516738c4058cdf9109e40a7812ef4239.png" alt></p><ul><li>len，记录了字符串长度。这样获取字符串长度的时候，只需要返回这个成员变量值就行，时间复杂度只需要 O（1）。</li><li>alloc，分配给字符数组的空间长度。这样在修改字符串的时候，可以通过 alloc - len 计算出剩余的空间大小，可以用来判断空间是否满足修改需求，如果不满足的话，就会自动将 SDS 的空间扩展至执行修改所需的大小，然后才执行实际的修改操作，所以使用 SDS 既不需要手动修改 SDS 的空间大小，也不会出现缓冲区溢出的问题。</li><li>flags，用来表示不同类型的 SDS。一共设计了 5 种类型，分别是 sdshdr5、sdshdr8、sdshdr16、sdshdr32 和 sdshdr64。</li><li>buf[]，字符数组，用来保存实际数据。不仅可以保存字符串，也可以保存二进制数据。</li></ul><h3 id="O（1）复杂度获取字符串长度"><a href="#O（1）复杂度获取字符串长度" class="headerlink" title="O（1）复杂度获取字符串长度"></a>O（1）复杂度获取字符串长度</h3><p>C 语言的字符串长度获取 strlen 函数，需要通过遍历的方式来统计字符串长度，时间复杂度是 O（N）。</p><p>而 Redis 的 SDS 结构因为加入了 len 成员变量，那么获取字符串长度的时候，直接返回这个成员变量的值就行，所以复杂度只有 O（1）。</p><h3 id="二进制安全"><a href="#二进制安全" class="headerlink" title="二进制安全"></a>二进制安全</h3><p>C语言用 “\0” 字符来标识字符串结尾。</p><p>因为 SDS 不需要用 “\0” 字符来标识字符串结尾了，而是有个专门的 len 成员变量来记录长度，所以可存储包含 “\0” 的数据。但是 SDS 为了兼容部分 C 语言标准库的函数， SDS 字符串结尾还是会加上 “\0” 字符。</p><p>通过使用二进制安全的 SDS，而不是 C 字符串，使得 Redis 不仅可以保存文本数据，也可以保存任意格式的二进制数据。</p><h3 id="不会发生缓冲区溢出"><a href="#不会发生缓冲区溢出" class="headerlink" title="不会发生缓冲区溢出"></a>不会发生缓冲区溢出</h3><p>Redis 的 SDS 结构里引入了 alloc 和 len 成员变量，这样 SDS API 通过 alloc - len 计算，可以算出剩余可用的空间大小，这样在对字符串做修改操作的时候，就可以由程序内部判断缓冲区大小是否足够用。</p><p>而且，当判断出缓冲区大小不够用时，Redis 会自动将扩大 SDS 的空间大小（小于 1MB 翻倍扩容，大于 1MB 按 1MB 扩容），以满足修改所需的大小。</p><p>在扩展 SDS 空间之前，SDS API 会优先检查未使用空间是否足够，如果不够的话，API 不仅会为 SDS 分配修改所必须要的空间，还会给 SDS 分配额外的「未使用空间」。</p><p>这样的好处是，下次在操作 SDS 时，如果 SDS 空间够的话，API 就会直接使用「未使用空间」，而无须执行内存分配，有效的减少内存分配次数。</p><p>所以，使用 SDS 即不需要手动修改 SDS 的空间大小，也不会出现缓冲区溢出的问题。</p><h3 id="节省内存空间"><a href="#节省内存空间" class="headerlink" title="节省内存空间"></a>节省内存空间</h3><p>SDS 结构中有个 flags 成员变量，表示的是 SDS 类型。</p><p>SDS 一共设计了 5 种类型，分别是 sdshdr5、sdshdr8、sdshdr16、sdshdr32 和 sdshdr64。</p><p>SDS 设计不同类型的结构体，是为了能灵活保存不同大小的字符串，从而有效节省内存空间。</p><blockquote><p>hash类型底层是哈希表或压缩列表，包含的元素数量较少，或者元素值不大的情况才会使用压缩列表，否则用哈希表。</p></blockquote><h2 id="哈希表"><a href="#哈希表" class="headerlink" title="哈希表"></a>哈希表</h2><p>哈希表是一种保存键值对（key-value）的数据结构。</p><p>哈希表优点在于，它能以 O(1) 的复杂度快速查询数据。将 key 通过 Hash 函数的计算，就能定位数据在表中的位置，因为哈希表实际上是数组，所以可以通过索引值快速查询到数据。</p><p>但是存在的风险也是有，在哈希表大小固定的情况下，随着数据不断增多，那么哈希冲突的可能性也会越高。</p><p>Redis 采用了「链式哈希」来解决哈希冲突，在不扩容哈希表的前提下，将具有相同哈希值的数据串起来，形成链接起，以便这些数据在表中仍然可以被查询到。</p><h3 id="哈希表结构设计"><a href="#哈希表结构设计" class="headerlink" title="哈希表结构设计"></a>哈希表结构设计</h3><pre><code>typedef struct dictht {    //哈希表数组    dictEntry **table;    //哈希表大小    unsigned long size;      //哈希表大小掩码，用于计算索引值    unsigned long sizemask;    //该哈希表已有的节点数量    unsigned long used;} dictht;</code></pre><p>可以看到，哈希表是一个数组（dictEntry **table），数组的每个元素是一个指向「哈希表节点（dictEntry）」的指针。</p><p><img src="/2022/10/25/2022-10-25-redis-shu-ju-jie-gou/dc495ffeaa3c3d8cb2e12129b3423118.png" alt></p><p>哈希表节点的结构如下：</p><pre><code>typedef struct dictEntry {    //键值对中的键    void *key;    //键值对中的值    union {        void *val;        uint64_t u64;        int64_t s64;        double d;    } v;    //指向下一个哈希表节点，形成链表    struct dictEntry *next;} dictEntry;</code></pre><p>dictEntry 结构里不仅包含指向键和值的指针，还包含了指向下一个哈希表节点的指针，这个指针可以将多个哈希值相同的键值对链接起来，以此来解决哈希冲突的问题，这就是链式哈希。</p><h3 id="哈希冲突"><a href="#哈希冲突" class="headerlink" title="哈希冲突"></a>哈希冲突</h3><p>哈希表实际上是一个数组，数组里多每一个元素就是一个哈希桶。</p><p>当一个键值对的键经过 Hash 函数计算后得到哈希值，再将(哈希值 % 哈希表大小)取模计算，得到的结果值就是该 key-value 对应的数组元素位置，也就是第几个哈希桶。</p><p><img src="/2022/10/25/2022-10-25-redis-shu-ju-jie-gou/753724a072e77d139c926ecf1f049b29.png" alt></p><p>当有两个以上数量的 kay 被分配到了哈希表中同一个哈希桶上时，此时称这些 key 发生了冲突。</p><h3 id="链式哈希"><a href="#链式哈希" class="headerlink" title="链式哈希"></a>链式哈希</h3><p>Redis 采用了「链式哈希」的方法来解决哈希冲突。</p><p>实现的方式就是每个哈希表节点都有一个 next 指针，用于指向下一个哈希表节点，因此多个哈希表节点可以用 next 指针构成一个单项链表，被分配到同一个哈希桶上的多个节点可以用这个单项链表连接起来，这样就解决了哈希冲突。</p><p><img src="/2022/10/25/2022-10-25-redis-shu-ju-jie-gou/675c23857a36b2dab26ed2e6a7b94b5d.png" alt></p><p>不过，链式哈希局限性也很明显，随着链表长度的增加，在查询这一位置上的数据的耗时就会增加，毕竟链表的查询的时间复杂度是 O(n)。</p><p>要想解决这一问题，就需要进行 rehash，也就是对哈希表的大小进行扩展。</p><h3 id="rehash"><a href="#rehash" class="headerlink" title="rehash"></a>rehash</h3><p>在实际使用哈希表时，Redis 定义一个 dict 结构体，这个结构体里定义了两个哈希表（ht[2]）。</p><pre><code>typedef struct dict {    …    //两个Hash表，交替使用，用于rehash操作    dictht ht[2];     …} dict;</code></pre><p>之所以定义了 2 个哈希表，是因为进行 rehash 的时候，需要用上 2 个哈希表了。</p><p><img src="/2022/10/25/2022-10-25-redis-shu-ju-jie-gou/2fedbc9cd4cb7236c302d695686dd478.png" alt></p><p>在正常服务请求阶段，插入的数据，都会写入到「哈希表 1」，此时的「哈希表 2 」 并没有被分配空间。</p><p>随着数据逐步增多，触发了 rehash 操作，这个过程分为三步：</p><ul><li>给「哈希表 2」 分配空间，一般会比「哈希表 1」 大 2 倍；</li><li>将「哈希表 1」的数据迁移到「哈希表 2」 中；</li><li>迁移完成后，「哈希表 1」的空间会被释放，并把「哈希表 2」 设置为「哈希表 1」，然后在「哈希表 2」 新创建一个空白的哈希表，为下次 rehash 做准备。</li></ul><p><img src="/2022/10/25/2022-10-25-redis-shu-ju-jie-gou/cabce0ce7e320bc9d9b5bde947b6811b.png" alt></p><p>如果「哈希表 1 」的数据量非常大，那么在迁移至「哈希表 2 」的时候，因为会涉及大量的数据拷贝，此时可能会对 Redis 造成阻塞，无法服务其他请求。</p><h3 id="渐进式-rehash"><a href="#渐进式-rehash" class="headerlink" title="渐进式 rehash"></a>渐进式 rehash</h3><p>为了避免 rehash 在数据迁移过程中，因拷贝数据的耗时，影响 Redis 性能的情况，所以 Redis 采用了渐进式 rehash，也就是将数据的迁移的工作不再是一次性迁移完成，而是分多次迁移。</p><p>渐进式 rehash 步骤如下：</p><ul><li>给「哈希表 2」 分配空间；</li><li>在 rehash 进行期间，每次哈希表元素进行新增、删除、查找或者更新操作时，Redis 除了会执行对应的操作之外，还会顺序将「哈希表 1 」中索引位置上的所有 key-value 迁移到「哈希表 2」 上；</li><li>随着处理客户端发起的哈希表操作请求数量越多，最终在某个时间点会把「哈希表 1 」的所有 key-value 迁移到「哈希表 2」，从而完成 rehash 操作。</li></ul><p>这样就巧妙地把一次性大量数据迁移工作的开销，分摊到了多次处理请求的过程中，避免了一次性 rehash 的耗时操作。</p><p>在进行渐进式 rehash 的过程中，会有两个哈希表，所以在渐进式 rehash 进行期间，哈希表元素的删除、查找、更新等操作都会在这两个哈希表进行。</p><p>比如，查找一个 key 的值的话，先会在「哈希表 1」 里面进行查找，如果没找到，就会继续到哈希表 2 里面进行找到。</p><p>另外，在渐进式 rehash 进行期间，新增一个 key-value 时，会被保存到「哈希表 2 」里面，而「哈希表 1」 则不再进行任何添加操作，这样保证了「哈希表 1 」的 key-value 数量只会减少，随着 rehash 操作的完成，最终「哈希表 1 」就会变成空表。</p><h3 id="rehash-触发条件"><a href="#rehash-触发条件" class="headerlink" title="rehash 触发条件"></a>rehash 触发条件</h3><p>rehash 的触发条件跟<strong>负载因子（load factor）</strong>有关系。</p><p>负载因子可以通过下面这个公式计算：</p><p><img src="/2022/10/25/2022-10-25-redis-shu-ju-jie-gou/85f597f7851b90d6c78bb0d8e39690fc.png" alt></p><ul><li>当负载因子大于等于 1 ，并且 Redis 没有执行 RDB 快照或没有进行 AOF 重写的时候，就会进行 rehash 操作。</li><li>当负载因子大于等于 5 时，此时说明哈希冲突非常严重了，不管有没有有在执行 RDB 快照或 AOF 重写，都会强制进行 rehash 操作。</li></ul><h2 id="压缩列表"><a href="#压缩列表" class="headerlink" title="压缩列表"></a>压缩列表</h2><p>压缩列表的最大特点，就是它被设计成一种内存紧凑型的数据结构，占用一块连续的内存空间，不仅可以利用 CPU 缓存，而且会针对不同长度的数据，进行相应编码，这种方法可以有效地节省内存开销。</p><p>但是，压缩列表的缺陷也是有的：</p><ul><li>不能保存过多的元素，否则查询效率就会降低；</li><li>新增或修改某个元素时，压缩列表占用的内存空间需要重新分配，甚至可能引发连锁更新的问题。</li></ul><p>因此，Redis 对象（List 对象、Hash 对象、Zset 对象）包含的元素数量较少，或者元素值不大的情况才会使用压缩列表作为底层数据结构。</p><h3 id="压缩列表结构设计"><a href="#压缩列表结构设计" class="headerlink" title="压缩列表结构设计"></a>压缩列表结构设计</h3><p>压缩列表是 Redis 为了节约内存而开发的，它是由连续内存块组成的顺序型数据结构，有点类似于数组。</p><p><img src="/2022/10/25/2022-10-25-redis-shu-ju-jie-gou/ab0b44f557f8b5bc7acb3a53d43ebfcb.png" alt></p><p>压缩列表在表头有三个字段：</p><ul><li>zlbytes，记录整个压缩列表占用对内存字节数；</li><li>zltail，记录压缩列表「尾部」节点距离起始地址由多少字节，也就是列表尾的偏移量；</li><li>zllen，记录压缩列表包含的节点数量；</li><li>zlend，标记压缩列表的结束点，固定值 0xFF（十进制255）。</li></ul><p>在压缩列表中，如果我们要查找定位第一个元素和最后一个元素，可以通过表头三个字段（zllen）的长度直接定位，复杂度是 O(1)。而查找其他元素时，就没有这么高效了，只能逐个查找，此时的复杂度就是 O(N) 了，因此压缩列表不适合保存过多的元素。</p><p>另外，压缩列表节点（entry）的构成如下：</p><p><img src="/2022/10/25/2022-10-25-redis-shu-ju-jie-gou/a3b1f6235cf0587115b21312fe60289c.png" alt></p><ul><li>prevlen，记录了「前一个节点」的长度，目的是为了实现从后向前遍历；</li><li>encoding，记录了当前节点实际数据的「类型和长度」，类型主要有两种：字符串和整数。</li><li>data，记录了当前节点的实际数据，类型和长度都由 encoding 决定；</li></ul><p>当我们往压缩列表中插入数据时，压缩列表就会根据数据类型是字符串还是整数，以及数据的大小，会使用不同空间大小的 prevlen 和 encoding 这两个元素里保存的信息，这种根据数据大小和类型进行不同的空间大小分配的设计思想，正是 Redis 为了节省内存而采用的。</p><blockquote><p>List类型底层是快速列表</p></blockquote><h2 id="quicklist（快速列表）"><a href="#quicklist（快速列表）" class="headerlink" title="quicklist（快速列表）"></a>quicklist（快速列表）</h2><p>其实 quicklist 就是「双向链表 + 压缩列表」组合，因为一个 quicklist 就是一个链表，而链表中的每个元素又是一个压缩列表。</p><h3 id="quicklist-结构设计"><a href="#quicklist-结构设计" class="headerlink" title="quicklist 结构设计"></a>quicklist 结构设计</h3><p>quicklist 的结构体跟链表的结构体类似，都包含了表头和表尾，区别在于 quicklist 的节点是 quicklistNode。</p><pre><code>typedef struct quicklist {    //quicklist的链表头    quicklistNode *head;      //quicklist的链表头    //quicklist的链表尾    quicklistNode *tail;     //所有压缩列表中的总元素个数    unsigned long count;    //quicklistNodes的个数    unsigned long len;           ...} quicklist;</code></pre><p>quicklistNode 的结构定义：</p><pre><code>typedef struct quicklistNode {    //前一个quicklistNode    struct quicklistNode *prev;     //前一个quicklistNode    //下一个quicklistNode    struct quicklistNode *next;     //后一个quicklistNode    //quicklistNode指向的压缩列表    unsigned char *zl;                  //压缩列表的的字节大小    unsigned int sz;                    //压缩列表的元素个数    unsigned int count : 16;        //ziplist中的元素个数     ....} quicklistNode;</code></pre><p>可以看到，quicklistNode 结构体里包含了前一个节点和下一个节点指针，这样每个 quicklistNode 形成了一个双向链表。但是链表节点的元素不再是单纯保存元素值，而是保存了一个压缩列表，所以 quicklistNode 结构体里有个指向压缩列表的指针 *zl。</p><p><img src="/2022/10/25/2022-10-25-redis-shu-ju-jie-gou/f46cbe347f65ded522f1cc3fd8dba549.png" alt></p><p>在向 quicklist 添加一个元素的时候，不会像普通的链表那样，直接新建一个链表节点。而是会检查插入位置的压缩列表是否能容纳该元素，如果能容纳就直接保存到 quicklistNode 结构里的压缩列表，如果不能容纳，才会新建一个新的 quicklistNode 结构。</p><blockquote><p>Set类型底层是整数集合或哈希表。如果集合对象的所有元素都是整数值，并且保存元素小于 512 个时，底层将使用整数集合。否则用哈希表。</p></blockquote><h2 id="整数集合"><a href="#整数集合" class="headerlink" title="整数集合"></a>整数集合</h2><p>整数集合是 Set 对象的底层实现之一。当一个 Set 对象只包含整数值元素，并且元素数量不大时，就会使用整数集这个数据结构作为底层实现。</p><h3 id="整数集合结构设计"><a href="#整数集合结构设计" class="headerlink" title="整数集合结构设计"></a>整数集合结构设计</h3><p>整数集合本质上是一块连续内存空间，它的结构定义如下：</p><pre><code>typedef struct intset {    //编码方式    uint32_t encoding;    //集合包含的元素数量    uint32_t length;    //保存元素的数组    int8_t contents[];} intset;</code></pre><p>可以看到，保存元素的容器是一个 contents 数组，虽然 contents 被声明为 int8_t 类型的数组，但是实际上 contents 数组并不保存任何 int8_t 类型的元素，contents 数组的真正类型取决于 intset 结构体里的 encoding 属性的值。比如：</p><ul><li>如果 encoding 属性值为 INTSET_ENC_INT16，那么 contents 就是一个 int16_t 类型的数组，数组中每一个元素的类型都是 int16_t；</li><li>如果 encoding 属性值为 INTSET_ENC_INT32，那么 contents 就是一个 int32_t 类型的数组，数组中每一个元素的类型都是 int32_t；</li><li>如果 encoding 属性值为 INTSET_ENC_INT64，那么 contents 就是一个 int64_t 类型的数组，数组中每一个元素的类型都是 int64_t；</li></ul><p>不同类型的 contents 数组，意味着数组的大小也会不同。</p><h3 id="整数集合的升级操作"><a href="#整数集合的升级操作" class="headerlink" title="整数集合的升级操作"></a>整数集合的升级操作</h3><p>整数集合会有一个升级规则，就是当我们将一个新元素加入到整数集合里面，如果新元素的类型（int32_t）比整数集合现有所有元素的类型（int16_t）都要长时，整数集合需要先进行升级，也就是按新元素的类型（int32_t）扩展 contents 数组的空间大小，然后才能将新元素加入到整数集合里，当然升级的过程中，也要维持整数集合的有序性。</p><p>整数集合升级的过程不会重新分配一个新类型的数组，而是在原本的数组上扩展空间，然后在将每个元素按间隔类型大小分割，如果 encoding 属性值为 INTSET_ENC_INT16，则每个元素的间隔就是 16 位。</p><p><img src="/2022/10/25/2022-10-25-redis-shu-ju-jie-gou/5dbdfa7cfbdd1d12a4d9458c6c90d472.png" alt></p><p>现在，往这个整数集合中加入一个新元素 65535，这个新元素需要用 int32_t 类型来保存，所以整数集合要进行升级操作，首先需要为 contents 数组扩容，在原本空间的大小之上再扩容多 80 位（4x32-3x16=80），这样就能保存下 4 个类型为 int32_t 的元素。</p><p><img src="/2022/10/25/2022-10-25-redis-shu-ju-jie-gou/e2e3e19fc934e70563fbdfde2af39a2b.png" alt></p><p>扩容完 contents 数组空间大小后，需要将之前的三个元素转换为 int32_t 类型，并将转换后的元素放置到正确的位上面，并且需要维持底层数组的有序性不变，整个转换过程如下：</p><p><img src="/2022/10/25/2022-10-25-redis-shu-ju-jie-gou/e84b052381e240eeb8cc97d6b729968b.png" alt></p><p>如果要让一个数组同时保存 int16_t、int32_t、int64_t 类型的元素，最简单做法就是直接使用 int64_t 类型的数组。不过这样的话，当如果元素都是 int16_t 类型的，就会造成内存浪费的情况。</p><p>整数集合升级就能避免这种情况，如果一直向整数集合添加 int16_t 类型的元素，那么整数集合的底层实现就一直是用 int16_t 类型的数组，只有在我们要将 int32_t 类型或 int64_t 类型的元素添加到集合时，才会对数组进行升级操作。</p><p>因此，整数集合升级的好处是节省内存资源。</p><blockquote><p>Zset类型的底层是压缩列表或跳表+哈希表，如果元素少于 128 个，且每个元素长度小于 64 字节，使用压缩列表。否则用跳表+哈希表。</p></blockquote><h2 id="跳表"><a href="#跳表" class="headerlink" title="跳表"></a>跳表</h2><p>Redis 只有 Zset 对象的底层实现用到了跳表，跳表的优势是能支持平均 O(logN) 复杂度的节点查找。</p><p>zset 结构体里有两个数据结构：一个是跳表，一个是哈希表。这样的好处是既能进行高效的范围查询，也能进行高效单点查询。</p><pre><code>typedef struct zset {    dict *dict;    zskiplist *zsl;} zset;</code></pre><p>Zset 对象在执行数据插入或是数据更新的过程中，会依次在跳表和哈希表中插入或更新相应的数据，从而保证了跳表和哈希表中记录的信息一致。</p><p>Zset 对象能支持范围查询（如 ZRANGEBYSCORE 操作），这是因为它的数据结构设计采用了跳表，而又能以常数复杂度获取元素权重（如 ZSCORE 操作），这是因为它同时采用了哈希表进行索引。</p><p>哈希表只是用于以常数复杂度获取元素权重，大部分操作都是跳表实现的。</p><h3 id="跳表结构设计"><a href="#跳表结构设计" class="headerlink" title="跳表结构设计"></a>跳表结构设计</h3><p>链表在查找元素的时候，因为需要逐一查找，所以查询效率非常低，时间复杂度是O(N)，于是就出现了跳表。跳表是在链表基础上改进过来的，实现了一种「多层」的有序链表，这样的好处是能快读定位数据。</p><p>一个层级为 3 的跳表如下：</p><p><img src="/2022/10/25/2022-10-25-redis-shu-ju-jie-gou/2ae0ed790c7e7403f215acb2bd82e884.png" alt></p><p>图中头节点有 L0~L2 三个头指针，分别指向了不同层级的节点，然后每个层级的节点都通过指针连接起来：</p><ul><li>L0 层级共有 5 个节点，分别是节点1、2、3、4、5；</li><li>L1 层级共有 3 个节点，分别是节点 2、3、5；</li><li>L2 层级只有 1 个节点，也就是节点 3 。</li></ul><p>如果我们要在链表中查找节点 4 这个元素，只能从头开始遍历链表，需要查找 4 次，而使用了跳表后，只需要查找 2 次就能定位到节点 4，因为可以在头节点直接从 L2 层级跳到节点 3，然后再往前遍历找到节点 4。</p><p>可以看到，这个查找过程就是在多个层级上跳来跳去，最后定位到元素。当数据量很大时，跳表的查找复杂度就是 O(logN)。</p><p>那跳表节点是怎么实现多层级的呢？这就需要看「跳表节点」的数据结构了，如下：</p><pre><code>typedef struct zskiplistNode {    //Zset 对象的元素值    sds ele;    //元素权重值    double score;    //后向指针    struct zskiplistNode *backward;    //节点的level数组，保存每层上的前向指针和跨度    struct zskiplistLevel {        struct zskiplistNode *forward;        unsigned long span;    } level[];} zskiplistNode;</code></pre><p>Zset 对象要同时保存「元素」和「元素的权重」，对应到跳表节点结构里就是 sds 类型的 ele 变量和 double 类型的 score 变量。每个跳表节点都有一个后向指针（struct zskiplistNode *backward），指向前一个节点，目的是为了方便从跳表的尾节点开始访问节点，这样倒序查找时很方便。</p><p>跳表是一个带有层级关系的链表，而且每一层级可以包含多个节点，每一个节点通过指针连接起来，实现这一特性就是靠跳表节点结构体中的zskiplistLevel 结构体类型的 level 数组。</p><p>level 数组中的每一个元素代表跳表的一层，也就是由 zskiplistLevel 结构体表示，比如 leve[0] 就表示第一层，leve[1] 就表示第二层。zskiplistLevel 结构体里定义了「指向下一个跳表节点的指针」和「跨度」，跨度时用来记录两个节点之间的距离。</p><p><img src="/2022/10/25/2022-10-25-redis-shu-ju-jie-gou/3%E5%B1%82%E8%B7%B3%E8%A1%A8-%E8%B7%A8%E5%BA%A6.webp" alt></p><p>跨度实际上是为了计算这个节点在跳表中的排位。具体怎么做的呢？因为跳表中的节点都是按序排列的，那么计算某个节点排位的时候，从头节点点到该结点的查询路径上，将沿途访问过的所有层的跨度累加起来，得到的结果就是目标节点在跳表中的排位。</p><p>举个例子，查找图中节点 3 在跳表中的排位，从头节点开始查找节点 3，查找的过程只经过了一个层（L2），并且层的跨度是 3，所以节点 3 在跳表中的排位是 3。</p><p>另外，图中的头节点其实也是 zskiplistNode 跳表节点，只不过头节点的后向指针、权重、元素值都没有用到，所以图中省略了这部分。</p><p>「跳表」结构体，如下所示：</p><pre><code>typedef struct zskiplist {    struct zskiplistNode *header, *tail;    unsigned long length;    int level;} zskiplist;</code></pre><ul><li>跳表的头尾节点，便于在O(1)时间复杂度内访问跳表的头节点和尾节点；</li><li>跳表的长度，便于在O(1)时间复杂度获取跳表节点的数量；</li><li>跳表的最大层数，便于在O(1)时间复杂度获取跳表中层高最大的那个节点的层数量；</li></ul><h3 id="跳表节点查询过程"><a href="#跳表节点查询过程" class="headerlink" title="跳表节点查询过程"></a>跳表节点查询过程</h3><p>查找一个跳表节点的过程时，跳表会从头节点的最高层开始，逐一遍历每一层。在遍历某一层的跳表节点时，会用跳表节点中的 SDS 类型的元素和元素的权重来进行判断，共有两个判断条件：</p><ul><li>如果当前节点的权重「小于」要查找的权重时，跳表就会访问该层上的下一个节点。</li><li>如果当前节点的权重「等于」要查找的权重时，并且当前节点的 SDS 类型数据「小于」要查找的数据时，跳表就会访问该层上的下一个节点。</li></ul><p>如果上面两个条件都不满足，或者下一个节点为空时，跳表就会使用目前遍历到的节点的 level 数组里的下一层指针，然后沿着下一层指针继续查找，这就相当于跳到了下一层接着查找。</p><p><img src="/2022/10/25/2022-10-25-redis-shu-ju-jie-gou/3%E5%B1%82%E8%B7%B3%E8%A1%A8-%E8%B7%A8%E5%BA%A6.drawio.webp" alt></p><p>如果要查找「元素：abcd，权重：4」的节点，查找的过程是这样的：</p><ul><li>先从头节点的最高层开始，L2 指向了「元素：abc，权重：3」节点，这个节点的权重比要查找节点的小，所以要访问该层上的下一个节点；</li><li>但是该层的下一个节点是空节点（ leve[2]指向的是空节点），于是就会跳到「元素：abc，权重：3」节点的下一层去找，也就是 leve[1];</li><li>「元素：abc，权重：3」节点的 leve[1] 的下一个指针指向了「元素：abcde，权重：4」的节点，然后将其和要查找的节点比较。虽然「元素：abcde，权重：4」的节点的权重和要查找的权重相同，但是当前节点的 SDS 类型数据「大于」要查找的数据，所以会继续跳到「元素：abc，权重：3」节点的下一层去找，也就是 leve[0]；</li><li>「元素：abc，权重：3」节点的 leve[0] 的下一个指针指向了「元素：abcd，权重：4」的节点，该节点正是要查找的节点，查询结束。</li></ul><h3 id="跳表节点层数设置"><a href="#跳表节点层数设置" class="headerlink" title="跳表节点层数设置"></a>跳表节点层数设置</h3><p>跳表的相邻两层的节点数量的比例会影响跳表的查询性能。</p><p>举个例子，下图的跳表，第二层的节点数量只有 1 个，而第一层的节点数量有 6 个。</p><p><img src="/2022/10/25/2022-10-25-redis-shu-ju-jie-gou/2802786ab4f52c1e248904e5cef33a74.png" alt></p><p>这时，如果想要查询节点 6，那基本就跟链表的查询复杂度一样，就需要在第一层的节点中依次顺序查找，复杂度就是 O(N) 了。所以，为了降低查询复杂度，我们就需要维持相邻层结点数间的关系。</p><p>跳表的相邻两层的节点数量最理想的比例是 2:1，查找复杂度可以降低到 O(logN)。</p><p>下图的跳表就是，相邻两层的节点数量的比例是 2 : 1</p><p><img src="/2022/10/25/2022-10-25-redis-shu-ju-jie-gou/cdc14698f629c74bf5a239cc8a611aeb.png" alt></p><p>那怎样才能维持相邻两层的节点数量的比例为 2 : 1 呢？</p><p>Redis 则采用一种巧妙的方法是，跳表在创建节点的时候，随机生成每个节点的层数，并没有严格维持相邻两层的节点数量比例为 2 : 1 的情况。</p><p>具体的做法是，跳表在创建节点时候，会生成范围为[0-1]的一个随机数，如果这个随机数小于 0.25（相当于概率 25%），那么层数就增加 1 层，然后继续生成下一个随机数，直到随机数的结果大于 0.25 结束，最终确定该节点的层数。</p><p>这样的做法，相当于每增加一层的概率不超过 25%，层数越高，概率越低，层高最大限制是 64。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="string-类型"><a href="#string-类型" class="headerlink" title="string 类型"></a>string 类型</h3><p>底层：简单动态字符串（SDS）。</p><h4 id="简单动态字符串（SDS）"><a href="#简单动态字符串（SDS）" class="headerlink" title="简单动态字符串（SDS）"></a>简单动态字符串（SDS）</h4><ul><li>O（1）复杂度获取字符串长度: 结构中定义len，记录了字符串长度。O（1）复杂度获取字符串长度。</li><li>减少内存分配次数: 结构中定义alloc，分配给字符数组的空间长度。这样在修改字符串的时候，可以通过 alloc - len 计算出剩余的空间大小，可以用来判断空间是否满足修改需求，如果不满足的话，就会自动将 SDS 的空间扩展至执行修改所需的大小，还会给 SDS 分配额外的「未使用空间」。这样的好处是，下次在操作 SDS 时，如果 SDS 空间够的话，API 就会直接使用「未使用空间」，而无须执行内存分配，有效的减少内存分配次数。</li><li>节省内存空间: 结构中定义flags 成员变量，表示的是 SDS 类型。根据不同大小的字符串保存对应的SDS类型，从而有效节省内存空间。</li><li>二进制安全: C语言用 “\0” 字符来标识字符串结尾。SDS有个专门的 len 成员变量来记录长度，所以可存储包含 “\0” 的数据。</li></ul><h3 id="hash-类型"><a href="#hash-类型" class="headerlink" title="hash 类型"></a>hash 类型</h3><p>底层：哈希表或压缩列表，包含的元素数量较少，或者元素值不大的情况才会使用压缩列表，否则用哈希表。</p><h4 id="压缩列表-1"><a href="#压缩列表-1" class="headerlink" title="压缩列表"></a>压缩列表</h4><p>压缩列表的最大特点，就是它被设计成一种内存紧凑型的数据结构，占用一块连续的内存空间，不仅可以利用 CPU 缓存，而且会针对不同长度的数据，进行相应编码，这种方法可以有效地节省内存开销。不能保存过多的元素，否则查询效率就会降低。</p><h4 id="哈希表-1"><a href="#哈希表-1" class="headerlink" title="哈希表"></a>哈希表</h4><ul><li>查询数据 O(1) 的复杂度：哈希表优点在于，它能以 O(1) 的复杂度快速查询数据。将 key 通过 Hash 函数的计算，就能定位数据在表中的位置，因为哈希表实际上是数组，所以可以通过索引值快速查询到数据。</li><li>链式哈希来解决哈希冲突：每个哈希表节点都有一个 next 指针，用于指向下一个哈希表节点，因此多个哈希表节点可以用 next 指针构成一个单项链表，被分配到同一个哈希桶上的多个节点可以用这个单项链表连接起来，这样就解决了哈希冲突。</li><li>rehash：如果哈希冲突严重，随着链表长度的增加，在查询这一位置上的数据的耗时就会增加，毕竟链表的查询的时间复杂度是 O(n)。通过rehash解决，对哈希表的大小进行扩展。结构定义了 2 个哈希表，是因为进行 rehash 的时候，需要用上 2 个哈希表了。在正常服务请求阶段，插入的数据，都会写入到「哈希表 1」，此时的「哈希表 2 」 并没有被分配空间。执行rehash的时候，给「哈希表 2」 分配空间，一般会比「哈希表 1」 大 2 倍；将「哈希表 1」的数据迁移到「哈希表 2」 中。</li><li>渐进式 rehash：如果「哈希表 1 」的数据量非常大，那么在迁移至「哈希表 2 」的时候，因为会涉及大量的数据拷贝，此时可能会对 Redis 造成阻塞，无法服务其他请求。Redis 采用了渐进式 rehash，也就是将数据的迁移的工作不再是一次性迁移完成，而是分多次迁移。渐进式 rehash 步骤：给「哈希表 2」 分配空间；在 rehash 进行期间，每次哈希表元素进行新增、删除、查找或者更新操作时，Redis 除了会执行对应的操作之外，还会顺序将「哈希表 1 」中索引位置上的所有 key-value 迁移到「哈希表 2」 上；随着处理客户端发起的哈希表操作请求数量越多，最终在某个时间点会把「哈希表 1 」的所有 key-value 迁移到「哈希表 2」，从而完成 rehash 操作。查找一个 key 的值的话，先会在「哈希表 1」 里面进行查找，如果没找到，就会继续到哈希表 2 里面进行找到。另外，在渐进式 rehash 进行期间，新增一个 key-value 时，会被保存到「哈希表 2 」里面，而「哈希表 1」 则不再进行任何添加操作，这样保证了「哈希表 1 」的 key-value 数量只会减少，随着 rehash 操作的完成，最终「哈希表 1 」就会变成空表。</li></ul><h3 id="list-类型"><a href="#list-类型" class="headerlink" title="list 类型"></a>list 类型</h3><p>底层：快速列表（QuickList）。</p><ul><li>quicklist 就是「双向链表 + 压缩列表」组合，因为一个 quicklist 就是一个链表，而链表中的每个元素又是一个压缩列表。</li><li>在向 quicklist 添加一个元素的时候，不会像普通的链表那样，直接新建一个链表节点。而是会检查插入位置的压缩列表是否能容纳该元素，如果能容纳就直接保存到 quicklistNode 结构里的压缩列表，如果不能容纳，才会新建一个新的 quicklistNode 结构。</li></ul><h3 id="set-类型"><a href="#set-类型" class="headerlink" title="set 类型"></a>set 类型</h3><p>底层：底层是整数集合或哈希表。如果集合对象的所有元素都是整数值，并且保存元素小于 512 个时，底层将使用整数集合。否则用哈希表。</p><h4 id="整数集合-1"><a href="#整数集合-1" class="headerlink" title="整数集合"></a>整数集合</h4><ul><li>当一个 Set 对象只包含整数值元素，并且元素数量不大时，就会使用整数集这个数据结构作为底层实现。</li><li>整数集合升级的好处是节省内存资源</li><li>整数集合会有一个升级规则，就是当我们将一个新元素加入到整数集合里面，如果新元素的类型（int32_t）比整数集合现有所有元素的类型（int16_t）都要长时，整数集合需要先进行升级，也就是按新元素的类型（int32_t）扩展 contents 数组的空间大小，然后才能将新元素加入到整数集合里，当然升级的过程中，也要维持整数集合的有序性。</li><li>整数集合升级的过程不会重新分配一个新类型的数组，而是在原本的数组上扩展空间，然后在将每个元素按间隔类型大小分割，如果 encoding 属性值为 INTSET_ENC_INT16，则每个元素的间隔就是 16 位。</li></ul><h3 id="zset-类型"><a href="#zset-类型" class="headerlink" title="zset 类型"></a>zset 类型</h3><p>底层：底层是压缩列表或跳表+哈希表，如果元素少于 128 个，且每个元素长度小于 64 字节，使用压缩列表。否则用跳表+哈希表。</p><h4 id="跳表-1"><a href="#跳表-1" class="headerlink" title="跳表"></a>跳表</h4><ul><li>支持平均 O(logN) 复杂度的节点查找</li><li>zset 结构体里有两个数据结构：一个是跳表，一个是哈希表。这样的好处是既能进行高效的范围查询，也能进行高效单点查询。</li><li>Zset 对象在执行数据插入或是数据更新的过程中，会依次在跳表和哈希表中插入或更新相应的数据，从而保证了跳表和哈希表中记录的信息一致。</li><li>Zset 对象能支持范围查询（如 ZRANGEBYSCORE 操作），这是因为它的数据结构设计采用了跳表，而又能以常数复杂度获取元素权重（如 ZSCORE 操作），这是因为它同时采用了哈希表进行索引。</li><li>哈希表只是用于以常数复杂度获取元素权重，大部分操作都是跳表实现的。</li><li>查找过程就是在多个层级上跳来跳去，最后定位到元素。当数据量很大时，跳表的查找复杂度就是 O(logN)。</li><li>查找一个跳表节点的过程时，跳表会从头节点的最高层开始，逐一遍历每一层。在遍历某一层的跳表节点时，会用跳表节点中的 SDS 类型的元素和元素的权重来进行判断，共有两个判断条件：- 如果当前节点的权重「小于」要查找的权重时，跳表就会访问该层上的下一个节点。- 如果当前节点的权重「等于」要查找的权重时，并且当前节点的 SDS 类型数据「小于」要查找的数据时，跳表就会访问该层上的下一个节点。如果上面两个条件都不满足，或者下一个节点为空时，跳表就会使用目前遍历到的节点的 level 数组里的下一层指针，然后沿着下一层指针继续查找，这就相当于跳到了下一层接着查找。</li></ul><p><img src="/2022/10/25/2022-10-25-redis-shu-ju-jie-gou/3%E5%B1%82%E8%B7%B3%E8%A1%A8-%E8%B7%A8%E5%BA%A6.drawio.webp" alt></p><p>如果要查找「元素：abcd，权重：4」的节点，查找的过程是这样的：</p><ul><li>先从头节点的最高层开始，L2 指向了「元素：abc，权重：3」节点，这个节点的权重比要查找节点的小，所以要访问该层上的下一个节点；</li><li>但是该层的下一个节点是空节点（ leve[2]指向的是空节点），于是就会跳到「元素：abc，权重：3」节点的下一层去找，也就是 leve[1];</li><li>「元素：abc，权重：3」节点的 leve[1] 的下一个指针指向了「元素：abcde，权重：4」的节点，然后将其和要查找的节点比较。虽然「元素：abcde，权重：4」的节点的权重和要查找的权重相同，但是当前节点的 SDS 类型数据「大于」要查找的数据，所以会继续跳到「元素：abc，权重：3」节点的下一层去找，也就是 leve[0]；</li><li>「元素：abc，权重：3」节点的 leve[0] 的下一个指针指向了「元素：abcd，权重：4」的节点，该节点正是要查找的节点，查询结束。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CompletableFuture</title>
      <link href="/2022/10/23/2022-10-23-completablefuture/"/>
      <url>/2022/10/23/2022-10-23-completablefuture/</url>
      
        <content type="html"><![CDATA[<p>CompletableFuture常见用法，CompletableFuture等待所有异步任务返回，包括常见的用法，结合自定义线程池使用。</p><pre><code>public class Test {    public static void main(String[] args) throws ExecutionException, InterruptedException {        // 1.创建CompletableFuture对象        //  方法名  功能描述        //  completedFuture(U value)    返回一个已经计算好的CompletableFuture        //  runAsync(Runnable runnable) 使用ForkJoinPool.commonPool()作为线程池执行任务，没有返回值        //  runAsync(Runnable runnable, Executor executor)  使用指定的线程池执行任务，没有返回值        //  supplyAsync(Supplier&lt;U&gt; supplier)   使用ForkJoinPool.commonPool()作为线程池执行任务，有返回值        //  supplyAsync(Supplier&lt;U&gt; supplier, Executor executor)    使用指定的线程池执行任务，有返回值        CompletableFuture&lt;Integer&gt; intFuture = CompletableFuture.completedFuture(100);        // 100        System.out.println(intFuture.get());        CompletableFuture&lt;Void&gt; voidFuture = CompletableFuture.runAsync(() -&gt; System.out.println(&quot;hello&quot;));        // null        System.out.println(voidFuture.get());        CompletableFuture&lt;String&gt; stringFuture = CompletableFuture.supplyAsync(() -&gt; &quot;hello&quot;);        // hello        System.out.println(&quot;stringFuture=&quot;+stringFuture.get());        // 2.计算结果完成时.可以执行的方法        // 方法名        // whenComplete(BiConsumer&lt;? super T,? super Throwable&gt; action)        // whenCompleteAsync(BiConsumer&lt;? super T,? super Throwable&gt; action)        // whenCompleteAsync(BiConsumer&lt;? super T,? super Throwable&gt; action, Executor executor)        CompletableFuture&lt;String&gt; future = CompletableFuture.supplyAsync(() -&gt; {            return &quot;hello&quot;;        }).whenComplete((v, e) -&gt; {            // hello            System.out.println(v);        });        // hello        System.out.println(future.get());        // 3.转换，消费，执行        // 方法名      功能描述        // thenApply    获取上一个任务的返回，并返回当前任务的值        // thenAccept   获取上一个任务的返回，单纯消费，没有返回值        // thenRun      上一个任务执行完成后，开始执行thenRun中的任务        CompletableFuture.supplyAsync(() -&gt; {            return &quot;hello &quot;;        }).thenAccept(str -&gt; {            // hello world            System.out.println(str + &quot;world&quot;);        }).thenRun(() -&gt; {            // task finish            System.out.println(&quot;task finish&quot;);        });        // 4.组合（两个任务都完成）        // 方法名  描述        // thenCombine  组合两个future，获取两个future的返回结果，并返回当前任务的返回值        // thenAcceptBoth   组合两个future，获取两个future任务的返回结果，然后处理任务，没有返回值        // runAfterBoth 组合两个future，不需要获取future的结果，只需两个future处理完任务后，处理该任务        CompletableFuture&lt;String&gt; futureA = CompletableFuture.supplyAsync(() -&gt; {            return &quot;欢迎关注 &quot;;        }).thenApply(t -&gt; {            return t + &quot;微信公众号 &quot;;        }).thenCombine(CompletableFuture.completedFuture(&quot;Java&quot;), (t, u) -&gt; {            return t + u;        }).whenComplete((t, e) -&gt; {            // 欢迎关注 微信公众号 Java            System.out.println(t);        });        // 5.组合（只需要一个任务完成）        // 方法名  描述        // applyToEither    两个任务有一个执行完成，获取它的返回值，处理任务并返回当前任务的返回值        // acceptEither 两个任务有一个执行完成，获取它的返回值，处理任务，没有返回值        // runAfterEither   两个任务有一个执行完成，不需要获取future的结果，处理任务，也没有返回值        CompletableFuture&lt;String&gt; future1 = CompletableFuture.supplyAsync(() -&gt; {            return &quot;欢迎关注微信公众号&quot;;        });        CompletableFuture&lt;String&gt; future2 = CompletableFuture.supplyAsync(() -&gt; {            return &quot;Java&quot;;        });        CompletableFuture&lt;String&gt; future3 = future1.applyToEither(future2, str -&gt; str);        // 欢迎关注微信公众号 Java 随机输出        System.out.println(future3.get());        //  6.多任务组合        //  方法名 描述        //  allOf   当所有的CompletableFuture完成后执行计算        //  anyOf   任意一个CompletableFuture完成后执行计算        //  allOf的使用        CompletableFuture&lt;String&gt; future1A = CompletableFuture.supplyAsync(() -&gt; {            sleepRandom();            return &quot;欢迎关注&quot;;        });        CompletableFuture&lt;String&gt; future2A = CompletableFuture.supplyAsync(() -&gt; {            sleepRandom();            return &quot;微信公众号&quot;;        });        CompletableFuture&lt;String&gt; future3A = CompletableFuture.supplyAsync(() -&gt; {            sleepRandom();            return &quot;Java识堂&quot;;        });        // 欢迎关注 微信公众号 Java        CompletableFuture.allOf(future1A, future2A, future3A)                .thenApply(v -&gt; {                            List&lt;Object&gt; collect = Stream.of(future1, future2, future3A)                                    .map(CompletableFuture::join)                                    .collect(Collectors.toList());                            return collect;                        }                )                .thenAccept(System.out::print);        // anyOf的使用        CompletableFuture&lt;String&gt; future1C = CompletableFuture.supplyAsync(() -&gt; {            sleepRandom();            return &quot;欢迎关注&quot;;        });        CompletableFuture&lt;String&gt; future2C = CompletableFuture.supplyAsync(() -&gt; {            sleepRandom();            return &quot;微信公众号&quot;;        });        CompletableFuture&lt;String&gt; future3C = CompletableFuture.supplyAsync(() -&gt; {            sleepRandom();            return &quot;Java&quot;;        });        CompletableFuture&lt;Object&gt; resultFuture = CompletableFuture.anyOf(future1C, future2C, future3C);        // 欢迎关注 微信公众号 Java 随机输出        System.out.println(resultFuture.get());        // 7.异常处理        // exceptionally    捕获异常，进行处理        CompletableFuture&lt;Integer&gt; futureC = CompletableFuture.supplyAsync(() -&gt; {            return 100 / 0;        }).thenApply(num -&gt; {            return num + 10;        }).exceptionally(throwable -&gt; {            return 0;        });        // 0        System.out.println(future.get());        // 当然有一些接口能捕获异常        CompletableFuture futureAAA = CompletableFuture.supplyAsync(() -&gt; {            String str = null;            return str.length();        }).whenComplete((v, e) -&gt; {            if (e == null) {                System.out.println(&quot;正常结果为&quot; + v);            } else {                // 发生异常了java.util.concurrent.CompletionException: java.lang.NullPointerException                System.out.println(&quot;发生异常了&quot; + e.toString());            }        });        // 8.集合业务使用示例，假设stringList为业务执行集合        ExecutorService executor = new ThreadPoolExecutor(10, 16, 10, TimeUnit.MICROSECONDS, new ArrayBlockingQueue&lt;&gt;(1000));        List&lt;String&gt; stringList = new ArrayList&lt;&gt;();        for (int i = 0; i &lt; 100; i++) {            stringList.add(&quot;a&quot; + i);        }        CompletableFuture&lt;Void&gt; all = null;        CompletableFuture&lt;Void&gt; all1 = null;        // 开始我们的业务处理        for (String personName : stringList) {            CompletableFuture&lt;Object&gt; stringCompletableFuture = CompletableFuture.supplyAsync(() -&gt; {                return null;            }).whenComplete((v, e) -&gt; {                if (e == null) {                    System.out.println(&quot;正常结果为&quot; + v);                } else {                    // 发生异常了java.util.concurrent.CompletionException: java.lang.NullPointerException                    System.out.println(&quot;发生异常了&quot; + e.toString());                }            });            all = CompletableFuture.allOf(stringCompletableFuture);            CompletableFuture&lt;String&gt; stringCompletableFuture1 = CompletableFuture.supplyAsync(() -&gt; {                // 模拟业务逻辑，say hello world                System.out.println(personName + &quot;: Hello World!&quot;);                return &quot;task finished!&quot;;            });            all1 = CompletableFuture.allOf(stringCompletableFuture1);        }        // 开始等待所有任务执行完成        all.join();        all1.join();        // 使用JDK 1.8的特性，stream()和Lambda表达式: (参数) -&gt; {表达式}        long start = System.currentTimeMillis();        if (CollectionUtils.isEmpty(stringList)) {            return;        }        final CompletableFuture[] completableFutures = stringList.stream().                map(t -&gt; CompletableFuture                        .supplyAsync(() -&gt; pause(t), executor)                        .whenComplete((result, th) -&gt; {                            System.out.println(&quot;hello&quot; + result);                        })).toArray(CompletableFuture[]::new);        // 开始等待所有任务执行完成        System.out.println(&quot;start block&quot;);        CompletableFuture.allOf(completableFutures).join();        System.out.println(&quot;block finish, consume time:&quot; + (System.currentTimeMillis() - start));        stringList.forEach(name -&gt; CompletableFuture.supplyAsync(() -&gt; {            // 封装了业务逻辑            System.out.println(&quot;name = &quot; + name);            return &quot;success&quot;;        }).exceptionally(e -&gt; {            System.out.println(e);            return &quot;false&quot;;        }));        // 关闭线程池        executor.shutdown();    }    private static void sleepRandom() {        System.out.println(&quot;测试方法&quot;);    }    public static String pause(String name) {        try {            Thread.sleep(300);        } catch (Exception e) {            e.printStackTrace();        }        return name;    }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java多线程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>高并发</title>
      <link href="/2022/10/20/2022-10-20-gao-bing-fa/"/>
      <url>/2022/10/20/2022-10-20-gao-bing-fa/</url>
      
        <content type="html"><![CDATA[<ul><li>高并发也就是系统访问用户增多，流量增大，导致服务器压力增大，出现性能瓶颈。</li><li>硬件方面：单体应用垂直扩容，如果服务器配置太低，升级服务器配置。</li><li>单体应用水平扩容，部署应用集群，负载均衡分流。</li><li>分布式redis缓存，主从架构+集群+哨兵，还可以再添加本地缓存，减少网络的请求IO。</li><li>数据库垂直拆分，数据库查询的最小单位是页，默认16k，如果一个表的字段太多，那么读进内存的行数就会少，会增加读的IO次数，把大表字段根据业务拆分多张表。数据库水平拆分，分表，行数减少降低了索引的层数，提高查询速度。像每天增加几千万的话，有钱就直接上TIDB吧，无限水平扩展和高可用性。</li><li>消息队列异步，缩短流程执行时间，消息队列削峰，平缓处理请求，避免系统因瞬间的巨大压力而压垮</li><li>网关结合redis令牌桶，根据ip，根据用户id，根据方法名限流。</li><li>线程池也减少资源消耗，提升系统性能。</li><li>代码性能优化，例如complateTableFuture并行执行，初始化map大小等。</li><li>sql性能分析与调优。尽量走到索引，避免慢sql。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模板模式 + 策略模式</title>
      <link href="/2022/10/19/2022-10-19-mo-ban-mo-shi/"/>
      <url>/2022/10/19/2022-10-19-mo-ban-mo-shi/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本篇主要记录和总结工作中用到模板 + 策略 + 工厂模式的一种实现方式</p></blockquote><h3 id="先抽象公共的方法模板"><a href="#先抽象公共的方法模板" class="headerlink" title="先抽象公共的方法模板"></a>先抽象公共的方法模板</h3><pre><code>/** * @author feiliang * @date 2022/4/20 21:56 */public abstract class AbstractService {    @Resource    private LevelFactory levelFactory;    /**     * 处理流程模板     */    protected String handle() {        // 1. 获取需要处理的数据        String data = this.getData();        // 2. 获取对象        Consumer consumer = this.getConsumer(data);        // 3. 获取等级策略处理        ILevelHandle levelHandler = levelFactory.get(consumer.getLevelCode());        // 4. 等级改变处理        String changeType = this.getChangeType();        boolean change = levelHandler.handle(consumer, changeType);        // 5. 记录用户等级变化DB操作        return null;    }    /**     * 获取需要处理的数据     */    protected abstract String getData();    /**     * 获取等级和金额     *     * @param consumerCode     * @return     */    protected abstract Consumer getConsumer(String consumerCode);    /**     * 获取升级或者降级类型 LevelChangeTypeConstants     *     * @return     */    protected abstract String getChangeType();    /**     * 1升级、2降级、3续会     *     * @return     */    protected abstract int getLevelChangeType();}</code></pre><h3 id="定义接口"><a href="#定义接口" class="headerlink" title="定义接口"></a>定义接口</h3><pre><code>/** * @author feiliang * @date 2022/4/20 13:53 */public interface ILevelService {    /**     * 等级变动计算     * @return     */    String changeLevel();}</code></pre><h3 id="接口实现"><a href="#接口实现" class="headerlink" title="接口实现"></a>接口实现</h3><pre><code>/** * @author feiliang * @date 2022/4/20 13:56 */@Slf4j@Servicepublic class UpgradeLevelServiceImpl extends AbstractLevelService implements ILevelService {    @Override    public String changeLevel(String changeType) {        return super.handle();    }    @Override    protected List&lt;String&gt; getData() {        return &quot;aaa&quot;;    }    @Override    protected ConsumerMember getConsumer(String consumerCode) {        // DB操作        return null;    }    @Override    protected String getChangeType() {        // 枚举返回值        return null;    }}</code></pre><h3 id="定义策略接口"><a href="#定义策略接口" class="headerlink" title="定义策略接口"></a>定义策略接口</h3><pre><code>/** * @author feiliang * @date 2022/4/20 17:39 */public interface ILevelHandle {    /**     * 升级     *     * @param consumer     * @param amount     * @return 是否有变更等级     */    Boolean upgrade(Consumer consumer, BigDecimal amount);    /**     * 降级     *     * @param consumer     * @param amount     * @return 是否有变更等级     */    Boolean demotion(Consumer consumer, BigDecimal amount);    /**     * 变动等级     *     * @param consumer     * @param changeType     * @param amount     * @return 是否有变更等级     */    default Boolean handle(Consumer consumer, String changeType, BigDecimal amount) {        valid(consumer, changeType, amount);        return changeLevel(consumerMember, changeType, amount);    }    /**     * 内部校验     *     * @param consumer     * @param changeType     * @param amount     */    default void valid(Consumer consumer, String changeType, BigDecimal amount) {        Assert.notNull(consumer, &quot;consumer object can not be null&quot;);        Assert.notNull(amount, &quot;amount object can not be null&quot;);        Assert.isTrue(StringUtil.isNotBlank(changeType), &quot;changeType can not be null&quot;);        Assert.isTrue(Sets.newHashSet(LevelChangeTypeConstants.UPGRADE, LevelChangeTypeConstants.DEMOTION).contains(changeType),                &quot;changeType is error&quot;);    }    /**     * 改变等级     *     * @param consumer     * @param changeType     * @param amount     * @return 是否有变更等级     */    default Boolean changeLevel(Consumer consumer, String changeType, BigDecimal amount) {        if (LevelChangeTypeConstants.UPGRADE.equals(changeType)) {            return upgrade(consumer, amount);        } else if (LevelChangeTypeConstants.DEMOTION.equals(changeType)) {            return demotion(consumer, amount);        }        return false;    }    /**     * expire_time设置为当前日期+12个月的日期时间     *     * @return 新的失效时间     */    default LocalDateTime updateExpireTime() {        return DateUtil.fromDate(new Date()).plusYears(1);    }}</code></pre><h3 id="策略接口实现"><a href="#策略接口实现" class="headerlink" title="策略接口实现"></a>策略接口实现</h3><pre><code>@Service(LevelChangeTypeConstants.BRONZE_MEDAL)public class BronzeMedalLevelHandle implements ILevelHandle {    @Override    public Boolean upgrade(Consumer consumer, BigDecimal memberLevelTotalAmount) {        // 升级逻辑        return true;    }    @Override    public Boolean demotion(Consumer consumer, BigDecimal memberLevelTotalAmount) {        // 降级逻辑        return true;    }}</code></pre><h3 id="策略工厂"><a href="#策略工厂" class="headerlink" title="策略工厂"></a>策略工厂</h3><pre><code>/** * @author feiliang * @date 2022/4/20 17:47 */@Componentpublic class LevelFactory {    @Lazy    @Resource    private Map&lt;String, ILevelService&gt; levelHandleMap;    /**     * 根据类型获取对应策略     *     * @param levelCode 等级类型     * @return     */    public IMemberLevelService get(String levelCode) {        return levelHandleMap.get(levelCode);    }}</code></pre><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本案例主要结合策略，工厂，模板模式解决公共流程的处理，本案例由于复杂业务导致策略模式较为复杂，平时中直接正常策略即可。</p>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线程池</title>
      <link href="/2022/06/15/2022-06-15-threadpool/"/>
      <url>/2022/06/15/2022-06-15-threadpool/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本篇主要记录和总结工作中不同场景需要的线程池实现</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>线程池是一种多线程处理形式，处理过程中将任务添加到队列，然后再创建线程后自动启动这些任务。</p><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>但不同的业务场景，线程池配置需要不一致。合理配置线程池，可以如虎添翼，更好的提高性能。</p><p>本篇中主要实现两种线程池，用于运用不同的场景需求；</p><ul><li>适合于业务处理需要远程资源的场景</li><li>比较适合于CPU密集型应用（比如runnable内部执行的操作都在JVM内部）</li></ul><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><h3 id="适合于CPU密集型应用"><a href="#适合于CPU密集型应用" class="headerlink" title="适合于CPU密集型应用"></a>适合于CPU密集型应用</h3><p>ThreadPoolExecutor: coreThread -&gt; queue -&gt; maxThread -&gt; reject</p><p>优先offer到queue，queue满后再扩充线程到maxThread，如果已经到了maxThread就reject</p><p>场景优势：比较适合于CPU密集型应用（比如runnable内部执行的操作都在JVM内部）</p><h3 id="适合于业务处理需要远程资源"><a href="#适合于业务处理需要远程资源" class="headerlink" title="适合于业务处理需要远程资源"></a>适合于业务处理需要远程资源</h3><p>StandardThreadExecutor：coreThread -&gt; maxThread -&gt; queue -&gt; reject</p><p>优先扩充线程到maxThread，再offer到queue，如果满了就reject</p><p>场景优势：比较适合于业务处理需要远程资源的场景</p><h3 id="LinkedTransferQueue"><a href="#LinkedTransferQueue" class="headerlink" title="LinkedTransferQueue"></a>LinkedTransferQueue</h3><p>LinkedTransferQueue 能保证更高性能，相比与LinkedBlockingQueue有明显提升。</p><p>采用一种预占模式。意思就是消费者线程取元素时，如果队列不为空，则直接取走数据，若队列为空，那就生成一个节点（节点元素为null）入队，然后消费者线程被等待在这个节点上，后面生产者线程入队时发现有一个元素为null的节点，生产者线程就不入队了，直接就将元素填充到该节点，并唤醒该节点等待的线程，被唤醒的消费者线程取走元素，从调用的方法返回。我们称这种节点操作为“匹配”方式。</p><p>缺点:没有队列长度控制，需要在外层协助控制</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="定义properties配置属性类"><a href="#定义properties配置属性类" class="headerlink" title="定义properties配置属性类"></a>定义properties配置属性类</h3><pre><code>package com.thread.config;import org.springframework.boot.context.properties.ConfigurationProperties;/** * Description: * &lt;br/&gt; * ThreadPoolProperties * * @author feiliang */@ConfigurationProperties(prefix = ThreadPoolProperties.THREAD_PREFIX)public class ThreadPoolProperties {    public final static String THREAD_PREFIX = &quot;thread.config&quot;;    /**     * 是否开启标准线程池     */    private boolean standardEnabled = Boolean.FALSE;    /**     * 默认最小核心线程数     */    private int minMumPoolSize = 20;    /**     * 默认最大核心线程数     */    private int maxiMumPoolSize = 200;    /**     * 队列大小     */    private int queueCapacity = 200;    /**     * keepAliveTime时间     */    private int keepAliveTime = 60 * 1000;    public static String getThreadPrefix() {        return THREAD_PREFIX;    }    public boolean isStandardEnabled() {        return standardEnabled;    }    public void setStandardEnabled(boolean standardEnabled) {        this.standardEnabled = standardEnabled;    }    public int getMinMumPoolSize() {        return minMumPoolSize;    }    public void setMinMumPoolSize(int minMumPoolSize) {        this.minMumPoolSize = minMumPoolSize;    }    public int getMaxiMumPoolSize() {        return maxiMumPoolSize;    }    public void setMaxiMumPoolSize(int maxiMumPoolSize) {        this.maxiMumPoolSize = maxiMumPoolSize;    }    public int getQueueCapacity() {        return queueCapacity;    }    public void setQueueCapacity(int queueCapacity) {        this.queueCapacity = queueCapacity;    }    public int getKeepAliveTime() {        return keepAliveTime;    }    public void setKeepAliveTime(int keepAliveTime) {        this.keepAliveTime = keepAliveTime;    }}</code></pre><h3 id="线程工厂命名声明"><a href="#线程工厂命名声明" class="headerlink" title="线程工厂命名声明"></a>线程工厂命名声明</h3><pre><code>package com.thread.config;import java.util.concurrent.ThreadFactory;import java.util.concurrent.atomic.AtomicInteger;/** * Description: 线程工厂命名声明 * &lt;br/&gt; * NamedThreadFactory * * @author feiliang */public class NamedThreadFactory implements ThreadFactory {    private static final AtomicInteger POOL_SEQ = new AtomicInteger(1);    private final AtomicInteger mThreadNum = new AtomicInteger(1);    private final String mPrefix;    private final boolean daemon;    private final ThreadGroup group;    public NamedThreadFactory() {        this(&quot;standard-pool-&quot; + POOL_SEQ.getAndIncrement(), false);    }    public NamedThreadFactory(String prefix) {        this(prefix, false);    }    public NamedThreadFactory(String prefix, boolean daemon) {        this.mPrefix = prefix + &quot;-thread-&quot;;        this.daemon = daemon;        SecurityManager s = System.getSecurityManager();        this.group = (s == null) ? Thread.currentThread().getThreadGroup() : s.getThreadGroup();    }    @Override    public Thread newThread(Runnable r) {        String name = mPrefix + mThreadNum.getAndIncrement();        Thread t = new Thread(group, r, name, 0);        t.setDaemon(daemon);        return t;    }}</code></pre><h3 id="线程池配置"><a href="#线程池配置" class="headerlink" title="线程池配置"></a>线程池配置</h3><pre><code>package com.thread;import com.thread.config.NamedThreadFactory;import com.thread.config.ThreadPoolProperties;import java.util.concurrent.LinkedTransferQueue;import java.util.concurrent.RejectedExecutionException;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicInteger;/** * Description: * * The Standard Thread Executor * * &lt;p&gt; * * ThreadPoolExecutor: coreThread -&gt; queue -&gt; maxThread -&gt; reject: * * 优先offer到queue，queue满后再扩充线程到maxThread，如果已经到了maxThread就reject * * 场景优势：比较适合于CPU密集型应用（比如runnable内部执行的操作都在JVM内部: memory copy or compute） * * &lt;p&gt; * * StandardThreadExecutor：coreThread -&gt; maxThread -&gt; queue -&gt; reject * * 优先扩充线程到maxThread，再offer到queue，如果满了就reject * * 场景优势：比较适合于业务处理需要远程资源的场景 * @author feiliang */public class StandardThreadExecutor extends ThreadPoolExecutor {    /**     * 提交处理中的任务数     */    private AtomicInteger submittedTasksCount;    /**     * 最大并发任务限制     */    private int maxSubmittedTaskCount;    public StandardThreadExecutor(ThreadPoolProperties threadPoolProperties) {        // CALLER_RUNS：不在新线程中执行任务，而是有调用者所在的线程来执行        super(threadPoolProperties.getMinMumPoolSize(), threadPoolProperties.getMaxiMumPoolSize(), threadPoolProperties.getKeepAliveTime(),                TimeUnit.MILLISECONDS, new StandardExecutorQueue(),                new NamedThreadFactory(), new ThreadPoolExecutor.CallerRunsPolicy());        ((StandardExecutorQueue) getQueue()).setStandardThreadExecutor(this);        submittedTasksCount = new AtomicInteger(0);        // 最大并发任务限制： 队列buffer数 + 最大线程数        maxSubmittedTaskCount = threadPoolProperties.getQueueCapacity() + threadPoolProperties.getMaxiMumPoolSize();    }    @Override    public void execute(Runnable command) {        int count = submittedTasksCount.incrementAndGet();        // 超过最大的并发任务限制，进行 reject        // 依赖的LinkedTransferQueue没有长度限制，因此这里进行控制        if (count &gt; maxSubmittedTaskCount) {            submittedTasksCount.decrementAndGet();            getRejectedExecutionHandler().rejectedExecution(command, this);        }        try {            super.execute(command);        } catch (Exception rx) {            if (!((StandardExecutorQueue) getQueue()).force(command)) {                submittedTasksCount.decrementAndGet();                getRejectedExecutionHandler().rejectedExecution(command, this);            }        }    }    public int getSubmittedTasksCount() {        return this.submittedTasksCount.get();    }    @Override    protected void afterExecute(Runnable r, Throwable t) {        submittedTasksCount.decrementAndGet();    }}/** * Description: * LinkedTransferQueue 能保证更高性能，相比与LinkedBlockingQueue有明显提升。 * 采用一种预占模式。意思就是消费者线程取元素时，如果队列不为空，则直接取走数据，若队列为空，那就生成一个节点（节点元素为null）入队，然后消费者线程被等待在这个节点上， * 后面生产者线程入队时发现有一个元素为null的节点，生产者线程就不入队了，直接就将元素填充到该节点，并唤醒该节点等待的线程，被唤醒的消费者线程取走元素，从调用的方法返回。我们称这种节点操作为“匹配”方式。 * &lt;p&gt; * 缺点:没有队列长度控制，需要在外层协助控制 */class StandardExecutorQueue extends LinkedTransferQueue&lt;Runnable&gt; {    private StandardThreadExecutor threadPoolExecutor;    StandardExecutorQueue() {        super();    }    void setStandardThreadExecutor(StandardThreadExecutor threadPoolExecutor) {        this.threadPoolExecutor = threadPoolExecutor;    }    boolean force(Runnable o) {        if (threadPoolExecutor.isShutdown()) {            throw new RejectedExecutionException(&quot;Executor not running, can&#39;t force a command into the queue&quot;);        }        return super.offer(o);    }    @Override    public boolean offer(Runnable o) {        int poolSize = threadPoolExecutor.getPoolSize();        // we are maxed out on threads, simply queue the object        if (poolSize == threadPoolExecutor.getMaximumPoolSize()) {            return super.offer(o);        }        // 任务少于当前线程数，不需要新开线程，任务直接放入队列        if (threadPoolExecutor.getSubmittedTasksCount() &lt;= poolSize) {            return super.offer(o);        }        // 任务大于当前线程数，且当前线程数少于最大线程数，新开线程处理任务，不放入队列了        if (poolSize &lt; threadPoolExecutor.getMaximumPoolSize()) {            return false;        }        // 任务放到队列中        return super.offer(o);    }}</code></pre><h3 id="spring-boot配置类"><a href="#spring-boot配置类" class="headerlink" title="spring boot配置类"></a>spring boot配置类</h3><pre><code>package com.thread.config;import com.thread.StandardThreadExecutor;import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean;import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;import org.springframework.boot.context.properties.EnableConfigurationProperties;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import java.util.concurrent.Executors;import java.util.concurrent.LinkedBlockingQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;/** * Description: * &lt;br/&gt; * ThreadPoolAutoConfig * * @author feiliang */@Configuration@EnableConfigurationProperties(value = {ThreadPoolProperties.class})public class ThreadPoolAutoConfig {    /**     * 比较适合于业务处理需要远程资源的场景     *     * @param threadPoolProperties     * @return     */    @Bean    @ConditionalOnProperty(prefix = ThreadPoolProperties.THREAD_PREFIX, name = &quot;standard-enabled&quot;, havingValue = &quot;true&quot;)    public StandardThreadExecutor standardThreadExecutor(ThreadPoolProperties threadPoolProperties) {        return new StandardThreadExecutor(threadPoolProperties);    }    /**     * 比较适合于CPU密集型应用（比如runnable内部执行的操作都在JVM内部: memory copy or compute）     *     * @param threadPoolProperties 线程池配置     * @return     */    @Bean    @ConditionalOnMissingBean(ThreadPoolExecutor.class)    public ThreadPoolExecutor threadPoolExecutor(ThreadPoolProperties threadPoolProperties) {        return new ThreadPoolExecutor(threadPoolProperties.getMinMumPoolSize(), threadPoolProperties.getMaxiMumPoolSize(), threadPoolProperties.getKeepAliveTime(), TimeUnit.MILLISECONDS,                new LinkedBlockingQueue&lt;&gt;(threadPoolProperties.getQueueCapacity()), Executors.defaultThreadFactory(), new ThreadPoolExecutor.CallerRunsPolicy());    }}</code></pre><h3 id="配置在resources-META-INF-spring-factories"><a href="#配置在resources-META-INF-spring-factories" class="headerlink" title="配置在resources/META-INF/spring.factories"></a>配置在resources/META-INF/spring.factories</h3><pre><code>org.springframework.boot.autoconfigure.EnableAutoConfiguration=\  com.thread.config.ThreadPoolAutoConfig</code></pre><h2 id="运用"><a href="#运用" class="headerlink" title="运用"></a>运用</h2><h3 id="引用依赖"><a href="#引用依赖" class="headerlink" title="引用依赖"></a>引用依赖</h3><pre><code>&lt;dependency&gt;    &lt;groupId&gt;org.example&lt;/groupId&gt;    &lt;artifactId&gt;threadpool-starter&lt;/artifactId&gt;    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt;</code></pre><h3 id="配置参数"><a href="#配置参数" class="headerlink" title="配置参数"></a>配置参数</h3><pre><code>thread:  config:    standard-enabled: true    min-mum-pool-size: 20    maxi-mum-pool-size: 200    queue-capacity: 200</code></pre><h3 id="场景应用"><a href="#场景应用" class="headerlink" title="场景应用"></a>场景应用</h3><pre><code>@Autowiredprivate StandardThreadExecutor standardThreadExecutor;@Autowiredprivate ThreadPoolExecutor threadPoolExecutor;standardThreadExecutor.execute(() -&gt; {    String name = Thread.currentThread().getName();    System.out.println(name);    if (!name.startsWith(&quot;standard&quot;)) {        System.out.println(&quot;-----------------&quot; + name);    }});</code></pre>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> starter组件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis知识</title>
      <link href="/2022/04/27/2022-04-27-redis-05/"/>
      <url>/2022/04/27/2022-04-27-redis-05/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《Redis深度历险》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>布隆过滤器专门用来解决去重问题的。它在起到去重的同时，在空间上还能节省 90% 以上，只是稍微有那么点不精确，也就是有一定的误判概率。</p><p>布隆过滤器可以理解为一个不怎么精确的 set 结构，当你使用它的 contains 方法判断某个对象是否存在时，它可能会误判。但是布隆过滤器也不是特别不精确，只要参数设置的合理，它的精确度可以控制的相对足够精确，只会有小小的误判概率。</p><p>当布隆过滤器说某个值存在时，这个值可能不存在；当它说不存在时，那就肯定不存在。</p><h2 id="Redis-中的布隆过滤器"><a href="#Redis-中的布隆过滤器" class="headerlink" title="Redis 中的布隆过滤器"></a>Redis 中的布隆过滤器</h2><p>Redis 官方提供的布隆过滤器到了 Redis 4.0 提供了插件功能之后才正式登场。布隆过滤器作为一个插件加载到 Redis Server 中，给 Redis 提供了强大的布隆去重功能。</p><p>docker安装：</p><pre><code>&gt; docker pull redislabs/rebloom  # 拉取镜像&gt; docker run -p6379:6379 redislabs/rebloom  # 运行容器&gt; redis-cli  # 连接容器中的 redis 服务</code></pre><h2 id="布隆过滤器基本使用"><a href="#布隆过滤器基本使用" class="headerlink" title="布隆过滤器基本使用"></a>布隆过滤器基本使用</h2><p>布隆过滤器有二个基本指令，bf.add 添加元素，bf.exists 查询元素是否存在，它的用法和 set 集合的 sadd 和 sismember 差不多。注意 bf.add 只能一次添加一个元素，如果想要一次添加多个，就需要用到 bf.madd 指令。同样如果需要一次查询多个元素是否存在，就需要用到 bf.mexists 指令。</p><pre><code>127.0.0.1:6379&gt; bf.add codehole user1(integer) 1127.0.0.1:6379&gt; bf.add codehole user2(integer) 1127.0.0.1:6379&gt; bf.add codehole user3(integer) 1127.0.0.1:6379&gt; bf.exists codehole user1(integer) 1127.0.0.1:6379&gt; bf.exists codehole user2(integer) 1127.0.0.1:6379&gt; bf.exists codehole user3(integer) 1127.0.0.1:6379&gt; bf.exists codehole user4(integer) 0127.0.0.1:6379&gt; bf.madd codehole user4 user5 user61) (integer) 12) (integer) 13) (integer) 1127.0.0.1:6379&gt; bf.mexists codehole user4 user5 user6 user71) (integer) 12) (integer) 13) (integer) 14) (integer) 0</code></pre><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><p>布隆过滤器的initial_size估计的过大，会浪费存储空间，估计的过小，就会影响准确率，用户在使用之前一定要尽可能地精确估计好元素数量，还需要加上一定的冗余空间以避免实际元素可能会意外高出估计值很多。</p><p>布隆过滤器的error_rate越小，需要的存储空间就越大，对于不需要过于精确的场合，error_rate设置稍大一点也无伤大雅。</p><h2 id="布隆过滤器的原理"><a href="#布隆过滤器的原理" class="headerlink" title="布隆过滤器的原理"></a>布隆过滤器的原理</h2><p><img src="/2022/04/27/2022-04-27-redis-05/1.webp" alt></p><p>每个布隆过滤器对应到 Redis 的数据结构里面就是一个大型的位数组和几个不一样的无偏 hash 函数。所谓无偏就是能够把元素的 hash 值算得比较均匀。</p><p>向布隆过滤器中添加 key 时，会使用多个 hash 函数对 key 进行 hash 算得一个整数索引值然后对位数组长度进行取模运算得到一个位置，每个 hash 函数都会算得一个不同的位置。再把位数组的这几个位置都置为 1 就完成了 add 操作。</p><p>向布隆过滤器询问 key 是否存在时，跟 add 一样，也会把 hash 的几个位置都算出来，看看位数组中这几个位置是否都为 1，只要有一个位为 0，那么说明布隆过滤器中这个 key 不存在。如果都是 1，这并不能说明这个 key 就一定存在，只是极有可能存在，因为这些位被置为 1 可能是因为其它的 key 存在所致。如果这个位数组比较稀疏，判断正确的概率就会很大，如果这个位数组比较拥挤，判断正确的概率就会降低。</p><p>使用时不要让实际元素远大于初始化大小，当实际元素开始超出初始化大小时，应该对布隆过滤器进行重建，重新分配一个 size 更大的过滤器，再将所有的历史元素批量 add 进去 (这就要求我们在其它的存储器中记录所有的历史元素)。因为 error_rate 不会因为数量超出就急剧增加，这就给我们重建过滤器提供了较为宽松的时间。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>布隆过滤器专门用来解决去重问题的。它在起到去重的同时，在空间上还能节省 90% 以上，只是稍微有那么点不精确，也就是有一定的误判概率。可应用在缓存穿透。</p>]]></content>
      
      
      <categories>
          
          <category> 中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis知识</title>
      <link href="/2022/04/25/2022-04-25-redis-04/"/>
      <url>/2022/04/25/2022-04-25-redis-04/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《Redis深度历险》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>为每一个页面一个独立的 set 集合来存储所有当天访问过此页面的用户 ID。当一个请求过来时，我们使用 sadd 将用户 ID 塞进去就可以了。通过 scard 可以取出这个集合的大小，这个数字就是这个页面的 UV 数据。没错，这是一个非常简单的方案。</p><p>但是，如果你的页面访问量非常大，比如一个爆款页面几千万的 UV，你需要一个很大的 set 集合来统计，这就非常浪费空间。如果这样的页面很多，那所需要的存储空间是惊人的。为这样一个去重功能就耗费这样多的存储空间，值得么？其实老板需要的数据又不需要太精确，105w 和 106w 这两个数字对于老板们来说并没有多大区别，So，有没有更好的解决方案呢？</p><p>这就是本节要引入的一个解决方案，Redis 提供了 HyperLogLog 数据结构就是用来解决这种统计问题的。HyperLogLog 提供不精确的去重计数方案，虽然不精确但是也不是非常不精确，标准误差是 0.81%，这样的精确度已经可以满足上面的 UV 统计需求了。</p><h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><p>HyperLogLog 提供了两个指令 pfadd 和 pfcount，根据字面意义很好理解，一个是增加计数，一个是获取计数。pfadd 用法和 set 集合的 sadd 是一样的，来一个用户 ID，就将用户 ID 塞进去就是。pfcount 和 scard 用法是一样的，直接获取计数值。</p><pre><code>127.0.0.1:6379&gt; pfadd codehole user1(integer) 1127.0.0.1:6379&gt; pfcount codehole(integer) 1127.0.0.1:6379&gt; pfadd codehole user2(integer) 1127.0.0.1:6379&gt; pfadd codehole user3 user4 user5 user16(integer) 1127.0.0.1:6379&gt; pfcount codehole(integer) 6</code></pre><p>HyperLogLog 提供不精确的去重计数方案，虽然不精确但是也不是非常不精确，标准误差是 0.81%。当数据量很大的时候，就会发现误差。</p><p>HyperLogLog 除了上面的 pfadd 和 pfcount 之外，还提供了第三个指令 pfmerge，用于将多个 pf 计数值累加在一起形成一个新的 pf 值。</p><p>比如在网站中我们有两个内容差不多的页面，运营说需要这两个页面的数据进行合并。其中页面的 UV 访问量也需要合并，那这个时候 pfmerge 就可以派上用场了。</p><h2 id="pf-的内存占用为什么是-12k"><a href="#pf-的内存占用为什么是-12k" class="headerlink" title="pf 的内存占用为什么是 12k"></a>pf 的内存占用为什么是 12k</h2><p>在 Redis 的 HyperLogLog 实现中用到的是 16384 个桶，也就是 2^14，每个桶的 maxbits 需要 6 个 bits 来存储，最大可以表示 maxbits=63，于是总共占用内存就是2^14 * 6 / 8 = 12k字节。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Hyperloglog主要提供不精确的去重计数方案，但会有一定的误差。</p>]]></content>
      
      
      <categories>
          
          <category> 中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis知识</title>
      <link href="/2022/04/23/2022-04-23-redis-03/"/>
      <url>/2022/04/23/2022-04-23-redis-03/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《Redis深度历险》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>在我们平时开发过程中，会有一些 bool 型数据需要存取，比如用户一年的签到记录，签了是 1，没签是 0，要记录 365 天。如果使用普通的 key/value，每个用户要记录 365 个，当用户上亿的时候，需要的存储空间是惊人的。</p><p>为了解决这个问题，Redis 提供了位图数据结构，这样每天的签到记录只占据一个位，365 天就是 365 个位，46 个字节 (一个稍长一点的字符串) 就可以完全容纳下，这就大大节约了存储空间。</p><p><img src="/2022/04/23/2022-04-23-redis-03/1.webp" alt></p><p>位图不是特殊的数据结构，它的内容其实就是普通的字符串，也就是 byte 数组。我们可以使用普通的 get/set 直接获取和设置整个位图的内容，也可以使用位图操作 getbit/setbit 等将 byte 数组看成「位数组」来处理。</p><p>当我们要统计月活的时候，因为需要去重，需要使用 set 来记录所有活跃用户的 id，这非常浪费内存。这时就可以考虑使用位图来标记用户的活跃状态。每个用户会都在这个位图的一个确定位置上，0 表示不活跃，1 表示活跃。然后到月底遍历一次位图就可以得到月度活跃用户数。不过这个方法也是有条件的，那就是 userid 是整数连续的，并且活跃占比较高，否则可能得不偿失。</p><h2 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h2><p>Redis 的位数组是自动扩展，如果设置了某个偏移位置超出了现有的内容范围，就会自动将位数组进行零扩充。</p><p>接下来我们使用位操作将字符串设置为 hello (不是直接使用 set 指令)，首先我们需要得到 hello 的 ASCII 码，用 Python 命令行可以很方便地得到每个字符的 ASCII 码的二进制值。</p><pre><code>&gt;&gt;&gt; bin(ord(&#39;h&#39;))&#39;0b1101000&#39;   # 高位 -&gt; 低位&gt;&gt;&gt; bin(ord(&#39;e&#39;))&#39;0b1100101&#39;&gt;&gt;&gt; bin(ord(&#39;l&#39;))&#39;0b1101100&#39;&gt;&gt;&gt; bin(ord(&#39;l&#39;))&#39;0b1101100&#39;&gt;&gt;&gt; bin(ord(&#39;o&#39;))&#39;0b1101111&#39;</code></pre><p><img src="/2022/04/23/2022-04-23-redis-03/2.webp" alt></p><p>接下来我们使用 redis-cli 设置第一个字符，也就是位数组的前 8 位，我们只需要设置值为 1 的位，如上图所示，h 字符只有 1/2/4 位需要设置，e 字符只有 9/10/13/15 位需要设置。值得注意的是位数组的顺序和字符的位顺序是相反的。</p><pre><code>127.0.0.1:6379&gt; setbit s 1 1(integer) 0127.0.0.1:6379&gt; setbit s 2 1(integer) 0127.0.0.1:6379&gt; setbit s 4 1(integer) 0127.0.0.1:6379&gt; setbit s 9 1(integer) 0127.0.0.1:6379&gt; setbit s 10 1(integer) 0127.0.0.1:6379&gt; setbit s 13 1(integer) 0127.0.0.1:6379&gt; setbit s 15 1(integer) 0127.0.0.1:6379&gt; get s&quot;he&quot;</code></pre><p>上面这个例子可以理解为「零存整取」，同样我们还也可以「零存零取」，「整存零取」。「零存」就是使用 setbit 对位值进行逐个设置，「整存」就是使用字符串一次性填充所有位数组，覆盖掉旧值。</p><h3 id="零存零取"><a href="#零存零取" class="headerlink" title="零存零取"></a>零存零取</h3><pre><code>127.0.0.1:6379&gt; setbit w 1 1(integer) 0127.0.0.1:6379&gt; setbit w 2 1(integer) 0127.0.0.1:6379&gt; setbit w 4 1(integer) 0127.0.0.1:6379&gt; getbit w 1  # 获取某个具体位置的值 0/1(integer) 1127.0.0.1:6379&gt; getbit w 2(integer) 1127.0.0.1:6379&gt; getbit w 4(integer) 1127.0.0.1:6379&gt; getbit w 5(integer) 0</code></pre><h3 id="整存零取"><a href="#整存零取" class="headerlink" title="整存零取"></a>整存零取</h3><pre><code>127.0.0.1:6379&gt; set w h  # 整存(integer) 0127.0.0.1:6379&gt; getbit w 1(integer) 1127.0.0.1:6379&gt; getbit w 2(integer) 1127.0.0.1:6379&gt; getbit w 4(integer) 1127.0.0.1:6379&gt; getbit w 5(integer) 0</code></pre><p>如果对应位的字节是不可打印字符，redis-cli 会显示该字符的 16 进制形式。</p><pre><code>127.0.0.1:6379&gt; setbit x 0 1(integer) 0127.0.0.1:6379&gt; setbit x 1 1(integer) 0127.0.0.1:6379&gt; get x&quot;\xc0&quot;</code></pre><h2 id="统计和查找"><a href="#统计和查找" class="headerlink" title="统计和查找"></a>统计和查找</h2><p>Redis 提供了位图统计指令 bitcount 和位图查找指令 bitpos，bitcount 用来统计指定位置范围内 1 的个数，bitpos 用来查找指定范围内出现的第一个 0 或 1。</p><p>比如我们可以通过 bitcount 统计用户一共签到了多少天，通过 bitpos 指令查找用户从哪一天开始第一次签到。如果指定了范围参数[start, end]，就可以统计在某个时间范围内用户签到了多少天，用户自某天以后的哪天开始签到。</p><p>遗憾的是， start 和 end 参数是字节索引，也就是说指定的位范围必须是 8 的倍数，而不能任意指定。因为这个设计，我们无法直接计算某个月内用户签到了多少天，而必须要将这个月所覆盖的字节内容全部取出来 (getrange 可以取出字符串的子串) 然后在内存里进行统计，这个非常繁琐</p><pre><code>127.0.0.1:6379&gt; set w helloOK127.0.0.1:6379&gt; bitcount w(integer) 21127.0.0.1:6379&gt; bitcount w 0 0  # 第一个字符中 1 的位数(integer) 3127.0.0.1:6379&gt; bitcount w 0 1  # 前两个字符中 1 的位数(integer) 7127.0.0.1:6379&gt; bitpos w 0  # 第一个 0 位(integer) 0127.0.0.1:6379&gt; bitpos w 1  # 第一个 1 位(integer) 1127.0.0.1:6379&gt; bitpos w 1 1 1  # 从第二个字符算起，第一个 1 位(integer) 9</code></pre><h2 id="魔术指令-bitfield"><a href="#魔术指令-bitfield" class="headerlink" title="魔术指令 bitfield"></a>魔术指令 bitfield</h2><p>前文我们设置 (setbit) 和获取 (getbit) 指定位的值都是单个位的，如果要一次操作多个位，就必须使用管道来处理。</p><p>不过 Redis 的 3.2 版本以后新增了一个功能强大的指令，有了这条指令，不用管道也可以一次进行多个位的操作。</p><p>bitfield 有三个子指令，分别是 get/set/incrby，它们都可以对指定位片段进行读写，但是最多只能处理 64 个连续的位，如果超过 64 位，就得使用多个子指令，bitfield 可以一次执行多个子指令。</p><p><img src="/2022/04/23/2022-04-23-redis-03/4.webp" alt></p><p>所谓有符号数是指获取的位数组中第一个位是符号位，剩下的才是值。如果第一位是 1，那就是负数。无符号数表示非负数，没有符号位，获取的位数组全部都是值。有符号数最多可以获取 64 位，无符号数只能获取 63 位 (因为 Redis 协议中的 integer 是有符号数，最大 64 位，不能传递 64 位无符号值)。如果超出位数限制，Redis 就会告诉你参数错误。</p><p>接下来我们一次执行多个子指令:</p><pre><code>127.0.0.1:6379&gt; bitfield w get u4 0 get u3 2 get i4 0 get i3 21) (integer) 62) (integer) 53) (integer) 64) (integer) -3</code></pre><p>然后我们使用 set 子指令将第二个字符 e 改成 a，a 的 ASCII 码是 97，返回旧值。</p><pre><code>127.0.0.1:6379&gt; bitfield w set u8 8 97  # 从第 9 个位开始，将接下来的 8 个位用无符号数 97 替换1) (integer) 101127.0.0.1:6379&gt; get w&quot;hallo&quot;</code></pre><p>再看第三个子指令 incrby，它用来对指定范围的位进行自增操作。既然提到自增，就有可能出现溢出。如果增加了正数，会出现上溢，如果增加的是负数，就会出现下溢出。Redis 默认的处理是折返。如果出现了溢出，就将溢出的符号位丢掉。如果是 8 位无符号数 255，加 1 后就会溢出，会全部变零。如果是 8 位有符号数 127，加 1 后就会溢出变成 -128。</p><pre><code>127.0.0.1:6379&gt; set w helloOK127.0.0.1:6379&gt; bitfield w incrby u4 2 1  # 从第三个位开始，对接下来的 4 位无符号数 +11) (integer) 11127.0.0.1:6379&gt; bitfield w incrby u4 2 11) (integer) 12127.0.0.1:6379&gt; bitfield w incrby u4 2 11) (integer) 13127.0.0.1:6379&gt; bitfield w incrby u4 2 11) (integer) 14127.0.0.1:6379&gt; bitfield w incrby u4 2 11) (integer) 15127.0.0.1:6379&gt; bitfield w incrby u4 2 1  # 溢出折返了1) (integer) 0</code></pre><h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><p>通过位图我们可以：</p><ul><li>为位图指定偏移量上的二进制位设置值，或者获取位图指定偏移量上的二进制位的值。</li><li>统计位图中有多少二进制位被设置成了1。</li><li>查找位图中第一个被设置为指定值的二进制位并返回它的偏移量。</li><li>对一个或者多个位图执行逻辑并、逻辑或、逻辑异或以及逻辑非运算。</li><li>将指定类型的整数存储到位图中</li></ul>]]></content>
      
      
      <categories>
          
          <category> 中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis知识</title>
      <link href="/2022/04/21/2022-04-21-redis-02/"/>
      <url>/2022/04/21/2022-04-21-redis-02/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《Redis深度历险》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="异步消息队列"><a href="#异步消息队列" class="headerlink" title="异步消息队列"></a>异步消息队列</h2><p>Redis 的 list(列表) 数据结构常用来作为异步消息队列使用，使用rpush/lpush操作入队列，使用lpop 和 rpop来出队列。</p><p><img src="/2022/04/21/2022-04-21-redis-02/1.webp" alt></p><pre><code>&gt; rpush notify-queue apple banana pear(integer) 3&gt; llen notify-queue(integer) 3&gt; lpop notify-queue&quot;apple&quot;&gt; llen notify-queue(integer) 2&gt; lpop notify-queue&quot;banana&quot;&gt; llen notify-queue(integer) 1&gt; lpop notify-queue&quot;pear&quot;&gt; llen notify-queue(integer) 0&gt; lpop notify-queue(nil)</code></pre><h2 id="队列空了怎么办"><a href="#队列空了怎么办" class="headerlink" title="队列空了怎么办"></a>队列空了怎么办</h2><p>客户端是通过队列的 pop 操作来获取消息，然后进行处理。处理完了再接着获取消息，再进行处理。如此循环往复，这便是作为队列消费者的客户端的生命周期</p><p>可是如果队列空了，客户端就会陷入 pop 的死循环，不停地 pop，没有数据，接着再 pop，又没有数据。这就是浪费生命的空轮询。空轮询不但拉高了客户端的 CPU，redis 的 QPS 也会被拉高，如果这样空轮询的客户端有几十来个，Redis 的慢查询可能会显著增多。</p><p>通常我们使用 sleep 来解决这个问题，让线程睡一会，睡个 1s 钟就可以了。不但客户端的 CPU 能降下来，Redis 的 QPS 也降下来了。</p><h2 id="队列延迟"><a href="#队列延迟" class="headerlink" title="队列延迟"></a>队列延迟</h2><p>用上面睡眠的办法可以解决问题。但是有个小问题，那就是睡眠会导致消息的延迟增大。如果只有 1 个消费者，那么这个延迟就是 1s。如果有多个消费者，这个延迟会有所下降，因为每个消费者的睡觉时间是岔开来的。</p><p>有没有什么办法能显著降低延迟呢？你当然可以很快想到：那就把睡觉的时间缩短点。这种方式当然可以，不过有没有更好的解决方案呢？当然也有，那就是 blpop/brpop。</p><p>这两个指令的前缀字符b代表的是blocking，也就是阻塞读。</p><p>阻塞读在队列没有数据的时候，会立即进入休眠状态，一旦数据到来，则立刻醒过来。消息的延迟几乎为零。用blpop/brpop替代前面的lpop/rpop，就完美解决了上面的问题。</p><h2 id="空闲连接自动断开"><a href="#空闲连接自动断开" class="headerlink" title="空闲连接自动断开"></a>空闲连接自动断开</h2><p>你以为上面的方案真的很完美么？</p><p>什么问题？—— 空闲连接的问题。</p><p>如果线程一直阻塞在哪里，Redis 的客户端连接就成了闲置连接，闲置过久，服务器一般会主动断开连接，减少闲置资源占用。这个时候blpop/brpop会抛出异常来。</p><p>所以编写客户端消费者的时候要小心，注意捕获异常，还要重试。</p><h2 id="锁冲突处理"><a href="#锁冲突处理" class="headerlink" title="锁冲突处理"></a>锁冲突处理</h2><p>分布式锁有 3 种策略来处理加锁失败：</p><ul><li>直接抛出异常，通知用户稍后重试；</li><li>sleep 一会再重试；</li><li>将请求转移至延时队列，过一会再试；</li></ul><h3 id="直接抛出特定类型的异常"><a href="#直接抛出特定类型的异常" class="headerlink" title="直接抛出特定类型的异常"></a>直接抛出特定类型的异常</h3><p>这种方式比较适合由用户直接发起的请求，用户看到错误对话框后，会先阅读对话框的内容，再点击重试，这样就可以起到人工延时的效果。如果考虑到用户体验，可以由前端的代码替代用户自己来进行延时重试控制。它本质上是对当前请求的放弃，由用户决定是否重新发起新的请求。</p><h3 id="sleep"><a href="#sleep" class="headerlink" title="sleep"></a>sleep</h3><p>sleep 会阻塞当前的消息处理线程，会导致队列的后续消息处理出现延迟。如果碰撞的比较频繁或者队列里消息比较多，sleep 可能并不合适。如果因为个别死锁的 key 导致加锁不成功，线程会彻底堵死，导致后续消息永远得不到及时处理。</p><h3 id="延时队列"><a href="#延时队列" class="headerlink" title="延时队列"></a>延时队列</h3><p>这种方式比较适合异步消息处理，将当前冲突的请求扔到另一个队列延后处理以避开冲突。</p><h2 id="延时队列的实现"><a href="#延时队列的实现" class="headerlink" title="延时队列的实现"></a>延时队列的实现</h2><p>延时队列可以通过 Redis 的 zset(有序列表) 来实现。我们将消息序列化成一个字符串作为 zset 的value，这个消息的到期处理时间作为score，然后用多个线程轮询 zset 获取到期的任务进行处理，多个线程是为了保障可用性，万一挂了一个线程还有其它线程可以继续处理。因为有多个线程，所以需要考虑并发争抢任务，确保任务不能被多次执行。</p><pre><code>import java.lang.reflect.Type;import java.util.Set;import java.util.UUID;import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.TypeReference;import redis.clients.jedis.Jedis;public class RedisDelayingQueue&lt;T&gt; {  static class TaskItem&lt;T&gt; {    public String id;    public T msg;  }  // fastjson 序列化对象中存在 generic 类型时，需要使用 TypeReference  private Type TaskType = new TypeReference&lt;TaskItem&lt;T&gt;&gt;() {  }.getType();  private Jedis jedis;  private String queueKey;  public RedisDelayingQueue(Jedis jedis, String queueKey) {    this.jedis = jedis;    this.queueKey = queueKey;  }  public void delay(T msg) {    TaskItem&lt;T&gt; task = new TaskItem&lt;T&gt;();    task.id = UUID.randomUUID().toString(); // 分配唯一的 uuid    task.msg = msg;    String s = JSON.toJSONString(task); // fastjson 序列化    jedis.zadd(queueKey, System.currentTimeMillis() + 5000, s); // 塞入延时队列 ,5s 后再试  }  public void loop() {    while (!Thread.interrupted()) {      // 只取一条      Set&lt;String&gt; values = jedis.zrangeByScore(queueKey, 0, System.currentTimeMillis(), 0, 1);      if (values.isEmpty()) {        try {          Thread.sleep(500); // 歇会继续        } catch (InterruptedException e) {          break;        }        continue;      }      String s = values.iterator().next();      if (jedis.zrem(queueKey, s) &gt; 0) { // 抢到了        TaskItem&lt;T&gt; task = JSON.parseObject(s, TaskType); // fastjson 反序列化        this.handleMsg(task.msg);      }    }  }  public void handleMsg(T msg) {    System.out.println(msg);  }  public static void main(String[] args) {    Jedis jedis = new Jedis();    RedisDelayingQueue&lt;String&gt; queue = new RedisDelayingQueue&lt;&gt;(jedis, &quot;q-demo&quot;);    Thread producer = new Thread() {      public void run() {        for (int i = 0; i &lt; 10; i++) {          queue.delay(&quot;codehole&quot; + i);        }      }    };    Thread consumer = new Thread() {      public void run() {        queue.loop();      }    };    producer.start();    consumer.start();    try {      producer.join();      Thread.sleep(6000);      consumer.interrupt();      consumer.join();    } catch (InterruptedException e) {    }  }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis知识</title>
      <link href="/2022/04/19/2022-04-19-redis-01/"/>
      <url>/2022/04/19/2022-04-19-redis-01/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《Redis深度历险》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="Redis-安装"><a href="#Redis-安装" class="headerlink" title="Redis 安装"></a>Redis 安装</h2><h3 id="Docker-方式"><a href="#Docker-方式" class="headerlink" title="Docker 方式"></a>Docker 方式</h3><pre><code># 拉取 redis 镜像&gt; docker pull redis# 运行 redis 容器&gt; docker run --name myredis -d -p6379:6379 redis# 执行容器中的 redis-cli，可以直接使用命令行操作 redis&gt; docker exec -it myredis redis-cli</code></pre><h3 id="直接安装方式"><a href="#直接安装方式" class="headerlink" title="直接安装方式"></a>直接安装方式</h3><pre><code># mac&gt; brew install redis# ubuntu&gt; apt-get install redis# redhat&gt; yum install redis# 运行客户端&gt; redis-cli</code></pre><h2 id="string-字符串"><a href="#string-字符串" class="headerlink" title="string (字符串)"></a>string (字符串)</h2><p>Redis 所有的数据结构都是以唯一的 key 字符串作为名称，然后通过这个唯一 key 值来获取相应的 value 数据。</p><p><img src="/2022/04/19/2022-04-19-redis-01/6.awebp.webp" alt></p><h3 id="内部结构"><a href="#内部结构" class="headerlink" title="内部结构"></a>内部结构</h3><p><img src="/2022/04/19/2022-04-19-redis-01/1.awebp.webp" alt></p><p>Redis 的字符串是动态字符串，是可以修改的字符串，内部结构实现上类似于 Java 的 ArrayList，采用预分配冗余空间的方式来减少内存的频繁分配，内部为当前字符串实际分配的空间 capacity 一般要高于实际字符串长度 len。当字符串长度小于 1M 时，扩容都是加倍现有的空间，如果超过 1M，扩容时一次只会多扩 1M 的空间。需要注意的是字符串最大长度为 512M。</p><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><h4 id="单个操作"><a href="#单个操作" class="headerlink" title="单个操作"></a>单个操作</h4><pre><code>&gt; set name codeholeOK&gt; get name&quot;codehole&quot;&gt; exists name(integer) 1&gt; del name(integer) 1&gt; get name(nil)</code></pre><h4 id="批量操作"><a href="#批量操作" class="headerlink" title="批量操作"></a>批量操作</h4><p>可以批量对多个字符串进行读写，节省网络耗时开销。</p><pre><code>&gt; set name1 codeholeOK&gt; set name2 holycoderOK&gt; mget name1 name2 name3 # 返回一个列表1) &quot;codehole&quot;2) &quot;holycoder&quot;3) (nil)&gt; mset name1 boy name2 girl name3 unknown&gt; mget name1 name2 name31) &quot;boy&quot;2) &quot;girl&quot;3) &quot;unknown&quot;</code></pre><h4 id="过期和-set-命令"><a href="#过期和-set-命令" class="headerlink" title="过期和 set 命令"></a>过期和 set 命令</h4><pre><code>&gt; set name codehole&gt; get name&quot;codehole&quot;&gt; expire name 5  # 5s 后过期...  # wait for 5s&gt; get name(nil)&gt; setex name 5 codehole  # 5s 后过期，等价于 set+expire&gt; get name&quot;codehole&quot;... # wait for 5s&gt; get name(nil)&gt; setnx name codehole  # 如果 name 不存在就执行 set 创建(integer) 1&gt; get name&quot;codehole&quot;&gt; setnx name holycoder(integer) 0  # 因为 name 已经存在，所以 set 创建不成功&gt; get name&quot;codehole&quot;  # 没有改变</code></pre><h4 id="计数"><a href="#计数" class="headerlink" title="计数"></a>计数</h4><pre><code>&gt; set age 30OK&gt; incr age(integer) 31&gt; incrby age 5(integer) 36&gt; incrby age -5(integer) 31</code></pre><h2 id="list-列表"><a href="#list-列表" class="headerlink" title="list (列表)"></a>list (列表)</h2><p>Redis 的列表相当于 Java 语言里面的 LinkedList，注意它是链表而不是数组。这意味着 list 的插入和删除操作非常快，时间复杂度为 O(1)，但是索引定位很慢，时间复杂度为 O(n)。</p><p>当列表弹出了最后一个元素之后，该数据结构自动被删除，内存被回收。</p><p><img src="/2022/04/19/2022-04-19-redis-01/9.awebp.webp" alt></p><h3 id="常用命令-1"><a href="#常用命令-1" class="headerlink" title="常用命令"></a>常用命令</h3><h4 id="右边进左边出：队列"><a href="#右边进左边出：队列" class="headerlink" title="右边进左边出：队列"></a>右边进左边出：队列</h4><pre><code>&gt; rpush books python java golang(integer) 3&gt; llen books(integer) 3&gt; lpop books&quot;python&quot;&gt; lpop books&quot;java&quot;&gt; lpop books&quot;golang&quot;&gt; lpop books(nil)</code></pre><h4 id="右边进右边出：栈"><a href="#右边进右边出：栈" class="headerlink" title="右边进右边出：栈"></a>右边进右边出：栈</h4><pre><code>&gt; rpush books python java golang(integer) 3&gt; rpop books&quot;golang&quot;&gt; rpop books&quot;java&quot;&gt; rpop books&quot;python&quot;&gt; rpop books(nil)</code></pre><h4 id="慢操作"><a href="#慢操作" class="headerlink" title="慢操作"></a>慢操作</h4><p>lindex 相当于 Java 链表的get(int index)方法，它需要对链表进行遍历，性能随着参数index增大而变差。</p><p>index 可以为负数，index=-1表示倒数第一个元素，同样index=-2表示倒数第二个元素。</p><p>ltrim 和字面上的含义不太一样，觉得它叫 lretain(保留) 更合适一些，因为 ltrim 跟的两个参数start_index和end_index定义了一个区间，在这个区间内的值，ltrim 要保留，区间之外统统砍掉。我们可以通过ltrim来实现一个定长的链表，这一点非常有用。</p><pre><code>&gt; rpush books python java golang(integer) 3&gt; lindex books 1  # O(n) 慎用&quot;java&quot;&gt; lrange books 0 -1  # 获取所有元素，O(n) 慎用1) &quot;python&quot;2) &quot;java&quot;3) &quot;golang&quot;&gt; ltrim books 1 -1 # O(n) 慎用OK&gt; lrange books 0 -11) &quot;java&quot;2) &quot;golang&quot;&gt; ltrim books 1 0 # 这其实是清空了整个列表，因为区间范围长度为负OK&gt; llen books(integer) 0</code></pre><h3 id="内部结构-1"><a href="#内部结构-1" class="headerlink" title="内部结构"></a>内部结构</h3><p><img src="/2022/04/19/2022-04-19-redis-01/2.awebp.webp" alt></p><p>Redis 底层存储的还不是一个简单的 linkedlist，而是称之为快速链表 quicklist 的一个结构。</p><p>首先在列表元素较少的情况下会使用一块连续的内存存储，这个结构是 ziplist，也即是压缩列表。它将所有的元素紧挨着一起存储，分配的是一块连续的内存。当数据量比较多的时候才会改成 quicklist。因为普通的链表需要的附加指针空间太大，会比较浪费空间，而且会加重内存的碎片化。比如这个列表里存的只是 int 类型的数据，结构上还需要两个额外的指针 prev 和 next 。所以 Redis 将链表和 ziplist 结合起来组成了 quicklist。也就是将多个 ziplist 使用双向指针串起来使用。这样既满足了快速的插入删除性能，又不会出现太大的空间冗余。</p><h2 id="hash-字典"><a href="#hash-字典" class="headerlink" title="hash (字典)"></a>hash (字典)</h2><p>Redis 的字典相当于 Java 语言里面的 HashMap，它是无序字典。</p><p><img src="/2022/04/19/2022-04-19-redis-01/8.awebp.webp" alt></p><p>hash 结构也可以用来存储用户信息，不同于字符串一次性需要全部序列化整个对象，hash 可以对用户结构中的每个字段单独存储。这样当我们需要获取用户信息时可以进行部分获取。而以整个字符串的形式去保存用户信息的话就只能一次性全部读取，这样就会比较浪费网络流量。</p><h3 id="常用命令-2"><a href="#常用命令-2" class="headerlink" title="常用命令"></a>常用命令</h3><pre><code>&gt; hset books java &quot;think in java&quot;  # 命令行的字符串如果包含空格，要用引号括起来(integer) 1&gt; hset books golang &quot;concurrency in go&quot;(integer) 1&gt; hset books python &quot;python cookbook&quot;(integer) 1&gt; hgetall books  # entries()，key 和 value 间隔出现1) &quot;java&quot;2) &quot;think in java&quot;3) &quot;golang&quot;4) &quot;concurrency in go&quot;5) &quot;python&quot;6) &quot;python cookbook&quot;&gt; hlen books(integer) 3&gt; hget books java&quot;think in java&quot;&gt; hset books golang &quot;learning go programming&quot;  # 因为是更新操作，所以返回 0(integer) 0&gt; hget books golang&quot;learning go programming&quot;&gt; hmset books java &quot;effective java&quot; python &quot;learning python&quot; golang &quot;modern golang programming&quot;  # 批量 setOK</code></pre><p>同字符串对象一样，hash 结构中的单个子 key 也可以进行计数，它对应的指令是 hincrby，和 incr 使用基本一样。</p><pre><code>&gt; hincrby user1 age 1(integer) 25</code></pre><h3 id="内部结构-2"><a href="#内部结构-2" class="headerlink" title="内部结构"></a>内部结构</h3><p>内部实现结构上同 Java 的 HashMap 也是一致的，同样的数组 + 链表二维结构。第一维 hash 的数组位置碰撞时，就会将碰撞的元素使用链表串接起来。</p><p><img src="/2022/04/19/2022-04-19-redis-01/5.awebp.webp" alt></p><p>不同的是，Redis 的字典的值只能是字符串，另外它们 rehash 的方式不一样，因为 Java 的 HashMap 在字典很大时，rehash 是个耗时的操作，需要一次性全部 rehash。Redis 为了高性能，不能堵塞服务，所以采用了渐进式 rehash 策略。</p><p><img src="/2022/04/19/2022-04-19-redis-01/4.awebp.webp" alt></p><p>渐进式 rehash 会在 rehash 的同时，保留新旧两个 hash 结构，查询时会同时查询两个 hash 结构，然后在后续的定时任务中以及 hash 操作指令中，循序渐进地将旧 hash 的内容一点点迁移到新的 hash 结构中。当搬迁完成了，就会使用新的hash结构取而代之。</p><h2 id="set-集合"><a href="#set-集合" class="headerlink" title="set (集合)"></a>set (集合)</h2><p>Redis 的集合相当于 Java 语言里面的 HashSet，它内部的键值对是无序的唯一的。它的内部实现相当于一个特殊的字典，字典中所有的 value 都是一个值NULL。</p><p><img src="/2022/04/19/2022-04-19-redis-01/7.awebp.webp" alt></p><h3 id="常用命令-3"><a href="#常用命令-3" class="headerlink" title="常用命令"></a>常用命令</h3><pre><code>&gt; sadd books python(integer) 1&gt; sadd books python  #  重复(integer) 0&gt; sadd books java golang(integer) 2&gt; smembers books  # 注意顺序，和插入的并不一致，因为 set 是无序的1) &quot;java&quot;2) &quot;python&quot;3) &quot;golang&quot;&gt; sismember books java  # 查询某个 value 是否存在，相当于 contains(o)(integer) 1&gt; sismember books rust(integer) 0&gt; scard books  # 获取长度相当于 count()(integer) 3&gt; spop books  # 弹出一个&quot;java&quot;</code></pre><h3 id="内部结构-3"><a href="#内部结构-3" class="headerlink" title="内部结构"></a>内部结构</h3><p>整数集合（IntSet）或字典（dict）。如果集合对象的所有元素都是整数值，并且保存元素小于 512 个时，底层将使用整数集合。否则用字典。</p><h2 id="zset-有序集合"><a href="#zset-有序集合" class="headerlink" title="zset (有序集合)"></a>zset (有序集合)</h2><p>类似于 Java 的 SortedSet 和 HashMap 的结合体，一方面它是一个 set，保证了内部 value 的唯一性，另一方面它可以给每个 value 赋予一个 score，代表这个 value 的排序权重。它的内部实现用的是一种叫做「跳跃列表」的数据结构。</p><h3 id="常用命令-4"><a href="#常用命令-4" class="headerlink" title="常用命令"></a>常用命令</h3><pre><code>&gt; zadd books 9.0 &quot;think in java&quot;(integer) 1&gt; zadd books 8.9 &quot;java concurrency&quot;(integer) 1&gt; zadd books 8.6 &quot;java cookbook&quot;(integer) 1&gt; zrange books 0 -1  # 按 score 排序列出，参数区间为排名范围1) &quot;java cookbook&quot;2) &quot;java concurrency&quot;3) &quot;think in java&quot;&gt; zrevrange books 0 -1  # 按 score 逆序列出，参数区间为排名范围1) &quot;think in java&quot;2) &quot;java concurrency&quot;3) &quot;java cookbook&quot;&gt; zcard books  # 相当于 count()(integer) 3&gt; zscore books &quot;java concurrency&quot;  # 获取指定 value 的 score&quot;8.9000000000000004&quot;  # 内部 score 使用 double 类型进行存储，所以存在小数点精度问题&gt; zrank books &quot;java concurrency&quot;  # 排名(integer) 1&gt; zrangebyscore books 0 8.91  # 根据分值区间遍历 zset1) &quot;java cookbook&quot;2) &quot;java concurrency&quot;&gt; zrangebyscore books -inf 8.91 withscores # 根据分值区间 (-∞, 8.91] 遍历 zset，同时返回分值。inf 代表 infinite，无穷大的意思。1) &quot;java cookbook&quot;2) &quot;8.5999999999999996&quot;3) &quot;java concurrency&quot;4) &quot;8.9000000000000004&quot;&gt; zrem books &quot;java concurrency&quot;  # 删除 value(integer) 1&gt; zrange books 0 -11) &quot;java cookbook&quot;2) &quot;think in java&quot;</code></pre><h3 id="内部结构-4"><a href="#内部结构-4" class="headerlink" title="内部结构"></a>内部结构</h3><p>zset 内部的排序功能是通过「跳跃列表」数据结构来实现的，它的结构非常特殊，也比较复杂。</p><p>请百度跳跃表</p><h2 id="容器型数据结构的通用规则"><a href="#容器型数据结构的通用规则" class="headerlink" title="容器型数据结构的通用规则"></a>容器型数据结构的通用规则</h2><p>list/set/hash/zset 这四种数据结构是容器型数据结构，它们共享下面两条通用规则：</p><h3 id="create-if-not-exists"><a href="#create-if-not-exists" class="headerlink" title="create if not exists"></a>create if not exists</h3><p>如果容器不存在，那就创建一个，再进行操作。比如 rpush 操作刚开始是没有列表的，Redis 就会自动创建一个，然后再 rpush 进去新元素。</p><h3 id="drop-if-no-elements"><a href="#drop-if-no-elements" class="headerlink" title="drop if no elements"></a>drop if no elements</h3><p>如果容器里元素没有了，那么立即删除元素，释放内存。这意味着 lpop 操作到最后一个元素，列表就消失了。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="string-类型"><a href="#string-类型" class="headerlink" title="string 类型"></a>string 类型</h3><p>常规 KV 结构，其中 V 可以是 string 或 number 类型。</p><p>容量：string 的一个 key 最大能存 512MB。</p><p>底层：动态字符串（SDS）。</p><p>应用场景：</p><ul><li>简单值缓存：token，验证码。</li><li>计数器：登录错误次数计数，产品库存。</li></ul><h3 id="hash-类型"><a href="#hash-类型" class="headerlink" title="hash 类型"></a>hash 类型</h3><p>hash 是一个的键值对集合，其中 V 相当于一个 HashMap，适用于存储对象数据。</p><p>容量：hash 的一个 key 最大可以存储 2^32 -1 个键值对（40 多亿）。</p><p>底层：字典（dict）。</p><p>应用场景：对象信息，如用户信息，产品信息。</p><h3 id="list-类型"><a href="#list-类型" class="headerlink" title="list 类型"></a>list 类型</h3><p>list 是有序列表，它可以压入头部和尾部，也可以弹出头部和尾部。</p><p>容量：list最多可存储 2^32 - 1 个元素元素（40 多亿）。</p><p>底层：快速列表（QuickList）。如果列表元素较少，则会使用一块连续的内存空间，即压缩列表。如果列表元素较多，则会使用快速列表，其原理就是双向链表+压缩列表。</p><p>应用场景：微博的关注列表，粉丝列表，消息列表</p><h3 id="set-类型"><a href="#set-类型" class="headerlink" title="set 类型"></a>set 类型</h3><p>set 是无序集合，其中 value 相当于 HashSet。适用于数据去重、排重、交集，并集，差集等场景。</p><p>容量：集合中最多可以存储 2^32 - 1 个元素元素（40 多亿）。</p><p>底层：整数集合（IntSet）或字典（dict）。如果集合对象的所有元素都是整数值，并且保存元素小于 512 个时，底层将使用整数集合。否则用字典。</p><p>应用场景：共同爱好等。</p><h3 id="zset-类型"><a href="#zset-类型" class="headerlink" title="zset 类型"></a>zset 类型</h3><p>zset 是排序集合，其中 value 相当于 TreeSet。适用场景和 set 相似，但数据是有序的。增加了一个权重参数score，使得集合中的元素能够按score进行有序排列。</p><p>容量：集合中最多可以存储 2^32 - 1 个元素元素（40 多亿）。</p><p>底层：压缩列表或字典+跳跃表。如果元素少于 128 个，且每个元素长度小于 64 字节，使用压缩列表。否则用字典+跳跃表。</p><p>应用场景：金额最多的前几位。</p>]]></content>
      
      
      <categories>
          
          <category> 中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式理论</title>
      <link href="/2021/11/15/2021-11-15-fen-bu-shi-li-lun/"/>
      <url>/2021/11/15/2021-11-15-fen-bu-shi-li-lun/</url>
      
        <content type="html"><![CDATA[<h2 id="CAP理论"><a href="#CAP理论" class="headerlink" title="CAP理论"></a>CAP理论</h2><p>CAP 也就是 Consistency（一致性）、Availability（可用性）、Partition Tolerance（分区容错性） 这三个单词首字母组合。</p><p><img src="/2021/11/15/2021-11-15-fen-bu-shi-li-lun/cap.png" alt></p><p>CAP 定理（CAP theorem）指出对于一个分布式系统来说，当设计读写操作时，只能同时满足以下三点中的两个：</p><ul><li>一致性（Consistency） : 所有节点访问同一份最新的数据副本</li><li>可用性（Availability）: 非故障的节点在合理的时间内返回合理的响应（不是错误或者超时的响应）。</li><li>分区容错性（Partition tolerance） : 分布式系统出现网络分区的时候，仍然能够对外提供服务。</li></ul><p>因此，分布式系统理论上不可能选择 CA 架构，只能选择 CP 或者 AP 架构。</p><p>比如 ZooKeeper、HBase 就是 CP 架构，Cassandra、Eureka 就是 AP 架构，Nacos 不仅支持 CP 架构也支持 AP 架构。</p><ul><li>ZooKeeper 保证的是 CP。 任何时刻对 ZooKeeper 的读请求都能得到一致性的结果，但是， ZooKeeper 不保证每次请求的可用性比如在 Leader 选举过程中或者半数以上的机器不可用的时候服务就是不可用的。 </li><li>Eureka 保证的则是 AP。 Eureka 在设计的时候就是优先保证 A （可用性）。在 Eureka 中不存在什么 Leader 节点，每个节点都是一样的、平等的。因此 Eureka 不会像 ZooKeeper 那样出现选举过程中或者半数以上的机器不可用的时候服务就是不可用的情况。 Eureka 保证即使大部分节点挂掉也不会影响正常提供服务，只要有一个节点是可用的就行了。只不过这个节点上的数据可能并不是最新的。 </li><li>Nacos 不仅支持 CP 也支持 AP。 # 总结</li></ul><p>选择 CP 还是 AP 的关键在于当前的业务场景，没有定论，比如对于需要确保强一致性的场景如银行一般会选择保证 CP 。</p><p>如果网络分区正常的话（系统在绝大部分时候所处的状态），也就说不需要保证 P 的时候，C 和 A 能够同时保证。</p><h3 id="什么是网络分区？"><a href="#什么是网络分区？" class="headerlink" title="什么是网络分区？"></a>什么是网络分区？</h3><p>分布式系统中，多个节点之前的网络本来是连通的，但是因为某些故障（比如部分节点网络出了问题）某些节点之间不连通了，整个网络就分成了几块区域，这就叫网络分区。</p><h2 id="BASE理论"><a href="#BASE理论" class="headerlink" title="BASE理论"></a>BASE理论</h2><p>BASE 是 Basically Available（基本可用） 、Soft-state（软状态） 和 Eventually Consistent（最终一致性） 三个短语的缩写。</p><h3 id="BASE-理论的核心思想"><a href="#BASE-理论的核心思想" class="headerlink" title="BASE 理论的核心思想"></a>BASE 理论的核心思想</h3><p>即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。</p><blockquote><p>也就是牺牲数据的一致性来满足系统的高可用性，系统中一部分数据不可用或者不一致时，仍需要保持系统整体“主要可用”。</p></blockquote><p>BASE 理论本质上是对 CAP 的延伸和补充，更具体地说，是对 CAP 中 AP 方案的一个补充。</p><p>AP 方案只是在系统发生分区的时候放弃一致性，而不是永远放弃一致性。在分区故障恢复后，系统应该达到最终一致性。这一点其实就是 BASE 理论延伸的地方。</p><h3 id="BASE-理论三要素"><a href="#BASE-理论三要素" class="headerlink" title="BASE 理论三要素"></a>BASE 理论三要素</h3><p><img src="/2021/11/15/2021-11-15-fen-bu-shi-li-lun/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOC81LzI0LzE2MzkxNDgwNmQ5ZTE1YzY.png" alt></p><h4 id="基本可用"><a href="#基本可用" class="headerlink" title="基本可用"></a>基本可用</h4><p>基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这绝不等价于系统不可用。</p><p>什么叫允许损失部分可用性呢？</p><ul><li>响应时间上的损失: 正常情况下，处理用户请求需要 0.5s 返回结果，但是由于系统出现故障，处理用户请求的时间变为 3 s。 </li><li>系统功能上的损失：正常情况下，用户可以使用系统的全部功能，但是由于系统访问量突然剧增，系统的部分非核心功能无法使用。</li></ul><h4 id="软状态"><a href="#软状态" class="headerlink" title="软状态"></a>软状态</h4><p>软状态指允许系统中的数据存在中间状态（CAP 理论中的数据不一致），并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。</p><h4 id="最终一致性"><a href="#最终一致性" class="headerlink" title="最终一致性"></a>最终一致性</h4><p>最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>ACID 是数据库事务完整性的理论，CAP 是分布式系统设计理论，BASE 是 CAP 理论中 AP 方案的延伸。</p>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Executor框架</title>
      <link href="/2021/11/13/2021-11-13-executor-kuang-jia/"/>
      <url>/2021/11/13/2021-11-13-executor-kuang-jia/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《Java并发编程的艺术》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="Executor框架简介"><a href="#Executor框架简介" class="headerlink" title="Executor框架简介"></a>Executor框架简介</h2><h3 id="Executor框架的两级调度模型"><a href="#Executor框架的两级调度模型" class="headerlink" title="Executor框架的两级调度模型"></a>Executor框架的两级调度模型</h3><p>在HotSpot VM的线程模型中，Java线程（java.lang.Thread）被一对一映射为本地操作系统线程。Java线程启动时会创建一个本地操作系统线程；当该Java线程终止时，这个操作系统线程也会被回收。操作系统会调度所有线程并将它们分配给可用的CPU。</p><p>在上层，Java多线程程序通常把应用分解为若干个任务，然后使用用户级的调度器（Executor框架）将这些任务映射为固定数量的线程；在底层，操作系统内核将这些线程映射到硬件处理器上。</p><p><img src="/2021/11/13/2021-11-13-executor-kuang-jia/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211118092903.png" alt></p><h3 id="Executor框架的结构"><a href="#Executor框架的结构" class="headerlink" title="Executor框架的结构"></a>Executor框架的结构</h3><p>Executor框架主要由3大部分组成如下</p><ul><li>任务:包括被执行任务需要实现的接口：Runnable接口或Callable接口。</li><li>任务的执行:包括任务执行机制的核心接口Executor，以及继承自Executor的ExecutorService接口。Executor框架有两个关键类实现了ExecutorService接口（ThreadPoolExecutor和ScheduledThreadPoolExecutor）。</li><li>异步计算的结果。包括接口Future和实现Future接口的FutureTask类。</li></ul><p>Executor是一个接口，它是Executor框架的基础，它将任务的提交与任务的执行分离开来。</p><p>ThreadPoolExecutor是线程池的核心实现类，用来执行被提交的任务。</p><p>ScheduledThreadPoolExecutor是一个实现类，可以在给定的延迟后运行命令，或者定期执行命令。</p><p>ScheduledThreadPoolExecutor比Timer更灵活，功能更强大。</p><p>Future接口和实现Future接口的FutureTask类，代表异步计算的结果。</p><p>Runnable接口和Callable接口的实现类，都可以被ThreadPoolExecutor或Scheduled-ThreadPoolExecutor执行。</p><p><img src="/2021/11/13/2021-11-13-executor-kuang-jia/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211118103721.png" alt></p><p><img src="/2021/11/13/2021-11-13-executor-kuang-jia/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211118103754.png" alt></p><h3 id="Executor框架的使用示意图"><a href="#Executor框架的使用示意图" class="headerlink" title="Executor框架的使用示意图"></a>Executor框架的使用示意图</h3><p>主线程首先要创建实现Runnable或者Callable接口的任务对象。工具类Executors可以把一个Runnable对象封装为一个Callable对象（Executors.callable（Runnable task）或Executors.callable（Runnable task，Object resule））。</p><p>然后可以把Runnable对象直接交给ExecutorService执行（ExecutorService.execute（Runnable command））；或者也可以把Runnable对象或Callable对象提交给ExecutorService执行（Executor-Service.submit（Runnable task）或ExecutorService.submit（Callable<t>task））。</t></p><p>如果执行ExecutorService.submit（…），ExecutorService将返回一个实现Future接口的对象（到目前为止的JDK中，返回的是FutureTask对象）。由于FutureTask实现了Runnable，程序员也可以创建FutureTask，然后直接交给ExecutorService执行。</p><p>最后，主线程可以执行FutureTask.get()方法来等待任务执行完成。主线程也可以执行FutureTask.cancel（boolean mayInterruptIfRunning）来取消此任务的执行。</p><h3 id="Executor框架的成员"><a href="#Executor框架的成员" class="headerlink" title="Executor框架的成员"></a>Executor框架的成员</h3><p>Executor框架的主要成员：ThreadPoolExecutor、ScheduledThreadPoolExecutor、Future接口、Runnable接口、Callable接口和Executors。</p><h4 id="ThreadPoolExecutor"><a href="#ThreadPoolExecutor" class="headerlink" title="ThreadPoolExecutor"></a>ThreadPoolExecutor</h4><p>ThreadPoolExecutor通常使用工厂类Executors来创建。Executors可以创建3种类型的ThreadPoolExecutor：SingleThreadExecutor、FixedThreadPool和CachedThreadPool。</p><ul><li>FixedThreadPool。创建使用固定线程数的FixedThreadPool的API。</li></ul><pre><code>public static ExecutorService newFixedThreadPool(int nThreads)public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactorythreadFactory)</code></pre><p>FixedThreadPool适用于为了满足资源管理的需求，而需要限制当前线程数量的应用场景，它适用于负载比较重的服务器。</p><ul><li>SingleThreadExecutor。创建使用单个线程的SingleThread-Executor的API。</li></ul><pre><code>public static ExecutorService newSingleThreadExecutor()public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory)</code></pre><p>SingleThreadExecutor适用于需要保证顺序地执行各个任务；并且在任意时间点，不会有多个线程是活动的应用场景。</p><ul><li>CachedThreadPool。创建一个会根据需要创建新线程的CachedThreadPool的API。</li></ul><pre><code>public static ExecutorService newCachedThreadPool()public static ExecutorService newCachedThreadPool(ThreadFactory threadFactory)</code></pre><p>CachedThreadPool是大小无界的线程池，适用于执行很多的短期异步任务的小程序，或者是负载较轻的服务器。</p><h4 id="ScheduledThreadPoolExecutor"><a href="#ScheduledThreadPoolExecutor" class="headerlink" title="ScheduledThreadPoolExecutor"></a>ScheduledThreadPoolExecutor</h4><p>ScheduledThreadPoolExecutor通常使用工厂类Executors来创建。Executors可以创建2种类型的ScheduledThreadPoolExecutor，如下。</p><ul><li>ScheduledThreadPoolExecutor。包含若干个线程的ScheduledThreadPoolExecutor。</li><li>SingleThreadScheduledExecutor。只包含一个线程的ScheduledThreadPoolExecutor。</li></ul><p>下面是工厂类Executors提供的，创建固定个数线程的ScheduledThreadPoolExecutor的API。</p><pre><code>public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize)public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize,ThreadFactory threadFactory)</code></pre><p>ScheduledThreadPoolExecutor适用于需要多个后台线程执行周期任务，同时为了满足资源管理的需求而需要限制后台线程的数量的应用场景。</p><p>下面是Executors提供的，创建单个线程的SingleThreadScheduledExecutor的API。</p><pre><code>public static ScheduledExecutorService newSingleThreadScheduledExecutor()public static ScheduledExecutorService newSingleThreadScheduledExecutor(ThreadFactory threadFactory)</code></pre><p>SingleThreadScheduledExecutor适用于需要单个后台线程执行周期任务，同时需要保证顺序地执行各个任务的应用场景。</p><h4 id="Future接口"><a href="#Future接口" class="headerlink" title="Future接口"></a>Future接口</h4><p>Future接口和实现Future接口的FutureTask类用来表示异步计算的结果。当我们把Runnable接口或Callable接口的实现类提交（submit）给ThreadPoolExecutor或ScheduledThreadPoolExecutor时，ThreadPoolExecutor或ScheduledThreadPoolExecutor会向我们返回一个FutureTask对象。下面是对应的API。</p><pre><code>&lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task)&lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result)Future&lt;&gt; submit(Runnable task)</code></pre><p>到目前最新的JDK 8为止，Java通过上述API返回的是一个FutureTask对象。但从API可以看到，Java仅仅保证返回的是一个实现了Future接口的对象。在将来的JDK实现中，返回的可能不一定是FutureTask。</p><h4 id="Runnable接口和Callable接口"><a href="#Runnable接口和Callable接口" class="headerlink" title="Runnable接口和Callable接口"></a>Runnable接口和Callable接口</h4><p>Runnable接口和Callable接口的实现类，都可以被ThreadPoolExecutor或Scheduled-ThreadPoolExecutor执行。它们之间的区别是Runnable不会返回结果，而Callable可以返回结果。</p><h3 id="ThreadPoolExecutor详解"><a href="#ThreadPoolExecutor详解" class="headerlink" title="ThreadPoolExecutor详解"></a>ThreadPoolExecutor详解</h3><p>Executor框架最核心的类是ThreadPoolExecutor，它是线程池的实现类，主要由下列4个组件构成。</p><ul><li>corePool：核心线程池的大小。</li><li>maximumPool：最大线程池的大小。</li><li>BlockingQueue：用来暂时保存任务的工作队列。</li><li>RejectedExecutionHandler：当ThreadPoolExecutor已经关闭或ThreadPoolExecutor已经饱和时（达到了最大线程池大小且工作队列已满），execute()方法将要调用的Handler。</li></ul><p>通过Executor框架的工具类Executors，可以创建3种类型的ThreadPoolExecutor。</p><ul><li>FixedThreadPool。</li><li>SingleThreadExecutor。</li><li>CachedThreadPool。</li></ul><h4 id="FixedThreadPool详解"><a href="#FixedThreadPool详解" class="headerlink" title="FixedThreadPool详解"></a>FixedThreadPool详解</h4><p>FixedThreadPool被称为可重用固定线程数的线程池。下面是FixedThreadPool的源代码实现。</p><pre><code>public static ExecutorService newFixedThreadPool(int nThreads) {    return new ThreadPoolExecutor(nThreads, nThreads,                                  0L, TimeUnit.MILLISECONDS,                                  new LinkedBlockingQueue&lt;Runnable&gt;());}</code></pre><p>FixedThreadPool的corePoolSize和maximumPoolSize都被设置为创建FixedThreadPool时指定的参数nThreads。</p><p>当线程池中的线程数大于corePoolSize时，keepAliveTime为多余的空闲线程等待新任务的最长时间，超过这个时间后多余的线程将被终止。这里把keepAliveTime设置为0L，意味着多余的空闲线程会被立即终止。</p><p><img src="/2021/11/13/2021-11-13-executor-kuang-jia/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211118110008.png" alt></p><ol><li>如果当前运行的线程数少于corePoolSize，则创建新线程来执行任务。</li><li>在线程池完成预热之后（当前运行的线程数等于corePoolSize），将任务加入LinkedBlockingQueue。</li><li>线程执行完1中的任务后，会在循环中反复从LinkedBlockingQueue获取任务来执行。</li></ol><p>FixedThreadPool使用无界队列LinkedBlockingQueue作为线程池的工作队列（队列的容量为Integer.MAX_VALUE）。使用无界队列作为工作队列会对线程池带来如下影响。</p><ol><li>当线程池中的线程数达到corePoolSize后，新任务将在无界队列中等待，因此线程池中的线程数不会超过corePoolSize。</li><li>由于1，使用无界队列时maximumPoolSize将是一个无效参数。</li><li>由于1和2，使用无界队列时keepAliveTime将是一个无效参数。</li><li>由于使用无界队列，运行中的FixedThreadPool（未执行方法shutdown()或shutdownNow()）不会拒绝任务（不会调用RejectedExecutionHandler.rejectedExecution方法）。</li></ol><h4 id="SingleThreadExecutor详解"><a href="#SingleThreadExecutor详解" class="headerlink" title="SingleThreadExecutor详解"></a>SingleThreadExecutor详解</h4><p>SingleThreadExecutor是使用单个worker线程的Executor。下面是SingleThreadExecutor的源代码实现。</p><pre><code>public static ExecutorService newSingleThreadExecutor() {    return new FinalizableDelegatedExecutorService        (new ThreadPoolExecutor(1, 1,                                0L, TimeUnit.MILLISECONDS,                                new LinkedBlockingQueue&lt;Runnable&gt;()));}</code></pre><p>SingleThreadExecutor的corePoolSize和maximumPoolSize被设置为1。其他参数与FixedThreadPool相同。SingleThreadExecutor使用无界队列LinkedBlockingQueue作为线程池的工作队列（队列的容量为Integer.MAX_VALUE）。</p><p><img src="/2021/11/13/2021-11-13-executor-kuang-jia/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211118111300.png" alt></p><ol><li>如果当前运行的线程数少于corePoolSize（即线程池中无运行的线程），则创建一个新线程来执行任务。</li><li>在线程池完成预热之后（当前线程池中有一个运行的线程），将任务加入Linked-BlockingQueue。</li><li>线程执行完1中的任务后，会在一个无限循环中反复从LinkedBlockingQueue获取任务来执行。</li></ol><h4 id="CachedThreadPool详解"><a href="#CachedThreadPool详解" class="headerlink" title="CachedThreadPool详解"></a>CachedThreadPool详解</h4><p>CachedThreadPool是一个会根据需要创建新线程的线程池。下面是创建CachedThread-Pool的源代码。</p><pre><code>public static ExecutorService newCachedThreadPool() {    return new ThreadPoolExecutor(0, Integer.MAX_VALUE,                                  60L, TimeUnit.SECONDS,                                  new SynchronousQueue&lt;Runnable&gt;());}</code></pre><p>CachedThreadPool的corePoolSize被设置为0，即corePool为空；maximumPoolSize被设置为Integer.MAX_VALUE，即maximumPool是无界的。这里把keepAliveTime设置为60L，意味着CachedThreadPool中的空闲线程等待新任务的最长时间为60秒，空闲线程超过60秒后将会被终止。</p><p>FixedThreadPool和SingleThreadExecutor使用无界队列LinkedBlockingQueue作为线程池的工作队列。CachedThreadPool使用没有容量的SynchronousQueue作为线程池的工作队列，但CachedThreadPool的maximumPool是无界的。这意味着，如果主<br>线程提交任务的速度高于maximumPool中线程处理任务的速度时，CachedThreadPool会不断创建新线程。极端情况下，CachedThreadPool会因为创建过多线程而耗尽CPU和内存资源。</p><p><img src="/2021/11/13/2021-11-13-executor-kuang-jia/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211118111546.png" alt></p><ol><li>首先执行SynchronousQueue.offer（Runnable task）。如果当前maximumPool中有空闲线程正在执行SynchronousQueue.poll（keepAliveTime，TimeUnit.NANOSECONDS），那么主线程执行offer操作与空闲线程执行的poll操作配对成功，主线程把任务交给空闲线程执行，execute()方法执行完成；否则执行下面的步骤2）。</li><li>当初始maximumPool为空，或者maximumPool中当前没有空闲线程时，将没有线程执行SynchronousQueue.poll（keepAliveTime，TimeUnit.NANOSECONDS）。这种情况下，步骤1）将失败。此时CachedThreadPool会创建一个新线程执行任务，execute()方法执行完成。</li><li>在步骤2）中新创建的线程将任务执行完后，会执行SynchronousQueue.poll（keepAliveTime，TimeUnit.NANOSECONDS）。这个poll操作会让空闲线程最多在SynchronousQueue中等待60秒钟。如果60秒钟内主线程提交了一个新任务（主线程执行步骤1）），那么这个空闲线程将执行主线程提交的新任务；否则，这个空闲线程将终止。由于空闲60秒的空闲线程会被终止，因此长时间保持空闲的CachedThreadPool不会使用任何资源。</li></ol><p>SynchronousQueue是一个没有容量的阻塞队列。每个插入操作必须等待另一个线程的对应移除操作，反之亦然。CachedThreadPool使用SynchronousQueue，把主线程提交的任务传递给空闲线程执行。</p><p><img src="/2021/11/13/2021-11-13-executor-kuang-jia/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211118111820.png" alt></p><h3 id="ScheduledThreadPoolExecutor详解"><a href="#ScheduledThreadPoolExecutor详解" class="headerlink" title="ScheduledThreadPoolExecutor详解"></a>ScheduledThreadPoolExecutor详解</h3><p>ScheduledThreadPoolExecutor继承自ThreadPoolExecutor。它主要用来在给定的延迟之后运行任务，或者定期执行任务。</p><h4 id="ScheduledThreadPoolExecutor的运行机制"><a href="#ScheduledThreadPoolExecutor的运行机制" class="headerlink" title="ScheduledThreadPoolExecutor的运行机制"></a>ScheduledThreadPoolExecutor的运行机制</h4><p>DelayQueue是一个无界队列，所以ThreadPoolExecutor的maximumPoolSize在Scheduled-ThreadPoolExecutor中没有什么意义（设置maximumPoolSize的大小没有什么效果）。</p><p>ScheduledThreadPoolExecutor的执行主要分为两大部分。</p><ul><li>当调用ScheduledThreadPoolExecutor的scheduleAtFixedRate()方法或者scheduleWith-FixedDelay()方法时，会向ScheduledThreadPoolExecutor的DelayQueue添加一个实现了RunnableScheduledFutur接口的ScheduledFutureTask。</li><li>线程池中的线程从DelayQueue中获取ScheduledFutureTask，然后执行任务。</li></ul><p><img src="/2021/11/13/2021-11-13-executor-kuang-jia/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211118112105.png" alt></p><h4 id="ScheduledThreadPoolExecutor的实现"><a href="#ScheduledThreadPoolExecutor的实现" class="headerlink" title="ScheduledThreadPoolExecutor的实现"></a>ScheduledThreadPoolExecutor的实现</h4><p>ScheduledThreadPoolExecutor会把待调度的任务（ScheduledFutureTask）放到一个DelayQueue中。<br>ScheduledFutureTask主要包含3个成员变量，如下。</p><ul><li>long型成员变量time，表示这个任务将要被执行的具体时间。</li><li>long型成员变量sequenceNumber，表示这个任务被添加到ScheduledThreadPoolExecutor中的序号。</li><li>long型成员变量period，表示任务执行的间隔周期。</li></ul><p>DelayQueue封装了一个PriorityQueue，这个PriorityQueue会对队列中的Scheduled-FutureTask进行排序。排序时，time小的排在前面（时间早的任务将被先执行）。如果两个ScheduledFutureTask的time相同，就比较sequenceNumber，sequenceNumber小的排在前面（也就是说，如果两个任务的执行时间相同，那么先提交的任务将被先执行）。</p><p><img src="/2021/11/13/2021-11-13-executor-kuang-jia/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211118112811.png" alt></p><ol><li>线程1从DelayQueue中获取已到期的ScheduledFutureTask（DelayQueue.take()）。到期任务是指ScheduledFutureTask的time大于等于当前时间。</li><li>线程1执行这个ScheduledFutureTask。</li><li>线程1修改ScheduledFutureTask的time变量为下次将要被执行的时间。</li><li>线程1把这个修改time之后的ScheduledFutureTask放回DelayQueue中（Delay-Queue.add()）。</li></ol><h3 id="FutureTask详解"><a href="#FutureTask详解" class="headerlink" title="FutureTask详解"></a>FutureTask详解</h3><p>Future接口和实现Future接口的FutureTask类，代表异步计算的结果。</p><h4 id="FutureTask简介"><a href="#FutureTask简介" class="headerlink" title="FutureTask简介"></a>FutureTask简介</h4><p>FutureTask除了实现Future接口外，还实现了Runnable接口。因此，FutureTask可以交给Executor执行，也可以由调用线程直接执行（FutureTask.run()）。根据FutureTask.run()方法被执行的时机，FutureTask可以处于下面3种状态。</p><ul><li>未启动。FutureTask.run()方法还没有被执行之前，FutureTask处于未启动状态。当创建一个FutureTask，且没有执行FutureTask.run()方法之前，这个FutureTask处于未启动状态。</li><li>已启动。FutureTask.run()方法被执行的过程中，FutureTask处于已启动状态。</li><li>已完成。FutureTask.run()方法执行完后正常结束，或被取消（FutureTask.cancel（…）），或执行FutureTask.run()方法时抛出异常而异常结束，FutureTask处于已完成状态。</li></ul><p><img src="/2021/11/13/2021-11-13-executor-kuang-jia/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211118114632.png" alt></p><p>当FutureTask处于未启动或已启动状态时，执行FutureTask.get()方法将导致调用线程阻塞；当FutureTask处于已完成状态时，执行FutureTask.get()方法将导致调用线程立即返回结果或抛出异常。</p><p>当FutureTask处于未启动状态时，执行FutureTask.cancel()方法将导致此任务永远不会被执行；当FutureTask处于已启动状态时，执行FutureTask.cancel（true）方法将以中断执行此任务线程的方式来试图停止任务；当FutureTask处于已启动状态时，执行FutureTask.cancel（false）方法将不会对正在执行此任务的线程产生影响（让正在执行的任务运行完成）；当FutureTask处于已完成状态时，执行FutureTask.cancel（…）方法将返回false。</p><p><img src="/2021/11/13/2021-11-13-executor-kuang-jia/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211118114724.png" alt></p><h4 id="FutureTask的使用"><a href="#FutureTask的使用" class="headerlink" title="FutureTask的使用"></a>FutureTask的使用</h4><p>可以把FutureTask交给Executor执行；也可以通过ExecutorService.submit（…）方法返回一个FutureTask，然后执行FutureTask.get()方法或FutureTask.cancel（…）方法。除此以外，还可以单独使用FutureTask。</p><p>当一个线程需要等待另一个线程把某个任务执行完后它才能继续执行，此时可以使用FutureTask。假设有多个线程执行若干任务，每个任务最多只能被执行一次。当多个线程试图同时执行同一个任务时，只允许一个线程执行任务，其他线程需要等待这个任务执行完后才能继续执行。下面是对应的示例代码。</p><h4 id="FutureTask的实现"><a href="#FutureTask的实现" class="headerlink" title="FutureTask的实现"></a>FutureTask的实现</h4><p>FutureTask的实现基于AbstractQueuedSynchronizer（以下简称为AQS）。java.util.concurrent中的很多可阻塞类（比如ReentrantLock）都是基于AQS来实现的。AQS是一个同步框架，它提供通用机制来原子性管理同步状态、阻塞和唤醒线程，以及维护被阻塞线程的队列。JDK 6中AQS被广泛使用，基于AQS实现的同步器包括：ReentrantLock、Semaphore、ReentrantReadWriteLock、CountDownLatch和FutureTask。</p><p>至少一个acquire操作。这个操作阻塞调用线程，除非/直到AQS的状态允许这个线程继续执行。FutureTask的acquire操作为get()/get（long timeout，TimeUnit unit）方法调用。</p><p>至少一个release操作。这个操作改变AQS的状态，改变后的状态可允许一个或多个阻塞线程被解除阻塞。FutureTask的release操作包括run()方法和cancel（…）方法。</p><p>基于“复合优先于继承”的原则，FutureTask声明了一个内部私有的继承于AQS的子类Sync，对FutureTask所有公有方法的调用都会委托给这个内部子类。</p><p>AQS被作为“模板方法模式”的基础类提供给FutureTask的内部子类Sync，这个内部子类只需要实现状态检查和状态更新的方法即可，这些方法将控制FutureTask的获取和释放操作。具体来说，Sync实现了AQS的tryAcquireShared（int）方法和tryReleaseShared（int）方法，Sync通过这两个方法来检查和更新同步状态。</p><p>Sync是FutureTask的内部私有类，它继承自AQS。创建FutureTask时会创建内部私有的成员对象Sync，FutureTask所有的的公有方法都直接委托给了内部私有的Sync。</p><p>FutureTask.get()方法会调用AQS.acquireSharedInterruptibly（int arg）方法，这个方法的执行过程如下。</p><ol><li>调用AQS.acquireSharedInterruptibly（int arg）方法，这个方法首先会回调在子类Sync中实现的tryAcquireShared()方法来判断acquire操作是否可以成功。acquire操作可以成功的条件为：state为执行完成状态RAN或已取消状态CANCELLED，且runner不为null。</li><li>如果成功则get()方法立即返回。如果失败则到线程等待队列中去等待其他线程执行release操作。</li><li>当其他线程执行release操作（比如FutureTask.run()或FutureTask.cancel（…））唤醒当前线程后，当前线程再次执行tryAcquireShared()将返回正值1，当前线程将离开线程等待队列并唤醒它的后继线程（这里会产生级联唤醒的效果，后面会介绍）。</li><li>最后返回计算的结果或抛出异常。</li></ol><p>FutureTask.run()的执行过程如下。</p><ol><li>执行在构造函数中指定的任务（Callable.call()）。</li><li>以原子方式来更新同步状态（调用AQS.compareAndSetState（int expect，int update），设置state为执行完成状态RAN）。如果这个原子操作成功，就设置代表计算结果的变量result的值为Callable.call()的返回值，然后调用AQS.releaseShared（int arg）。</li><li>AQS.releaseShared（int arg）首先会回调在子类Sync中实现的tryReleaseShared（arg）来执行release操作（设置运行任务的线程runner为null，然会返回true）；AQS.releaseShared（int arg），然后唤醒线程等待队列中的第一个线程。</li><li>调用FutureTask.done()。</li></ol>]]></content>
      
      
      <categories>
          
          <category> JAVA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 并发编程的艺术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java中的线程池</title>
      <link href="/2021/11/11/2021-11-11-java-zhong-de-xian-cheng-chi/"/>
      <url>/2021/11/11/2021-11-11-java-zhong-de-xian-cheng-chi/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《Java并发编程的艺术》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="线程池的好处"><a href="#线程池的好处" class="headerlink" title="线程池的好处"></a>线程池的好处</h2><p>Java中的线程池是运用场景最多的并发框架，几乎所有需要异步或并发执行任务的程序都可以使用线程池。在开发过程中，合理地使用线程池能够带来3个好处。</p><ul><li>降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。</li><li>提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。</li><li>提高线程的可管理性。线程是稀缺资源，如果无限制地创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一分配、调优和监控。但是，要做到合理利用线程池，必须对其实现原理了如指掌。</li></ul><h2 id="线程池的实现原理"><a href="#线程池的实现原理" class="headerlink" title="线程池的实现原理"></a>线程池的实现原理</h2><p>当提交一个新任务到线程池时，线程池的处理流程如下：</p><ol><li>线程池判断核心线程池里的线程是否都在执行任务。如果不是，则创建一个新的工作线程来执行任务。如果核心线程池里的线程都在执行任务，则进入下个流程。</li><li>线程池判断工作队列是否已经满。如果工作队列没有满，则将新提交的任务存储在这个工作队列里。如果工作队列满了，则进入下个流程。</li><li>线程池判断线程池的线程是否都处于工作状态。如果没有，则创建一个新的工作线程来执行任务。如果已经满了，则交给饱和策略来处理这个任务。</li></ol><p><img src="/2021/11/11/2021-11-11-java-zhong-de-xian-cheng-chi/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211111112607.png" alt></p><p><img src="/2021/11/11/2021-11-11-java-zhong-de-xian-cheng-chi/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211111113329.png" alt></p><p>ThreadPoolExecutor执行execute方法分下面4种情况。</p><ol><li>如果当前运行的线程少于corePoolSize，则创建新线程来执行任务（注意，执行这一步骤需要获取全局锁）。</li><li>如果运行的线程等于或多于corePoolSize，则将任务加入BlockingQueue。</li><li>如果无法将任务加入BlockingQueue（队列已满），则创建新的线程来处理任务（注意，执行这一步骤需要获取全局锁）。</li><li>如果创建新线程将使当前运行的线程超出maximumPoolSize，任务将被拒绝，并调用RejectedExecutionHandler.rejectedExecution()方法。</li></ol><p>ThreadPoolExecutor采取上述步骤的总体设计思路，是为了在执行execute()方法时，尽可能地避免获取全局锁（那将会是一个严重的可伸缩瓶颈）。在ThreadPoolExecutor完成预热之后（当前运行的线程数大于等于corePoolSize），几乎所有的execute()方法调用都是执行步骤2，而步骤2不需要获取全局锁。</p><h2 id="线程池的使用"><a href="#线程池的使用" class="headerlink" title="线程池的使用"></a>线程池的使用</h2><h3 id="线程池的创建"><a href="#线程池的创建" class="headerlink" title="线程池的创建"></a>线程池的创建</h3><p>通过ThreadPoolExecutor来创建一个线程池:</p><pre><code>new  ThreadPoolExecutor(corePoolSize, maximumPoolSize, keepAliveTime,milliseconds,runnableTaskQueue, handler);</code></pre><ol><li><p>corePoolSize（线程池的基本大小）：当提交一个任务到线程池时，线程池会创建一个线程来执行任务，即使其他空闲的基本线程能够执行新任务也会创建线程，等到需要执行的任务数大于线程池基本大小时就不再创建。如果调用了线程池的prestartAllCoreThreads()方法，线程池会提前创建并启动所有基本线程。</p></li><li><p>runnableTaskQueue（任务队列）：用于保存等待执行的任务的阻塞队列。可以选择以下几个阻塞队列。</p></li></ol><ul><li>ArrayBlockingQueue：是一个基于数组结构的有界阻塞队列，此队列按FIFO（先进先出）原则对元素进行排序。</li><li>LinkedBlockingQueue：一个基于链表结构的阻塞队列，此队列按FIFO排序元素，吞吐量通常要高于ArrayBlockingQueue。静态工厂方法Executors.newFixedThreadPool()使用了这个队列。</li><li>SynchronousQueue：一个不存储元素的阻塞队列。每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于Linked-BlockingQueue，静态工厂方法Executors.newCachedThreadPool使用了这个队列。</li><li>PriorityBlockingQueue：一个具有优先级的无限阻塞队列。</li></ul><ol start="3"><li><p>maximumPoolSize（线程池最大数量）：线程池允许创建的最大线程数。如果队列满了，并且已创建的线程数小于最大线程数，则线程池会再创建新的线程执行任务。值得注意的是，如果使用了无界的任务队列这个参数就没什么效果。</p></li><li><p>ThreadFactory：用于设置创建线程的工厂，可以通过线程工厂给每个创建出来的线程设置更有意义的名字。使用开源框架guava提供的ThreadFactoryBuilder可以快速给线程池里的线程设置有意义的名字，代码如下。</p></li></ol><pre><code>new ThreadFactoryBuilder().setNameFormat(&quot;XX-task-%d&quot;).build();</code></pre><ol start="5"><li>RejectedExecutionHandler（饱和策略）：当队列和线程池都满了，说明线程池处于饱和状态，那么必须采取一种策略处理提交的新任务。这个策略默认情况下是AbortPolicy，表示无法处理新任务时抛出异常。在JDK 1.5中Java线程池框架提供了以下4种策略。</li></ol><ul><li>AbortPolicy：直接抛出异常。</li><li>CallerRunsPolicy：只用调用者所在线程来运行任务。</li><li>DiscardOldestPolicy：丢弃队列里最近的一个任务，并执行当前任务。</li><li>DiscardPolicy：不处理，丢弃掉。</li></ul><ol start="6"><li><p>keepAliveTime（线程活动保持时间）：线程池的工作线程空闲后，保持存活的时间。所以，如果任务很多，并且每个任务执行的时间比较短，可以调大时间，提高线程的利用率。</p></li><li><p>TimeUnit（线程活动保持时间的单位）：可选的单位有天（DAYS）、小时（HOURS）、分钟（MINUTES）、毫秒（MILLISECONDS）、微秒（MICROSECONDS，千分之一毫秒）和纳秒（NANOSECONDS，千分之一微秒）。</p></li></ol><h3 id="向线程池提交任务"><a href="#向线程池提交任务" class="headerlink" title="向线程池提交任务"></a>向线程池提交任务</h3><p>可以使用两个方法向线程池提交任务，分别为execute()和submit()方法。</p><p>execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功。通过以下代码可知execute()方法输入的任务是一个Runnable类的实例。</p><pre><code>threadsPool.execute(new Runnable() {        @Override        public void run() {                // TODO Auto-generated method stub        }});</code></pre><p>submit()方法用于提交需要返回值的任务。线程池会返回一个future类型的对象，通过这个future对象可以判断任务是否执行成功，并且可以通过future的get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用get（long timeout，TimeUnit unit）方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。</p><pre><code>Future&lt;Object&gt; future = executor.submit(harReturnValuetask);    try {            Object s = future.get();    } catch (InterruptedException e) {            // 处理中断异常    } catch (ExecutionException e) {            // 处理无法执行任务异常    } finally {            // 关闭线程池            executor.shutdown();    }}</code></pre><h3 id="关闭线程池"><a href="#关闭线程池" class="headerlink" title="关闭线程池"></a>关闭线程池</h3><p>可以通过调用线程池的shutdown或shutdownNow方法来关闭线程池。它们的原理是遍历线程池中的工作线程，然后逐个调用线程的interrupt方法来中断线程，所以无法响应中断的任务可能永远无法终止。但是它们存在一定的区别，shutdown只是将线程池的状态设置为SHUTWDOWN状态，正在执行的任务会继续执行下去，没有被执行的则中断。而shutdownNow则是将线程池的状态设置为STOP，正在执行的任务则被停止，没被执行任务的则返回。</p><p>只要调用了这两个关闭方法中的任意一个，isShutdown方法就会返回true。当所有的任务都已关闭后，才表示线程池关闭成功，这时调用isTerminaed方法会返回true。至于应该调用哪一种方法来关闭线程池，应该由提交到线程池的任务特性决定，通常调用shutdown方法来关闭线程池，如果任务不一定要执行完，则可以调用shutdownNow方法。</p><h3 id="合理地配置线程池"><a href="#合理地配置线程池" class="headerlink" title="合理地配置线程池"></a>合理地配置线程池</h3><p>要想合理地配置线程池，就必须首先分析任务特性，可以从以下几个角度来分析。</p><ul><li>任务的性质：CPU密集型任务、IO密集型任务和混合型任务。</li><li>任务的优先级：高、中和低。</li><li>任务的执行时间：长、中和短。</li><li>任务的依赖性：是否依赖其他系统资源，如数据库连接。</li></ul><p>性质不同的任务可以用不同规模的线程池分开处理。CPU密集型任务应配置尽可能小的线程，如配置Ncpu+1个线程的线程池。由于IO密集型任务线程并不是一直在执行任务，则应配置尽可能多的线程，如2*Ncpu。混合型的任务，如果可以拆分，将其拆分成一个CPU密集型任务和一个IO密集型任务，只要这两个任务执行的时间相差不是太大，那么分解后执行的吞吐量将高于串行执行的吞吐量。如果这两个任务执行时间相差太大，则没必要进行分解。可以通过Runtime.getRuntime().availableProcessors()方法获得当前设备的CPU个数。</p><h3 id="线程池的监控"><a href="#线程池的监控" class="headerlink" title="线程池的监控"></a>线程池的监控</h3><p>如果在系统中大量使用线程池，则有必要对线程池进行监控，方便在出现问题时，可以根据线程池的使用状况快速定位问题。可以通过线程池提供的参数进行监控，在监控线程池的时候可以使用以下属性。</p><ul><li>taskCount：线程池需要执行的任务数量。</li><li>completedTaskCount：线程池在运行过程中已完成的任务数量，小于或等于taskCount。</li><li>largestPoolSize：线程池里曾经创建过的最大线程数量。通过这个数据可以知道线程池是否曾经满过。如该数值等于线程池的最大大小，则表示线程池曾经满过。</li><li>getPoolSize：线程池的线程数量。如果线程池不销毁的话，线程池里的线程不会自动销毁，所以这个大小只增不减。</li><li>getActiveCount：获取活动的线程数。</li></ul><p>通过扩展线程池进行监控。可以通过继承线程池来自定义线程池，重写线程池的beforeExecute、afterExecute和terminated方法，也可以在任务执行前、执行后和线程池关闭前执行一些代码来进行监控。例如，监控任务的平均执行时间、最大执行时间和最小执行时间等。这几个方法在线程池里是空方法。</p>]]></content>
      
      
      <categories>
          
          <category> JAVA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 并发编程的艺术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java中的并发工具类</title>
      <link href="/2021/11/09/2021-11-09-java-zhong-de-bing-fa-gong-ju-lei/"/>
      <url>/2021/11/09/2021-11-09-java-zhong-de-bing-fa-gong-ju-lei/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《Java并发编程的艺术》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="等待多线程完成的CountDownLatch"><a href="#等待多线程完成的CountDownLatch" class="headerlink" title="等待多线程完成的CountDownLatch"></a>等待多线程完成的CountDownLatch</h2><p>CountDownLatch允许一个或多个线程等待其他线程完成操作。</p><p>CountDownLatch的构造函数接收一个int类型的参数作为计数器，如果你想等待N个点完成，这里就传入N。</p><p>当我们调用CountDownLatch的countDown方法时，N就会减1，CountDownLatch的await方法会阻塞当前线程，直到N变成零。由于countDown方法可以用在任何地方，所以这里说的N个点，可以是N个线程，也可以是1个线程里的N个执行步骤。用在多个线程时，只需要把这个CountDownLatch的引用传递到线程里即可。</p><h2 id="同步屏障CyclicBarrier"><a href="#同步屏障CyclicBarrier" class="headerlink" title="同步屏障CyclicBarrier"></a>同步屏障CyclicBarrier</h2><p>CyclicBarrier的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续运行。</p><p>CyclicBarrier默认的构造方法是CyclicBarrier（int parties），其参数表示屏障拦截的线程数量，每个线程调用await方法告诉CyclicBarrier我已经到达了屏障，然后当前线程被阻塞。</p><h2 id="CyclicBarrier和CountDownLatch的区别"><a href="#CyclicBarrier和CountDownLatch的区别" class="headerlink" title="CyclicBarrier和CountDownLatch的区别"></a>CyclicBarrier和CountDownLatch的区别</h2><p>CountDownLatch的计数器只能使用一次，而CyclicBarrier的计数器可以使用reset()方法重置。所以CyclicBarrier能处理更为复杂的业务场景。例如，如果计算发生错误，可以重置计数器，并让线程重新执行一次。</p><p>CyclicBarrier还提供其他有用的方法，比如getNumberWaiting方法可以获得Cyclic-Barrier阻塞的线程数量。isBroken()方法用来了解阻塞的线程是否被中断。</p><h2 id="控制并发线程数的Semaphore"><a href="#控制并发线程数的Semaphore" class="headerlink" title="控制并发线程数的Semaphore"></a>控制并发线程数的Semaphore</h2><p>Semaphore（信号量）是用来控制同时访问特定资源的线程数量，它通过协调各个线程，以保证合理的使用公共资源。</p><p>Semaphore的用法也很简单，首先线程使用Semaphore的acquire()方法获取一个许可证，使用完之后调用release()方法归还许可证。</p><p>Semaphore还提供一些其他方法，具体如下。</p><ul><li>intavailablePermits()：返回此信号量中当前可用的许可证数。</li><li>intgetQueueLength()：返回正在等待获取许可证的线程数。</li><li>booleanhasQueuedThreads()：是否有线程正在等待获取许可证。</li><li>void reducePermits（int reduction）：减少reduction个许可证，是个protected方法。</li><li>Collection getQueuedThreads()：返回所有等待获取许可证的线程集合，是个protected方法。</li></ul><h2 id="线程间交换数据的Exchanger"><a href="#线程间交换数据的Exchanger" class="headerlink" title="线程间交换数据的Exchanger"></a>线程间交换数据的Exchanger</h2><p>Exchanger（交换者）是一个用于线程间协作的工具类。Exchanger用于进行线程间的数据交换。它提供一个同步点，在这个同步点，两个线程可以交换彼此的数据。这两个线程通过exchange方法交换数据，如果第一个线程先执行exchange()方法，它会一直等待第二个线程也执行exchange方法，当两个线程都到达同步点时，这两个线程就可以交换数据，将本线程生产出来的数据传递给对方。</p><p>如果两个线程有一个没有执行exchange()方法，则会一直等待，如果担心有特殊情况发生，避免一直等待，可以使用exchange（V x，longtimeout，TimeUnit unit）设置最大等待时长。</p>]]></content>
      
      
      <categories>
          
          <category> JAVA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 并发编程的艺术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java中的13个原子操作类</title>
      <link href="/2021/11/07/2021-11-07-java-zhong-de-13-ge-yuan-zi-cao-zuo-lei/"/>
      <url>/2021/11/07/2021-11-07-java-zhong-de-13-ge-yuan-zi-cao-zuo-lei/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《Java并发编程的艺术》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="原子更新基本类型类"><a href="#原子更新基本类型类" class="headerlink" title="原子更新基本类型类"></a>原子更新基本类型类</h2><p>使用原子的方式更新基本类型，Atomic包提供了以下3个类。</p><ul><li>AtomicBoolean：原子更新布尔类型。</li><li>AtomicInteger：原子更新整型。</li><li>AtomicLong：原子更新长整型。</li></ul><p>仅以AtomicInteger为例进行讲解，AtomicInteger的常用方法如下</p><ul><li>int addAndGet（int delta）：以原子方式将输入的数值与实例中的值（AtomicInteger里的value）相加，并返回结果。</li><li>boolean compareAndSet（int expect，int update）：如果输入的数值等于预期值，则以原子方式将该值设置为输入的值。</li><li>int getAndIncrement()：以原子方式将当前值加1，注意，这里返回的是自增前的值。</li><li>int getAndIncrement()：以原子方式将当前值加1，注意，这里返回的是自增前的值。</li><li>void lazySet（int newValue）：最终会设置成newValue，使用lazySet设置值后，可能导致其他线程在之后的一小段时间内还是可以读到旧的值。</li><li>int getAndSet（int newValue）：以原子方式设置为newValue的值，并返回旧值。</li></ul><p>getAndIncrement是如何实现原子操作的呢</p><pre><code>public final int getAndIncrement() {    for (;;) {        int current = get();        int next = current + 1;        if (compareAndSet(current, next))            return current;    }}public final boolean compareAndSet(int expect, int update) {    return unsafe.compareAndSwapInt(this, valueOffset, expect, update);}</code></pre><p>源码中for循环体的第一步先取得AtomicInteger里存储的数值，第二步对AtomicInteger的当前数值进行加1操作，关键的第三步调用compareAndSet方法来进行原子更新操作，该方法先检查当前数值是否等于current，等于意味着AtomicInteger的值没有被其他线程修改过，则将AtomicInteger的当前数值更新成next的值，如果不等compareAndSet方法会返回false，程序会进入for循环重新进行compareAndSet操作。</p><p>Atomic包里的类基本都是使用Unsafe实现的</p><pre><code>** * 如果当前数值是expected，则原子的将Java变量更新成x * @return 如果更新成功则返回true */public final native boolean compareAndSwapObject(Object o,                                                 long offset,                                                 Object expected,                                                 Object x);public final native boolean compareAndSwapInt(Object o, long offset,                                              int expected,                                              int x);public final native boolean compareAndSwapLong(Object o, long offset,                                               long expected,                                               long x);</code></pre><p>Unsafe只提供了3种CAS方法：compareAndSwapObject、compare-AndSwapInt和compareAndSwapLong，再看AtomicBoolean源码，发现它是先把Boolean转换成整型，再使用compareAndSwapInt进行CAS，所以原子更新char、float和double变量也可以用类似的思路来实现。</p><h3 id="原子更新数组"><a href="#原子更新数组" class="headerlink" title="原子更新数组"></a>原子更新数组</h3><p>通过原子的方式更新数组里的某个元素，Atomic包提供了以下类。</p><ul><li>AtomicIntegerArray：原子更新整型数组里的元素。</li><li>AtomicLongArray：原子更新长整型数组里的元素。</li><li>AtomicReferenceArray：原子更新引用类型数组里的元素。</li></ul><p>AtomicIntegerArray类主要是提供原子的方式更新数组里的整型。</p><ul><li>int addAndGet（int i，int delta）：以原子方式将输入值与数组中索引i的元素相加。</li><li>boolean compareAndSet（int i，int expect，int update）：如果当前值等于预期值，则以原子方式将数组位置i的元素设置成update值。</li></ul><h3 id="原子更新引用类型"><a href="#原子更新引用类型" class="headerlink" title="原子更新引用类型"></a>原子更新引用类型</h3><p>原子更新基本类型的AtomicInteger，只能更新一个变量，如果要原子更新多个变量，就需要使用这个原子更新引用类型提供的类。Atomic包提供了以下3个类。</p><ul><li>AtomicReference：原子更新引用类型。</li><li>AtomicReferenceFieldUpdater：原子更新引用类型里的字段。</li><li>AtomicMarkableReference：原子更新带有标记位的引用类型。可以原子更新一个布尔类型的标记位和引用类型。构造方法是AtomicMarkableReference（V initialRef，boolean initialMark）。</li></ul><h3 id="原子更新字段类"><a href="#原子更新字段类" class="headerlink" title="原子更新字段类"></a>原子更新字段类</h3><p>如果需原子地更新某个类里的某个字段时，就需要使用原子更新字段类，Atomic包提供了以下3个类进行原子字段更新。</p><ul><li>AtomicIntegerFieldUpdater：原子更新整型的字段的更新器。</li><li>AtomicLongFieldUpdater：原子更新长整型字段的更新器。</li><li>AtomicStampedReference：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于原子的更新数据和数据的版本号，可以解决使用CAS进行原子更新时可能出现的ABA问题。</li></ul><pre><code>import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicReference;import java.util.concurrent.atomic.AtomicStampedReference;public class ABADemo {    /**     * 普通的原子引用包装类     */    static AtomicReference&lt;Integer&gt; atomicReference = new AtomicReference&lt;&gt;(100);    // 传递两个值，一个是初始值，一个是初始版本号    static AtomicStampedReference&lt;Integer&gt; atomicStampedReference = new AtomicStampedReference&lt;&gt;(100, 1);    public static void main(String[] args) {        System.out.println(&quot;============以下是ABA问题的产生==========&quot;);        new Thread(() -&gt; {            // 把100 改成 101 然后在改成100，也就是ABA            atomicReference.compareAndSet(100, 101);            atomicReference.compareAndSet(101, 100);        }, &quot;t1&quot;).start();        new Thread(() -&gt; {            try {                // 睡眠一秒，保证t1线程，完成了ABA操作                TimeUnit.SECONDS.sleep(1);            } catch (InterruptedException e) {                e.printStackTrace();            }            // 把100 改成 101 然后在改成100，也就是ABA            System.out.println(atomicReference.compareAndSet(100, 2019) + &quot;\t&quot; + atomicReference.get());        }, &quot;t2&quot;).start();        /        try {            TimeUnit.SECONDS.sleep(2);        } catch (Exception e) {            e.printStackTrace();        }        /        System.out.println(&quot;============以下是ABA问题的解决==========&quot;);        new Thread(() -&gt; {            // 获取版本号            int stamp = atomicStampedReference.getStamp();            System.out.println(Thread.currentThread().getName() + &quot;\t 第一次版本号&quot; + stamp);            // 暂停t3一秒钟            try {                TimeUnit.SECONDS.sleep(1);            } catch (InterruptedException e) {                e.printStackTrace();            }            // 传入4个值，期望值，更新值，期望版本号，更新版本号            atomicStampedReference.compareAndSet(100, 101, atomicStampedReference.getStamp(),                    atomicStampedReference.getStamp() + 1);            System.out.println(Thread.currentThread().getName() + &quot;\t 第二次版本号&quot; + atomicStampedReference.getStamp());            atomicStampedReference.compareAndSet(101, 100, atomicStampedReference.getStamp(),                    atomicStampedReference.getStamp() + 1);            System.out.println(Thread.currentThread().getName() + &quot;\t 第三次版本号&quot; + atomicStampedReference.getStamp());        }, &quot;t3&quot;).start();        new Thread(() -&gt; {            // 获取版本号            int stamp = atomicStampedReference.getStamp();            System.out.println(Thread.currentThread().getName() + &quot;\t 第一次版本号&quot; + stamp);            // 暂停t4 3秒钟，保证t3线程也进行一次ABA问题            try {                TimeUnit.SECONDS.sleep(3);            } catch (InterruptedException e) {                e.printStackTrace();            }            boolean result = atomicStampedReference.compareAndSet(100, 2019, stamp, stamp + 1);            System.out.println(Thread.currentThread().getName() + &quot;\t 修改成功否：&quot; + result + &quot;\t 当前最新实际版本号：&quot;                    + atomicStampedReference.getStamp());            System.out.println(Thread.currentThread().getName() + &quot;\t 当前实际最新值&quot; + atomicStampedReference.getReference());        }, &quot;t4&quot;).start();    }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> JAVA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 并发编程的艺术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java并发容器和框架</title>
      <link href="/2021/11/05/2021-11-05-java-bing-fa-rong-qi-he-kuang-jia/"/>
      <url>/2021/11/05/2021-11-05-java-bing-fa-rong-qi-he-kuang-jia/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《Java并发编程的艺术》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="ConcurrentHashMap的实现原理与使用"><a href="#ConcurrentHashMap的实现原理与使用" class="headerlink" title="ConcurrentHashMap的实现原理与使用"></a>ConcurrentHashMap的实现原理与使用</h2><p>ConcurrentHashMap是线程安全且高效的HashMap。</p><h3 id="为什么要使用ConcurrentHashMap"><a href="#为什么要使用ConcurrentHashMap" class="headerlink" title="为什么要使用ConcurrentHashMap"></a>为什么要使用ConcurrentHashMap</h3><p>在并发编程中使用HashMap可能导致程序死循环。而使用线程安全的HashTable效率又非常低下，基于以上两个原因，便有了ConcurrentHashMap的登场机会。</p><h4 id="线程不安全的HashMap"><a href="#线程不安全的HashMap" class="headerlink" title="线程不安全的HashMap"></a>线程不安全的HashMap</h4><p>在多线程环境下，使用HashMap进行put操作会引起死循环，导致CPU利用率接近100%，所以在并发情况下不能使用HashMap。例</p><p>HashMap在并发执行put操作时会引起死循环，是因为多线程会导致HashMap的Entry链表形成环形数据结构，一旦形成环形数据结构，Entry的next节点永远不为空，就会产生死循环获取Entry。</p><h4 id="效率低下的HashTable"><a href="#效率低下的HashTable" class="headerlink" title="效率低下的HashTable"></a>效率低下的HashTable</h4><p>HashTable容器使用synchronized来保证线程安全，但在线程竞争激烈的情况下HashTable的效率非常低下。因为当一个线程访问HashTable的同步方法，其他线程也访问HashTable的同步方法时，会进入阻塞或轮询状态。如线程1使用put进行元素添加，线程2不但不能使用put方法添加元素，也不能使用get方法来获取元素，所以竞争越激烈效率越低。</p><h4 id="ConcurrentHashMap的锁分段技术可有效提升并发访问率"><a href="#ConcurrentHashMap的锁分段技术可有效提升并发访问率" class="headerlink" title="ConcurrentHashMap的锁分段技术可有效提升并发访问率"></a>ConcurrentHashMap的锁分段技术可有效提升并发访问率</h4><p>HashTable容器在竞争激烈的并发环境下表现出效率低下的原因是所有访问HashTable的线程都必须竞争同一把锁，假如容器里有多把锁，每一把锁用于锁容器其中一部分数据，那么当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效提高并发访问效率，这就是ConcurrentHashMap所使用的锁分段技术。首先将数据分成一段一段地存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。</p><h3 id="ConcurrentHashMap的结构"><a href="#ConcurrentHashMap的结构" class="headerlink" title="ConcurrentHashMap的结构"></a>ConcurrentHashMap的结构</h3><p>ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成。Segment是一种可重入锁（ReentrantLock），在ConcurrentHashMap里扮演锁的角色；HashEntry则用于存储键值对数据。一个ConcurrentHashMap里包含一个Segment数组。Segment的结构和HashMap类似，是一种数组和链表结构。一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元素，每个Segment守护着一个HashEntry数组里的元素，当对HashEntry数组的数据进行修改时，必须首先获得与它对应的Segment锁，</p><p><img src="/2021/11/05/2021-11-05-java-bing-fa-rong-qi-he-kuang-jia/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211108163207.png" alt></p><h3 id="ConcurrentHashMap的初始化"><a href="#ConcurrentHashMap的初始化" class="headerlink" title="ConcurrentHashMap的初始化"></a>ConcurrentHashMap的初始化</h3><p>ConcurrentHashMap初始化方法是通过initialCapacity、loadFactor和concurrencyLevel等几个参数来初始化segment数组、段偏移量segmentShift、段掩码segmentMask和每个segment里的HashEntry数组来实现的。</p><ol><li>初始化segments数组</li></ol><p>segments数组的长度ssize是通过concurrencyLevel计算得出的。为了能通过按位与的散列算法来定位segments数组的索引，必须保证segments数组的长度是2的N次方（power-of-two size），所以必须计算出一个大于或等于concurrencyLevel的最小的2的N次方值来作为segments数组的长度。假如concurrencyLevel等于14、15或16，ssize都会等于16，即容器里锁的个数也是16。</p><ol start="2"><li><p>初始化segmentShift和segmentMask</p></li><li><p>初始化每个segment</p></li></ol><p>输入参数initialCapacity是ConcurrentHashMap的初始化容量，loadfactor是每个segment的负载因子，在构造方法里需要通过这两个参数来初始化数组中的每个segment。</p><h3 id="定位Segment"><a href="#定位Segment" class="headerlink" title="定位Segment"></a>定位Segment</h3><p>既然ConcurrentHashMap使用分段锁Segment来保护不同段的数据，那么在插入和获取元素的时候，必须先通过散列算法定位到Segment。可以看到ConcurrentHashMap会首先使用Wang/Jenkins hash的变种算法对元素的hashCode进行一次再散列。</p><p>之所以进行再散列，目的是减少散列冲突，使元素能够均匀地分布在不同的Segment上，从而提高容器的存取效率。假如散列的质量差到极点，那么所有的元素都在一个Segment中，不仅存取元素缓慢，分段锁也会失去意义。</p><h3 id="ConcurrentHashMap的操作"><a href="#ConcurrentHashMap的操作" class="headerlink" title="ConcurrentHashMap的操作"></a>ConcurrentHashMap的操作</h3><h4 id="get操作"><a href="#get操作" class="headerlink" title="get操作"></a>get操作</h4><p>Segment的get操作实现非常简单和高效。先经过一次再散列，然后使用这个散列值通过散列运算定位到Segment，再通过散列算法定位到元素，代码如下</p><pre><code>public V get(Object key) {    int hash = hash(key.hashCode());    return segmentFor(hash).get(key, hash);}</code></pre><p>get操作的高效之处在于整个get过程不需要加锁，除非读到的值是空才会加锁重读。我们知道HashTable容器的get方法是需要加锁的，那么ConcurrentHashMap的get操作是如何做到不加锁的呢？原因是它的get方法里将要使用的共享变量都定义成volatile类型，如用于统计当前Segement大小的count字段和用于存储值的HashEntry的value。定义成volatile的变量，能够在线程之间保持可见性，能够被多线程同时读，并且保证不会读到过期的值，但是只能被单线程写（有一种情况可以被多线程写，就是写入的值不依赖于原值），在get操作里只需要读不需要写共享变量count和value，所以可以不用加锁。</p><h4 id="put操作"><a href="#put操作" class="headerlink" title="put操作"></a>put操作</h4><p>由于put方法里需要对共享变量进行写入操作，所以为了线程安全，在操作共享变量时必须加锁。put方法首先定位到Segment，然后在Segment里进行插入操作。插入操作需要经历两个步骤，第一步判断是否需要对Segment里的HashEntry数组进行扩容，第二步定位添加元素的位置，然后将其放在HashEntry数组里。</p><ol><li>是否需要扩容</li></ol><p>在插入元素前会先判断Segment里的HashEntry数组是否超过容量（threshold），如果超过阈值，则对数组进行扩容。值得一提的是，Segment的扩容判断比HashMap更恰当，因为HashMap是在插入元素后判断元素是否已经到达容量的，如果到达了就进行扩容，但是很有可能扩容之后没有新元素插入，这时HashMap就进行了一次无效的扩容。</p><ol start="2"><li>如何扩容</li></ol><p>在扩容的时候，首先会创建一个容量是原来容量两倍的数组，然后将原数组里的元素进行再散列后插入到新的数组里。为了高效，ConcurrentHashMap不会对整个容器进行扩容，而只对某个segment进行扩容。</p><h4 id="size操作"><a href="#size操作" class="headerlink" title="size操作"></a>size操作</h4><p>如果要统计整个ConcurrentHashMap里元素的大小，就必须统计所有Segment里元素的大小后求和。Segment里的全局变量count是一个volatile变量，那么在多线程场景下，是不是直接把所有Segment的count相加就可以得到整个ConcurrentHashMap大小了呢？不是的，虽然相加时可以获取每个Segment的count的最新值，但是可能累加前使用的count发生了变化，那么统计结果就不准了。所以，最安全的做法是在统计size的时候把所有Segment的put、remove和clean方法全部锁住，但是这种做法显然非常低效。</p><p>因为在累加count操作过程中，之前累加过的count发生变化的几率非常小，所以ConcurrentHashMap的做法是先尝试2次通过不锁住Segment的方式来统计各个Segment大小，如果统计的过程中，容器的count发生了变化，则再采用加锁的方式来统计所有Segment的大小。</p><p>那么ConcurrentHashMap是如何判断在统计的时候容器是否发生了变化呢？使用modCount变量，在put、remove和clean方法里操作元素前都会将变量modCount进行加1，那么在统计size前后比较modCount是否发生变化，从而得知容器的大小是否发生变化。</p><h2 id="ConcurrentLinkedQueue"><a href="#ConcurrentLinkedQueue" class="headerlink" title="ConcurrentLinkedQueue"></a>ConcurrentLinkedQueue</h2><p>在并发编程中，有时候需要使用线程安全的队列。如果要实现一个线程安全的队列有两种方式：一种是使用阻塞算法，另一种是使用非阻塞算法。使用阻塞算法的队列可以用一个锁（入队和出队用同一把锁）或两个锁（入队和出队用不同的锁）等方式来实现。非阻塞的实现方式则可以使用循环CAS的方式来实现。</p><p>ConcurrentLinkedQueue是一个基于链接节点的无界线程安全队列，它采用先进先出的规则对节点进行排序，当我们添加一个元素的时候，它会添加到队列的尾部；当我们获取一个元素时，它会返回队列头部的元素。它采用了“wait-free”算法（即CAS算法）来实现</p><h3 id="ConcurrentLinkedQueue的结构"><a href="#ConcurrentLinkedQueue的结构" class="headerlink" title="ConcurrentLinkedQueue的结构"></a>ConcurrentLinkedQueue的结构</h3><p>ConcurrentLinkedQueue由head节点和tail节点组成，每个节点（Node）由节点元素（item）和指向下一个节点（next）的引用组成，节点与节点之间就是通过这个next关联起来，从而组成一张链表结构的队列。默认情况下head节点存储的元素为空，tail节点等于head节点。</p><h3 id="入队列"><a href="#入队列" class="headerlink" title="入队列"></a>入队列</h3><p>入队列就是将入队节点添加到队列的尾部。</p><p>整个入队过程主要做两件事情：第一是定位出尾节点；第二是使用CAS算法将入队节点设置成尾节点的next节点，如不成功则重试。</p><h3 id="出队列"><a href="#出队列" class="headerlink" title="出队列"></a>出队列</h3><p>出队列的就是从队列里返回一个节点元素，并清空该节点对元素的引用。</p><h2 id="Java中的阻塞队列"><a href="#Java中的阻塞队列" class="headerlink" title="Java中的阻塞队列"></a>Java中的阻塞队列</h2><p>阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。这两个附加的操作支持阻塞的插入和移除方法。</p><ul><li>支持阻塞的插入方法：意思是当队列满时，队列会阻塞插入元素的线程，直到队列不满。</li><li>支持阻塞的移除方法：意思是在队列为空时，获取元素的线程会等待队列变为非空。</li></ul><p>阻塞队列常用于生产者和消费者的场景，生产者是向队列里添加元素的线程，消费者是从队列里取元素的线程。阻塞队列就是生产者用来存放元素、消费者用来获取元素的容器。</p><p><img src="/2021/11/05/2021-11-05-java-bing-fa-rong-qi-he-kuang-jia/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211108172423.png" alt></p><ul><li>抛出异常：当队列满时，如果再往队列里插入元素，会抛出IllegalStateException（”Queue full”）异常。当队列空时，从队列里获取元素会抛出NoSuchElementException异常。</li><li>返回特殊值：当往队列插入元素时，会返回元素是否插入成功，成功返回true。如果是移除方法，则是从队列里取出一个元素，如果没有则返回null。</li><li>一直阻塞：当阻塞队列满时，如果生产者线程往队列里put元素，队列会一直阻塞生产者线程，直到队列可用或者响应中断退出。当队列空时，如果消费者线程从队列里take元素，队列会阻塞住消费者线程，直到队列不为空。</li><li>超时退出：当阻塞队列满时，如果生产者线程往队列里插入元素，队列会阻塞生产者线程一段时间，如果超过了指定的时间，生产者线程就会退出。</li></ul><h3 id="Java里的阻塞队列"><a href="#Java里的阻塞队列" class="headerlink" title="Java里的阻塞队列"></a>Java里的阻塞队列</h3><p>JDK 7提供了7个阻塞队列，如下:</p><ul><li>ArrayBlockingQueue：一个由数组结构组成的有界阻塞队列。</li><li>LinkedBlockingQueue：一个由链表结构组成的有界阻塞队列。</li><li>PriorityBlockingQueue：一个支持优先级排序的无界阻塞队列。</li><li>DelayQueue：一个使用优先级队列实现的无界阻塞队列。</li><li>SynchronousQueue：一个不存储元素的阻塞队列。</li><li>LinkedTransferQueue：一个由链表结构组成的无界阻塞队列。</li><li>LinkedBlockingDeque：一个由链表结构组成的双向阻塞队列。</li></ul><h4 id="ArrayBlockingQueue"><a href="#ArrayBlockingQueue" class="headerlink" title="ArrayBlockingQueue"></a>ArrayBlockingQueue</h4><p>ArrayBlockingQueue是一个用数组实现的有界阻塞队列。此队列按照先进先出（FIFO）的原则对元素进行排序。</p><p>默认情况下不保证线程公平的访问队列，所谓公平访问队列是指阻塞的线程，可以按照阻塞的先后顺序访问队列，即先阻塞线程先访问队列。非公平性是对先等待的线程是非公平的，当队列可用时，阻塞的线程都可以争夺访问队列的资格，有可能先阻塞的线程最后才访问队列。为了保证公平性，通常会降低吞吐量。</p><h4 id="LinkedBlockingQueue"><a href="#LinkedBlockingQueue" class="headerlink" title="LinkedBlockingQueue"></a>LinkedBlockingQueue</h4><p>LinkedBlockingQueue是一个用链表实现的有界阻塞队列。此队列的默认和最大长度为Integer.MAX_VALUE。此队列按照先进先出的原则对元素进行排序。</p><h4 id="PriorityBlockingQueue"><a href="#PriorityBlockingQueue" class="headerlink" title="PriorityBlockingQueue"></a>PriorityBlockingQueue</h4><p>PriorityBlockingQueue是一个支持优先级的无界阻塞队列。默认情况下元素采取自然顺序升序排列。也可以自定义类实现compareTo()方法来指定元素排序规则，或者初始化PriorityBlockingQueue时，指定构造参数Comparator来对元素进行排序。需要注意的是不能保证同优先级元素的顺序。</p><h4 id="DelayQueue"><a href="#DelayQueue" class="headerlink" title="DelayQueue"></a>DelayQueue</h4><p>DelayQueue是一个支持延时获取元素的无界阻塞队列。队列使用PriorityQueue来实现。队列中的元素必须实现Delayed接口，在创建元素时可以指定多久才能从队列中获取当前元素。只有在延迟期满时才能从队列中提取元素。</p><h4 id="SynchronousQueue"><a href="#SynchronousQueue" class="headerlink" title="SynchronousQueue"></a>SynchronousQueue</h4><p>SynchronousQueue是一个不存储元素的阻塞队列。每一个put操作必须等待一个take操作，否则不能继续添加元素。</p><p>它支持公平访问队列。默认情况下线程采用非公平性策略访问队列。</p><h4 id="LinkedTransferQueue"><a href="#LinkedTransferQueue" class="headerlink" title="LinkedTransferQueue"></a>LinkedTransferQueue</h4><p>LinkedTransferQueue是一个由链表结构组成的无界阻塞TransferQueue队列。相对于其他阻塞队列，LinkedTransferQueue多了tryTransfer和transfer方法。</p><p>如果当前有消费者正在等待接收元素（消费者使用take()方法或带时间限制的poll()方法时），transfer方法可以把生产者传入的元素立刻transfer（传输）给消费者。如果没有消费者在等待接收元素，transfer方法会将元素存放在队列的tail节点，并等到该元素被消费者消费了才返回。</p><p>tryTransfer方法是用来试探生产者传入的元素是否能直接传给消费者。如果没有消费者等待接收元素，则返回false。和transfer方法的区别是tryTransfer方法无论消费者是否接收，方法立即返回，而transfer方法是必须等到消费者消费了才返回。</p><h4 id="LinkedBlockingDeque"><a href="#LinkedBlockingDeque" class="headerlink" title="LinkedBlockingDeque"></a>LinkedBlockingDeque</h4><p>LinkedBlockingDeque是一个由链表结构组成的双向阻塞队列。所谓双向队列指的是可以从队列的两端插入和移出元素。双向队列因为多了一个操作队列的入口，在多线程同时入队时，也就减少了一半的竞争。相比其他的阻塞队列，LinkedBlockingDeque多了addFirst、addLast、offerFirst、offerLast、peekFirst和peekLast等方法，以First单词结尾的方法，表示插入、获取（peek）或移除双端队列的第一个元素。以Last单词结尾的方法，表示插入、获取或移除双端队列的最后一个元素。另外，插入方法add等同于addLast，移除方法remove等效于removeFirst。但是take方法却等同于takeFirst，不知道是不是JDK的bug，使用时还是用带有First和Last后缀的方法更清楚。</p><h3 id="阻塞队列的实现原理"><a href="#阻塞队列的实现原理" class="headerlink" title="阻塞队列的实现原理"></a>阻塞队列的实现原理</h3><p>使用通知模式实现。所谓通知模式，就是当生产者往满的队列里添加元素时会阻塞住生产者，当消费者消费了一个队列中的元素后，会通知生产者当前队列可用。</p><p>当往队列里插入一个元素时，如果队列不可用，那么阻塞生产者主要通过LockSupport.park（this）来实现。</p><p>发现调用setBlocker先保存一下将要阻塞的线程，然后调用unsafe.park阻塞当前线程。</p><h2 id="Fork-Join框架"><a href="#Fork-Join框架" class="headerlink" title="Fork/Join框架"></a>Fork/Join框架</h2><h3 id="什么是Fork-Join框架"><a href="#什么是Fork-Join框架" class="headerlink" title="什么是Fork/Join框架"></a>什么是Fork/Join框架</h3><p>Fork/Join框架是Java 7提供的一个用于并行执行任务的框架，是一个把大任务分割成若干个小任务，最终汇总每个小任务结果后得到大任务结果的框架。</p><p>我们再通过Fork和Join这两个单词来理解一下Fork/Join框架。Fork就是把一个大任务切分为若干子任务并行的执行，Join就是合并这些子任务的执行结果，最后得到这个大任务的结果。比如计算1+2+…+10000，可以分割成10个子任务，每个子任务分别对1000个数进行求和，最终汇总这10个子任务的结果。</p><p><img src="/2021/11/05/2021-11-05-java-bing-fa-rong-qi-he-kuang-jia/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211109143734.png" alt></p><h3 id="工作窃取算法"><a href="#工作窃取算法" class="headerlink" title="工作窃取算法"></a>工作窃取算法</h3><p>工作窃取（work-stealing）算法是指某个线程从其他队列里窃取任务来执行。那么，为什么需要使用工作窃取算法呢？假如我们需要做一个比较大的任务，可以把这个任务分割为若干互不依赖的子任务，为了减少线程间的竞争，把这些子任务分别放到不同的队列里，并为每个队列创建一个单独的线程来执行队列里的任务，线程和队列一一对应。比如A线程负责处理A队列里的任务。但是，有的线程会先把自己队列里的任务干完，而其他线程对应的队列里还有任务等待处理。干完活的线程与其等着，不如去帮其他线程干活，于是它就去其他线程的队列里窃取一个任务来执行。而在这时它们会访问同一个队列，所以为了减少窃取任务线程和被窃取任务线程之间的竞争，通常会使用双端队列，被窃取任务线程永远从双端队列的头部拿任务执行，而窃取任务的线程永远从双端队列的尾部拿任务执行。</p><p><img src="/2021/11/05/2021-11-05-java-bing-fa-rong-qi-he-kuang-jia/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211109160819.png" alt></p><p>工作窃取算法的优点：充分利用线程进行并行计算，减少了线程间的竞争。</p><p>工作窃取算法的缺点：在某些情况下还是存在竞争，比如双端队列里只有一个任务时。并且该算法会消耗了更多的系统资源，比如创建多个线程和多个双端队列。</p><h3 id="Fork-Join框架的设计"><a href="#Fork-Join框架的设计" class="headerlink" title="Fork/Join框架的设计"></a>Fork/Join框架的设计</h3><h4 id="分割任务"><a href="#分割任务" class="headerlink" title="分割任务"></a>分割任务</h4><p>首先我们需要有一个fork类来把大任务分割成子任务，有可能子任务还是很大，所以还需要不停地分割，直到分割出的子任务足够小。</p><h4 id="执行任务并合并结果"><a href="#执行任务并合并结果" class="headerlink" title="执行任务并合并结果"></a>执行任务并合并结果</h4><p>分割的子任务分别放在双端队列里，然后几个启动线程分别从双端队列里获取任务执行。子任务执行完的结果都统一放在一个队列里，启动一个线程从队列里拿数据，然后合并这些数据。</p><h3 id="使用Fork-Join框架"><a href="#使用Fork-Join框架" class="headerlink" title="使用Fork/Join框架"></a>使用Fork/Join框架</h3><h4 id="ForkJoinTask"><a href="#ForkJoinTask" class="headerlink" title="ForkJoinTask"></a>ForkJoinTask</h4><p>我们要使用ForkJoin框架，必须首先创建一个ForkJoin任务。它提供在任务中执行fork()和join()操作的机制。通常情况下，我们不需要直接继承ForkJoinTask类，只需要继承它的子类，Fork/Join框架提供了以下两个子类。</p><ul><li>RecursiveAction：用于没有返回结果的任务。</li><li>RecursiveTask：用于有返回结果的任务。</li></ul><h4 id="ForkJoinPool"><a href="#ForkJoinPool" class="headerlink" title="ForkJoinPool"></a>ForkJoinPool</h4><p>ForkJoinTask需要通过ForkJoinPool来执行。</p><h3 id="Fork-Join框架的异常处理"><a href="#Fork-Join框架的异常处理" class="headerlink" title="Fork/Join框架的异常处理"></a>Fork/Join框架的异常处理</h3><p>ForkJoinTask在执行的时候可能会抛出异常，但是我们没办法在主线程里直接捕获异常，所以ForkJoinTask提供了isCompletedAbnormally()方法来检查任务是否已经抛出异常或已经被取消了，并且可以通过ForkJoinTask的getException方法获取异常。使用如下代码。</p><p>getException方法返回Throwable对象，如果任务被取消了则返回CancellationException。如果任务没有完成或者没有抛出异常则返回null。</p><h3 id="Fork-Join框架的实现原理"><a href="#Fork-Join框架的实现原理" class="headerlink" title="Fork/Join框架的实现原理"></a>Fork/Join框架的实现原理</h3><p>ForkJoinPool由ForkJoinTask数组和ForkJoinWorkerThread数组组成，ForkJoinTask数组负责将存放程序提交给ForkJoinPool的任务，而ForkJoinWorkerThread数组负责执行这些任务。</p><h4 id="ForkJoinTask的fork方法实现原理"><a href="#ForkJoinTask的fork方法实现原理" class="headerlink" title="ForkJoinTask的fork方法实现原理"></a>ForkJoinTask的fork方法实现原理</h4><p>当我们调用ForkJoinTask的fork方法时，程序会调用ForkJoinWorkerThread的pushTask方法异步地执行这个任务，然后立即返回结果。代码如下。</p><pre><code>public final ForkJoinTask&lt;V&gt; fork() {    ((ForkJoinWorkerThread) Thread.currentThread())        .pushTask(this);    return this;}</code></pre><p>pushTask方法把当前任务存放在ForkJoinTask数组队列里。然后再调用ForkJoinPool的signalWork()方法唤醒或创建一个工作线程来执行任务。</p><h4 id="ForkJoinTask的join方法实现原理"><a href="#ForkJoinTask的join方法实现原理" class="headerlink" title="ForkJoinTask的join方法实现原理"></a>ForkJoinTask的join方法实现原理</h4><p>Join方法的主要作用是阻塞当前线程并等待获取结果。</p><pre><code>public final V join() {    if (doJoin() != NORMAL)        return reportResult();    else        return getRawResult();}private V reportResult() {    int s; Throwable ex;    if ((s = status) == CANCELLED)        throw new CancellationException();    if (s == EXCEPTIONAL &amp;&amp; (ex = getThrowableException()) != null)        UNSAFE.throwException(ex);    return getRawResult();}</code></pre><p>首先，它调用了doJoin()方法，通过doJoin()方法得到当前任务的状态来判断返回什么结果，任务状态有4种：已完成（NORMAL）、被取消（CANCELLED）、信号（SIGNAL）和出现异常（EXCEPTIONAL）。</p><ul><li>如果任务状态是已完成，则直接返回任务结果。</li><li>如果任务状态是被取消，则直接抛出CancellationException。</li><li>如果任务状态是抛出异常，则直接抛出对应的异常。</li></ul><p>在doJoin()方法里，首先通过查看任务的状态，看任务是否已经执行完成，如果执行完成，则直接返回任务状态；如果没有执行完，则从任务数组里取出任务并执行。如果任务顺利执行完成，则设置任务状态为NORMAL，如果出现异常，则记录异常，并将任务状态设置为EXCEPTIONAL。</p>]]></content>
      
      
      <categories>
          
          <category> JAVA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 并发编程的艺术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java中的锁</title>
      <link href="/2021/11/03/2021-11-03-java-zhong-de-suo/"/>
      <url>/2021/11/03/2021-11-03-java-zhong-de-suo/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《Java并发编程的艺术》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="Lock接口"><a href="#Lock接口" class="headerlink" title="Lock接口"></a>Lock接口</h2><p>锁是用来控制多个线程访问共享资源的方式，一般来说，一个锁能够防止多个线程同时访问共享资源（但是有些锁可以允许多个线程并发的访问共享资源，比如读写锁）。在Lock接口出现之前，Java程序是靠synchronized关键字实现锁功能的，而Java SE 5之后，并发包中新增了Lock接口（以及相关实现类）用来实现锁功能，它提供了与synchronized关键字类似的同步功能，只是在使用时需要显式地获取和释放锁。虽然它缺少了（通过synchronized块或者方法所提供的）隐式获取释放锁的便捷性，但是却拥有了锁获取与释放的可操作性、可中断的获取锁以及超时获取锁等多种synchronized关键字所不具备的同步特性。</p><h3 id="队列同步器"><a href="#队列同步器" class="headerlink" title="队列同步器"></a>队列同步器</h3><p>队列同步器AbstractQueuedSynchronizer（以下简称同步器），是用来构建锁或者其他同步组件的基础框架，它使用了一个int成员变量表示同步状态，通过内置的FIFO队列来完成资源获取线程的排队工作，并发包的作者（Doug Lea）期望它能够成为实现大部分同步需求的基础。</p><p>同步器的主要使用方式是继承，子类通过继承同步器并实现它的抽象方法来管理同步状态，在抽象方法的实现过程中免不了要对同步状态进行更改，这时就需要使用同步器提供的3个方法（getState()、setState(int newState)和compareAndSetState(int expect,int update)）来进行操作，因为它们能够保证状态的改变是安全的。子类推荐被定义为自定义同步组件的静态内部类，同步器自身没有实现任何同步接口，它仅仅是定义了若干同步状态获取和释放的方法来供自定义同步组件使用，同步器既可以支持独占式地获取同步状态，也可以支持共享式地获取同步状态，这样就可以方便实现不同类型的同步组件（ReentrantLock、ReentrantReadWriteLock和CountDownLatch等）。</p><p>同步器是实现锁（也可以是任意同步组件）的关键，在锁的实现中聚合同步器，利用同步器实现锁的语义。可以这样理解二者之间的关系：锁是面向使用者的，它定义了使用者与锁交互的接口（比如可以允许两个线程并行访问），隐藏了实现细节；同步器面向的是锁的实现者，它简化了锁的实现方式，屏蔽了同步状态管理、线程的排队、等待与唤醒等底层操作。锁和同步器很好地隔离了使用者和实现者所需关注的领域。</p><h3 id="队列同步器的接口与示例"><a href="#队列同步器的接口与示例" class="headerlink" title="队列同步器的接口与示例"></a>队列同步器的接口与示例</h3><p>同步器的设计是基于模板方法模式的，也就是说，使用者需要继承同步器并重写指定的方法，随后将同步器组合在自定义同步组件的实现中，并调用同步器提供的模板方法，而这些模板方法将会调用使用者重写的方法。</p><p>重写同步器指定的方法时，需要使用同步器提供的如下3个方法来访问或修改同步状态。</p><ul><li>getState()：获取当前同步状态。</li><li>setState(int newState)：设置当前同步状态。</li><li>compareAndSetState(int expect,int update)：使用CAS设置当前状态，该方法能够保证状态设置的原子性。</li></ul><p><img src="/2021/11/03/2021-11-03-java-zhong-de-suo/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211102171403.png" alt></p><p><img src="/2021/11/03/2021-11-03-java-zhong-de-suo/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211102171425.png" alt></p><p><img src="/2021/11/03/2021-11-03-java-zhong-de-suo/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211102171538.png" alt></p><p>同步器提供的模板方法基本上分为3类：独占式获取与释放同步状态、共享式获取与释放同步状态和查询同步队列中的等待线程情况。自定义同步组件将使用同步器提供的模板方法来实现自己的同步语义。</p><p>独占锁就是在同一时刻只能有一个线程获取到锁，而其他获取锁的线程只能处于同步队列中等待，只有获取锁的线程释放了锁，后继的线程才能够获取锁</p><h3 id="队列同步器的实现分析"><a href="#队列同步器的实现分析" class="headerlink" title="队列同步器的实现分析"></a>队列同步器的实现分析</h3><h4 id="同步队列"><a href="#同步队列" class="headerlink" title="同步队列"></a>同步队列</h4><p>同步器依赖内部的同步队列（一个FIFO双向队列）来完成同步状态的管理，当前线程获取同步状态失败时，同步器会将当前线程以及等待状态等信息构造成为一个节点（Node）并将其加入同步队列，同时会阻塞当前线程，当同步状态释放时，会把首节点中的线程唤醒，使其再次尝试获取同步状态。</p><p><img src="/2021/11/03/2021-11-03-java-zhong-de-suo/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211104111031.png" alt></p><p>节点是构成同步队列的基础，同步器拥有首节点（head）和尾节点（tail），没有成功获取同步状态的线程将会成为节点加入该队列的尾部</p><p><img src="/2021/11/03/2021-11-03-java-zhong-de-suo/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211104151040.png" alt></p><p>同步器包含了两个节点类型的引用，一个指向头节点，而另一个指向尾节点。试想一下，当一个线程成功地获取了同步状态（或者锁），其他线程将无法获取到同步状态，转而被构造成为节点并加入到同步队列中，而这个加入队列的过程必须要保证线程安全，因此同步器提供了一个基于CAS的设置尾节点的方法：compareAndSetTail(Node expect,Node update)，它需要传递当前线程“认为”的尾节点和当前节点，只有设置成功后，当前节点才正式与之前的尾节点建立关联。</p><p><img src="/2021/11/03/2021-11-03-java-zhong-de-suo/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211104165637.png" alt></p><p>同步队列遵循FIFO，首节点是获取同步状态成功的节点，首节点的线程在释放同步状态时，将会唤醒后继节点，而后继节点将会在获取同步状态成功时将自己设置为首节点</p><p><img src="/2021/11/03/2021-11-03-java-zhong-de-suo/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211104170134.png" alt></p><p>设置首节点是通过获取同步状态成功的线程来完成的，由于只有一个线程能够成功获取到同步状态，因此设置头节点的方法并不需要使用CAS来保证，它只需要将首节点设置成为原首节点的后继节点并断开原首节点的next引用即可。</p><h4 id="独占式同步状态获取与释放"><a href="#独占式同步状态获取与释放" class="headerlink" title="独占式同步状态获取与释放"></a>独占式同步状态获取与释放</h4><p>通过调用同步器的acquire(int arg)方法可以获取同步状态，该方法对中断不敏感，也就是由于线程获取同步状态失败后进入同步队列中，后续对线程进行中断操作时，线程不会从同步队列中移出</p><p><img src="/2021/11/03/2021-11-03-java-zhong-de-suo/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211104172009.png" alt></p><h2 id="重入锁"><a href="#重入锁" class="headerlink" title="重入锁"></a>重入锁</h2><p>重入锁ReentrantLock，顾名思义，就是支持重进入的锁，它表示该锁能够支持一个线程对资源的重复加锁。除此之外，该锁的还支持获取锁时的公平和非公平性选择。</p><p>ReentrantLock虽然没能像synchronized关键字一样支持隐式的重进入，但是在调用lock()方法时，已经获取到锁的线程，能够再次调用lock()方法获取锁而不被阻塞。</p><h3 id="实现重进入"><a href="#实现重进入" class="headerlink" title="实现重进入"></a>实现重进入</h3><p>重进入是指任意线程在获取到锁之后能够再次获取该锁而不会被锁所阻塞，该特性的实现需要解决以下两个问题:</p><ul><li>线程再次获取锁。锁需要去识别获取锁的线程是否为当前占据锁的线程，如果是，则再次成功获取。</li><li>锁的最终释放。线程重复n次获取了锁，随后在第n次释放该锁后，其他线程能够获取到该锁。锁的最终释放要求锁对于获取进行计数自增，计数表示当前锁被重复获取的次数，而锁被释放时，计数自减，当计数等于0时表示锁已经成功释放。</li></ul><h3 id="公平与非公平获取锁的区别"><a href="#公平与非公平获取锁的区别" class="headerlink" title="公平与非公平获取锁的区别"></a>公平与非公平获取锁的区别</h3><p>公平性与否是针对获取锁而言的，如果一个锁是公平的，那么锁的获取顺序就应该符合请求的绝对时间顺序，也就是FIFO。</p><p>对于非公平锁，只要CAS设置同步状态成功，则表示当前线程获取了锁，而公平锁则不同</p><h2 id="读写锁"><a href="#读写锁" class="headerlink" title="读写锁"></a>读写锁</h2><p>之前提到锁（如Mutex和ReentrantLock）基本都是排他锁，这些锁在同一时刻只允许一个线程进行访问，而读写锁在同一时刻可以允许多个读线程访问，但是在写线程访问时，所有的读线程和其他写线程均被阻塞。读写锁维护了一对锁，一个读锁和一个写锁，通过分离读锁和写锁，使得并发性相比一般的排他锁有了很大提升。</p><h2 id="LockSupport工具"><a href="#LockSupport工具" class="headerlink" title="LockSupport工具"></a>LockSupport工具</h2><p>当需要阻塞或唤醒一个线程的时候，都会使用LockSupport工具类来完成相应工作。LockSupport定义了一组的公共静态方法，这些方法提供了最基本的线程阻塞和唤醒功能，而LockSupport也成为构建同步组件的基础工具。</p><p>LockSupport定义了一组以park开头的方法用来阻塞当前线程，以及unpark(Thread thread)方法来唤醒一个被阻塞的线程。</p><p><img src="/2021/11/03/2021-11-03-java-zhong-de-suo/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211108114153.png" alt></p><p>LockSupport增加了park(Object blocker)、parkNanos(Object blocker,long nanos)和parkUntil(Object blocker,long deadline)3个方法，用于实现阻塞当前线程的功能，其中参数blocker是用来标识当前线程在等待的对象（以下称为阻塞对象），该对象主要用于问题排查和系统监控。这是由于在Java 5之前，当线程阻塞（使用synchronized关键字）在一个对象上时，通过线程dump能够查看到该线程的阻塞对象，方便问题定位，而Java 5推出的Lock等并发工具时却遗漏了这一点，致使在线程dump时无法提供阻塞对象的信息。因此，在Java 6中，LockSupport新增了上述3个含有阻塞对象的park方法，用以替代原有的park方法。</p><h2 id="Condition接口"><a href="#Condition接口" class="headerlink" title="Condition接口"></a>Condition接口</h2><p>任意一个Java对象，都拥有一组监视器方法（定义在java.lang.Object上），主要包括wait()、wait(long timeout)、notify()以及notifyAll()方法，这些方法与synchronized同步关键字配合，可以实现等待/通知模式。Condition接口也提供了类似Object的监视器方法，与Lock配合可以实现等待/通知模式，但是这两者在使用方式以及功能特性上还是有差别的。</p><p><img src="/2021/11/03/2021-11-03-java-zhong-de-suo/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211108140634.png" alt></p><p>Condition定义了等待/通知两种类型的方法，当前线程调用这些方法时，需要提前获取到Condition对象关联的锁。Condition对象是由Lock对象（调用Lock对象的newCondition()方法）创建出来的，换句话说，Condition是依赖Lock对象的。</p><p>如示例所示，一般都会将Condition对象作为成员变量。当调用await()方法后，当前线程会<strong>释放锁</strong>并在此等待，而其他线程调用Condition对象的signal()方法，通知当前线程后，当前线程才从await()方法返回，并且在返回前已经获取了锁。</p><h3 id="Condition的实现分析"><a href="#Condition的实现分析" class="headerlink" title="Condition的实现分析"></a>Condition的实现分析</h3><p>ConditionObject是同步器AbstractQueuedSynchronizer的内部类，因为Condition的操作需要获取相关联的锁，所以作为同步器的内部类也较为合理。每个Condition对象都包含着一个队列（以下称为等待队列），该队列是Condition对象实现等待/通知功能的关键。</p><p>等待队列是一个FIFO的队列，在队列中的每个节点都包含了一个线程引用，该线程就是在Condition对象上等待的线程，如果一个线程调用了Condition.await()方法，那么该线程将会释放锁、构造成节点加入等待队列并进入等待状态。事实上，节点的定义复用了同步器中节点的定义，也就是说，同步队列和等待队列中节点类型都是同步器的静态内部类AbstractQueuedSynchronizer.Node。</p><p>一个Condition包含一个等待队列，Condition拥有首节点（firstWaiter）和尾节点（lastWaiter）。当前线程调用Condition.await()方法，将会以当前线程构造节点，并将节点从尾部加入等待队列</p><p><img src="/2021/11/03/2021-11-03-java-zhong-de-suo/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211108144637.png" alt></p><p>Condition拥有首尾节点的引用，而新增节点只需要将原有的尾节点nextWaiter指向它，并且更新尾节点即可。上述节点引用更新的过程并没有使用CAS保证，原因在于调用await()方法的线程必定是获取了锁的线程，也就是说该过程是由锁来保证线程安全的。</p><p>在Object的监视器模型上，一个对象拥有一个同步队列和等待队列，而并发包中的Lock（更确切地说是同步器）拥有一个同步队列和多个等待队列</p><p><img src="/2021/11/03/2021-11-03-java-zhong-de-suo/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211108153450.png" alt></p><p>Condition的实现是同步器的内部类，因此每个Condition实例都能够访问同步器提供的方法，相当于每个Condition都拥有所属同步器的引用。</p><h3 id="等待"><a href="#等待" class="headerlink" title="等待"></a>等待</h3><p>调用Condition的await()方法（或者以await开头的方法），会使当前线程进入等待队列并释放锁，同时线程状态变为等待状态。当从await()方法返回时，当前线程一定获取了Condition相关联的锁。</p><p>如果从队列（同步队列和等待队列）的角度看await()方法，当调用await()方法时，相当于同步队列的首节点（获取了锁的节点）移动到Condition的等待队列中。</p><h3 id="通知"><a href="#通知" class="headerlink" title="通知"></a>通知</h3><p>调用Condition的signal()方法，将会唤醒在等待队列中等待时间最长的节点（首节点），在唤醒节点之前，会将节点移到同步队列中。</p>]]></content>
      
      
      <categories>
          
          <category> JAVA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 并发编程的艺术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java并发编程线程</title>
      <link href="/2021/11/01/2021-11-01-java-bing-fa-bian-cheng-xian-cheng/"/>
      <url>/2021/11/01/2021-11-01-java-bing-fa-bian-cheng-xian-cheng/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《Java并发编程的艺术》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="线程简介"><a href="#线程简介" class="headerlink" title="线程简介"></a>线程简介</h2><h3 id="什么是线程"><a href="#什么是线程" class="headerlink" title="什么是线程"></a>什么是线程</h3><p>操作系统在运行一个程序时，会为其创建一个进程。例如，启动一个Java程序，操作系统就会创建一个Java进程。现代操作系统调度的最小单元是线程，也叫轻量级进程（Light Weight Process），在一个进程里可以创建多个线程，这些线程都拥有各自的计数器、堆栈和局部变量等属性，并且能够访问共享的内存变量。处理器在这些线程上高速切换，让使用者感觉到这些线程在同时执行。</p><h3 id="为什么要使用多线程"><a href="#为什么要使用多线程" class="headerlink" title="为什么要使用多线程"></a>为什么要使用多线程</h3><ul><li>更多的处理器核心</li></ul><p>线程是大多数操作系统调度的基本单元，一个程序作为一个进程来运行，程序运行过程中能够创建多个线程，而一个线程在一个时刻只能运行在一个处理器核心上。试想一下，一个单线程程序在运行时只能使用一个处理器核心，那么再多的处理器核心加入也无法显著提升该程序的执行效率。相反，如果该程序使用多线程技术，将计算逻辑分配到多个处理器核心上，就会显著减少程序的处理时间，并且随着更多处理器核心的加入而变得更有效率。</p><ul><li>更快的响应时间</li></ul><p>以使用多线程技术，即将数据一致性不强的操作派发给其他线程处理（也可以使用消息队列），如生成订单快照、发送邮件等。这样做的好处是响应用户请求的线程能够尽可能快地处理完成，缩短了响应时间，提升了用户体验。</p><h3 id="线程优先级"><a href="#线程优先级" class="headerlink" title="线程优先级"></a>线程优先级</h3><p>现代操作系统基本采用时分的形式调度运行的线程，操作系统会分出一个个时间片，线程会分配到若干时间片，当线程的时间片用完了就会发生线程调度，并等待着下次分配。线程分配到的时间片多少也就决定了线程使用处理器资源的多少，而线程优先级就是决定线程需要多或者少分配一些处理器资源的线程属性。</p><p>在Java线程中，通过一个整型成员变量priority来控制优先级，优先级的范围从1~10，在线程构建的时候可以通过setPriority(int)方法来修改优先级，默认优先级是5，优先级高的线程分配时间片的数量要多于优先级低的线程。</p><h3 id="线程的状态"><a href="#线程的状态" class="headerlink" title="线程的状态"></a>线程的状态</h3><p>Java线程在运行的生命周期中可能处于6种不同的状态，在给定的一个时刻，线程只能处于其中的一个状态。</p><p><img src="/2021/11/01/2021-11-01-java-bing-fa-bian-cheng-xian-cheng/%E6%9C%AA%E5%91%BD%E5%90%8D1635821483.png" alt></p><h3 id="Daemon线程"><a href="#Daemon线程" class="headerlink" title="Daemon线程"></a>Daemon线程</h3><p>Daemon线程是一种支持型线程，因为它主要被用作程序中后台调度以及支持性工作。这意味着，当一个Java虚拟机中不存在非Daemon线程的时候，Java虚拟机将会退出。可以通过调用Thread.setDaemon(true)将线程设置为Daemon线程。</p><blockquote><p>Daemon属性需要在启动线程之前设置，不能在启动线程之后设置。</p></blockquote><p>Daemon线程被用作完成支持性工作，但是在Java虚拟机退出时Daemon线程中的finally块并不一定会执行，在构建Daemon线程时，不能依靠finally块中的内容来确保执行关闭或清理资源的逻辑。</p><h3 id="构造线程"><a href="#构造线程" class="headerlink" title="构造线程"></a>构造线程</h3><p>在运行线程之前首先要构造一个线程对象，线程对象在构造的时候需要提供线程所需要的属性，如线程所属的线程组、线程优先级、是否是Daemon线程等信息。</p><h3 id="启动线程"><a href="#启动线程" class="headerlink" title="启动线程"></a>启动线程</h3><p>线程对象在初始化完成之后，调用start()方法就可以启动这个线程。线程start()方法的含义是：当前线程（即parent线程）同步告知Java虚拟机，只要线程规划器空闲，应立即启动调用start()方法的线程。</p><p>启动一个线程前，最好为这个线程设置线程名称，因为这样在使用jstack分析程序或者进行问题排查时，就会给开发人员提供一些提示，自定义的线程最好能够起个名字。</p><h3 id="理解中断"><a href="#理解中断" class="headerlink" title="理解中断"></a>理解中断</h3><p>中断可以理解为线程的一个标识位属性，它表示一个运行中的线程是否被其他线程进行了中断操作。中断好比其他线程对该线程打了个招呼，其他线程通过调用该线程的interrupt()方法对其进行中断操作。</p><h3 id="过期的suspend-、resume-和stop"><a href="#过期的suspend-、resume-和stop" class="headerlink" title="过期的suspend()、resume()和stop()"></a>过期的suspend()、resume()和stop()</h3><p>suspend()、resume()和stop()方法完成了线程的暂停、恢复和终止工作，但是这些API是过期的，也就是不建议使用的。</p><p>不建议使用的原因主要有：以suspend()方法为例，在调用后，线程不会释放已经占有的资源（比如锁），而是占有着资源进入睡眠状态，这样容易引发死锁问题。同样，stop()方法在终结一个线程时不会保证线程的资源正常释放，通常是没有给予线程完成资源释放工作的机会，因此会导致程序可能工作在不确定状态下。</p><p>正因为suspend()、resume()和stop()方法带来的副作用，这些方法才被标注为不建议使用的过期方法，而暂停和恢复操作可以用后面提到的等待/通知机制来替代。</p><h2 id="线程间通信"><a href="#线程间通信" class="headerlink" title="线程间通信"></a>线程间通信</h2><h3 id="volatile和synchronized关键字"><a href="#volatile和synchronized关键字" class="headerlink" title="volatile和synchronized关键字"></a>volatile和synchronized关键字</h3><p>Java支持多个线程同时访问一个对象或者对象的成员变量，由于每个线程可以拥有这个变量的拷贝（虽然对象以及成员变量分配的内存是在共享内存中的，但是每个执行的线程还是可以拥有一份拷贝，这样做的目的是加速程序的执行，这是现代多核处理器的一个显著特性），所以程序在执行过程中，一个线程看到的变量并不一定是最新的。</p><p>关键字volatile可以用来修饰字段（成员变量），就是告知程序任何对该变量的访问均需要从共享内存中获取，而对它的改变必须同步刷新回共享内存，它能保证所有线程对变量访问的可见性。</p><p>关键字synchronized可以修饰方法或者以同步块的形式来进行使用，它主要确保多个线程在同一个时刻，只能有一个线程处于方法或者同步块中，它保证了线程对变量访问的可见性和排他性。</p><p>任意一个对象都拥有自己的监视器，当这个对象由同步块或者这个对象的同步方法调用时，执行方法的线程必须先获取到该对象的监视器才能进入同步块或者同步方法，而没有获取到监视器（执行该方法）的线程将会被阻塞在同步块和同步方法的入口处，进入BLOCKED状态。</p><p><img src="/2021/11/01/2021-11-01-java-bing-fa-bian-cheng-xian-cheng/%E6%9C%AA%E5%91%BD%E5%90%8D1635839946.png" alt></p><p>任意线程对Object（Object由synchronized保护）的访问，首先要获得Object的监视器。如果获取失败，线程进入同步队列，线程状态变为BLOCKED。当访问Object的前驱（获得了锁的线程）释放了锁，则该释放操作唤醒阻塞在同步队列中的线程，使其重新尝试对监视器的获取。</p><h3 id="等待-通知机制"><a href="#等待-通知机制" class="headerlink" title="等待/通知机制"></a>等待/通知机制</h3><p><img src="/2021/11/01/2021-11-01-java-bing-fa-bian-cheng-xian-cheng/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211102160201.png" alt></p><p>等待/通知机制，是指一个线程A调用了对象O的wait()方法进入等待状态，而另一个线程B调用了对象O的notify()或者notifyAll()方法，线程A收到通知后从对象O的wait()方法返回，进而执行后续操作。上述两个线程通过对象O来完成交互，而对象上的wait()和notify/notifyAll()的关系就如同开关信号一样，用来完成等待方和通知方之间的交互工作。</p><p>调用wait()、notify()以及notifyAll()时需要注意的细节:</p><ul><li>使用wait()、notify()和notifyAll()时需要先对调用对象加锁。</li><li>调用wait()方法后，线程状态由RUNNING变为WAITING，并将当前线程放置到对象的等待队列。</li><li>notify()或notifyAll()方法调用后，等待线程依旧不会从wait()返回，需要调用notify()或notifAll()的线程释放锁之后，等待线程才有机会从wait()返回。</li><li>notify()方法将等待队列中的一个等待线程从等待队列中移到同步队列中，而notifyAll()方法则是将等待队列中所有的线程全部移到同步队列，被移动的线程状态由WAITING变为BLOCKED。</li><li>从wait()方法返回的前提是获得了调用对象的锁。</li></ul><p>WaitThread首先获取了对象的锁，然后调用对象的wait()方法，从而放弃了锁并进入了对象的等待队列WaitQueue中，进入等待状态。由于WaitThread释放了对象的锁，NotifyThread随后获取了对象的锁，并调用对象的notify()方法，将WaitThread从WaitQueue移到SynchronizedQueue中，此时WaitThread的状态变为阻塞状态。NotifyThread释放了锁之后，WaitThread再次获取到锁并从wait()方法返回继续执行。</p><h3 id="等待-通知的经典范式"><a href="#等待-通知的经典范式" class="headerlink" title="等待/通知的经典范式"></a>等待/通知的经典范式</h3><p>等待方遵循如下原则:</p><ul><li>获取对象的锁。</li><li>如果条件不满足，那么调用对象的wait()方法，被通知后仍要检查条件。</li><li>条件满足则执行对应的逻辑。</li></ul><pre><code>synchronized(对象) {    while(条件不满足) {        对象.wait();    }    对应的处理逻辑}</code></pre><p>通知方遵循如下原则:</p><ul><li>获得对象的锁。</li><li>改变条件。</li><li>通知所有等待在对象上的线程。</li></ul><pre><code>synchronized(对象) {    改变条件    对象.notifyAll();}</code></pre><h3 id="Thread-join-的使用"><a href="#Thread-join-的使用" class="headerlink" title="Thread.join()的使用"></a>Thread.join()的使用</h3><p>如果一个线程A执行了thread.join()语句，其含义是：当前线程A等待thread线程终止之后才从thread.join()返回。线程Thread除了提供join()方法之外，还提供了join(long millis)和join(long millis,int nanos)两个具备超时特性的方法。这两个超时方法表示，如果线程thread在给定的超时时间里没有终止，那么将会从该超时方法中返回。</p><p>每个线程终止的前提是前驱线程的终止，每个线程等待前驱线程终止后，才从join()方法返回，这里涉及了等待/通知机制（等待前驱线程结束，接收前驱线程结束通知）。</p><h3 id="ThreadLocal的使用"><a href="#ThreadLocal的使用" class="headerlink" title="ThreadLocal的使用"></a>ThreadLocal的使用</h3><p>ThreadLocal，即线程变量，是一个以ThreadLocal对象为键、任意对象为值的存储结构。这个结构被附带在线程上，也就是说一个线程可以根据一个ThreadLocal对象查询到绑定在这个线程上的一个值。可以通过set(T)方法来设置一个值，在当前线程下再通过get()方法获取到原先设置的值。</p>]]></content>
      
      
      <categories>
          
          <category> JAVA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 并发编程的艺术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java内存模型</title>
      <link href="/2021/10/30/2021-10-30-java-nei-cun-mo-xing/"/>
      <url>/2021/10/30/2021-10-30-java-nei-cun-mo-xing/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《Java并发编程的艺术》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="Java内存模型的基础"><a href="#Java内存模型的基础" class="headerlink" title="Java内存模型的基础"></a>Java内存模型的基础</h2><h3 id="并发编程模型的两个关键问题"><a href="#并发编程模型的两个关键问题" class="headerlink" title="并发编程模型的两个关键问题"></a>并发编程模型的两个关键问题</h3><p>在并发编程中，需要处理两个关键问题：线程之间如何通信及线程之间如何同步（这里的线程是指并发执行的活动实体）。通信是指线程之间以何种机制来交换信息。在命令式编程中，线程之间的通信机制有两种：共享内存和消息传递。</p><p>在共享内存的并发模型里，线程之间共享程序的公共状态，通过写-读内存中的公共状态进行隐式通信。在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过发送消息来显式进行通信。</p><p>同步是指程序中用于控制不同线程间操作发生相对顺序的机制。在共享内存并发模型里，同步是显式进行的。程序员必须显式指定某个方法或某段代码需要在线程之间互斥执行。在消息传递的并发模型里，由于消息的发送必须在消息的接收之前，因此同步是隐式进行的。</p><h3 id="Java内存模型的抽象结构"><a href="#Java内存模型的抽象结构" class="headerlink" title="Java内存模型的抽象结构"></a>Java内存模型的抽象结构</h3><p>在Java中，所有实例域、静态域和数组元素都存储在堆内存中，堆内存在线程之间共享（本章用“共享变量”这个术语代指实例域，静态域和数组元素）。局部变量（Local Variables），方法定义参数（Java语言规范称之为Formal Method Parameters）和异常处理器参数（Exception Handler Parameters）不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。</p><p>Java线程之间的通信由Java内存模型（本文简称为JMM）控制，JMM决定一个线程对共享变量的写入何时对另一个线程可见。从抽象的角度来看，JMM定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存（Main Memory）中，每个线程都有一个私有的本地内存（Local Memory），本地内存中存储了该线程以读/写共享变量的副本。本地内存是JMM的一个抽象概念，并不真实存在。它涵盖了缓存、写缓冲区、寄存器以及其他的硬件和编译器优化。</p><p><img src="/2021/10/30/2021-10-30-java-nei-cun-mo-xing/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211028164209.png" alt></p><p>线程A与线程B之间要通信的话，必须要经历下面2个步骤:</p><ol><li>线程A把本地内存A中更新过的共享变量刷新到主内存中去。</li><li>线程B到主内存中去读取线程A之前已更新过的共享变量。</li></ol><p><img src="/2021/10/30/2021-10-30-java-nei-cun-mo-xing/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211028164319.png" alt></p><p>本地内存A和本地内存B由主内存中共享变量x的副本。假设初始时，这3个内存中的x值都为0。线程A在执行时，把更新后的x值（假设值为1）临时存放在自己的本地内存A中。当线程A和线程B需要通信时，线程A首先会把自己本地内存中修改后的x值刷新到主内存中，此时主内存中的x值变为了1。随后，线程B到主内存中去读取线程A更新后的x值，此时线程B的本地内存的x值也变为了1。</p><h3 id="从源代码到指令序列的重排序"><a href="#从源代码到指令序列的重排序" class="headerlink" title="从源代码到指令序列的重排序"></a>从源代码到指令序列的重排序</h3><p>在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排序。重排序分3种类型。</p><ul><li>编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。</li><li>指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism，ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。</li><li>内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。</li></ul><p><img src="/2021/10/30/2021-10-30-java-nei-cun-mo-xing/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211028164650.png" alt></p><p>上述的1属于编译器重排序，2和3属于处理器重排序。</p><p><img src="/2021/10/30/2021-10-30-java-nei-cun-mo-xing/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211028172802.png" alt></p><p>StoreLoad Barriers是一个“全能型”的屏障，它同时具有其他3个屏障的效果。</p><h3 id="happens-before简介"><a href="#happens-before简介" class="headerlink" title="happens-before简介"></a>happens-before简介</h3><p>在JMM中，如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须要存在happens-before关系。这里提到的两个操作既可以是在一个线程之内，也可以是在不同线程之间。</p><ul><li>程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作。</li><li>监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。</li><li>volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。</li><li>传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。</li></ul><p>两个操作之间具有happens-before关系，并不意味着前一个操作必须要在后一个操作之前执行！happens-before仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前（the first is visible to and ordered before the second）。</p><h2 id="重排序"><a href="#重排序" class="headerlink" title="重排序"></a>重排序</h2><p>重排序是指编译器和处理器为了优化程序性能而对指令序列进行重新排序的一种手段。</p><h3 id="数据依赖性"><a href="#数据依赖性" class="headerlink" title="数据依赖性"></a>数据依赖性</h3><p>如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。</p><p><img src="/2021/10/30/2021-10-30-java-nei-cun-mo-xing/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211028175208.png" alt></p><p>上面3种情况，只要重排序两个操作的执行顺序，程序的执行结果就会被改变。</p><p>编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序。</p><h3 id="as-if-serial语义"><a href="#as-if-serial语义" class="headerlink" title="as-if-serial语义"></a>as-if-serial语义</h3><p>as-if-serial语义的意思是：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器、runtime和处理器都必须遵守as-if-serial语义。</p><p>为了遵守as-if-serial语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是，如果操作之间不存在数据依赖关系，这些操作就可能被编译器和处理器重排序。</p><h3 id="程序顺序规则"><a href="#程序顺序规则" class="headerlink" title="程序顺序规则"></a>程序顺序规则</h3><ul><li>A　happens-before B。</li><li>B　happens-before C。</li><li>A　happens-before C。</li></ul><p>这里的第3个happens-before关系，是根据happens-before的传递性推导出来的。</p><h2 id="顺序一致性"><a href="#顺序一致性" class="headerlink" title="顺序一致性"></a>顺序一致性</h2><p>顺序一致性内存模型是一个理论参考模型，在设计的时候，处理器的内存模型和编程语言的内存模型都会以顺序一致性内存模型作为参照。</p><h3 id="数据竞争与顺序一致性"><a href="#数据竞争与顺序一致性" class="headerlink" title="数据竞争与顺序一致性"></a>数据竞争与顺序一致性</h3><p>当程序未正确同步时，就可能会存在数据竞争。Java内存模型规范对数据竞争的定义如下。</p><p>在一个线程中写一个变量，在另一个线程读同一个变量，而且写和读没有通过同步来排序。</p><p>当代码中包含数据竞争时，程序的执行往往产生违反直觉的结果。如果一个多线程程序能正确同步，这个程序将是一个没有数据竞争的程序。</p><p>JMM对正确同步的多线程程序的内存一致性做了如下保证。</p><p>如果程序是正确同步的，程序的执行将具有顺序一致性（Sequentially Consistent）——即程序的执行结果与该程序在顺序一致性内存模型中的执行结果相同。马上我们就会看到，这对于程序员来说是一个极强的保证。这里的同步是指广义上的同步，包括对常用同步原语（synchronized、volatile和final）的正确使用。</p><h3 id="顺序一致性内存模型"><a href="#顺序一致性内存模型" class="headerlink" title="顺序一致性内存模型"></a>顺序一致性内存模型</h3><p>顺序一致性内存模型有两大特性。</p><ul><li>一个线程中的所有操作必须按照程序的顺序来执行。</li><li>（不管程序是否同步）所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。</li></ul><p><img src="/2021/10/30/2021-10-30-java-nei-cun-mo-xing/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211029102733.png" alt></p><p>在概念上，顺序一致性模型有一个单一的全局内存，这个内存通过一个左右摆动的开关可以连接到任意一个线程，同时每一个线程必须按照程序的顺序来执行内存读/写操作。从上面的示意图可以看出，在任意时间点最多只能有一个线程可以连接到内存。当多个线程并发执行时，图中的开关装置能把所有线程的所有内存读/写操作串行化</p><p>但是，在JMM中就没有这个保证。未同步程序在JMM中不但整体的执行顺序是无序的，而且所有线程看到的操作执行顺序也可能不一致。比如，在当前线程把写过的数据缓存在本地内存中，在没有刷新到主内存之前，这个写操作仅对当前线程可见；从其他线程的角度来观察，会认为这个写操作根本没有被当前线程执行。只有当前线程把本地内存中写过的数据刷新到主内存之后，这个写操作才能对其他线程可见。在这种情况下，当前线程和其他线程看到的操作执行顺序将不一致。</p><h2 id="volatile的内存语义"><a href="#volatile的内存语义" class="headerlink" title="volatile的内存语义"></a>volatile的内存语义</h2><ul><li>可见性：对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。</li><li>原子性：对任意单个volatile变量的读/写具有原子性，但类似于volatile++这种复合操作不具有原子性。</li></ul><p>当第二个操作是volatile写时，不管第一个操作是什么，都不能重排序。这个规则确保volatile写之前的操作不会被编译器重排序到volatile写之后。</p><p>当第一个操作是volatile读时，不管第二个操作是什么，都不能重排序。这个规则确保volatile读之后的操作不会被编译器重排序到volatile读之前。</p><p>当第一个操作是volatile写，第二个操作是volatile读时，不能重排序。</p><p>为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。对于编译器来说，发现一个最优布置来最小化插入屏障的总数几乎不可能。为此，JMM采取保守策略。下面是基于保守策略的JMM内存屏障插入策略。</p><ul><li>在每个volatile写操作的前面插入一个StoreStore屏障。</li><li>在每个volatile写操作的后面插入一个StoreLoad屏障。</li><li>在每个volatile读操作的后面插入一个LoadLoad屏障。</li><li>在每个volatile读操作的后面插入一个LoadStore屏障。</li></ul><p><img src="/2021/10/30/2021-10-30-java-nei-cun-mo-xing/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211029153002.png" alt></p><p><img src="/2021/10/30/2021-10-30-java-nei-cun-mo-xing/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211029153634.png" alt></p><h2 id="锁的内存语义"><a href="#锁的内存语义" class="headerlink" title="锁的内存语义"></a>锁的内存语义</h2><p>锁除了让临界区互斥执行外，还可以让释放锁的线程向获取同一个锁的线程发送消息。</p><p>当线程释放锁时，JMM会把该线程对应的本地内存中的共享变量刷新到主内存中。</p><p>当线程获取锁时，JMM会把该线程对应的本地内存置为无效。从而使得被监视器保护的临界区代码必须从主内存中读取共享变量。</p><p><img src="/2021/10/30/2021-10-30-java-nei-cun-mo-xing/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211029155454.png" alt></p><p>锁释放与volatile写有相同的内存语义；锁获取与volatile读有相同的内存语义。</p><h2 id="happens-before"><a href="#happens-before" class="headerlink" title="happens-before"></a>happens-before</h2><ul><li>程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作。</li><li>监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。</li><li>volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。</li><li>传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。</li><li>start()规则：如果线程A执行操作ThreadB.start()（启动线程B），那么A线程的ThreadB.start()操作happens-before于线程B中的任意操作。</li><li>join()规则：如果线程A执行操作ThreadB.join()并成功返回，那么线程B中的任意操作happens-before于线程A从ThreadB.join()操作成功返回。</li></ul>]]></content>
      
      
      <categories>
          
          <category> JAVA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 并发编程的艺术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>并发机制的底层实现原理</title>
      <link href="/2021/10/28/2021-10-28-bing-fa-ji-zhi-de-di-ceng-shi-xian-yuan-li/"/>
      <url>/2021/10/28/2021-10-28-bing-fa-ji-zhi-de-di-ceng-shi-xian-yuan-li/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《Java并发编程的艺术》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><p>Java代码在编译后会变成Java字节码，字节码被类加载器加载到JVM里，JVM执行字节码，最终需要转化为汇编指令在CPU上执行，Java中所使用的并发机制依赖于JVM的实现和CPU的指令。</p><h2 id="volatile的应用"><a href="#volatile的应用" class="headerlink" title="volatile的应用"></a>volatile的应用</h2><p>volatile是轻量级的synchronized，它在多处理器开发中保证了共享变量的“可见性”和避免了指令重排。可见性的意思是当一个线程修改一个共享变量时，另外一个线程能读到这个修改的值。如果volatile变量修饰符使用恰当的话，它比synchronized的使用和执行成本更低，因为它不会引起线程上下文的切换和调度。避免指令重排主要是靠内存屏障来实现。</p><h3 id="volatile的定义与实现原理"><a href="#volatile的定义与实现原理" class="headerlink" title="volatile的定义与实现原理"></a>volatile的定义与实现原理</h3><p>Java语言提供了volatile，在某些情况下比锁要更加方便。如果一个字段被声明成volatile，Java线程内存模型确保所有线程看到这个变量的值是一致的。</p><p>有volatile变量修饰的共享变量进行写操作的时候:</p><ol><li>将当前处理器缓存行的数据写回到系统内存。</li><li>这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效。</li></ol><p>为了提高处理速度，处理器不直接和内存进行通信，而是先将系统内存的数据读到内部缓存（L1，L2或其他）后再进行操作，但操作完不知道何时会写到内存。如果对声明了volatile的变量进行写操作，JVM就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写回到系统内存。但是，就算写回到内存，如果其他处理器缓存的值还是旧的，再执行计算操作就会有问题。所以，在多处理器下，为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存里。</p><h2 id="synchronized的实现原理与应用"><a href="#synchronized的实现原理与应用" class="headerlink" title="synchronized的实现原理与应用"></a>synchronized的实现原理与应用</h2><p>Java中的每一个对象都可以作为锁。具体表现为以下3种形式:</p><ul><li>对于普通同步方法，锁是当前实例对象。</li><li>对于静态同步方法，锁是当前类的Class对象。</li><li>对于同步方法块，锁是Synchonized括号里配置的对象。</li></ul><p>当一个线程试图访问同步代码块时，它首先必须得到锁，退出或抛出异常时必须释放锁。</p><p>JVM基于进入和退出Monitor对象来实现方法同步和代码块同步，但两者的实现细节不一样。代码块同步是使用monitorenter和monitorexit指令实现的，而方法同步是使用另外一种方式实现的，细节在JVM规范里并没有详细说明。但是，方法的同步同样可以使用这两个指令来实现。</p><p>monitorenter指令是在编译后插入到同步代码块的开始位置，而monitorexit是插入到方法结束处和异常处，JVM要保证每个monitorenter必须有对应的monitorexit与之配对。任何对象都有一个monitor与之关联，当且一个monitor被持有后，它将处于锁定状态。线程执行到monitorenter指令时，将会尝试获取对象所对应的monitor的所有权，即尝试获得对象的锁。</p><h3 id="Java对象头"><a href="#Java对象头" class="headerlink" title="Java对象头"></a>Java对象头</h3><p>synchronized用的锁是存在Java对象头里的。如果对象是数组类型，则虚拟机用3个字宽（Word）存储对象头，如果对象是非数组类型，则用2字宽存储对象头。在32位虚拟机中，1字宽等于4字节，即32bit</p><p>Java对象头里的Mark Word里默认存储对象的HashCode、分代年龄和锁标记位。</p><p><img src="/2021/10/28/2021-10-28-bing-fa-ji-zhi-de-di-ceng-shi-xian-yuan-li/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211028104134.png" alt></p><p>在运行期间，Mark Word里存储的数据会随着锁标志位的变化而变化。Mark Word可能变化为存储以下4种数据</p><p><img src="/2021/10/28/2021-10-28-bing-fa-ji-zhi-de-di-ceng-shi-xian-yuan-li/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211028104424.png" alt></p><h3 id="锁的升级与对比"><a href="#锁的升级与对比" class="headerlink" title="锁的升级与对比"></a>锁的升级与对比</h3><p>锁一共有4种状态，级别从低到高依次是：无锁状态、偏向锁状态、轻量级锁状态和重量级锁状态，这几个状态会随着竞争情况逐渐升级。锁可以升级但不能降级，意味着偏向锁升级成轻量级锁后不能降级成偏向锁。<strong>这种锁升级却不能降级的策略，目的是为了提高获得锁和释放锁的效率</strong></p><h4 id="偏向锁"><a href="#偏向锁" class="headerlink" title="偏向锁"></a>偏向锁</h4><p>大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低而引入了偏向锁。当一个线程访问同步块并获取锁时，会在对象头和栈帧中的锁记录里存储锁偏向的线程ID，以后该线程在进入和退出同步块时不需要进行CAS操作来加锁和解锁，只需简单地测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁。如果测试成功，表示线程已经获得了锁。如果测试失败，则需要再测试一下Mark Word中偏向锁的标识是否设置成1（表示当前是偏向锁）：如果没有设置，则使用CAS竞争锁；如果设置了，则尝试使用CAS将对象头的偏向锁指向当前线程。</p><ul><li>偏向锁的撤销</li></ul><p>偏向锁使用了一种等到竞争出现才释放锁的机制，所以当其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有正在执行的字节码）。它会首先暂停拥有偏向锁的线程，然后检查持有偏向锁的线程是否活着，如果线程不处于活动状态，则将对象头设置成无锁状态；如果线程仍然活着，拥有偏向锁的栈会被执行。遍历偏向对象的锁记录，栈中的锁记录和对象头的Mark Word要么重新偏向于其他线程，要么恢复到无锁或者标记对象不适合作为偏向锁，最后唤醒暂停的线程。</p><p><img src="/2021/10/28/2021-10-28-bing-fa-ji-zhi-de-di-ceng-shi-xian-yuan-li/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211028105754.png" alt></p><h4 id="轻量级锁"><a href="#轻量级锁" class="headerlink" title="轻量级锁"></a>轻量级锁</h4><ul><li>轻量级锁加锁</li></ul><p>线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中，然后线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。</p><ul><li>轻量级锁解锁</li></ul><p>轻量级解锁时，会使用原子的CAS操作将Displaced Mark Word替换回到对象头，如果成功，则表示没有竞争发生。如果失败，表示当前锁存在竞争，锁就会膨胀成重量级锁。</p><p><img src="/2021/10/28/2021-10-28-bing-fa-ji-zhi-de-di-ceng-shi-xian-yuan-li/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211028110652.png" alt></p><p>因为自旋会消耗CPU，为了避免无用的自旋（比如获得锁的线程被阻塞住了），一旦锁升级成重量级锁，就不会再恢复到轻量级锁状态。当锁处于这个状态下，其他线程试图获取锁时，都会被阻塞住，当持有锁的线程释放锁之后会唤醒这些线程，被唤醒的线程就会进行新一轮的夺锁之争。</p><h4 id="锁的优缺点对比"><a href="#锁的优缺点对比" class="headerlink" title="锁的优缺点对比"></a>锁的优缺点对比</h4><p><img src="/2021/10/28/2021-10-28-bing-fa-ji-zhi-de-di-ceng-shi-xian-yuan-li/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211028112307.png" alt></p><h2 id="原子操作的实现原理"><a href="#原子操作的实现原理" class="headerlink" title="原子操作的实现原理"></a>原子操作的实现原理</h2><h3 id="使用总线锁保证原子性"><a href="#使用总线锁保证原子性" class="headerlink" title="使用总线锁保证原子性"></a>使用总线锁保证原子性</h3><p>第一个机制是通过总线锁保证原子性。如果多个处理器同时对共享变量进行读改写操作（i++就是经典的读改写操作），那么共享变量就会被多个处理器同时进行操作，这样读改写操作就不是原子的，操作完之后共享变量的值会和期望的不一致。举个例子，如果i=1，我们进行两次i++操作，我们期望的结果是3，但是有可能结果是2</p><p><img src="/2021/10/28/2021-10-28-bing-fa-ji-zhi-de-di-ceng-shi-xian-yuan-li/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211028161047.png" alt></p><p>原因可能是多个处理器同时从各自的缓存中读取变量i，分别进行加1操作，然后分别写入系统内存中。那么，想要保证读改写共享变量的操作是原子的，就必须保证CPU1读改写共享变量的时候，CPU2不能操作缓存了该共享变量内存地址的缓存。</p><p>处理器使用总线锁就是来解决这个问题的。所谓总线锁就是使用处理器提供的一个LOCK＃信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住，那么该处理器可以独占共享内存。</p><h3 id="使用缓存锁保证原子性"><a href="#使用缓存锁保证原子性" class="headerlink" title="使用缓存锁保证原子性"></a>使用缓存锁保证原子性</h3><p>第二个机制是通过缓存锁定来保证原子性。在同一时刻，我们只需保证对某个内存地址的操作是原子性即可，但总线锁定把CPU和内存之间的通信锁住了，这使得锁定期间，其他处理器不能操作其他内存地址的数据，所以总线锁定的开销比较大，目前处理器在某些场合下使用缓存锁定代替总线锁定来进行优化。</p><h3 id="Java如何实现原子操作"><a href="#Java如何实现原子操作" class="headerlink" title="Java如何实现原子操作"></a>Java如何实现原子操作</h3><p>在Java中可以通过锁和循环CAS的方式来实现原子操作。</p><h4 id="使用循环CAS实现原子操作"><a href="#使用循环CAS实现原子操作" class="headerlink" title="使用循环CAS实现原子操作"></a>使用循环CAS实现原子操作</h4><p>JVM中的CAS操作正是利用了处理器提供的CMPXCHG指令实现的。自旋CAS实现的基本思路就是循环进行CAS操作直到成功为止</p><p>从Java 1.5开始，JDK的并发包里提供了一些类来支持原子操作，如AtomicBoolean（用原子方式更新的boolean值）、AtomicInteger（用原子方式更新的int值）和AtomicLong（用原子方式更新的long值）。这些原子包装类还提供了有用的工具方法，比如以原子的方式将当前值自增1和自减1。</p><p>CAS实现原子操作的三大问题：ABA问题，循环时间长开销大，以及只能保证一个共享变量的原子操作。</p><ul><li>ABA问题。</li></ul><p>因为CAS需要在操作值的时候，检查值有没有发生变化，如果没有发生变化则更新，但是如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值没有发生变化，但是实际上却变化了。ABA问题的解决思路就是使用版本号。在变量前面追加上版本号，每次变量更新的时候把版本号加1，那么A→B→A就会变成1A→2B→3A。从Java 1.5开始，JDK的Atomic包里提供了一个类AtomicStampedReference来解决ABA问题。这个类的compareAndSet方法的作用是首先检查当前引用是否等于预期引用，并且检查当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。</p><ul><li>循环时间长开销大</li></ul><p>自旋CAS如果长时间不成功，会给CPU带来非常大的执行开销。如果JVM能支持处理器提供的pause指令，那么效率会有一定的提升。pause指令有两个作用：第一，它可以延迟流水线执行指令（de-pipeline），使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零；第二，它可以避免在退出循环的时候因内存顺序冲突（Memory Order Violation）而引起CPU流水线被清空（CPU Pipeline Flush），从而提高CPU的执行效率。</p><ul><li>只能保证一个共享变量的原子操作。</li></ul><p>当对一个共享变量执行操作时，我们可以使用循环CAS的方式来保证原子操作，但是对多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候就可以用锁。还有一个取巧的办法，就是把多个共享变量合并成一个共享变量来操作。比如，有两个共享变量i＝2，j=a，合并一下ij=2a，然后用CAS来操作ij。从Java 1.5开始，JDK提供了AtomicReference类来保证引用对象之间的原子性，就可以把多个变量放在一个对象里来进行CAS操作。</p><h4 id="使用锁机制实现原子操作"><a href="#使用锁机制实现原子操作" class="headerlink" title="使用锁机制实现原子操作"></a>使用锁机制实现原子操作</h4><p>锁机制保证了只有获得锁的线程才能够操作锁定的内存区域。JVM内部实现了很多种锁机制，有偏向锁、轻量级锁和互斥锁。有意思的是除了偏向锁，JVM实现锁的方式都用了循环CAS，即当一个线程想进入同步块的时候使用循环CAS的方式来获取锁，当它退出同步块的时候使用循环CAS释放锁。</p>]]></content>
      
      
      <categories>
          
          <category> JAVA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 并发编程的艺术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>并发编程的挑战</title>
      <link href="/2021/10/27/2021-10-27-bing-fa-bian-cheng-de-tiao-zhan/"/>
      <url>/2021/10/27/2021-10-27-bing-fa-bian-cheng-de-tiao-zhan/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《Java并发编程的艺术》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="上下文切换"><a href="#上下文切换" class="headerlink" title="上下文切换"></a>上下文切换</h2><p>即使是单核处理器也支持多线程执行代码，CPU通过给每个线程分配CPU时间片来实现这个机制。时间片是CPU分配给各个线程的时间，因为时间片非常短，所以CPU通过不停地切换线程执行，让我们感觉多个线程是同时执行的，时间片一般是几十毫秒（ms）。</p><p>CPU通过时间片分配算法来循环执行任务，当前任务执行一个时间片后会切换到下一个任务。但是，在切换前会保存上一个任务的状态，以便下次切换回这个任务时，可以再加载这个任务的状态。所以任务从保存到再加载的过程就是一次上下文切换。</p><p>多线程不一定快，因为线程有创建和上下文切换的开销。</p><h2 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h2><p>锁是个非常有用的工具，运用场景非常多，因为它使用起来非常简单，而且易于理解。但同时它也会带来一些困扰，那就是可能会引起死锁，一旦产生死锁，就会造成系统功能不可用。</p><p>避免死锁的几个常见方法。</p><ul><li>避免一个线程同时获取多个锁。</li><li>避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源。</li><li>尝试使用定时锁，使用lock.tryLock（timeout）来替代使用内部锁机制。</li><li>对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况。</li></ul>]]></content>
      
      
      <categories>
          
          <category> JAVA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 并发编程的艺术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hashmap核心知识</title>
      <link href="/2021/10/19/2021-10-19-hashmap/"/>
      <url>/2021/10/19/2021-10-19-hashmap/</url>
      
        <content type="html"><![CDATA[<h2 id="扰动函数"><a href="#扰动函数" class="headerlink" title="扰动函数"></a>扰动函数</h2><p>HashMap存放元素时候有这样一段代码来处理哈希值，这是java 8的散列值扰动函数，用于优化散列效果</p><pre><code>static final int hash(Object key) {    int h;    return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);}</code></pre><h3 id="为什么使用扰动函数"><a href="#为什么使用扰动函数" class="headerlink" title="为什么使用扰动函数"></a>为什么使用扰动函数</h3><p>理论上来说字符串的hashCode是一个int类型值，那可以直接作为数组下标了，且不会出现碰撞。但是这个hashCode的取值范围是[-2147483648, 2147483647]，有将近40亿的长度，谁也不能把数组初始化的这么大，内存也是放不下的。</p><p>我们默认初始化的Map大小是16个长度 DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4，所以获取的Hash值并不能直接作为下标使用，需要与数组长度进行取模运算得到一个下标值。</p><p>hashMap源码这里不只是直接获取哈希值，还进行了一次扰动计算，(h = key.hashCode()) ^ (h &gt;&gt;&gt; 16)。把哈希值右移16位，也就正好是自己长度的一半，之后与原哈希值做异或运算，这样就混合了原哈希值中的高位和低位，增大了随机性。计算方式如下图；</p><p><img src="/2021/10/19/2021-10-19-hashmap/interview-4-02.png" alt></p><p><strong>使用扰动函数就是为了增加随机性，让数据元素更加均衡的散列，减少碰撞。</strong></p><h2 id="初始化容量和负载因子"><a href="#初始化容量和负载因子" class="headerlink" title="初始化容量和负载因子"></a>初始化容量和负载因子</h2><p>选择一个合理的大小下进行扩容，默认值0.75就是说当阀值容量占了3/4时赶紧扩容，减少Hash碰撞。</p><p>同时0.75是一个默认构造值，在创建HashMap也可以调整，比如你希望用更多的空间换取时间，可以把负载因子调的更小一些，减少碰撞。</p><h2 id="扩容元素拆分"><a href="#扩容元素拆分" class="headerlink" title="扩容元素拆分"></a>扩容元素拆分</h2><p>扩容最直接的问题，就是需要把元素拆分到新的数组中。拆分元素的过程中，原jdk1.7中会需要重新计算哈希值，但是到jdk1.8中已经进行优化，不在需要重新计算，提升了拆分的性能，设计的还是非常巧妙的。</p><p>当put时，如果发现目前的bucket占用程度已经超过了Load Factor所希望的比例，那么就会发生resize。在resize的过程，简单的说就是把bucket扩充为2倍，之后重新计算index，把节点再放到新的bucket中。</p><p>当超过限制的时候会resize，然而又因为我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。</p><p><img src="/2021/10/19/2021-10-19-hashmap/2-HashMap-4d03d.png" alt></p><p>因此元素在重新计算 hash 之后，因为n变为2倍，那么 n-1 的 mask 范围在高位多1bit(红色)，因此新的index就会发生这样的变化：</p><p><img src="/2021/10/19/2021-10-19-hashmap/2-HashMap-03719.png" alt></p><p>因此，我们在扩充HashMap的时候，不需要重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”。可以看看下图为16扩充为32的resize示意图：</p><p><img src="/2021/10/19/2021-10-19-hashmap/2-HashMap-4fb68.png" alt></p><h3 id="扩容并发死循环情况"><a href="#扩容并发死循环情况" class="headerlink" title="扩容并发死循环情况"></a>扩容并发死循环情况</h3><p>HashMap1.7当中，扩容的时候，采用的是头插法转移结点，在多线程并发的情况下会造成链表死循环的问题。</p><p>假设有两个线程，线程1和线程2，两个线程进行hashMap的put操作，触发了扩容。</p><p>下面是扩容的时候结点转移的关键代码</p><pre><code>void transfer(Entry[] newTable) {      Entry[] src = table;       int newCapacity = newTable.length;      for (int j = 0; j &lt; src.length; j++) {           Entry&lt;K,V&gt; e = src[j];                     if (e != null) {//两个线程都先进入if              src[j] = null;               do {                   Entry&lt;K,V&gt; next = e.next;                  int i = indexFor(e.hash, newCapacity);                 e.next = newTable[i]; //线程1 这里还没执行 停下                 newTable[i] = e;                   e = next;                          } while (e != null);         }     } }</code></pre><p>线程1和线程2 都进入if，然后线程1没有拿到cpu的资源在上面代码注释的地方停下了。此时的变量指针如下图所示：</p><p><img src="/2021/10/19/2021-10-19-hashmap/20210113183932506.png" alt></p><p>记住 线程1中 E变量指向a结点，next变量指向b结点。</p><p>下面是线程2 拿到cpu的资源，执行结点转移</p><p><img src="/2021/10/19/2021-10-19-hashmap/20210113183945982.png" alt></p><p><img src="/2021/10/19/2021-10-19-hashmap/20210113183959741.png" alt></p><p><img src="/2021/10/19/2021-10-19-hashmap/20210113184039586.png" alt></p><p><img src="/2021/10/19/2021-10-19-hashmap/20210113184049658.png" alt></p><p>线程2停下，轮到线程1</p><p>因为之前线程1中E变量指向的是a结点，next变量指向的是b结点，所以如下图所示：</p><p><img src="/2021/10/19/2021-10-19-hashmap/20210113184106784.png" alt></p><p>再来看看 刚才线程是在e.next = newTable[i] 这句代码还没执行的时候停下的，那么现在就要执行这一句代码</p><pre><code>void transfer(Entry[] newTable) {      Entry[] src = table;       int newCapacity = newTable.length;      for (int j = 0; j &lt; src.length; j++) {           Entry&lt;K,V&gt; e = src[j];                     if (e != null) {//两个线程都先进入if              src[j] = null;               do {                   Entry&lt;K,V&gt; next = e.next;                  int i = indexFor(e.hash, newCapacity);                 e.next = newTable[i]; //线程1刚才在这里停下，所以现在从这一句代码开始执行                 newTable[i] = e;                   e = next;                          } while (e != null);         }     } }</code></pre><p>此时线程1 执行代码之后，就造成了链表的死循环，结果如下：</p><p><img src="/2021/10/19/2021-10-19-hashmap/20210113184120471.png" alt></p><p>java1.8改为尾插法，防止环化。</p>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 集合 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpringCloudGateway</title>
      <link href="/2021/10/17/2021-10-17-springcloudgateway/"/>
      <url>/2021/10/17/2021-10-17-springcloudgateway/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>SpringCloud Gateway 是 Spring Cloud 的一个全新项目，该项目是基于 Spring 5.0，Spring Boot 2.0 和 Project Reactor 等技术开发的网关，它旨在为微服务架构提供一种简单有效的统一的 API 路由管理方式。</p><p>SpringCloud Gateway 作为 Spring Cloud 生态系统中的网关，目标是替代 Zuul，在Spring Cloud 2.0以上版本中，没有对新版本的Zuul 2.0以上最新高性能版本进行集成，仍然还是使用的Zuul 2.0之前的非Reactor模式的老版本。而为了提升网关的性能，SpringCloud Gateway是基于WebFlux框架实现的，而WebFlux框架底层则使用了高性能的Reactor模式通信框架Netty。</p><p>Spring Cloud Gateway 的目标，不仅提供统一的路由方式，并且基于 Filter 链的方式提供了网关基本的功能，例如：安全，监控/指标，和限流。</p><h2 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h2><p>SpringCloud官方，对SpringCloud Gateway 特征介绍如下：</p><ol><li>基于 Spring Framework 5，Project Reactor 和 Spring Boot 2.0</li><li>集成 Hystrix 断路器</li><li>集成 Spring Cloud DiscoveryClient</li><li>Predicates 和 Filters 作用于特定路由，易于编写的 Predicates 和 Filters</li><li>具备一些网关的高级功能：动态路由、限流、路径重写</li></ol><p>从以上的特征来说，和Zuul的特征差别不大。SpringCloud Gateway和Zuul主要的区别，还是在底层的通信框架上。</p><h3 id="Filter（过滤器）"><a href="#Filter（过滤器）" class="headerlink" title="Filter（过滤器）"></a>Filter（过滤器）</h3><p>和Zuul的过滤器在概念上类似，可以使用它拦截和修改请求，并且对上游的响应，进行二次处理。过滤器为org.springframework.cloud.gateway.filter.GatewayFilter类的实例。</p><h3 id="Route（路由）"><a href="#Route（路由）" class="headerlink" title="Route（路由）"></a>Route（路由）</h3><p>网关配置的基本组成模块，和Zuul的路由配置模块类似。一个Route模块由一个 ID，一个目标 URI，一组断言和一组过滤器定义。如果断言为真，则路由匹配，目标URI会被访问。</p><h3 id="Predicate（断言）"><a href="#Predicate（断言）" class="headerlink" title="Predicate（断言）"></a>Predicate（断言）</h3><p>这是一个 Java 8 的 Predicate，可以使用它来匹配来自 HTTP 请求的任何内容，例如 headers 或参数。断言的输入类型是一个 ServerWebExchange。</p><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>Spring在2017年下半年迎来了Webflux，Webflux的出现填补了Spring在响应式编程上的空白，Webflux的响应式编程不仅仅是编程风格的改变，而且对于一系列的著名框架，都提供了响应式访问的开发包，比如Netty、Redis等等。</p><p>SpringCloud Gateway 使用的Webflux中的reactor-netty响应式编程组件，底层使用了Netty通讯框架。</p><p><img src="/2021/10/17/2021-10-17-springcloudgateway/19816137-8758f092be21e6f7.gif" alt></p><h3 id="Zuul的IO模型"><a href="#Zuul的IO模型" class="headerlink" title="Zuul的IO模型"></a>Zuul的IO模型</h3><p>Springcloud中所集成的Zuul版本，采用的是Tomcat容器，使用的是传统的Servlet IO处理模型。</p><p>servlet由servlet container进行生命周期管理。container启动时构造servlet对象并调用servlet init()进行初始化；container关闭时调用servlet destory()销毁servlet；container运行时接受请求，并为每个请求分配一个线程（一般从线程池中获取空闲线程）然后调用service()。</p><p>弊端：servlet是一个简单的网络IO模型，当请求进入servlet container时，servlet container就会为其绑定一个线程，在并发不高的场景下这种模型是适用的，但是一旦并发上升，线程数量就会上涨，而线程资源代价是昂贵的（上线文切换，内存消耗大）严重影响请求的处理时间。在一些简单的业务场景下，不希望为每个request分配一个线程，只需要1个或几个线程就能应对极大并发的请求，这种业务场景下servlet模型没有优势。</p><p><img src="/2021/10/17/2021-10-17-springcloudgateway/19816137-bb466f6b0135bb71.jpg" alt></p><p>所以Springcloud Zuul 是基于servlet之上的一个阻塞式处理模型，即spring实现了处理所有request请求的一个servlet（DispatcherServlet），并由该servlet阻塞式处理处理。所以Springcloud Zuul无法摆脱servlet模型的弊端。虽然Zuul 2.0开始，使用了Netty，并且已经有了大规模Zuul 2.0集群部署的成熟案例，但是，Springcloud官方已经没有集成改版本的计划了。</p><h3 id="Webflux-服务器"><a href="#Webflux-服务器" class="headerlink" title="Webflux 服务器"></a>Webflux 服务器</h3><p>Webflux模式替换了旧的Servlet线程模型。用少量的线程处理request和response io操作，这些线程称为Loop线程，而业务交给响应式编程框架处理，响应式编程是非常灵活的，用户可以将业务中阻塞的操作提交到响应式框架的work线程中执行，而不阻塞的操作依然可以在Loop线程中进行处理，大大提高了Loop线程的利用率。官方结构图：</p><p><img src="/2021/10/17/2021-10-17-springcloudgateway/19816137-dad0e43fc31f4536.jpg" alt></p><p>Webflux虽然可以兼容多个底层的通信框架，但是一般情况下，底层使用的还是Netty，毕竟，Netty是目前业界认可的最高性能的通信框架。而Webflux的Loop线程，正好就是著名的Reactor 模式IO处理模型的Reactor线程，如果使用的是高性能的通信框架Netty，这就是Netty的EventLoop线程。</p><h3 id="Spring-Cloud-Gateway的处理流程"><a href="#Spring-Cloud-Gateway的处理流程" class="headerlink" title="Spring Cloud Gateway的处理流程"></a>Spring Cloud Gateway的处理流程</h3><p>客户端向 Spring Cloud Gateway 发出请求。然后在 Gateway Handler Mapping 中找到与请求相匹配的路由，将其发送到 Gateway Web Handler。Handler 再通过指定的过滤器链来将请求发送到我们实际的服务执行业务逻辑，然后返回。过滤器之间用虚线分开是因为过滤器可能会在发送代理请求之前（“pre”）或之后（“post”）执行业务逻辑。</p><p><img src="/2021/10/17/2021-10-17-springcloudgateway/19816137-eeedbd49be096c05.jpg" alt></p><h2 id="路由配置方式"><a href="#路由配置方式" class="headerlink" title="路由配置方式"></a>路由配置方式</h2><h3 id="基础URI路由配置方式"><a href="#基础URI路由配置方式" class="headerlink" title="基础URI路由配置方式"></a>基础URI路由配置方式</h3><p>如果请求的目标地址，是单个的URI资源路径，配置文件示例如下：</p><pre><code>server:  port: 8080spring:  application:    name: api-gateway  cloud:    gateway:      routes:        - id: url-proxy-1          uri: https://blog.csdn.net          predicates:            - Path=/csdn</code></pre><ul><li>id：我们自定义的路由 ID，保持唯一</li><li>uri：目标服务地址</li><li>predicates：路由条件，Predicate 接受一个输入参数，返回一个布尔值结果。该接口包含多种默认方法来将 Predicate 组合成其他复杂的逻辑（比如：与，或，非）。</li></ul><p>上面这段配置的意思是，配置了一个 id 为 url-proxy-1的URI代理规则，路由的规则为：</p><p>当访问地址<a href="http://localhost:8080/csdn/1.jsp时，会路由到上游地址https://blog.csdn.net/1.jsp。" target="_blank" rel="noopener">http://localhost:8080/csdn/1.jsp时，会路由到上游地址https://blog.csdn.net/1.jsp。</a></p><h3 id="基于代码的路由配置方式"><a href="#基于代码的路由配置方式" class="headerlink" title="基于代码的路由配置方式"></a>基于代码的路由配置方式</h3><p>转发功能同样可以通过代码来实现，我们可以在启动类 GateWayApplication 中添加方法 customRouteLocator() 来定制转发规则。</p><pre><code>@SpringBootApplicationpublic class GatewayApplication {    public static void main(String[] args) {        SpringApplication.run(GatewayApplication.class, args);    }    @Bean    public RouteLocator customRouteLocator(RouteLocatorBuilder builder) {        return builder.routes()                .route(&quot;path_route&quot;, r -&gt; r.path(&quot;/csdn&quot;)                        .uri(&quot;https://blog.csdn.net&quot;))                .build();    }}</code></pre><p>我们在yaml配置文件中注销掉相关路由的配置，重启服务，访问链接：<a href="http://localhost:8080/" target="_blank" rel="noopener">http://localhost:8080/</a> csdn， 可以看到和上面一样的页面，证明我们测试成功。</p><h3 id="和注册中心相结合的路由配置方式"><a href="#和注册中心相结合的路由配置方式" class="headerlink" title="和注册中心相结合的路由配置方式"></a>和注册中心相结合的路由配置方式</h3><p>在uri的schema协议部分为自定义的lb:类型，表示从微服务注册中心（如Eureka）订阅服务，并且进行服务的路由。</p><pre><code>server:  port: 8084spring:  cloud:    gateway:      routes:      -id: seckill-provider-route        uri: lb://seckill-provider        predicates:        - Path=/seckill-provider/**      -id: message-provider-route        uri: lb://message-provider        predicates:        -Path=/message-provider/**application:  name: cloud-gatewayeureka:  instance:    prefer-ip-address: true  client:    service-url:      defaultZone: http://localhost:8888/eureka/</code></pre><p>注册中心相结合的路由配置方式，与单个URI的路由配置，区别其实很小，仅仅在于URI的schema协议不同。单个URI的地址的schema协议，一般为http或者https协议。</p><h2 id="路由匹配规则"><a href="#路由匹配规则" class="headerlink" title="路由匹配规则"></a>路由匹配规则</h2><p>Spring Cloud Gateway 的功能很强大，我们仅仅通过 Predicates 的设计就可以看出来，前面我们只是使用了 predicates 进行了简单的条件匹配，其实 Spring Cloud Gataway 帮我们内置了很多 Predicates 功能。</p><p>Spring Cloud Gateway 是通过 Spring WebFlux 的 HandlerMapping 做为底层支持来匹配到转发路由，Spring Cloud Gateway 内置了很多 Predicates 工厂，这些 Predicates 工厂通过不同的 HTTP 请求参数来匹配，多个 Predicates 工厂可以组合使用。</p><p><img src="/2021/10/17/2021-10-17-springcloudgateway/20200527213652534.png" alt></p><p>gateWay的主要功能之一是转发请求，转发规则的定义主要包含三个部分</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Route（路由）</td><td>路由是网关的基本单元，由ID、URI、一组Predicate、一组Filter组成，根据Predicate进行匹配转发。</td></tr><tr><td>Predicate（谓语、断言）</td><td>路由转发的判断条件，目前SpringCloud Gateway支持多种方式，常见如：Path、Query、Method、Header等，写法必须遵循 key=vlue的形式</td></tr><tr><td>Filter（过滤器）</td><td>过滤器是路由转发请求时所经过的过滤逻辑，可用于修改请求、响应内容</td></tr></tbody></table><blockquote><p>其中Route和Predicate必须同时申明</p></blockquote><pre><code>//通过配置文件配置spring:  cloud:    gateway:      routes:        - id: gate_route          uri: http://localhost:9023          predicates:          ## 当请求的路径为gate、rule开头的时，转发到http://localhost:9023服务器上            - Path=/gate/**,/rule/**        ### 请求路径前加上/app          filters:          - PrefixPath=/app</code></pre><h3 id="Predicate-断言条件-转发规则-介绍"><a href="#Predicate-断言条件-转发规则-介绍" class="headerlink" title="Predicate 断言条件(转发规则)介绍"></a>Predicate 断言条件(转发规则)介绍</h3><p>Predicate 来源于 Java 8，是 Java 8 中引入的一个函数，Predicate 接受一个输入参数，返回一个布尔值结果。该接口包含多种默认方法来将 Predicate 组合成其他复杂的逻辑（比如：与，或，非）。可以用于接口请求参数校验、判断新老数据是否有变化需要进行更新操作。</p><p>在 Spring Cloud Gateway 中 Spring 利用 Predicate 的特性实现了各种路由匹配规则，有通过 Header、请求参数等不同的条件来进行作为条件匹配到对应的路由。网上有一张图总结了 Spring Cloud 内置的几种 Predicate 的实现。</p><p><img src="/2021/10/17/2021-10-17-springcloudgateway/19816137-bb046dbf19bee1b4.gif" alt></p><p>说白了 Predicate 就是为了实现一组匹配规则，方便让请求过来找到对应的 Route 进行处理，接下来我们接下 Spring Cloud GateWay 内置几种 Predicate 的使用。</p><h4 id="转发规则（predicates）"><a href="#转发规则（predicates）" class="headerlink" title="转发规则（predicates）"></a>转发规则（predicates）</h4><table><thead><tr><th>规则</th><th>实例</th><th>说明</th></tr></thead><tbody><tr><td>Path</td><td>- Path=/gate/<strong>,/rule/</strong></td><td>## 当请求的路径为gate、rule开头的时，转发到<a href="http://localhost:9023服务器上" target="_blank" rel="noopener">http://localhost:9023服务器上</a></td></tr><tr><td>Before</td><td>- Before=2017-01-20T17:42:47.789-07:00[America/Denver]</td><td>在某个时间之前的请求才会被转发到 <a href="http://localhost:9023服务器上" target="_blank" rel="noopener">http://localhost:9023服务器上</a></td></tr><tr><td>After</td><td>- After=2017-01-20T17:42:47.789-07:00[America/Denver]</td><td>在某个时间之后的请求才会被转发</td></tr><tr><td>Between</td><td>- Between=2017-01-20T17:42:47.789-07:00[America/Denver],2017-01-21T17:42:47.789-07:00[America/Denver]</td><td>在某个时间段之间的才会被转发</td></tr><tr><td>Cookie</td><td>- Cookie=chocolate, ch.p</td><td>名为chocolate的表单或者满足正则ch.p的表单才会被匹配到进行请求转发</td></tr><tr><td>Header</td><td>- Header=X-Request-Id, \d+</td><td>携带参数X-Request-Id或者满足\d+的请求头才会匹配</td></tr><tr><td>Host</td><td>- Host=<a href="http://www.hd123.com" target="_blank" rel="noopener">www.hd123.com</a></td><td>当主机名为<a href="http://www.hd123.com的时候直接转发到http://localhost:9023服务器上" target="_blank" rel="noopener">www.hd123.com的时候直接转发到http://localhost:9023服务器上</a></td></tr><tr><td>Method</td><td>- Method=GET</td><td>只有GET方法才会匹配转发请求，还可以限定POST、PUT等请求方式</td></tr></tbody></table><h4 id="通过请求参数匹配"><a href="#通过请求参数匹配" class="headerlink" title="通过请求参数匹配"></a>通过请求参数匹配</h4><p>Query Route Predicate 支持传入两个参数，一个是属性名一个为属性值，属性值可以是正则表达式。</p><pre><code>server:  port: 8080spring:  application:       name: api-gateway  cloud:    gateway:      routes:        -id: gateway-service          uri: https://www.baidu.com          order: 0          predicates:            -Query=smile</code></pre><p>这样配置，只要请求中包含 smile 属性的参数即可匹配路由。</p><p>localhost:8080?smile=x&amp;id=2</p><p>经过测试发现只要请求汇总带有 smile 参数即会匹配路由，不带 smile 参数则不会匹配。</p><p>还可以将 Query 的值以键值对的方式进行配置，这样在请求过来时会对属性值和正则进行匹配，匹配上才会走路由。</p><pre><code>server:  port: 8080spring:  application:    name: api-gateway  cloud:    gateway:      routes:        -id: gateway-service          uri: https://www.baidu.com          order: 0          predicates:            -Query=keep, pu.</code></pre><h4 id="通过-Header-属性匹配"><a href="#通过-Header-属性匹配" class="headerlink" title="通过 Header 属性匹配"></a>通过 Header 属性匹配</h4><pre><code>server:  port: 8080spring:  application:    name: api-gateway  cloud:    gateway:      routes:        -id: gateway-service          uri: https://www.baidu.com          order: 0          predicates:            - Header=X-Request-Id, \d+</code></pre><p>curl <a href="http://localhost:8080" target="_blank" rel="noopener">http://localhost:8080</a> -H “X-Request-Id:88”</p><p>则返回页面代码证明匹配成功。将参数-H “X-Request-Id:88”改为-H “X-Request-Id:spring”再次执行时返回404证明没有匹配。</p><h4 id="通过-Cookie-匹配"><a href="#通过-Cookie-匹配" class="headerlink" title="通过 Cookie 匹配"></a>通过 Cookie 匹配</h4><p>Cookie Route Predicate 可以接收两个参数，一个是 Cookie name ,一个是正则表达式，路由规则会通过获取对应的 Cookie name 值和正则表达式去匹配，如果匹配上就会执行路由，如果没有匹配上则不执行。</p><pre><code>server:  port: 8080spring:  application:    name: api-gateway  cloud:    gateway:      routes:        -id: gateway-service          uri: https://www.baidu.com          order: 0          predicates:            - Cookie=sessionId, test</code></pre><p>curl <a href="http://localhost:8080" target="_blank" rel="noopener">http://localhost:8080</a> –cookie “sessionId=test”</p><p>则会返回页面代码，如果去掉–cookie “sessionId=test”，后台汇报 404 错误。</p><h4 id="通过-Host-匹配"><a href="#通过-Host-匹配" class="headerlink" title="通过 Host 匹配"></a>通过 Host 匹配</h4><p>Host Route Predicate 接收一组参数，一组匹配的域名列表，这个模板是一个 ant 分隔的模板，用.号作为分隔符。它通过参数中的主机地址作为匹配规则。</p><pre><code>server:  port: 8080spring:  application:    name: api-gateway  cloud:    gateway:      routes:        -id: gateway-service          uri: https://www.baidu.com          order: 0          predicates:            - Host=**.baidu.com</code></pre><p>curl <a href="http://localhost:8080" target="_blank" rel="noopener">http://localhost:8080</a> -H “Host: <a href="http://www.baidu.com&quot;" target="_blank" rel="noopener">www.baidu.com&quot;</a></p><p>经测试以上可匹配到 host_route 路由，去掉 host 参数则会报 404 错误。</p><h4 id="通过请求方式匹配"><a href="#通过请求方式匹配" class="headerlink" title="通过请求方式匹配"></a>通过请求方式匹配</h4><p>可以通过是 POST、GET、PUT、DELETE 等不同的请求方式来进行路由。</p><pre><code>server:  port: 8080spring:  application:    name: api-gateway  cloud:    gateway:      routes:        -id: gateway-service          uri: https://www.baidu.com          order: 0          predicates:            - Method=GET</code></pre><p>curl -X POST <a href="http://localhost:8080" target="_blank" rel="noopener">http://localhost:8080</a></p><p>返回 404 没有找到，证明没有匹配上路由</p><h4 id="通过请求路径匹配"><a href="#通过请求路径匹配" class="headerlink" title="通过请求路径匹配"></a>通过请求路径匹配</h4><p>Path Route Predicate 接收一个匹配路径的参数来判断是否走路由。</p><pre><code>server:  port: 8080spring:  application:    name: api-gateway  cloud:    gateway:      routes:        -id: gateway-service          uri: http://ityouknow.com          order: 0          predicates:            -Path=/foo/{segment}</code></pre><p>curl <a href="http://localhost:8080/foo/xx" target="_blank" rel="noopener">http://localhost:8080/foo/xx</a></p><p>经过测试命令可以正常获取到页面返回值</p><h4 id="通过请求-ip-地址进行匹配"><a href="#通过请求-ip-地址进行匹配" class="headerlink" title="通过请求 ip 地址进行匹配"></a>通过请求 ip 地址进行匹配</h4><p>Predicate 也支持通过设置某个 ip 区间号段的请求才会路由，RemoteAddr Route Predicate 接受 cidr 符号(IPv4 或 IPv6 )字符串的列表(最小大小为1)，例如 192.168.0.1/16 (其中 192.168.0.1 是 IP 地址，16 是子网掩码)。</p><pre><code>server:  port: 8080spring:  application:    name: api-gateway  cloud:    gateway:      routes:        - id: gateway-service          uri: https://www.baidu.com          order: 0          predicates:            - RemoteAddr=192.168.1.1/24</code></pre><p>如果请求的远程地址是 192.168.1.10，则此路由将匹配。</p><h4 id="组合使用"><a href="#组合使用" class="headerlink" title="组合使用"></a>组合使用</h4><pre><code>server:  port: 8080spring:  application:    name: api-gateway  cloud:    gateway:      routes:        - id: gateway-service          uri: https://www.baidu.com          order: 0          predicates:            - Host=**.foo.org            - Path=/headers            - Method=GET            - Header=X-Request-Id, \d+            - Query=foo, ba.            - Query=baz            - Cookie=chocolate, ch.p</code></pre><p>各种 Predicates 同时存在于同一个路由时，请求必须同时满足所有的条件才被这个路由匹配。</p><p>一个请求满足多个路由的断言条件时，请求只会被首个成功匹配的路由转发</p><h3 id="过滤器规则（Filter）"><a href="#过滤器规则（Filter）" class="headerlink" title="过滤器规则（Filter）"></a>过滤器规则（Filter）</h3><table><thead><tr><th>过滤规则</th><th>实例</th><th>说明</th></tr></thead><tbody><tr><td>PrefixPath</td><td>- PrefixPath=/app</td><td>在请求路径前加上app</td></tr><tr><td>RewritePath</td><td>- RewritePath=/test, /app/test</td><td>访问localhost:9022/test,请求会转发到localhost:8001/app/test</td></tr><tr><td>SetPath</td><td>SetPath=/app/{path}</td><td>通过模板设置路径，转发的规则时会在路径前增加app，{path}表示原请求路径</td></tr><tr><td>RedirectTo</td><td></td><td>重定向</td></tr><tr><td>RemoveRequestHeader</td><td></td><td>去掉某个请求头信息</td></tr></tbody></table><blockquote><p>当配置多个filter时，优先定义的会被调用，剩余的filter将不会生效</p></blockquote><h4 id="PrefixPath"><a href="#PrefixPath" class="headerlink" title="PrefixPath"></a>PrefixPath</h4><p>对所有的请求路径添加前缀：</p><pre><code>spring:  cloud:    gateway:      routes:      - id: prefixpath_route        uri: https://example.org        filters:        - PrefixPath=/mypath</code></pre><p>访问/hello的请求被发送到<a href="https://example.org/mypath/hello。" target="_blank" rel="noopener">https://example.org/mypath/hello。</a></p><h4 id="RedirectTo"><a href="#RedirectTo" class="headerlink" title="RedirectTo"></a>RedirectTo</h4><p>重定向，配置包含重定向的返回码和地址：</p><pre><code>spring:  cloud:    gateway:      routes:      - id: prefixpath_route        uri: https://example.org        filters:        - RedirectTo=302, https://acme.org</code></pre><h4 id="RemoveRequestHeader"><a href="#RemoveRequestHeader" class="headerlink" title="RemoveRequestHeader"></a>RemoveRequestHeader</h4><p>去掉某个请求头信息：</p><pre><code>spring:  cloud:    gateway:      routes:      - id: removerequestheader_route        uri: https://example.org        filters:        - RemoveRequestHeader=X-Request-Foo</code></pre><h4 id="RemoveRequestParameter"><a href="#RemoveRequestParameter" class="headerlink" title="RemoveRequestParameter"></a>RemoveRequestParameter</h4><p>去掉某个请求参数信息：</p><pre><code>spring:  cloud:    gateway:      routes:      - id: removerequestparameter_route        uri: https://example.org        filters:        - RemoveRequestParameter=red</code></pre><h4 id="RewritePath"><a href="#RewritePath" class="headerlink" title="RewritePath"></a>RewritePath</h4><p>改写路径：</p><pre><code>spring:  cloud:    gateway:      routes:      - id: rewrite_filter        uri: http://localhost:8081        predicates:        - Path=/test/**        filters:        - RewritePath=/where(?&lt;segment&gt;/?.*), /test(?&lt;segment&gt;/?.*)</code></pre><h4 id="SetPath"><a href="#SetPath" class="headerlink" title="SetPath"></a>SetPath</h4><p>设置请求路径，与RewritePath类似。</p><pre><code>spring:  cloud:    gateway:      routes:      - id: setpath_route        uri: https://example.org        predicates:        - Path=/red/{segment}        filters:        - SetPath=/{segment}</code></pre><p>如/red/blue的请求被转发到/blue。</p><h4 id="SetRequestHeader"><a href="#SetRequestHeader" class="headerlink" title="SetRequestHeader"></a>SetRequestHeader</h4><p>设置请求头信息。</p><pre><code>spring:  cloud:    gateway:      routes:      - id: setrequestheader_route        uri: https://example.org        filters:        - SetRequestHeader=X-Request-Red, Blue</code></pre><h4 id="SetStatus"><a href="#SetStatus" class="headerlink" title="SetStatus"></a>SetStatus</h4><p>设置回执状态码。</p><pre><code>spring:  cloud:    gateway:      routes:      - id: setstatusint_route        uri: https://example.org        filters:        - SetStatus=401</code></pre><h4 id="StripPrefix"><a href="#StripPrefix" class="headerlink" title="StripPrefix"></a>StripPrefix</h4><pre><code>spring:  cloud:    gateway:      routes:      - id: nameRoot        uri: https://nameservice        predicates:        - Path=/name/**        filters:        - StripPrefix=2</code></pre><p>请求/name/blue/red会转发到/red。</p><h4 id="RequestSize"><a href="#RequestSize" class="headerlink" title="RequestSize"></a>RequestSize</h4><pre><code>spring:  cloud:    gateway:      routes:      - id: request_size_route        uri: http://localhost:8080/upload        predicates:        - Path=/upload        filters:        - name: RequestSize          args:            maxSize: 5000000</code></pre><p>超过5M的请求会返回413错误。</p><h4 id="Default-filters"><a href="#Default-filters" class="headerlink" title="Default-filters"></a>Default-filters</h4><p>对所有请求添加过滤器。</p><pre><code>spring:  cloud:    gateway:      default-filters:      - AddResponseHeader=X-Response-Default-Red, Default-Blue      - PrefixPath=/httpbin</code></pre><h2 id="实现熔断降级"><a href="#实现熔断降级" class="headerlink" title="实现熔断降级"></a>实现熔断降级</h2><p>在分布式系统中，网关作为流量的入口，因此会有大量的请求进入网关，向其他服务发起调用，其他服务不可避免的会出现调用失败（超时、异常），失败时不能让请求堆积在网关上，需要快速失败并返回给客户端，想要实现这个要求，就必须在网关上做熔断、降级操作。</p><pre><code>server.port: 8082spring:  application:    name: gateway  redis:      host: localhost      port: 6379      password: 123456  cloud:    gateway:      routes:        - id: rateLimit_route          uri: http://localhost:8000          order: 0          predicates:            - Path=/test/**          filters:            - StripPrefix=1            - name: Hystrix              args:                name: fallbackCmdA                fallbackUri: forward:/fallbackA  hystrix.command.fallbackCmdA.execution.isolation.thread.timeoutInMilliseconds: 5000</code></pre><p>这里的配置，使用了两个过滤器：</p><ol><li>过滤器StripPrefix，作用是去掉请求路径的最前面n个部分截取掉。</li></ol><p>StripPrefix=1就代表截取路径的个数为1，比如前端过来请求/test/good/1/view，匹配成功后，路由到后端的请求路径就会变成<a href="http://localhost:8888/good/1/view。" target="_blank" rel="noopener">http://localhost:8888/good/1/view。</a></p><ol start="2"><li>过滤器Hystrix，作用是通过Hystrix进行熔断降级</li></ol><p>当上游的请求，进入了Hystrix熔断降级机制时，就会调用fallbackUri配置的降级地址。需要注意的是，还需要单独设置Hystrix的commandKey的超时时间</p><p>fallbackUri配置的降级地址的代码如下：</p><pre><code>@RestControllerpublic class FallbackController {    @GetMapping(&quot;/fallbackA&quot;)    public Response fallbackA() {        Response response = new Response();        response.setCode(&quot;100&quot;);        response.setMessage(&quot;服务暂时不可用&quot;);        return response;    }}</code></pre><h2 id="分布式限流"><a href="#分布式限流" class="headerlink" title="分布式限流"></a>分布式限流</h2><p>从某种意义上讲，令牌桶算法是对漏桶算法的一种改进，桶算法能够限制请求调用的速率，而令牌桶算法能够在限制调用的平均速率的同时还允许一定程度的突发调用。在令牌桶算法中，存在一个桶，用来存放固定数量的令牌。算法中存在一种机制，以一定的速率往桶中放令牌。每次请求调用需要先获取令牌，只有拿到令牌，才有机会继续执行，否则选择选择等待可用的令牌、或者直接拒绝。放令牌这个动作是持续不断的进行，如果桶中令牌数达到上限，就丢弃令牌，所以就存在这种情况，桶中一直有大量的可用令牌，这时进来的请求就可以直接拿到令牌执行，比如设置qps为100，那么限流器初始化完成一秒后，桶中就已经有100个令牌了，这时服务还没完全启动好，等启动完成对外提供服务时，该限流器可以抵挡瞬时的100个请求。所以，只有桶中没有令牌时，请求才会进行等待，最后相当于以一定的速率执行。</p><p><img src="/2021/10/17/2021-10-17-springcloudgateway/19816137-89297e59a487540d.jpg" alt></p><p>在Spring Cloud Gateway中，有Filter过滤器，因此可以在“pre”类型的Filter中自行实现上述三种过滤器。但是限流作为网关最基本的功能，Spring Cloud Gateway官方就提供了RequestRateLimiterGatewayFilterFactory这个类，适用在Redis内的通过执行Lua脚本实现了令牌桶的方式。具体实现逻辑在RequestRateLimiterGatewayFilterFactory类中，lua脚本在如下图所示的文件夹中：</p><p>首先在工程的pom文件中引入gateway的起步依赖和redis的reactive依赖，代码如下：</p><pre><code>server:  port: 8081spring:  cloud:    gateway:      routes:      - id: limit_route        uri: http://httpbin.org:80/get        predicates:        - After=2017-01-20T17:42:47.789-07:00[America/Denver]        filters:        - name: RequestRateLimiter          args:            key-resolver: &#39;#{@userKeyResolver}&#39;            redis-rate-limiter.replenishRate: 1            redis-rate-limiter.burstCapacity: 3  application:    name: cloud-gateway  redis:    host: localhost    port: 6379    database: 0</code></pre><p>在上面的配置文件，指定程序的端口为8081，配置了 redis的信息，并配置了RequestRateLimiter的限流过滤器，该过滤器需要配置三个参数：</p><ul><li>burstCapacity，令牌桶总容量。</li><li>replenishRate，令牌桶每秒填充平均速率。</li><li>key-resolver，用于限流的键的解析器的 Bean 对象的名字。它使用 SpEL 表达式根据#{@beanName}从 Spring 容器中获取 Bean 对象。</li></ul><p>这里根据用户ID限流，请求路径中必须携带userId参数</p><pre><code>@BeanKeyResolver userKeyResolver() {  return exchange -&gt; Mono.just(exchange.getRequest().getQueryParams().getFirst(&quot;user&quot;));}</code></pre><p>KeyResolver需要实现resolve方法，比如根据userid进行限流，则需要用userid去判断。实现完KeyResolver之后，需要将这个类的Bean注册到Ioc容器中。</p><p>如果需要根据IP限流，定义的获取限流Key的bean为：</p><pre><code>@Beanpublic KeyResolver ipKeyResolver() {  return exchange -&gt; Mono.just(exchange.getRequest().getRemoteAddress().getHostName());}</code></pre><p>通过exchange对象可以获取到请求信息，这边用了HostName，如果你想根据用户来做限流的话这边可以获取当前请求的用户ID或者用户名就可以了，比如：</p><p>如果需要根据接口的URI进行限流，则需要获取请求地址的uri作为限流key，定义的Bean对象为：</p><pre><code>@BeanKeyResolver apiKeyResolver() {  return exchange -&gt; Mono.just(exchange.getRequest().getPath().value());}</code></pre><h2 id="健康检查配置"><a href="#健康检查配置" class="headerlink" title="健康检查配置"></a>健康检查配置</h2><p>admin-client、actuator健康检查配置，为之后的功能提供支持，此部分比较简单，不再赘述，加入以下maven依赖和配置</p><pre><code>&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;de.codecentric&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-admin-starter-client&lt;/artifactId&gt;    &lt;version&gt;2.1.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;    &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt;</code></pre><p>配置文件</p><pre><code>spring:  application:    name: mas-cloud-gateway  boot:    admin:      client:      ### 本地搭建的admin-server        url: http://localhost:8011eureka:  client:    registerWithEureka: true    fetchRegistry: true    healthcheck:      enabled: true    serviceUrl:      defaultZone: http://localhost:6887/eureka/    enabled: truefeign:  sentinel:    enabled: truemanagement:  endpoints:    web:      exposure:        include: &#39;*&#39;  endpoint:    health:      show-details: ALWAYS</code></pre><p>若转发的目标地址为微服务中组件，不为具体ip:port形式的，应写成lb://mas-openapi-service形式，目标地址会从注册中心直接拉取</p><h2 id="统一配置跨域请求"><a href="#统一配置跨域请求" class="headerlink" title="统一配置跨域请求"></a>统一配置跨域请求</h2><p>现在的请求通过经过gateWay网关时，需要在网关统一配置跨域请求，需求所有请求通过</p><pre><code>spring:  cloud:    gateway:      globalcors:        cors-configurations:          &#39;[/**]&#39;:            allowed-origins: &quot;*&quot;            allowed-headers: &quot;*&quot;            allow-credentials: true            allowed-methods:              - GET              - POST              - DELETE              - PUT              - OPTION</code></pre><h2 id="整合Nacos"><a href="#整合Nacos" class="headerlink" title="整合Nacos"></a>整合Nacos</h2><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;parent&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;        &lt;version&gt;2.1.9.RELEASE&lt;/version&gt;        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;    &lt;/parent&gt;    &lt;groupId&gt;com.example&lt;/groupId&gt;    &lt;artifactId&gt;nacos_gateway&lt;/artifactId&gt;    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;    &lt;packaging&gt;war&lt;/packaging&gt;    &lt;name&gt;nacos_gateway&lt;/name&gt;    &lt;description&gt;Demo project for Spring Boot&lt;/description&gt;    &lt;properties&gt;        &lt;java.version&gt;1.8&lt;/java.version&gt;        &lt;spring-cloud.version&gt;Greenwich.SR3&lt;/spring-cloud.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;        &lt;!--gateway--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--nacos dicovery--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;dependencyManagement&gt;        &lt;dependencies&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;                &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;                &lt;version&gt;${spring-cloud.version}&lt;/version&gt;                &lt;type&gt;pom&lt;/type&gt;                &lt;scope&gt;import&lt;/scope&gt;            &lt;/dependency&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;                &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt;                &lt;version&gt;0.2.2.RELEASE&lt;/version&gt;                &lt;type&gt;pom&lt;/type&gt;                &lt;scope&gt;import&lt;/scope&gt;            &lt;/dependency&gt;        &lt;/dependencies&gt;    &lt;/dependencyManagement&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre><p>需要注意在Gateway服务中的pom.xml文件中不要存在这个jar</p><pre><code>&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt;    &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;</code></pre><p>否则调用接口时会报以下错误因为gateway使用的是webflux,默认使用netty,所以从依赖中排除 tomcat相关的依赖</p><pre><code>java.lang.ClassCastException: org.springframework.core.io.buffer.DefaultDataBufferFactory cannot be cast to org.springframework.core.io.buffer.NettyDataBufferFactory    at org.springframework.cloud.gateway.filter.NettyWriteResponseFilter.lambda$filter$1(NettyWriteResponseFilter.java:82) ~[spring-cloud-gateway-core-2.1.3.RELEASE.jar:2.1.3.RELEASE]    at reactor.core.publisher.MonoDefer.subscribe(MonoDefer.java:44) [reactor-core-3.2.12.RELEASE.jar:3.2.12.RELEASE]</code></pre><p>错误2 是由于 spring-boot-starter-web 引起</p><h3 id="服务发现配置：从Nacos获取微服务提供者清单"><a href="#服务发现配置：从Nacos获取微服务提供者清单" class="headerlink" title="服务发现配置：从Nacos获取微服务提供者清单"></a>服务发现配置：从Nacos获取微服务提供者清单</h3><pre><code>server:  port: 8087spring:  application:    name: nacos_gateway  cloud:    nacos:      discovery:        server-addr: 127.0.0.1:8848    gateway:      discovery:        locator:          enabled: true  #表明gateway开启服务注册和发现的功能，并且spring cloud gateway自动根据服务发现为每一个服务创建了一个router，这个router将以服务名开头的请求路径转发到对应的服务          lower-case-service-id: true  #是将请求路径上的服务名配置为小写（因为服务注册的时候，向注册中心注册时将服务名转成大写的了      routes:          -id: apiuser          #          uri: lb://nacos-consumer-user          predicates:          # http://localhost:6601/user/user/users/2, 必须加上StripPrefix=1，否则访问服务时会带上user          - Path=/user/** # 转发该路径           #以下是配置例子            # - id: 163                     #网关路由到网易官网            #  uri: http://www.163.com/            #  predicates:                - Path=/163/**        #      - id: ORDER-SERVICE           #网关路由到订单服务order-service        #        uri: lb://ORDER-SERVICE        #        predicates:        #          - Path=/ORDER-SERVICE/**        #      - id: USER-SERVICE            #网关路由到用户服务user-service        #        uri: lb://USER-SERVICE        #        predicates:        #          - Pach=/USER-SERVICE/**</code></pre><h2 id="整合Swagger"><a href="#整合Swagger" class="headerlink" title="整合Swagger"></a>整合Swagger</h2><p>整合Swagger聚合微服务系统API文档</p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;cloud-center-alibaba&lt;/artifactId&gt;        &lt;groupId&gt;com.crazymaker.springcloud&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;groupId&gt;com.crazymaker.springcloud&lt;/groupId&gt;    &lt;artifactId&gt;springcloud-gateway-demo&lt;/artifactId&gt;    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;name&gt;springcloud-gateway-demo&lt;/name&gt;    &lt;packaging&gt;jar&lt;/packaging&gt;    &lt;dependencies&gt;        &lt;!--gateway 网关依赖,内置webflux 依赖 --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--新增sentinel--&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt;            &lt;artifactId&gt;sentinel-spring-cloud-gateway-adapter&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt;            &lt;artifactId&gt;sentinel-transport-simple-http&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!-- nacos服务注册发现依赖--&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;groupId&gt;com.google.guava&lt;/groupId&gt;                    &lt;artifactId&gt;guava&lt;/artifactId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;mysql&lt;/groupId&gt;            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;            &lt;version&gt;${mysql.connector.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- nacos配置服务依赖--&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;groupId&gt;com.google.guava&lt;/groupId&gt;                    &lt;artifactId&gt;guava&lt;/artifactId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;cn.hutool&lt;/groupId&gt;            &lt;artifactId&gt;hutool-all&lt;/artifactId&gt;            &lt;version&gt;${hutool.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;io.springfox&lt;/groupId&gt;            &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt;            &lt;version&gt;${swagger.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;io.springfox&lt;/groupId&gt;            &lt;artifactId&gt;springfox-swagger-common&lt;/artifactId&gt;            &lt;version&gt;${swagger.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.github.xiaoymin&lt;/groupId&gt;            &lt;artifactId&gt;swagger-bootstrap-ui&lt;/artifactId&gt;            &lt;version&gt;${swagger-ui.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-commons&lt;/artifactId&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;                &lt;configuration&gt;                    &lt;mainClass&gt;com.crazymaker.cloud.nacos.demo.gateway.starter.GatewayProviderApplication&lt;/mainClass&gt;                &lt;/configuration&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;goals&gt;                            &lt;goal&gt;repackage&lt;/goal&gt;                        &lt;/goals&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;            &lt;plugin&gt;                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;                &lt;version&gt;2.4.1&lt;/version&gt;                &lt;configuration&gt;                    &lt;descriptors&gt;                        &lt;descriptor&gt;src/main/assembly/assembly.xml&lt;/descriptor&gt;                    &lt;/descriptors&gt;                &lt;/configuration&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;id&gt;make-assembly&lt;/id&gt;                        &lt;phase&gt;package&lt;/phase&gt;                        &lt;goals&gt;                            &lt;goal&gt;single&lt;/goal&gt;                        &lt;/goals&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;            &lt;!-- 添加docker-maven插件 --&gt;            &lt;plugin&gt;                &lt;groupId&gt;com.spotify&lt;/groupId&gt;                &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt;                &lt;version&gt;1.1.1&lt;/version&gt;                &lt;configuration&gt;                    &lt;imageName&gt;dockerlocal:5000/${project.artifactId}:${project.version}&lt;/imageName&gt;                    &lt;baseImage&gt;dockerlocal:5000/java&lt;/baseImage&gt;                    &lt;entryPoint&gt;[&quot;java&quot;, &quot;-jar&quot;, &quot;/${project.build.finalName}.jar&quot;]&lt;/entryPoint&gt;                    &lt;dockerDirectory&gt;docker&lt;/dockerDirectory&gt;                    &lt;resources&gt;                        &lt;resource&gt;                            &lt;targetPath&gt;/&lt;/targetPath&gt;                            &lt;directory&gt;${project.build.directory}&lt;/directory&gt;                            &lt;include&gt;${project.build.finalName}.jar&lt;/include&gt;                        &lt;/resource&gt;                    &lt;/resources&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre><p>配置文件</p><pre><code>@Component@Primary@AllArgsConstructorpublic class SwaggerConfig implements SwaggerResourcesProvider{    public static final String API_URI = &quot;/v2/api-docs&quot;;    private final RouteLocator routeLocator;    private final GatewayProperties gatewayProperties;    @Override    public List&lt;SwaggerResource&gt; get()    {        /**         * 网关应用名称,不需要在网关的swagger 上展示         */        String appName = &quot;springcloud-gateway&quot;;        List&lt;SwaggerResource&gt; resources = new ArrayList&lt;&gt;();        List&lt;String&gt; routes = new ArrayList&lt;&gt;();        // 取出gateway的route        routeLocator.getRoutes().subscribe(route -&gt; routes.add(route.getId()));        // 结合配置的route-路径(Path)，和route过滤，只获取有效的route节点        // 打开下面注释可以自动扫描接入gateway的服务，为了演示，只扫描system        // gatewayProperties.getRoutes().stream().filter(routeDefinition -&gt;        // routes.contains(routeDefinition.getId()))        gatewayProperties.getRoutes().stream()                .filter(route -&gt; route.getUri().getHost() != null)                .filter(route -&gt; !appName.equals(route.getUri().getHost()))                .forEach(routeDefinition -&gt; routeDefinition.getPredicates().stream()                        .filter(predicateDefinition -&gt; (&quot;Path&quot;).equalsIgnoreCase(predicateDefinition.getName()))                        .forEach(predicateDefinition -&gt; resources                                .add(swaggerResource(routeDefinition.getId(), predicateDefinition.getArgs()                                        .get(NameUtils.GENERATED_NAME_PREFIX + &quot;0&quot;).replace(&quot;/**&quot;, API_URI)))));        return resources;    }    private SwaggerResource swaggerResource(String name, String location)    {        SwaggerResource swaggerResource = new SwaggerResource();        swaggerResource.setName(name);        swaggerResource.setLocation(location);        swaggerResource.setSwaggerVersion(&quot;2.0&quot;);        return swaggerResource;    }}</code></pre><h2 id="Gatway-网关的过滤器开发"><a href="#Gatway-网关的过滤器开发" class="headerlink" title="Gatway 网关的过滤器开发"></a>Gatway 网关的过滤器开发</h2><h3 id="过滤器的执行次序"><a href="#过滤器的执行次序" class="headerlink" title="过滤器的执行次序"></a>过滤器的执行次序</h3><p>Spring-Cloud-Gateway 基于过滤器实现，同 zuul 类似，有pre和post两种方式的 filter,分别处理前置逻辑和后置逻辑。客户端的请求先经过pre类型的 filter，然后将请求转发到具体的业务服务，收到业务服务的响应之后，再经过post类型的 filter 处理，最后返回响应到客户端。</p><p>过滤器执行流程如下，order 越大，优先级越低</p><p><img src="/2021/10/17/2021-10-17-springcloudgateway/spring-cloud-gateway-fliter-order.png" alt></p><p>分为全局过滤器和局部过滤器</p><h4 id="全局过滤器："><a href="#全局过滤器：" class="headerlink" title="全局过滤器："></a>全局过滤器：</h4><ul><li>对所有路由生效</li><li>接口用时统计</li></ul><h4 id="局部过滤器："><a href="#局部过滤器：" class="headerlink" title="局部过滤器："></a>局部过滤器：</h4><ul><li>对指定路由生效</li></ul><h3 id="定义全局过滤器"><a href="#定义全局过滤器" class="headerlink" title="定义全局过滤器"></a>定义全局过滤器</h3><p>实现 GlobalFilter 和 Ordered，重写相关方法，加入到spring容器管理即可，无需配置，全局过滤器对所有的路由都有效。</p><pre><code>@Configurationpublic class FilterConfig{    @Bean    @Order(-1)    public GlobalFilter a()    {        return new AFilter();    }    @Bean    @Order(0)    public GlobalFilter b()    {        return new BFilter();    }    @Bean    @Order(1)    public GlobalFilter c()    {        return new CFilter();    }    @Slf4j    public class AFilter implements GlobalFilter, Ordered    {        @Override        public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain)        {            log.info(&quot;AFilter前置逻辑&quot;);            return chain.filter(exchange).then(Mono.fromRunnable(() -&gt;            {                log.info(&quot;AFilter后置逻辑&quot;);            }));        }        //   值越小，优先级越高//    int HIGHEST_PRECEDENCE = -2147483648;//    int LOWEST_PRECEDENCE = 2147483647;        @Override        public int getOrder()        {            return HIGHEST_PRECEDENCE + 100;        }    }    @Slf4j    public class BFilter implements GlobalFilter, Ordered    {        @Override        public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain)        {            log.info(&quot;BFilter前置逻辑&quot;);            return chain.filter(exchange).then(Mono.fromRunnable(() -&gt;            {                log.info(&quot;BFilter后置逻辑&quot;);            }));        }        //   值越小，优先级越高//    int HIGHEST_PRECEDENCE = -2147483648;//    int LOWEST_PRECEDENCE = 2147483647;        @Override        public int getOrder()        {            return HIGHEST_PRECEDENCE + 200;        }    }    @Slf4j    public class CFilter implements GlobalFilter, Ordered    {        @Override        public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain)        {            log.info(&quot;CFilter前置逻辑&quot;);            return chain.filter(exchange).then(Mono.fromRunnable(() -&gt;            {                log.info(&quot;CFilter后置逻辑&quot;);            }));        }        //   值越小，优先级越高//    int HIGHEST_PRECEDENCE = -2147483648;//    int LOWEST_PRECEDENCE = 2147483647;        @Override        public int getOrder()        {            return HIGHEST_PRECEDENCE + 300;        }    }}</code></pre><h3 id="定义局部过滤器"><a href="#定义局部过滤器" class="headerlink" title="定义局部过滤器"></a>定义局部过滤器</h3><ol><li>需要实现GatewayFilter, Ordered，实现相关的方法</li><li>加入到过滤器工厂，并且注册到spring容器中。</li><li>在配置文件中进行配置，如果不配置则不启用此过滤器规则。</li></ol><p>局部过滤器举例, 对请求头部的 user-id 进行校验，代码如下：</p><h4 id="需要实现GatewayFilter-Ordered，实现相关的方法"><a href="#需要实现GatewayFilter-Ordered，实现相关的方法" class="headerlink" title="需要实现GatewayFilter, Ordered，实现相关的方法"></a>需要实现GatewayFilter, Ordered，实现相关的方法</h4><pre><code>//@Component@Slf4jpublic class UserIdCheckGateWayFilter implements GatewayFilter, Ordered{    @Override    public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain)    {        String url = exchange.getRequest().getPath().pathWithinApplication().value();        log.info(&quot;请求URL:&quot; + url);        log.info(&quot;method:&quot; + exchange.getRequest().getMethod());       /*   String secret = exchange.getRequest().getHeaders().getFirst(&quot;secret&quot;);        if (StringUtils.isBlank(secret))        {            return chain.filter(exchange);        }*/         //获取param 请求参数        String uname = exchange.getRequest().getQueryParams().getFirst(&quot;uname&quot;);        //获取header        String userId = exchange.getRequest().getHeaders().getFirst(&quot;user-id&quot;);        log.info(&quot;userId：&quot; + userId);        if (StringUtils.isBlank(userId))        {            log.info(&quot;*****头部验证不通过，请在头部输入  user-id&quot;);            //终止请求，直接回应            exchange.getResponse().setStatusCode(HttpStatus.NOT_ACCEPTABLE);            return exchange.getResponse().setComplete();        }        return chain.filter(exchange);    }    //   值越小，优先级越高//    int HIGHEST_PRECEDENCE = -2147483648;//    int LOWEST_PRECEDENCE = 2147483647;    @Override    public int getOrder()    {        return HIGHEST_PRECEDENCE;    }}</code></pre><h4 id="加入到过滤器工厂，并且注册到spring容器中。"><a href="#加入到过滤器工厂，并且注册到spring容器中。" class="headerlink" title="加入到过滤器工厂，并且注册到spring容器中。"></a>加入到过滤器工厂，并且注册到spring容器中。</h4><pre><code>@Componentpublic class UserIdCheckGatewayFilterFactory extends AbstractGatewayFilterFactory&lt;Object&gt;{    @Override    public GatewayFilter apply(Object config)    {        return new UserIdCheckGateWayFilter();    }}</code></pre><h4 id="在配置文件中进行配置，如果不配置则不启用此过滤器规则。"><a href="#在配置文件中进行配置，如果不配置则不启用此过滤器规则。" class="headerlink" title="在配置文件中进行配置，如果不配置则不启用此过滤器规则。"></a>在配置文件中进行配置，如果不配置则不启用此过滤器规则。</h4><pre><code>- id: service_provider_demo_route_filter    uri: lb://service-provider-demo    predicates:    - Path=/filter/**    filters:    - RewritePath=/filter/(?&lt;segment&gt;.*), /provider/$\{segment}    - UserIdCheckGateWayFilter</code></pre>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringCloud </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>springboot自动装配</title>
      <link href="/2021/10/15/2021-10-15-springboot-zi-dong-zhuang-pei/"/>
      <url>/2021/10/15/2021-10-15-springboot-zi-dong-zhuang-pei/</url>
      
        <content type="html"><![CDATA[<h2 id="自动装配是什么"><a href="#自动装配是什么" class="headerlink" title="自动装配是什么"></a>自动装配是什么</h2><p>SpringBoot 定义了一套接口规范，这套规范规定：SpringBoot 在启动时会扫描外部引用 jar 包中的META-INF/spring.factories文件，将文件中配置的类型信息加载到 Spring 容器，并执行类中定义的各种操作。对于外部 jar 来说，只需要按照 SpringBoot 定义的标准，就能将自己的功能装置进 SpringBoot。</p><p>没有 Spring Boot 的情况下，如果我们需要引入第三方依赖，需要手动配置，非常麻烦。但是，Spring Boot 中，我们直接引入一个 starter 即可。比如你想要在项目中使用 redis 的话，直接在项目中引入对应的 starter 即可。</p><pre><code>&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;</code></pre><p>引入 starter 之后，我们通过少量注解和一些简单的配置就能使用第三方组件提供的功能了。</p><p>在我看来，自动装配可以简单理解为：<strong>通过注解或者一些简单的配置就能在 Spring Boot 的帮助下实现某块功能</strong>。</p><h2 id="自动装配如何实现"><a href="#自动装配如何实现" class="headerlink" title="自动装配如何实现"></a>自动装配如何实现</h2><p>先看一下 SpringBoot 的核心注解 SpringBootApplication</p><pre><code>@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited&lt;1.&gt;@SpringBootConfiguration&lt;2.&gt;@ComponentScan&lt;3.&gt;@EnableAutoConfigurationpublic @interface SpringBootApplication {}@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@Configuration //实际上它也是一个配置类public @interface SpringBootConfiguration {}</code></pre><p>大概可以把 @SpringBootApplication看作是 @Configuration、@EnableAutoConfiguration、@ComponentScan 注解的集合。根据 SpringBoot 官网，这三个注解的作用分别是：</p><ul><li>@EnableAutoConfiguration：启用 SpringBoot 的自动配置机制</li><li>@Configuration：允许在上下文中注册额外的 bean 或导入其他配置类</li><li>@ComponentScan： 扫描被@Component (@Service,@Controller)注解的 bean，注解默认会扫描启动类所在的包下所有的类 ，可以自定义不扫描某些 bean。如下图所示，容器中将排除TypeExcludeFilter和AutoConfigurationExcludeFilter。</li></ul><h3 id="EnableAutoConfiguration核心注解"><a href="#EnableAutoConfiguration核心注解" class="headerlink" title="@EnableAutoConfiguration核心注解"></a>@EnableAutoConfiguration核心注解</h3><p>@EnableAutoConfiguration:实现自动装配的核心注解</p><p>EnableAutoConfiguration 只是一个简单地注解，自动装配核心功能的实现实际是通过 AutoConfigurationImportSelector类。</p><pre><code>@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@AutoConfigurationPackage //作用：将main包下的所欲组件注册到容器中@Import({AutoConfigurationImportSelector.class}) //加载自动装配类 xxxAutoconfigurationpublic @interface EnableAutoConfiguration {    String ENABLED_OVERRIDE_PROPERTY = &quot;spring.boot.enableautoconfiguration&quot;;    Class&lt;?&gt;[] exclude() default {};    String[] excludeName() default {};}</code></pre><h4 id="AutoConfigurationImportSelector-加载自动装配类"><a href="#AutoConfigurationImportSelector-加载自动装配类" class="headerlink" title="AutoConfigurationImportSelector:加载自动装配类"></a>AutoConfigurationImportSelector:加载自动装配类</h4><p>AutoConfigurationImportSelector类的继承体系如下：</p><pre><code>public class AutoConfigurationImportSelector implements DeferredImportSelector, BeanClassLoaderAware, ResourceLoaderAware, BeanFactoryAware, EnvironmentAware, Ordered {}public interface DeferredImportSelector extends ImportSelector {}public interface ImportSelector {    String[] selectImports(AnnotationMetadata var1);}</code></pre><p>可以看出，AutoConfigurationImportSelector 类实现了 ImportSelector接口，也就实现了这个接口中的 selectImports方法，该方法主要用于获取所有符合条件的类的全限定类名，这些类需要被加载到 IoC 容器中。</p><pre><code>private static final String[] NO_IMPORTS = new String[0];public String[] selectImports(AnnotationMetadata annotationMetadata) {        // &lt;1&gt;.判断自动装配开关是否打开        if (!this.isEnabled(annotationMetadata)) {            return NO_IMPORTS;        } else {          //&lt;2&gt;.获取所有需要装配的bean            AutoConfigurationMetadata autoConfigurationMetadata = AutoConfigurationMetadataLoader.loadMetadata(this.beanClassLoader);            AutoConfigurationImportSelector.AutoConfigurationEntry autoConfigurationEntry = this.getAutoConfigurationEntry(autoConfigurationMetadata, annotationMetadata);            return StringUtils.toStringArray(autoConfigurationEntry.getConfigurations());        }    }</code></pre><p>这里我们需要重点关注一下getAutoConfigurationEntry()方法，这个方法主要负责加载自动配置类的。</p><p>该方法调用链如下：</p><p><img src="/2021/10/15/2021-10-15-springboot-zi-dong-zhuang-pei/3c1200712655443ca4b38500d615bb70~tplv-k3u1fbpfcp-watermark.image.png" alt></p><p>现在我们结合getAutoConfigurationEntry()的源码来详细分析一下：</p><pre><code>private static final AutoConfigurationEntry EMPTY_ENTRY = new AutoConfigurationEntry();AutoConfigurationEntry getAutoConfigurationEntry(AutoConfigurationMetadata autoConfigurationMetadata, AnnotationMetadata annotationMetadata) {        //&lt;1&gt;.        if (!this.isEnabled(annotationMetadata)) {            return EMPTY_ENTRY;        } else {            //&lt;2&gt;.            AnnotationAttributes attributes = this.getAttributes(annotationMetadata);            //&lt;3&gt;.            List&lt;String&gt; configurations = this.getCandidateConfigurations(annotationMetadata, attributes);            //&lt;4&gt;.            configurations = this.removeDuplicates(configurations);            Set&lt;String&gt; exclusions = this.getExclusions(annotationMetadata, attributes);            this.checkExcludedClasses(configurations, exclusions);            configurations.removeAll(exclusions);            configurations = this.filter(configurations, autoConfigurationMetadata);            this.fireAutoConfigurationImportEvents(configurations, exclusions);            return new AutoConfigurationImportSelector.AutoConfigurationEntry(configurations, exclusions);        }    }</code></pre><ol><li>判断自动装配开关是否打开。默认spring.boot.enableautoconfiguration=true，可在 application.properties 或 application.yml 中设置</li></ol><p><img src="/2021/10/15/2021-10-15-springboot-zi-dong-zhuang-pei/77aa6a3727ea4392870f5cccd09844ab~tplv-k3u1fbpfcp-watermark.image.png" alt></p><ol start="2"><li>用于获取EnableAutoConfiguration注解中的 exclude 和 excludeName。</li></ol><p><img src="/2021/10/15/2021-10-15-springboot-zi-dong-zhuang-pei/3d6ec93bbda1453aa08c52b49516c05a~tplv-k3u1fbpfcp-zoom-1.image.png" alt></p><ol start="3"><li>获取需要自动装配的所有配置类，读取META-INF/spring.factories</li></ol><p><img src="/2021/10/15/2021-10-15-springboot-zi-dong-zhuang-pei/58c51920efea4757aa1ec29c6d5f9e36~tplv-k3u1fbpfcp-watermark.image.png" alt></p><p>从下图可以看到这个文件的配置内容都被我们读取到了。XXXAutoConfiguration的作用就是按需加载组件。</p><p><img src="/2021/10/15/2021-10-15-springboot-zi-dong-zhuang-pei/94d6e1a060ac41db97043e1758789026~tplv-k3u1fbpfcp-watermark.image.png" alt></p><p>不光是这个依赖下的META-INF/spring.factories被读取到，所有 Spring Boot Starter 下的META-INF/spring.factories都会被读取到。</p><ol start="4"><li>spring.factories中这么多配置，每次启动都要全部加载么？”。</li></ol><p>很明显，这是不现实的。我们 debug 到后面你会发现，configurations 的值变小了。</p><p><img src="/2021/10/15/2021-10-15-springboot-zi-dong-zhuang-pei/267f8231ae2e48d982154140af6437b0~tplv-k3u1fbpfcp-watermark.image.png" alt></p><p>因为，这一步有经历了一遍筛选，@ConditionalOnXXX 中的所有条件都满足，该类才会生效。</p><pre><code>@Configuration// 检查相关的类：RabbitTemplate 和 Channel是否存在// 存在才会加载@ConditionalOnClass({ RabbitTemplate.class, Channel.class })@EnableConfigurationProperties(RabbitProperties.class)@Import(RabbitAnnotationDrivenConfiguration.class)public class RabbitAutoConfiguration {}</code></pre><ul><li>@ConditionalOnBean：当容器里有指定 Bean 的条件下</li><li>@ConditionalOnMissingBean：当容器里没有指定 Bean 的情况下</li><li>@ConditionalOnSingleCandidate：当指定 Bean 在容器中只有一个，或者虽然有多个但是指定首选 Bean</li><li>@ConditionalOnClass：当类路径下有指定类的条件下</li><li>@ConditionalOnMissingClass：当类路径下没有指定类的条件下</li><li>@ConditionalOnProperty：指定的属性是否有指定的值</li><li>@ConditionalOnResource：类路径是否有指定的值</li><li>@ConditionalOnExpression：基于 SpEL 表达式作为判断条件</li><li>@ConditionalOnJava：基于 Java 版本作为判断条件</li><li>@ConditionalOnJndi：在 JNDI 存在的条件下差在指定的位置</li><li>@ConditionalOnNotWebApplication：当前项目不是 Web 项目的条件下</li><li>@ConditionalOnWebApplication：当前项目是 Web 项 目的条件下</li></ul><h2 id="如何实现一个-Starter"><a href="#如何实现一个-Starter" class="headerlink" title="如何实现一个 Starter"></a>如何实现一个 Starter</h2><h3 id="新创建一个maven项目"><a href="#新创建一个maven项目" class="headerlink" title="新创建一个maven项目"></a>新创建一个maven项目</h3><p>这里创建一个threadpool-spring-boot-starter</p><p><img src="/2021/10/15/2021-10-15-springboot-zi-dong-zhuang-pei/1ff0ebe7844f40289eb60213af72c5a6~tplv-k3u1fbpfcp-watermark.image.png" alt></p><h3 id="引入-Spring-Boot-相关依赖"><a href="#引入-Spring-Boot-相关依赖" class="headerlink" title="引入 Spring Boot 相关依赖"></a>引入 Spring Boot 相关依赖</h3><pre><code>&lt;groupId&gt;org.example&lt;/groupId&gt;    &lt;artifactId&gt;threadpool-spring-boot-starter&lt;/artifactId&gt;    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;packaging&gt;jar&lt;/packaging&gt;    &lt;parent&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;        &lt;version&gt;2.2.6.RELEASE&lt;/version&gt;    &lt;/parent&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;            &lt;version&gt;2.2.6.RELEASE&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;</code></pre><h3 id="创建ThreadPoolAutoConfiguration"><a href="#创建ThreadPoolAutoConfiguration" class="headerlink" title="创建ThreadPoolAutoConfiguration"></a>创建ThreadPoolAutoConfiguration</h3><pre><code>@Configurationpublic class ThreadPoolAutoConfiguration {    // 第一种方式    @Bean    @ConditionalOnMissingBean    public MyThreadPool myThreadPool1() {        return new MyThreadPool();    }    // 第二种方式    @Bean    @ConditionalOnClass(ThreadPoolExecutor.class)    public ThreadPoolExecutor myThreadPool() {        // rejection-policy：当pool已经达到max size的时候，如何处理新任务        // CALLER_RUNS：不在新线程中执行任务，而是有调用者所在的线程来执行        // 一下参数可配置从配置文件获取，引用方配置参数即可@value(&amp;{xxx.xxxx.corePoolSize})        return new ThreadPoolExecutor(10,                10,                10,                TimeUnit.SECONDS,                new LinkedBlockingQueue&lt;Runnable&gt;(100),                new ThreadPoolExecutor.CallerRunsPolicy());    }}</code></pre><pre><code>public class MyThreadPool {    // 第一种方式    public ThreadPoolExecutor get() {        // rejection-policy：当pool已经达到max size的时候，如何处理新任务        // CALLER_RUNS：不在新线程中执行任务，而是有调用者所在的线程来执行        return new ThreadPoolExecutor(10,                10,                10,                TimeUnit.SECONDS,                new LinkedBlockingQueue&lt;Runnable&gt;(100),                new ThreadPoolExecutor.CallerRunsPolicy());    }}</code></pre><h3 id="配置spring-factories"><a href="#配置spring-factories" class="headerlink" title="配置spring.factories"></a>配置spring.factories</h3><p>在threadpool-spring-boot-starter工程的 resources 包下创建META-INF/spring.factories文件</p><pre><code>org.springframework.boot.autoconfigure.EnableAutoConfiguration=threadPool.starter.ThreadPoolAutoConfiguration</code></pre><h3 id="install-threadpool-spring-boot-starter工程"><a href="#install-threadpool-spring-boot-starter工程" class="headerlink" title="install threadpool-spring-boot-starter工程"></a>install threadpool-spring-boot-starter工程</h3><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><p>其他工程引入pom.xml</p><pre><code>&lt;dependency&gt;    &lt;groupId&gt;org.example&lt;/groupId&gt;    &lt;artifactId&gt;threadpool-spring-boot-starter&lt;/artifactId&gt;    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt;</code></pre><pre><code>    @Resource    private MyThreadPool myThreadPool;    @Resource    private ThreadPoolExecutor threadPoolExecutor;    @Test    public void test111() {        System.out.println(myThreadPool.get().getCorePoolSize());        System.out.println(threadPoolExecutor.getCorePoolSize());    }</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Spring Boot启动类里面有个@SpringBootApplication注解，这个注解是@Configuration、@EnableAutoConfiguration、@ComponentScan 注解的集合。</p><p>@EnableAutoConfiguration注解里面@Import({AutoConfigurationImportSelector.class}) //加载自动装配类 xxxAutoconfiguration。AutoConfigurationImportSelector 类实现了 ImportSelector接口，也就实现了这个接口中的 selectImports方法，该方法主要用于获取所有符合条件的类的全限定类名，这些类需要被加载到 IoC 容器中。</p><p>不光是这个依赖下的META-INF/spring.factories被读取到，所有 Spring Boot Starter 下的META-INF/spring.factories都会被读取到。但需要过滤，@ConditionalOnXXX。</p><p>Spring Boot 通过@EnableAutoConfiguration开启自动装配，通过 SpringFactoriesLoader 最终加载META-INF/spring.factories中的自动配置类实现自动装配，自动配置类其实就是通过@Conditional按需加载的配置类，想要其生效必须引入spring-boot-starter-xxx包实现起步依赖</p>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二叉堆</title>
      <link href="/2021/09/24/2021-09-24-dui-pai-xu/"/>
      <url>/2021/09/24/2021-09-24-dui-pai-xu/</url>
      
        <content type="html"><![CDATA[<h2 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h2><p>假设我们要对一个数组从小到大进行排序，首先我们可以将原数组中的数据建立成一个大顶堆。这样，最大的元素就会在数组的首位。正常情况下，从堆中删除一个元素，是直接将堆顶元素弹出，然后将堆中最后一位的元素放到堆顶，再做向下调整的。这样的话，原来堆中的末尾元素位置就空了出来。</p><p><img src="/2021/09/24/2021-09-24-dui-pai-xu/5a36da1091252111c51yy86948208006.webp" alt></p><p>现在，由于要对原数组进行排序，因此我们可以把弹出的堆顶元素与堆中的末尾元素进行位置交换，再向下做调整。也就是将图中的元素 9 和 4 做调换，再对 4 做向下调整。经过一轮这样的操作，我们就可以将一个堆顶的最大值放到正确的排序位置上。我在下图中给出了三轮操作以后，数组中元素的排序情况：</p><p><img src="/2021/09/24/2021-09-24-dui-pai-xu/11e68f1aa9f83a403b279f8a805fbb1e.webp" alt></p><p>经过三轮弹出大顶堆顶元素的操作以后，原数组中最大的三个值就被放置到了最后三位。当大顶堆中元素弹空时，也就完成了对原数组排序的过程。</p><h2 id="堆排序的流程"><a href="#堆排序的流程" class="headerlink" title="堆排序的流程"></a>堆排序的流程</h2><ol><li>在原数组上建立堆结构</li><li>将堆顶元素与堆末元素进行调换，再对堆顶元素进行向下调整</li><li>经过 n 轮操作以后，数组中的元素就有序了</li></ol><p>对于第 1 步，如果想在一个数组上建立一个堆结构，我们要怎么做呢？</p><p>一种最直接的方式，就是我们先将原数组分成两部分，前半部分是堆，后半部分是数组中的元素。然后通过堆的向上调整策略，我们依次将后面的元素插入到前面的堆结构中。下图展示的就是用这种尾插法建堆的前三轮数组中的元素情况：</p><p>一种最直接的方式，就是我们先将原数组分成两部分，前半部分是堆，后半部分是数组中的元素。然后通过堆的向上调整策略，我们依次将后面的元素插入到前面的堆结构中。下图展示的就是用这种尾插法建堆的前三轮数组中的元素情况：</p><p><img src="/2021/09/24/2021-09-24-dui-pai-xu/88182559c591392400e37e40e0d80bec.webp" alt></p><p>这种建堆的方法比较直观，所以建堆的时间复杂度我们很容易就可以计算出来，就是 O(nlogn)。</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 好好学算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二叉堆</title>
      <link href="/2021/09/22/2021-09-22-er-cha-dui/"/>
      <url>/2021/09/22/2021-09-22-er-cha-dui/</url>
      
        <content type="html"><![CDATA[<h2 id="完全二叉树"><a href="#完全二叉树" class="headerlink" title="完全二叉树"></a>完全二叉树</h2><p>堆就是一种具有特殊性质的完全二叉树。</p><p>完全二叉树是由满二叉树去掉最后一层右侧的若干节点而形成的二叉树结构。</p><p><img src="/2021/09/22/2021-09-22-er-cha-dui/be70f37ca4ab58fbdcf3d01dd1b5c3f2.webp" alt></p><p>完全二叉树有一个很优秀的性质，就是可以被存储在一片连续的数组空间中。</p><p>上面这个示意图，如果我们对它的每个节点进行编号，采用从上到下、从左到右的顺序依次标上 1 到 6。那么你仔细观察其中父节点编号与子节点编号之间的关系就会发现，如果父节点编号是 i，其左孩子的编号就是 2 * i，右孩子的编号就是 2 * i + 1。例如，我们以 3 号节点作为父节点，其左孩子的编号就是 2 * 3 = 6 号，如果有右孩子，那它右孩子的编号就一定是 2 * 3 + 1 = 7。</p><p>在一棵有 n 个节点的完全二叉树中，节点编号应该为 1 到 n。这样，我们就可以使用数组的 1 到 n 位对应于这 n 个节点。由于完全二叉树父节点与子节点编号之间的特殊计算关系，因此只要我们知道父节点编号，就可以通过计算得到子节点编号。</p><p>即使我们将完全二叉树的所有数据，存储在一个连续的数组空间中，也不会破坏其特殊的树形结构信息。也就是说，一棵完全二叉树可以对应到一段连续的数组空间，而根据数组空间的内容，我们也可以唯一地还原成一棵完全二叉树。</p><p><img src="/2021/09/22/2021-09-22-er-cha-dui/9754d1c6579002df4ba6ca5894509300.webp" alt></p><p>实际上，在计算机中，我们会把数组作为完全二叉树的实际存储结构，而完全二叉树，则是我们重新看待这段数组信息的思维逻辑结构。</p><h2 id="堆结构的定义"><a href="#堆结构的定义" class="headerlink" title="堆结构的定义"></a>堆结构的定义</h2><p>堆可以分为两类，小顶堆和大顶堆。</p><ul><li>小顶堆:如果在一棵完全二叉树中，每个父节点的值都要小于其两个子节点的值，我们就管这种结构叫做小顶堆。</li><li>大顶堆:大顶堆就是每个父节点的值要大于其两个子节点的值。</li></ul><p><img src="/2021/09/22/2021-09-22-er-cha-dui/be5a6957yyc34996239ef34745489537.webp" alt></p><p>根据小顶堆的性质定义，我们可以轻松得知，小顶堆中的最小值一定放在了根节点，也就是存储在数组中的第一个位置。如果我们将数组中的所有元素看成一个集合的话，那小顶堆的作用就非常明显了，就是维护这个集合中的最小值。</p><p>堆的实际存储结构是数组，这个数组是一段从下标 1 开始的连续存储空间。</p><h2 id="堆的插入操作"><a href="#堆的插入操作" class="headerlink" title="堆的插入操作"></a>堆的插入操作</h2><p>假设我们想要在堆中放入一个新的元素 1，那么我们可以将这个新的元素，放置到整个数组的最后一位。对应到完全二叉树的思维逻辑结构中，就是向树中的最后一层添加了一个新的叶节点。</p><p><img src="/2021/09/22/2021-09-22-er-cha-dui/0de0a63918eabaea63846eab343b62ee.webp" alt></p><p>这一步的操作叫做元素放置。你会看到这么做之后，数组的结构就不满足小顶堆的性质了。所以下一步，我们要调整数组的结构，让它依然满足堆的性质。</p><p>由于堆的性质定义中，只规定了父节点与子节点之间的大小关系，所以，我们的调整操作只需要维护父子节点之间的大小关系即可。也就是说，新插入的元素 1，只需要和其父节点进行比较。</p><p>结合上面的示意图，我来说一下具体的操作。由于 1 比 4 小，所以我们把 1 交换到 4 的位置，然后让 1 再继续向上跟当前的父节点比较。因为 1 比 2 小，所以再让它们交换。这一步叫做向上调整，它的原理就是在当前元素值小于其父节点值的时候，交换子节点与父节点值的位置，就这样一直向上调整，直到当前节点大于父节点的值或者调整到了堆顶。</p><p>这样一来，经过元素放置 以及 2 次 向上调整 以后，堆中就增加了一个新元素，还依然满足堆的性质定义。这样，我们就完成了向堆中插入元素的操作。具体过程如图所示：</p><p><img src="/2021/09/22/2021-09-22-er-cha-dui/56ca8f6bf9c294d085bbba9a0be0d544.webp" alt></p><h2 id="堆的删除最值操作"><a href="#堆的删除最值操作" class="headerlink" title="堆的删除最值操作"></a>堆的删除最值操作</h2><p>堆的删除操作是有局限性的，这怎么理解呢？我们可以从删除操作的定义入手。堆的删除操作也叫做删除最值元素，对小顶堆进行删除操作就是删除最小的元素，其实就是删除数组中第一位的元素。</p><p>那为了保证删除最值元素以后，整个堆结构的存储还是从数组的第 1 位开始的，所以我们第一步要做的就是元素覆盖，也就是用堆的最后一位元素，覆盖掉堆顶元素。</p><p><img src="/2021/09/22/2021-09-22-er-cha-dui/1bc73192812611ceefd3bf2f2c1709b5.webp" alt></p><p>接下来，我们就需要通过适当调整，让它重新满足堆的性质。调整的方法其实也很简单，就是从堆顶位置开始，每次从当前元素所在三元组中找到一个最小值，与当前元素交换。交换后，让当前位置的元素继续和下面两个元素比较，如果这个三元组中依然有最小值，那我们就继续向下调整，直到当前元素是三元组中的最小值为止。</p><p><img src="/2021/09/22/2021-09-22-er-cha-dui/2e41d9d82610ecba8e3df50fb3531dab.webp" alt></p><p>于是，针对这个小顶堆，我们通过 1 次 元素覆盖 和 1 次 向下调整，就完成了删除最值元素的操作。</p><h2 id="优先队列"><a href="#优先队列" class="headerlink" title="优先队列"></a>优先队列</h2><p>堆其实就是一个数组，在插入元素时，从数组的末尾放入元素，而删除元素时，是从数组的头部移出元素。</p><p><img src="/2021/09/22/2021-09-22-er-cha-dui/ecb6760c6f8yy2cb1def10ff63972bd2.webp" alt></p><p>队列结构就是从尾部入元素，从头部出元素。而且堆这种结构，每次从头部移出的都是当前堆中的最值元素。所以，如果我们把堆的存储结构看成是一个队列的话，那堆就是一种可以灵活控制元素出队优先级的数据结构，我们管这种结构就叫做优先队列。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>堆是一种特殊的完全二叉树，它可以分为两类，分别是大顶堆和小顶堆。在小顶堆中，每个父节点的值都要小于其两个子节点的值，大顶堆则相反。</li><li>堆有两种基本的结构操作，插入操作和删除最值操作。在插入操作中，我们是将新元素放到整个堆结构的末尾，然后对新插入的元素执行向上调整的操作，一直调整到满足堆的结构性质为止。而在删除操作中，我们是将堆顶元素弹出以后，再将堆的尾部元素移动到堆顶，对其执行向下调整的操作，一直调整到满足堆的结构性质为止。</li><li>堆是实现优先队列的其中一种方式，优先队列每次出队的元素，都是队列中优先级最大的值。</li><li>堆是用来维护集合最值的高效数据结构。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 好好学算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>快速排序优化</title>
      <link href="/2021/09/20/2021-09-20-kuai-su-pai-xu-you-hua/"/>
      <url>/2021/09/20/2021-09-20-kuai-su-pai-xu-you-hua/</url>
      
        <content type="html"><![CDATA[<h2 id="单边递归优化"><a href="#单边递归优化" class="headerlink" title="单边递归优化"></a>单边递归优化</h2><p>在快排函数的实现过程中，当本层完成了 partition 操作以后，剩余的工作就是等待着左边和右边的排序完成。代码如下所示：</p><pre><code>quick_sort(arr, l, x - 1); // 对左半边排序quick_sort(arr, x + 1 , r); // 对右半边排序</code></pre><p>这段代码就分别对基准值的左右两边进行了排序的递归调用。从程序的运行时间来考虑的话，我们每次函数调用，都会消耗掉一部分运行时间。那只要我们可以减少函数调用的次数，其实就可以加快一点程序运行的速度。</p><p>因此，单边递归优化的方式，就是当本层完成了 partition 操作以后，让本层继续完成基准值左边的 partition 操作，而基准值右边的排序工作交给下一层递归函数去处理。</p><p><img src="/2021/09/20/2021-09-20-kuai-su-pai-xu-you-hua/f5f8c3de54e917a5f11d706b95e4f35e.webp" alt></p><p>经过单边递归优化后生成的递归树如上图所示，为了方便你理解，我把在同一个函数调用内的操作用同样的颜色表示。一共用了黄色、红色、蓝色、绿色这 4 种颜色，也就是说我们实际调用了 4 次函数。而如果不采用单边递归法，实际发生的函数调用次数就是图中二叉树的节点个数，也就是 7 次。可见，我们采用了单边递归法以后，函数实际调用次数减少了一半。</p><p>下面是一段单边递归法的代码：</p><pre><code>void quick_sort(int *arr, int l, int r) {    while (l &lt; r) {        // 进行一轮 partition 操作        // 获得基准值的位置        int ind = partition(arr, l, r);        // 右侧正常调用递归函数         quick_sort(arr, ind + 1, r);        // 用本层处理左侧的排序        r = ind - 1;    }    return ;}</code></pre><p>从代码中可知，l 和 r 是数组中待排序的区间范围，ind 是本轮 partition 操作后基准值的位置。当找到基准值的位置以后，对于右侧从 ind + 1 到 r 位置，我们就正常调用递归函数。然后，我们通过将 r 设置为 ind - 1，直接利用本层 while 循环逻辑，继续对左侧进行 partition 等相关排序操作。</p><h2 id="基准值选取优化"><a href="#基准值选取优化" class="headerlink" title="基准值选取优化"></a>基准值选取优化</h2><p>如果基准值选取不合理的话，快速排序的时间复杂度有可能达到 O(n2) 这个量级，也就是退化成和选择排序、插入排序等算法一样的时间复杂度。只有当基准值每次都能将排序区间中的数据平分时，时间复杂度才是最好情况下的 O(nlogn)。</p><p><img src="/2021/09/20/2021-09-20-kuai-su-pai-xu-you-hua/c4a0920318d94089bd516cf2925bfdd8.webp" alt></p><p>所谓三点取中法，就是每一轮取排序区间的头、尾和中间元素这三个值，然后把它们排序以后的中间值作为本轮的基准值。当然，你也可以根据自己的理解，调整要选取的这三个值的位置。我们就以上图为例，假设本轮的三个值分别为 2、9、7，中间值是 7，所以，本轮的基准值就是 7。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>单边递归法可以使快排过程中的递归调用次数减少一半，并且，这种优化方法也可以使用在所有和快速排序类似的程序结构中；</li><li>三点取中法能帮助我们选出更加合理的基准值，保证快速排序的运行效率。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 好好学算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>快速排序</title>
      <link href="/2021/09/18/2021-09-18-kuai-su-pai-xu/"/>
      <url>/2021/09/18/2021-09-18-kuai-su-pai-xu/</url>
      
        <content type="html"><![CDATA[<h2 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h2><p>快速排序算法思想很简单，核心就是一句话：<strong>找到基准值的位置</strong>。</p><p>可以分成三步：</p><ol><li>第一步，选择一个值作为基准值；</li><li>第二步，找到基准值的位置，并将小于基准值的元素放在基准值的前面，大于基准值的元素放在基准值的后面；</li><li>第三步，对基准值的左右两侧递归地进行这个过程。</li></ol><h3 id="1：选择一个值作为基准值"><a href="#1：选择一个值作为基准值" class="headerlink" title="1：选择一个值作为基准值"></a>1：选择一个值作为基准值</h3><p>最简单的选择方法，一定是选择待排序区间的头部元素作为基准值。</p><p><img src="/2021/09/18/2021-09-18-kuai-su-pai-xu/c5aeb573fd47e58e1a8b74e79f503ec2.webp" alt></p><h3 id="2：将小于基准值的元素放在基准值的前面，将大于基准值的元素放在基准值的后面"><a href="#2：将小于基准值的元素放在基准值的前面，将大于基准值的元素放在基准值的后面" class="headerlink" title="2：将小于基准值的元素放在基准值的前面，将大于基准值的元素放在基准值的后面"></a>2：将小于基准值的元素放在基准值的前面，将大于基准值的元素放在基准值的后面</h3><p>这一步通常被叫做 partition 操作，中文直译过来就是分割操作，也就是用基准值将原数组分割成前后两部分。</p><p>partition 操作简单来说，就是空出一个位置，反复地前后调换元素。</p><p>当我们选择了基准值以后，原先基准值的位置就相当于被空出来了，也就是说数组的第一位是空着的。</p><p><img src="/2021/09/18/2021-09-18-kuai-su-pai-xu/2e43506d0d8516bce42393c0612cf911.webp" alt></p><p><strong>将后面小于基准值的元素放到前面的空位上，这样后面就空出一位了。然后，我们再将前面大于基准值的元素放到后面这个空位上。就这样交替进行，直到空位前面的值都小于基准值，空位后面的值都大于基准值为止。</strong></p><p><img src="/2021/09/18/2021-09-18-kuai-su-pai-xu/ce5d08f986007bf3b0e178d8d0445856.webp" alt></p><h3 id="3：对基准值的左右两侧，递归地进行第一步和第二步"><a href="#3：对基准值的左右两侧，递归地进行第一步和第二步" class="headerlink" title="3：对基准值的左右两侧，递归地进行第一步和第二步"></a>3：对基准值的左右两侧，递归地进行第一步和第二步</h3><p>要分别对 6、3、7、2 和 10、9、12 这两部分，再做选择基准值、找基准值位置和递归这三步。由于每次 partition 操作中，都会确定一个值，也就是基准值的正确位置，所以，经过有限次递归操作以后，整个数组也就变成了一个有序数组。</p><h2 id="分析快速排序的时间复杂度"><a href="#分析快速排序的时间复杂度" class="headerlink" title="分析快速排序的时间复杂度"></a>分析快速排序的时间复杂度</h2><p>在讲快速排序算法过程的时候，我们说其中最关键的步骤是理解 parition 操作。因此，分析快速排序的时间复杂度，我们也要先来分析 partition 操作这一步的时间复杂度。</p><p>在 partition 操作的过程中，头指针会循环扫描到基准值最后放置的位置，尾指针也会扫描到最后基准值放置的位置。这样，头尾指针扫描加在一起，其实相当于扫描了整个待排序数组的区域。因此，我们就能得出单次 partition 操作的时间复杂度为 O(n)。也就是说，当前数组区间中有 10 个元素时，我们大概操作 10 次就能找到基准值的位置了。清楚了单次操作的时间复杂度以后，我们就能知道总体的时间复杂度了。</p><p>首先，我们要确定总体时间复杂度的公式。我们用 T(n)表示对 n个元素的数组进行快速排序所用的时间，那么 T(n) 中应该包括了单次的 partition 操作用时，以及 parition 操作以后，我们对左右两个子数组分别做快速排序所用的时间，也就是 T(n) = n + T(L) + T(R)。其中 n是单次 partition 操作的用时，T(L)和 T(R)分别是对左右区间进行快速排序的用时，L 和 R分别代表左区间和右区间中元素的数量。</p><p><img src="/2021/09/18/2021-09-18-kuai-su-pai-xu/0396fd48cee04847bd335b9b6ecddee0.webp" alt></p><p>接着，我们借助二叉树的结构来求一下 T(n) 。首先，我们可以将基准值看成是由 n 个元素组成的二叉树的根节点，那么 partition 操作就是找到这个根节点的正确位置，总用时就是 n。如果我们将这个用时 n 当做二叉树根节点的独立用时，那么左子树根节点的独立用时就是 L，右子树根节点的独立用时就是 R。这样，我们就得到了这个二叉树第二层上所有节点的独立用时：L + R = n - 1。我们可以将这个值大致看成是 n。依照此方法，你会得到接下来各层二叉树节点的独立用时，关系如图所示：</p><p><img src="/2021/09/18/2021-09-18-kuai-su-pai-xu/0b7b917d6e9a691305c272cd23442f88.webp" alt></p><p>其中，每个节点上的数值代表了这个节点的独立用时。 n = L + R + 1，L = LL + LR + 1， R = RL + RR + 1。这也就意味着，第一层上节点的独立用时总和是 n，第二层上节点的独立用时总和是 L + R = n - 1，第三层上节点的独立用时总和是 LL + LR + RL + RR = n - 3。</p><p>其实，如果 n 足够大，那每一层上的所有节点的独立用时总和，我们都可以让其约等于 n。那快速排序的总用时，就可以约等于 n 乘上树的层数，也就是树的高度。因此，这棵树的高度越低，快速排序的效率越好，而树的高度越高，快速排序的效率也就越差。这样一来，我们就让树高这个直观的量和快速排序的效率有了概念上的关联。因此，我们只需要分析树高，就能分析清楚快速排序的执行效率了。</p><p>那针对有 n 个节点的二叉树（每个节点代表一个基准值，n 个节点就代表确定了 n 个基准值的位置），我们该怎么确定树高的范围呢？稍微一分析，你会发现，树高最低是 log2​(n+1) ，也就是每个节点的左右两棵子树，所包含节点数量都差不多。这就意味着，我们每次选择基准值的时候，都要尽可能选择处在待排序数组中间的数字。也只有这样，快速排序算法才会达到最好的时间复杂度，也就是 O(nlog2​n)。</p><p>结合二叉树的结构，我们分析了快速排序的最好时间复杂度。而快速排序的最坏时间复杂度是 O(n^2) </p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>先选择一个基准值，一般选择第一个元素，基准值位置空缺；</li><li>基准值和最后的元素开始比较，如果小于基准值，则移动到空缺的位置;</li><li>基准值和最前的元素开始比较，如果大于基准值，则移动到空缺的位置;</li><li>不断执行第二和第三步骤，确定基准值的位置；</li><li>对基准值两边的元素集合递归执行1234步骤。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 好好学算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何学算法</title>
      <link href="/2021/09/17/2021-09-17-ru-he-xue-suan-fa/"/>
      <url>/2021/09/17/2021-09-17-ru-he-xue-suan-fa/</url>
      
        <content type="html"><![CDATA[<ol><li>学习算法的目的：为了在职业发展道路走的更稳更远。</li><li>学习算法绝不是走弯路，反倒是捷径。</li><li>跟老师学的意义在于，好的老师可以帮助提高学习效率。</li><li>学习算法，到底在学什么呢？其实是学习算法的设计过程。要有理解的实践。</li><li>学习算法的过程中，要明白重点要学什么？掌握算法思维，才是算法学习的重中之重。</li><li>学习算法本身是有挑战的，是挑战自己”智力 + 毅力”的过程。</li><li>要学会正确的锻炼方法，还需要不断地训练巩固。</li><li>算法没有会或者不会一说，只有程度深、浅一说。</li><li>坚持锻炼算法思维的你，总会比那个曾经的你要更强。这不是算法难或者简单的问题，而是你自己如何选择的问题。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 好好学算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式ID</title>
      <link href="/2021/07/11/2021-07-11-fen-bu-shi-id/"/>
      <url>/2021/07/11/2021-07-11-fen-bu-shi-id/</url>
      
        <content type="html"><![CDATA[<h2 id="基于UUID"><a href="#基于UUID" class="headerlink" title="基于UUID"></a>基于UUID</h2><p>像用作订单号UUID这样的字符串没有丝毫的意义，看不出和订单相关的有用信息；而对于数据库来说用作业务主键ID，它不仅是太长还是字符串，存储性能差查询也很耗时，所以不推荐用作分布式ID。</p><p>优点：</p><ul><li>生成足够简单，本地生成无网络消耗，具有唯一性</li></ul><p>缺点：</p><ul><li>无序的字符串，不具备趋势自增特性</li><li>没有具体的业务含义</li><li>长度过长16 字节128位，36位长度的字符串，存储以及查询对MySQL的性能消耗较大，MySQL官方明确建议主键要尽量越短越好，作为数据库主键 UUID 的无序性会导致数据位置频繁变动，严重影响性能。</li></ul><h2 id="基于数据库自增ID"><a href="#基于数据库自增ID" class="headerlink" title="基于数据库自增ID"></a>基于数据库自增ID</h2><p>基于数据库的auto_increment自增ID完全可以充当分布式ID，具体实现：需要一个单独的MySQL实例用来生成ID</p><p>当我们需要一个ID的时候，向表中插入一条记录返回主键ID，但这种方式有一个比较致命的缺点，访问量激增时MySQL本身就是系统的瓶颈，用它来实现分布式服务风险比较大，不推荐！</p><p>优点：</p><ul><li>实现简单，ID单调自增，数值类型查询速度快</li></ul><p>缺点：</p><ul><li>DB单点存在宕机风险，无法扛住高并发场景</li></ul><h2 id="基于数据库集群模式"><a href="#基于数据库集群模式" class="headerlink" title="基于数据库集群模式"></a>基于数据库集群模式</h2><p>前边说了单点数据库方式不可取，那对上边的方式做一些高可用优化，换成主从模式集群。害怕一个主节点挂掉没法用，那就做双主模式集群，也就是两个Mysql实例都能单独的生产自增ID。</p><p>那这样还会有个问题，两个MySQL实例的自增ID都从1开始，会生成重复的ID怎么办？</p><p>解决方案：设置起始值和自增步长</p><p>MySQL_1 配置：</p><pre><code>set @@auto_increment_offset = 1;     -- 起始值set @@auto_increment_increment = 2;  -- 步长</code></pre><p>MySQL_2 配置：</p><pre><code>set @@auto_increment_offset = 2;     -- 起始值set @@auto_increment_increment = 2;  -- 步长</code></pre><p>这样两个MySQL实例的自增ID分别就是：</p><pre><code>1、3、5、7、9 2、4、6、8、10</code></pre><p>那如果集群后的性能还是扛不住高并发咋办？就要进行MySQL扩容增加节点，这是一个比较麻烦的事。</p><p><img src="/2021/07/11/2021-07-11-fen-bu-shi-id/640.webp" alt></p><p>增加第三台MySQL实例需要人工修改一、二两台MySQL实例的起始值和步长</p><p>优点：</p><ul><li>解决DB单点问题</li></ul><p>缺点：</p><ul><li>不利于后续扩容，而且实际上单个数据库自身压力还是大，依旧无法满足高并发场景。</li></ul><h2 id="基于数据库的号段模式"><a href="#基于数据库的号段模式" class="headerlink" title="基于数据库的号段模式"></a>基于数据库的号段模式</h2><p>号段模式是当下分布式ID生成器的主流实现方式之一，号段模式可以理解为从数据库批量的获取自增ID，每次从数据库取出一个号段范围，例如 (1,1000] 代表1000个ID，具体的业务服务将本号段，生成1~1000的自增ID并加载到内存。表结构如下：</p><pre><code>CREATE TABLE id_generator (  id int(10) NOT NULL,  max_id bigint(20) NOT NULL COMMENT &#39;当前最大id&#39;,  step int(20) NOT NULL COMMENT &#39;号段的布长&#39;,  biz_type    int(20) NOT NULL COMMENT &#39;业务类型&#39;,  version int(20) NOT NULL COMMENT &#39;版本号&#39;,  PRIMARY KEY (`id`)) </code></pre><p>version ：是一个乐观锁，每次都更新version，保证并发时数据的正确性</p><p>等这批号段ID用完，再次向数据库申请新号段，对max_id字段做一次update操作，update max_id= max_id + step，update成功则说明新号段获取成功，新的号段范围是(max_id ,max_id +step]。</p><pre><code>update id_generator set max_id = #{max_id+step}, version = version + 1 where version = # {version} and biz_type = XXX</code></pre><p>由于多业务端可能同时操作，所以采用版本号version乐观锁方式更新，这种分布式ID生成方式不强依赖于数据库，不会频繁的访问数据库，对数据库的压力小很多。</p><h2 id="基于Redis模式"><a href="#基于Redis模式" class="headerlink" title="基于Redis模式"></a>基于Redis模式</h2><p>Redis也同样可以实现，原理就是利用redis的 incr命令实现ID的原子性自增。</p><pre><code>127.0.0.1:6379&gt; set seq_id 1     // 初始化自增ID为1OK127.0.0.1:6379&gt; incr seq_id      // 增加1，并返回递增后的数值(integer) 2</code></pre><p>用redis实现需要注意一点，要考虑到redis持久化的问题。redis有两种持久化方式RDB和AOF</p><ul><li>RDB会定时打一个快照进行持久化，假如连续自增但redis没及时持久化，而这会Redis挂掉了，重启Redis后会出现ID重复的情况。</li><li>AOF会对每条写命令进行持久化，即使Redis挂掉了也不会出现ID重复的情况，但由于incr命令的特殊性，会导致Redis重启恢复的数据时间过长。</li></ul><h2 id="基于雪花算法（Snowflake）模式"><a href="#基于雪花算法（Snowflake）模式" class="headerlink" title="基于雪花算法（Snowflake）模式"></a>基于雪花算法（Snowflake）模式</h2><p><img src="/2021/07/11/2021-07-11-fen-bu-shi-id/1234.webp" alt></p><p>雪花算法（Snowflake）是twitter公司内部分布式项目采用的ID生成算法</p><p>Snowflake生成的是Long类型的ID，一个Long类型占8个字节，每个字节占8比特，也就是说一个Long类型占64个比特。</p><p>Snowflake ID组成结构：正数位（占1比特）+ 时间戳（占41比特）+ 机器ID（占5比特）+ 数据中心（占5比特）+ 自增值（占12比特），总共64比特组成的一个Long类型。</p><ul><li>第一个bit位（1bit）：Java中long的最高位是符号位代表正负，正数是0，负数是1，一般生成ID都为正数，所以默认为0。</li><li>时间戳部分（41bit）：毫秒级的时间，不建议存当前时间戳，而是用（当前时间戳 - 固定开始时间戳）的差值，可以使产生的ID从更小的值开始；41位的时间戳可以使用69年，(1L &lt;&lt; 41) / (1000L * 60 * 60 * 24 * 365) = 69年</li><li>工作机器id（10bit）：也被叫做workId，这个可以灵活配置，机房或者机器号组合都可以。</li><li>序列号部分（12bit），自增值支持同一毫秒内同一个节点可以生成4096个ID</li></ul><p>根据这个算法的逻辑，只需要将这个算法用Java语言实现出来，封装为一个工具方法，那么各个业务应用可以直接使用该工具方法来获取分布式ID，只需保证每个业务应用有自己的工作机器id即可，而不需要单独去搭建一个获取分布式ID的应用。</p><pre><code>/** * Twitter的SnowFlake算法,使用SnowFlake算法生成一个整数，然后转化为62进制变成一个短地址URL * * https://github.com/beyondfengyu/SnowFlake */public class SnowFlakeShortUrl {    /**     * 起始的时间戳     */    private final static long START_TIMESTAMP = 1480166465631L;    /**     * 每一部分占用的位数     */    private final static long SEQUENCE_BIT = 12;   //序列号占用的位数    private final static long MACHINE_BIT = 5;     //机器标识占用的位数    private final static long DATA_CENTER_BIT = 5; //数据中心占用的位数    /**     * 每一部分的最大值     */    private final static long MAX_SEQUENCE = -1L ^ (-1L &lt;&lt; SEQUENCE_BIT);    private final static long MAX_MACHINE_NUM = -1L ^ (-1L &lt;&lt; MACHINE_BIT);    private final static long MAX_DATA_CENTER_NUM = -1L ^ (-1L &lt;&lt; DATA_CENTER_BIT);    /**     * 每一部分向左的位移     */    private final static long MACHINE_LEFT = SEQUENCE_BIT;    private final static long DATA_CENTER_LEFT = SEQUENCE_BIT + MACHINE_BIT;    private final static long TIMESTAMP_LEFT = DATA_CENTER_LEFT + DATA_CENTER_BIT;    private long dataCenterId;  //数据中心    private long machineId;     //机器标识    private long sequence = 0L; //序列号    private long lastTimeStamp = -1L;  //上一次时间戳    private long getNextMill() {        long mill = getNewTimeStamp();        while (mill &lt;= lastTimeStamp) {            mill = getNewTimeStamp();        }        return mill;    }    private long getNewTimeStamp() {        return System.currentTimeMillis();    }    /**     * 根据指定的数据中心ID和机器标志ID生成指定的序列号     *     * @param dataCenterId 数据中心ID     * @param machineId    机器标志ID     */    public SnowFlakeShortUrl(long dataCenterId, long machineId) {        if (dataCenterId &gt; MAX_DATA_CENTER_NUM || dataCenterId &lt; 0) {            throw new IllegalArgumentException(&quot;DtaCenterId can&#39;t be greater than MAX_DATA_CENTER_NUM or less than 0！&quot;);        }        if (machineId &gt; MAX_MACHINE_NUM || machineId &lt; 0) {            throw new IllegalArgumentException(&quot;MachineId can&#39;t be greater than MAX_MACHINE_NUM or less than 0！&quot;);        }        this.dataCenterId = dataCenterId;        this.machineId = machineId;    }    /**     * 产生下一个ID     *     * @return     */    public synchronized long nextId() {        long currTimeStamp = getNewTimeStamp();        if (currTimeStamp &lt; lastTimeStamp) {            throw new RuntimeException(&quot;Clock moved backwards.  Refusing to generate id&quot;);        }        if (currTimeStamp == lastTimeStamp) {            //相同毫秒内，序列号自增            sequence = (sequence + 1) &amp; MAX_SEQUENCE;            //同一毫秒的序列数已经达到最大            if (sequence == 0L) {                currTimeStamp = getNextMill();            }        } else {            //不同毫秒内，序列号置为0            sequence = 0L;        }        lastTimeStamp = currTimeStamp;        return (currTimeStamp - START_TIMESTAMP) &lt;&lt; TIMESTAMP_LEFT //时间戳部分                | dataCenterId &lt;&lt; DATA_CENTER_LEFT       //数据中心部分                | machineId &lt;&lt; MACHINE_LEFT             //机器标识部分                | sequence;                             //序列号部分    }    public static void main(String[] args) {        SnowFlakeShortUrl snowFlake = new SnowFlakeShortUrl(2, 3);        for (int i = 0; i &lt; (1 &lt;&lt; 4); i++) {            //10进制            System.out.println(snowFlake.nextId());        }    }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis</title>
      <link href="/2021/07/07/2021-07-07-redis-duo-xian-cheng-mo-xing/"/>
      <url>/2021/07/07/2021-07-07-redis-duo-xian-cheng-mo-xing/</url>
      
        <content type="html"><![CDATA[<h2 id="Redis-6-0-之前为什么不使用多线程？"><a href="#Redis-6-0-之前为什么不使用多线程？" class="headerlink" title="Redis 6.0 之前为什么不使用多线程？"></a>Redis 6.0 之前为什么不使用多线程？</h2><p>官方答复：</p><ul><li>使用 Redis 时，几乎不存在 CPU 成为瓶颈的情况， Redis 主要受限于内存和网络。</li><li>在一个普通的 Linux 系统上，Redis 通过使用pipelining 每秒可以处理 100 万个请求，所以如果应用程序主要使用 O(N) 或O(log(N)) 的命令，它几乎不会占用太多 CPU。</li><li>使用了单线程后，可维护性高。多线程模型虽然在某些方面表现优异，但是它却引入了程序执行顺序的不确定性，带来了并发读写的一系列问题，增加了系统复杂度、同时可能存在线程切换、甚至加锁解锁、死锁造成的性能损耗。</li></ul><h2 id="Redis-6-0-之前单线程指的是-Redis-只有一个线程干活么？"><a href="#Redis-6-0-之前单线程指的是-Redis-只有一个线程干活么？" class="headerlink" title="Redis 6.0 之前单线程指的是 Redis 只有一个线程干活么？"></a>Redis 6.0 之前单线程指的是 Redis 只有一个线程干活么？</h2><p>不是。Redis 在处理客户端的请求时，包括获取 (socket 读)、解析、执行、内容返回 (socket 写) 等都由一个顺序串行的主线程处理，这就是所谓的「单线程」。</p><p>其中执行命令阶段，由于 Redis 是单线程来处理命令的，所有每一条到达服务端的命令不会立刻执行，所有的命令都会进入一个 Socket 队列中，当 socket 可读则交给单线程事件分发器逐个被执行。</p><p><img src="/2021/07/07/2021-07-07-redis-duo-xian-cheng-mo-xing/640.webp" alt></p><p>此外，有些命令操作可以用后台线程或子进程执行（比如数据删除、快照生成、AOF 重写）。</p><h2 id="那-Redis-6-0-为啥要引入多线程呀？"><a href="#那-Redis-6-0-为啥要引入多线程呀？" class="headerlink" title="那 Redis 6.0 为啥要引入多线程呀？"></a>那 Redis 6.0 为啥要引入多线程呀？</h2><p>随着硬件性能提升，Redis 的性能瓶颈可能出现网络 IO 的读写，也就是：单个线程处理网络读写的速度跟不上底层网络硬件的速度。</p><p>读写网络的 read/write 系统调用占用了Redis 执行期间大部分CPU 时间，瓶颈主要在于网络的 IO 消耗, 优化主要有两个方向:</p><ul><li>提高网络 IO 性能，典型的实现比如使用 DPDK来替代内核网络栈的方式。</li><li>使用多线程充分利用多核，提高网络请求读写的并行度，典型的实现比如 Memcached。</li></ul><p>Redis 采用多个 IO 线程来处理网络请求，提高网络请求处理的并行度。</p><p><strong>需要注意的是，Redis 多 IO 线程模型只用来处理网络读写请求，对于 Redis 的读写命令，依然是单线程处理。</strong></p><p>这是因为，网络处理经常是瓶颈，通过多线程并行处理可提高性能。</p><p>而继续使用单线程执行读写命令，不需要为了保证 Lua 脚本、事务、等开发多线程安全机制，实现更简单。</p><p><img src="/2021/07/07/2021-07-07-redis-duo-xian-cheng-mo-xing/641.webp" alt></p><h2 id="主线程与-IO-多线程是如何实现协作呢？"><a href="#主线程与-IO-多线程是如何实现协作呢？" class="headerlink" title="主线程与 IO 多线程是如何实现协作呢？"></a>主线程与 IO 多线程是如何实现协作呢？</h2><p><img src="/2021/07/07/2021-07-07-redis-duo-xian-cheng-mo-xing/1712130-20200516174905348-1186276910.png" alt></p><p><img src="/2021/07/07/2021-07-07-redis-duo-xian-cheng-mo-xing/642.webp" alt></p><p>流程简述：</p><ol><li>主线程负责接收建立连接请求，获取 Socket 放入全局等待读处理队列。</li><li>主线程处理完读事件之后，通过 RR（Round Robin）将这些连接分配给这些 IO 线程。</li><li>主线程阻塞等待 IO 线程读取 Socket 完毕。</li><li>主线程通过单线程的方式执行请求命令，请求数据读取并解析完成，但并不执行。</li><li>主线程阻塞等待 IO 线程将数据回写 Socket 完毕。</li><li>解除绑定，清空等待队列。</li></ol><p>该设计有如下特点：</p><ol><li>IO 线程要么同时在读 Socket，要么同时在写，不会同时读或写。</li><li>IO 线程只负责读写 Socket 解析命令，不负责命令处理。</li></ol><p>思路：将主线程 IO 读写任务拆分出来给一组独立的线程处理，使得多个 socket 读写可以并行化，但是 Redis 命令还是主线程串行执行。</p><h2 id="如何开启多线程呢？"><a href="#如何开启多线程呢？" class="headerlink" title="如何开启多线程呢？"></a>如何开启多线程呢？</h2><p>Redis 6.0 的多线程默认是禁用的，只使用主线程。如需开启需要修改 redis.conf 配置文件：io-threads-do-reads yes。</p><h2 id="线程数是不是越多越好？"><a href="#线程数是不是越多越好？" class="headerlink" title="线程数是不是越多越好？"></a>线程数是不是越多越好？</h2><p>当然不是，关于线程数的设置，官方有一个建议：4 核的机器建议设置为 2 或 3 个线程，8核的建议设置为 6 个线程，线程数一定要小于机器核数。</p><p>另外，开启多线程后，还需要设置线程数，否则是不生效的。</p><pre><code>io-threads 4</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="/2021/07/07/2021-07-07-redis-duo-xian-cheng-mo-xing/1712130-20200516174905348-1186276910.png" alt></p><p>Redis 6.0引入多线程模型。将等待队列中的事件分给IO线程组执行，主线程阻塞。主线程轮询所有IO线程执行完读socket和解析后，按顺序执行命令，然后IO线程组回写socket。</p>]]></content>
      
      
      <categories>
          
          <category> 中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对象类型</title>
      <link href="/2021/07/05/2021-07-05-qian-xi-vo-dto-do-po/"/>
      <url>/2021/07/05/2021-07-05-qian-xi-vo-dto-do-po/</url>
      
        <content type="html"><![CDATA[<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><ul><li>VO（View Object）：视图对象，用于展示层，它的作用是把某个指定页面（或组件）的所有数据封装起来。</li><li>DTO（Data Transfer Object）：数据传输对象，泛指用于展示层与服务层之间的数据传输对象。</li><li>DO（Domain Object）：<strong>领域对象</strong>，就是从现实世界中抽象出来的有形或无形的业务实体。</li><li>PO（Persistent Object）：持久化对象，它跟持久层（通常是关系型数据库）的数据结构形成一一对应的映射关系，如果持久层是关系型数据库，那么，数据表中的每个字段（或若干个）就对应PO的一个（或若干个）属性。</li></ul><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><img src="/2021/07/05/2021-07-05-qian-xi-vo-dto-do-po/640.webp" alt></p><ol><li>用户发出请求（可能是填写表单），表单的数据在展示层被匹配为VO。</li><li>展示层把VO转换为服务层对应方法所要求的DTO，传送给服务层。</li><li>服务层首先根据DTO的数据构造（或重建）一个DO，调用DO的业务方法完成具体业务。</li><li>服务层把DO转换为持久层对应的PO（可以使用ORM工具，也可以不用），调用持久层的持久化方法，把PO传递给它，完成持久化操作。</li></ol>]]></content>
      
      
      <categories>
          
          <category> JAVA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL</title>
      <link href="/2021/06/03/2021-06-03-chong-xue-mysql/"/>
      <url>/2021/06/03/2021-06-03-chong-xue-mysql/</url>
      
        <content type="html"><![CDATA[<h2 id="基础架构"><a href="#基础架构" class="headerlink" title="基础架构"></a>基础架构</h2><p><img src="/2021/06/03/2021-06-03-chong-xue-mysql/2020060101.png" alt="MySQL 的逻辑架构图"></p><ul><li>mysql分为Server层和引擎层</li><li>Server层：连接器、查询缓存、分析器、优化器、执行器等 以及所有内置的函数（eg:日期、时间、数学和加密函数等）。所有跨存储引擎的功能都在这一层实现，eg：存储过程、触发器、视图</li><li>连接器：一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。</li><li>查询缓存：mysql拿到一个查询后，先查询缓存，（缓存保存形式KV（K为查询的语句，V为查询的结果）） 。但建议关闭查询缓存（show variables like ‘%query_cache_type%’; set GLOBAL query_cache_type=’OFF’;）。失效频繁，而且对于压力大的数据库，命中率非常低。mysql8.0版本取消了查询缓存的整个功能模块。</li><li>分析器：<ul><li>词法分析：识别出里面的字符串分别是什么代表什么。</li><li>语法分析: 根据词法分析的结果和语法规则，判断是否满足Mysql语法。</li></ul></li><li>优化器：分析器知道要做什么，优化器在便利有多个索引时，决定使用哪个索引；在一个语句有多表关联（join）时，决定各个表的连接顺序。</li><li>执行器：优化器知道怎么做了，执行器进行执行语句。开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限</li></ul><h2 id="存储引擎"><a href="#存储引擎" class="headerlink" title="存储引擎"></a>存储引擎</h2><h3 id="MyISAM和InnoDB区别："><a href="#MyISAM和InnoDB区别：" class="headerlink" title="MyISAM和InnoDB区别："></a>MyISAM和InnoDB区别：</h3><ul><li>锁方面: MyISAM 只有表级锁，而InnoDB 支持行级锁和表级锁,默认为行级锁。</li><li>事务方面：MyISAM 强调的是性能，每次查询具有原子性,其执行速度比InnoDB类型更快，但是不提供事务支持。InnoDB 支持事务</li><li>外键方面：MyISAM不支持，而InnoDB支持。</li><li>索引方面：InnoDB的主键是聚集索引，数据是和主键索引绑在一起的，必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，再回表查询数据。而MyISAM是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针。</li><li>查询效率方面：Innodb不支持全文索引，而MyISAM支持全文索引，查询效率上MyISAM要高</li></ul><h3 id="如何选择"><a href="#如何选择" class="headerlink" title="如何选择"></a>如何选择</h3><p>如果没有特别的需求，使用默认的 Innodb 即可。</p><p>MyISAM：以读写插入为主的应用程序。</p><p>Innodb：更新（删除）操作频率也高，或者要保证数据的完整性；并发量高，支持事务和外键。</p><h2 id="日志系统"><a href="#日志系统" class="headerlink" title="日志系统"></a>日志系统</h2><ul><li>redo log 是 InnoDB 引擎特有的；binlog是 MySQL 的 Server 层实现的，所有引擎都可以使用。</li><li>redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。</li><li>redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。</li></ul><h2 id="事物的四大特性-ACID"><a href="#事物的四大特性-ACID" class="headerlink" title="事物的四大特性(ACID)"></a>事物的四大特性(ACID)</h2><ul><li>原子性（Atomicity）： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；</li><li>一致性（Consistency）： 执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的；</li><li>隔离性（Isolation）： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的；</li><li>持久性（Durability）： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。</li></ul><h2 id="ACID的原理"><a href="#ACID的原理" class="headerlink" title="ACID的原理"></a>ACID的原理</h2><ul><li>原子性：redo log(重做日志)和undo log(回滚日志)。redo log用于保证事务持久性；undo log则是事务原子性和隔离性实现的基础。</li><li>持久性：redo log(重做日志)。当数据修改时，除了修改Buffer Pool中的数据，还会在redo log记录这次操作；当事务提交时，会调用fsync接口对redo log进行刷盘。如果MySQL宕机，重启时可以读取redo log中的数据，对数据库进行恢复。</li><li>隔离性：(一个事务)写操作对(另一个事务)写操作的影响：锁机制保证隔离性；(一个事务)写操作对(另一个事务)读操作的影响：MVCC(多版本并发控制)保证隔离性。</li><li>一致性：保证原子性、持久性和隔离性</li></ul><h2 id="事务隔离级别"><a href="#事务隔离级别" class="headerlink" title="事务隔离级别"></a>事务隔离级别</h2><ul><li>READ-UNCOMMITTED(读未提交)： 一个事务还未提交，它所做的变更就可以被别的事务看到。可能会导致脏读、幻读或不可重复读。</li><li>READ-COMMITTED(读已提交)： 一个事务提交之后，它所做的变更才可以被别的事务看到。可以阻止脏读，但是幻读或不可重复读仍有可能发生。</li><li>REPEATABLE-READ(可重复读)： 一个事务执行过程中看到的数据是一致的。未提交的更改对其他事务是不可见的。可以阻止脏读和不可重复读，但幻读仍有<strong>可能</strong>发生（其他事务插入数据，当前事务进行当前读就会产生幻读情况）。</li></ul><blockquote><p>以下情况下，Mysql InnoDB存储引擎在RR隔离级别下，是解决了幻读问题的。<br>使用MVCC解决了快照读情况下的幻读问题。快照读是不会产生幻读情况的。<br>使用Next-Key Lock解决了当前读情况下的幻读问题。如当前事务lock in share mode或for update进行当前读时，其他事务插入数据就会被Next-key Lock阻塞</p></blockquote><ul><li>SERIALIZABLE(串行化)： 对应一个记录会加读写锁，出现冲突的时候，后访问的事务必须等前一个事务执行完成才能继续执行。该级别可以防止脏读、不可重复读以及幻读。</li></ul><p>不可重复读是在查询方面，多次读取一条记录发现其中某些列的值被修改。幻读是在于新增或者删除方面，范围查询发现记录增多或减少了。</p><h2 id="事务其他知识"><a href="#事务其他知识" class="headerlink" title="事务其他知识"></a>事务其他知识</h2><ul><li>事务隔离的实现：每条记录在更新的时候都会同时记录一条回滚操作。同一条记录在系统中可以存在多个版本，这就是数据库的多版本并发控制（MVCC）。</li><li>回滚日志什么时候删除？系统会判断当没有事务需要用到这些回滚日志的时候，回滚日志会被删除。</li><li>什么时候不需要了？当系统里没有比这个回滚日志更早的read-view的时候。</li><li>为什么尽量不要使用长事务。长事务意味着系统里面会存在很老的事务视图，在这个事务提交之前，回滚记录都要保留，这会导致大量占用存储空间。除此之外，长事务还占用锁资源，可能会拖垮库。</li><li>事务启动方式：一、显式启动事务语句，begin或者start transaction,提交commit，回滚rollback；二、set autocommit=0，该命令会把这个线程的自动提交关掉。这样只要执行一个select语句，事务就启动，并不会自动提交，直到主动执行commit或rollback或断开连接。</li><li>建议使用方法一，如果考虑多一次交互问题，可以使用commit work and chain语法。在autocommit=1的情况下用begin显式启动事务，如果执行commit则提交事务。如果执行commit work and chain则提交事务并自动启动下一个事务。</li><li>InnoDB 的行数据有多个版本，每个数据版本有自己的 row trx_id，每个事务或者语句有自己的一致性视图。普通查询语句是一致性读，一致性读会根据 row trx_id 和一致性视图确定数据版本的可见性。</li><li>对于可重复读，查询只承认在事务启动前就已经提交完成的数据；</li><li>对于读提交，查询只承认在语句启动前就已经提交完成的数据；</li><li>而当前读，总是读取已经提交完成的最新版本。</li><li>更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。</li><li>begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动</li></ul><h2 id="innodb是如何实现事物的"><a href="#innodb是如何实现事物的" class="headerlink" title="innodb是如何实现事物的"></a>innodb是如何实现事物的</h2><p>innodb 通过buffer pool, logBuffer, Redo Log, Undo Log 来实现事务的, 以update语句举例:</p><ol><li>innodb 在收到一个update语句后, 会先根据条件找到数据所在的页, 并将该页缓存在buffer pool中</li><li>执行update语句, 修改buffer pool中的数据, 即内存中的数据</li><li>针对update语句生成一个redoLog对象, 并存入LogBuffer中</li><li>针对update语句生成undo Log日志, 用于事务回滚</li><li>如果事务提交, 那么则把redo Log对象及逆行持久化, 后续还有其他机制将buffer pool中修改的数据页持久化到磁盘中</li><li>如果事务回滚, 则利用undo log 日志进行回滚</li></ol><h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><ul><li>索引类型：主键索引、非主键索引主键索引的叶子节点存的是整行的数据(聚簇索引)，非主键索引的叶子节点内容是主键的值(二级索引)</li><li>主键索引和普通索引的区别：主键索引只要搜索ID这个B+Tree即可拿到数据。普通索引先搜索索引拿到主键值，再到主键索引树搜索一次(回表)</li><li>一个数据页满了，按照B+Tree算法，新增加一个数据页，叫做页分裂，会导致性能下降。空间利用率降低大概50%。当相邻的两个数据页利用率很低的时候会做数据页合并，合并的过程是分裂过程的逆过程。</li><li>索引是排好序的快速查找的数据结构。</li><li>Hash结构：查询速度快，查询时间复杂度为O(1)。但不支持范围查询和排序。</li><li>二叉树：查询时间复杂度O(log(N))，但容易造成左倾或右倾两边不平衡情况，导致树高过高，磁盘IO成本高。</li><li>平衡二叉树：自动调节两边平衡。但每个节点只有两个子节点，当数据过多的时候，树高也过高，磁IO成本也相对高。</li><li>B树： 每个节点都存储key和data，要找到具体的数据，需要进行一次中序遍历按序来扫，导致查询效率不稳定。并且查询的最小单位是页，每页默认16K，数据页的大小有限，节点包含数据信息，会导致查询的数据变少，IO读写次数变多。</li><li>B+树：只有叶子节点存储data。每个数据的查询效率稳定相等。非叶子节点不带有数据信息，一次性读入内存的需要查找的关键字也就越多，相对IO读写次数就降低了。</li><li>覆盖索引：某索引已经覆盖了查询需求，称为覆盖索引，例如：select ID,K from T where k between 3 and 5，K是普通索引</li><li>最左前缀原则：在MySQL建立联合索引时会遵守最左前缀匹配原则，即最左优先，在检索数据时从联合索引的最左边开始匹配。例如已经有了联合索引（a,b），如果既有联合查询，又有基于 a、b 各自的查询，那么当查询条件里面只有 b 的语句，是无法使用 (a,b) 这个联合索引的，这时候需要同时维护 (a,b)、(b) 这两个索引。</li><li>索引下推：在MySQL5.6之前，只能从根据最左前缀查询，查到主键后到主键索引上找出数据行，再对比条件字段值。MySQL5.6引入的索引下推优化，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。例如mysql&gt; select * from tuser where name like ‘张%’ and age=10 and ismale=1; 在此之前已经建立（name, age）联合索引，此SQL明显可知只用到了name索引，age索引用不到。当没有用到索引下推时，只根据name索引查找出主键，再回表查询数据判断age和ismale条件字段是否符合。当用到索引下推时，会对索引中包含的字段先做判断，判断联合索引值的age=10，只有当索引值name like ‘张%’ &amp;&amp; age=10，才找出主键回表查找数据，再判断ismale条件字段是否符合。</li></ul><h2 id="什么情况下应不建或少建索引"><a href="#什么情况下应不建或少建索引" class="headerlink" title="什么情况下应不建或少建索引"></a>什么情况下应不建或少建索引</h2><ul><li>表记录太少，因为索引也要存储空间</li><li>经常插入、删除、修改的表，因为更新表时，不仅要保存数据，还要保存索引文件</li><li>数据重复且分布平均的表字段，假如一个表有10万行记录，有一个字段A只有T和F两种值，且每个值的分布概率大约为50%，那么对这种表A字段建索引一般不会提高数据库的查询速度。</li></ul><h2 id="普通索引和唯一索引选择"><a href="#普通索引和唯一索引选择" class="headerlink" title="普通索引和唯一索引选择"></a>普通索引和唯一索引选择</h2><h3 id="选择普通索引还是唯一索引"><a href="#选择普通索引还是唯一索引" class="headerlink" title="选择普通索引还是唯一索引"></a>选择普通索引还是唯一索引</h3><ul><li>对于查询过程来说：<br>a、普通索引，查到满足条件的第一个记录后，继续查找下一个记录，直到第一个不满足条件的记录<br>b、唯一索引，由于索引唯一性，查到第一个满足条件的记录后，停止检索<br>但是，两者的性能差距微乎其微。因为InnoDB根据数据页来读写的。</li><li>对于更新过程来说：<br>引入概念：change buffer<br>当需要更新一个数据页，如果数据页在内存中就直接更新，如果不在内存中，在不影响数据一致性的前提下，InnoDB会将这些更新操作缓存在change buffer中。下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中的与这个页有关的操作。<br>change buffer是可以持久化的数据。在内存中有拷贝，也会被写入到磁盘上。<br>merge:将change buffer中的操作应用到原数据页上，得到最新结果的过程，访问这个数据页会触发merge，系统有后台线程定期merge，在数据库正常关闭的过程中，也会执行merge。<br>唯一索引的更新不能使用change buffer。对于唯一索引来说，例如插入一条数据，需要将数据页读入内存，判断到没有冲突，插入这个值，都已经读入到内存了，那直接更新内存会更快，就没必要使用 change buffer 了。<br>change buffer用的是buffer pool里的内存，change buffer的大小，可以通过参数innodb_change_buffer_max_size来动态设置。这个参数设置为50的时候，表示change buffer的大小最多只能占用buffer pool的50%。<br>将数据从磁盘读入内存涉及随机IO的访问，是数据库里面成本最高的操作之一。<br>change buffer 因为减少了随机磁盘访问，所以对更新性能的提升很明显。</li></ul><h3 id="change-buffer使用场景"><a href="#change-buffer使用场景" class="headerlink" title="change buffer使用场景"></a>change buffer使用场景</h3><p>在一个数据页做merge之前，change buffer记录的变更越多，收益就越大。<br>对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。<br>反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在change buffer,但之后由于马上要访问这个数据页，会立即触发merge过程。这样随机访问IO的次数不会减少，反而增加了change buffer的维护代价。所以，对于这种业务模式来说，change buffer反而起到了副作用。</p><h3 id="索引的选择和实践"><a href="#索引的选择和实践" class="headerlink" title="索引的选择和实践"></a>索引的选择和实践</h3><p>尽可能使用普通索引。</p><h2 id="MySQL选错索引"><a href="#MySQL选错索引" class="headerlink" title="MySQL选错索引"></a>MySQL选错索引</h2><ul><li><p>优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句。在数据库里面，扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的 CPU 资源越少。</p></li><li><p>MySQL在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数。MySQL 采样统计的方法。</p></li><li><p>采样统计的时候，InnoDB 默认会选择 N 个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。</p></li><li><p>对于由于索引统计信息不准确导致的问题，你可以用 analyze table t来解决。</p></li><li><p>而对于其他优化器误判的情况，你可以在应用端用 force index  （select * from t force index(a) where ….）来强行指定索引，也可以通过修改语句来引导优化器，还可以通过增加或者删除索引来绕过这个问题。</p></li></ul><h2 id="合理建立索引"><a href="#合理建立索引" class="headerlink" title="合理建立索引"></a>合理建立索引</h2><p>ALTER TABLE table_name ADD INDEX index_name (column_list);</p><ul><li><p>直接创建完整索引，这样可能比较占用空间；<br>  这种方式最简单，如果性能没问题，我会这么创建，简单直接且存储空间的费用越来越低</p></li><li><p>创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引；<br>  这种方式需要判断出前缀的长度多少合适，需要根据自己的业务来定，主要是看区分度多少合适</p></li><li><p>倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题；<br>  这种方式用于前缀区分度不高后缀区分度高的场景，目的还是要提高索引的区分度，使用这种方式不适合范围检索</p></li><li><p>创建 hash 字段索引，查询性能稳定，有额外的存储和计算消耗，不支持范围扫描。</p></li></ul><h2 id="为什么索引没用上"><a href="#为什么索引没用上" class="headerlink" title="为什么索引没用上"></a>为什么索引没用上</h2><p>字段发生了转换,导致本该使用索引而没有用到索引</p><h3 id="条件字段函数操作"><a href="#条件字段函数操作" class="headerlink" title="条件字段函数操作"></a>条件字段函数操作</h3><p>对索引字段做函数操作，MySQL 无法再使用索引快速定位功能，而只能使用全索引扫描（不使用索引）。</p><h3 id="隐式类型转换"><a href="#隐式类型转换" class="headerlink" title="隐式类型转换"></a>隐式类型转换</h3><p>在 MySQL 中，字符串和数字做比较的话，是将字符串转换成数字。<br>select * from tradelog where tradeid=110717;<br>将tradeid从String转换为int，对索引做了函数操作，不走索引；<br>select * from tradelog where id=”83126”;<br>将『83126』String转int，没涉及到索引函数操作，还是走索引</p><h3 id="隐式字符编码转换"><a href="#隐式字符编码转换" class="headerlink" title="隐式字符编码转换"></a>隐式字符编码转换</h3><p>如果驱动表的字符集比被驱动表得字符集小，关联列就能用到索引,如果更大,需要发生隐式编码转换,则不能用到索引,latin&lt;gbk&lt;utf8&lt;utf8mb4</p><h2 id="锁"><a href="#锁" class="headerlink" title="锁"></a>锁</h2><p>根据加锁范围：MySQL里面的锁可以分为：全局锁、表级锁、行级锁</p><h3 id="全局锁"><a href="#全局锁" class="headerlink" title="全局锁"></a>全局锁</h3><p>对整个数据库实例加锁。</p><p>MySQL提供加全局读锁的方法：Flush tables with read lock(FTWRL)。这个命令可以使整个库处于只读状态。使用该命令之后，数据更新语句、数据定义语句和更新类事务的提交语句等操作都会被阻塞。</p><p>使用场景：</p><ul><li>全库逻辑备份。</li></ul><p>风险：</p><ul><li>如果在主库备份，在备份期间不能更新，业务停摆；</li><li>如果在从库备份，备份期间不能执行主库同步的binlog，导致主从延迟。</li></ul><p>如果要全库只读，为什么不使用set global readonly=true的方式？</p><ul><li>在有些系统中，readonly的值会被用来做其他逻辑，比如判断主备库。所以修改global变量的方式影响太大；</li><li>在异常处理机制上有差异。如果执行FTWRL命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个库长时间处于不可写状态，风险较高。</li></ul><h3 id="表级锁"><a href="#表级锁" class="headerlink" title="表级锁"></a>表级锁</h3><p>MySQL里面表级锁有两种，一种是表锁，一种是元数据锁(meta data lock,MDL)</p><p>表锁的语法是:lock tables … read/write</p><p>可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。</p><p>对于InnoDB这种支持行锁的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面还是太大。</p><h3 id="行锁"><a href="#行锁" class="headerlink" title="行锁"></a>行锁</h3><p>在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放， 而是要等到事务结束时才释放。建议：如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。</p><p>死锁：当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态。解决方案：</p><ul><li>通过参数 innodb_lock_wait_timeout 根据实际业务场景来设置超时时间，InnoDB引擎默认值是50s。</li><li>发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑（默认是开启状态）。</li></ul><p><strong>innodb行级锁是通过锁索引记录实现的，如果操作的列没建索引是会锁住整个表的。</strong></p><h3 id="间隙锁"><a href="#间隙锁" class="headerlink" title="间隙锁"></a>间隙锁</h3><h3 id="幻读"><a href="#幻读" class="headerlink" title="幻读"></a>幻读</h3><p>幻读是指在同一个事务中，存在前后两次查询同一个范围的数据，但是第二次查询却看到了第一次查询没看到的行。</p><p>出现场景：</p><ul><li>事务的隔</li><li>幻读仅专指新插入的行</li></ul><h3 id="怎么避免幻读？"><a href="#怎么避免幻读？" class="headerlink" title="怎么避免幻读？"></a>怎么避免幻读？</h3><p>存储引擎采用加间隙锁的方式来避免出现幻读</p><h3 id="为啥会出现幻读？"><a href="#为啥会出现幻读？" class="headerlink" title="为啥会出现幻读？"></a>为啥会出现幻读？</h3><p>行锁只能锁定存在的行，针对新插入的操作没有限定</p><h3 id="间隙锁-1"><a href="#间隙锁-1" class="headerlink" title="间隙锁"></a>间隙锁</h3><p>间隙锁，是专门用于解决幻读这种问题的锁，它锁的了行与行之间的间隙，能够阻塞新插入的操作</p><p>间隙锁的引入也带来了一些新的问题，比如：降低并发度，可能导致死锁。</p><p>注意，读读不互斥，读写/写读/写写是互斥的，但是间隙锁之间是不冲突的，间隙锁会阻塞插入操作</p><p><strong>间隙锁在可重复读级别下才是有效的</strong></p><p>举个栗子：有三个字段id,c,d，其中c为索引，d不建立索引，现在有三条数据（0，0，0），（5，5，5），（10，10，10），总共有四个区间(-∞,0）、(0,5）、(5,10）、（10，+∞）。一个事务中select * from t where d（c） = 6 for update或者update t set c = 6 where id = 6，查询中的条件d的情况会全表扫描，产生所有区间，查询条件c的情况和update情况会产生（5，10）区间间隙锁，其他事务无法在区间插入新数据。</p><p>RR模式下，在一个事务中，select * from t where d = 4 for update，如果d为索引，则锁住这行数据，如果d不为索引，则锁住全表遍历的行数据。<strong>innodb行级锁是通过锁索引记录实现的，如果操作的列没建索引是会锁住整个表的。</strong></p><h2 id="next-key-lock"><a href="#next-key-lock" class="headerlink" title="next-key lock"></a>next-key lock</h2><p>间隙锁和行锁合称 next-key lock</p><ul><li>原则 1：加锁的基本单位是 next-key lock，next-key lock 是前开后闭区间。间隙锁是前开后开区间。</li><li>原则 2：查找过程中访问到的对象才会加锁。</li><li>优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。</li><li>优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。</li><li>一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。</li><li><strong>delete后面加limit可以减小next-key lock的范围。</strong>因为没有继续往下访问，所以后面的区间没有加锁。</li></ul><h2 id="慢查询"><a href="#慢查询" class="headerlink" title="慢查询"></a>慢查询</h2><ol><li>查看慢SQL是否启用，查看命令：show variables like ‘log_slow_queries’; </li><li>开启慢查询命令：set global log_slow_queries = on;  </li><li>查看慢查询参数，即设置超过多少秒的查询归为了慢查询。参数为：long_query_time，查询命令： show global  variables like ‘long_query_time’;</li><li>这里设置时间为1秒，即超过1秒就会被认为慢查询。设置命令：set global long_query_time =1;用命令设置的，会立即生效，不用重启mysql服务。但重启mysql服务后就会失效。</li><li>查看慢查询存放日志，命令： show variables like ‘slow_query_log_file’;去相应目录下查看即可。</li></ol><h2 id="EXPLAIN参数"><a href="#EXPLAIN参数" class="headerlink" title="EXPLAIN参数"></a>EXPLAIN参数</h2><p>explain + SQL语句即可分析SQL性能</p><h3 id="id"><a href="#id" class="headerlink" title="id"></a>id</h3><p>id代表执行select子句或操作表的顺序,id分别有三种不同的执行结果,分别如下: </p><ul><li>id相同,执行顺序由上至下</li></ul><p><img src="/2021/06/03/2021-06-03-chong-xue-mysql/20180521160555926.png" alt></p><ul><li>id不同,如果是子查询,id的序号会递增,id值越大,优先级越高,越先被执行</li></ul><p><img src="/2021/06/03/2021-06-03-chong-xue-mysql/20180521161124499.png" alt></p><ul><li>id相同和不同,同时存在,遵从优先级高的优先执行,优先级相同的按照由上至下的顺序执行</li></ul><p><img src="/2021/06/03/2021-06-03-chong-xue-mysql/20180521161430126.png" alt></p><h3 id="select-type"><a href="#select-type" class="headerlink" title="select_type"></a>select_type</h3><p>查询的类型,主要用于区别普通查询,联合查询,子查询等复杂查询</p><ul><li>simple:简单的select查询,查询中不包含子查询或union查询</li><li>primary:查询中若包含任何复杂的子部分,最外层查询则被标记为primary</li><li>subquery 在select 或where 列表中包含了子查询</li><li>derived 在from列表中包含的子查询被标记为derived,mysql会递归这些子查询,把结果放在临时表里</li><li>union 做第二个select出现在union之后,则被标记为union,若union包含在from子句的子查询中,外层select将被标记为derived</li><li>union result 从union表获取结果的select</li></ul><h3 id="table"><a href="#table" class="headerlink" title="table"></a>table</h3><p>显示一行的数据时关于哪张表的</p><h3 id="type"><a href="#type" class="headerlink" title="type"></a>type</h3><p><strong>查询类型从最好到最差依次是:system&gt;const&gt;eq_ref&gt;ref&gt;range&gt;index&gt;All,一般情况下,得至少保证达到range级别,最好能达到ref</strong></p><ul><li><p>system:表只有一行记录,这是const类型的特例,平时不会出现</p></li><li><p>const:表示通过索引一次就找到了,const即常量,它用于比较primary key或unique索引,因为只匹配一行数据,所以效率很快,如将主键置于where条件中,mysql就能将该查询转换为一个常量</p><p>  <img src="/2021/06/03/2021-06-03-chong-xue-mysql/2018052116390992.png" alt></p></li><li><p>eq_ref:唯一性索引扫描,对于每个索引键,表中只有一条记录与之匹配,常见于主键或唯一索引扫描</p></li><li><p>ref:非唯一性索引扫描,返回匹配某个单独值的行,它可能会找到多个符合条件的行,所以他应该属于查找和扫描的混合体</p></li><li><p>range:只检索给定范围的行,使用一个索引来选择行,如where语句中出现了between,&lt;,&gt;,in等查询,这种范围扫描索引比全表扫描要好，因为它只需要开始于索引的某一点，而结束于另一点，不用扫描全部索引。</p></li><li><p>index:index类型只遍历索引树,这通常比All快,因为索引文件通常比数据文件小,index是从索引中读取,all从硬盘中读取</p></li><li><p>all:全表扫描,是最差的一种查询类型</p></li></ul><h3 id="possible-keys"><a href="#possible-keys" class="headerlink" title="possible_keys"></a>possible_keys</h3><p>显示可能应用在这张表中的索引,一个或多个,查询到的索引不一定是真正被用到的</p><h3 id="key"><a href="#key" class="headerlink" title="key"></a>key</h3><p>实际使用的索引,如果为null,则没有使用索引,因此会出现possible_keys列有可能被用到的索引,但是key列为null,表示实际没用索引。</p><h3 id="key-len"><a href="#key-len" class="headerlink" title="key_len"></a>key_len</h3><p>表示索引中使用的字节数,而通过该列计算查询中使用的 索引长度,在不损失精确性的情况下,长度越短越好,key_len显示的值为索引字段的最大可能长度,并非实际使用长度,即,key_len是根据表定义计算而得么不是通过表内检索出的</p><h3 id="ref"><a href="#ref" class="headerlink" title="ref"></a>ref</h3><p>显示索引的哪一列被使用了,如果可能的话是一个常数,哪些列或常量被用于查找索引列上的值</p><h3 id="rows"><a href="#rows" class="headerlink" title="rows"></a>rows</h3><p>根据表统计信息及索引选用情况,大只估算出找到所需的记录所需要读取的行数</p><h3 id="Extra"><a href="#Extra" class="headerlink" title="Extra"></a>Extra</h3><ul><li>Using filesort:说明mysql会对数据使用一个外部的索引排序,而不是按照表内的索引顺序进行读取,mysql中无法利用索引完成的排序操作称为”文件排序”</li><li>Using temporary :使用了临时表保存中间结果,mysql在对查询结果排序时使用临时表,常见于order by和分组查询group by</li><li>Using index:表示相应的select操作中使用了覆盖索引（Covering Index），避免访问了表的数据行，效率不错。如果同时出现using where，表明索引被用来执行索引键值的查找；如果没有同时出现using where，表明索引用来读取数据而非执行查找动作。 其中的覆盖索引含义是所查询的列是和建立的索引字段和个数是一一对应的</li><li>Using where:表明使用了where过滤</li><li>Using join buffer:表明使用了连接缓存,如在查询的时候会有多次join,则可能会产生临时表</li><li>impossible where:表示where子句的值总是false,不能用来获取任何元祖</li><li>select tables optimized away:在没有GROUPBY子句的情况下，基于索引优化MIN/MAX操作或者对于MyISAM存储引擎优化COUNT(*)操作，不必等到执行阶段再进行计算，查询执行计划生成的阶段即完成优化。</li><li>distinct:优化distinct操作，在找到第一匹配的元组后即停止找同样值的动作</li></ul><h2 id="sql的执行顺序"><a href="#sql的执行顺序" class="headerlink" title="sql的执行顺序"></a>sql的执行顺序</h2><p><img src="/2021/06/03/2021-06-03-chong-xue-mysql/1066538-20191005155410162-1650888418.png" alt></p><h2 id="查询性能不稳定"><a href="#查询性能不稳定" class="headerlink" title="查询性能不稳定"></a>查询性能不稳定</h2><h3 id="MySQL为什么会查询不稳定"><a href="#MySQL为什么会查询不稳定" class="headerlink" title="MySQL为什么会查询不稳定"></a>MySQL为什么会查询不稳定</h3><p>因为运行的不正常，或者不稳定，需要花费更多的资源处理别的事情，会使SQL语句的执行效率明显变慢。</p><p>针对innoDB导致MySQL查询性能不稳定的原因，主要是InnoDB 会在后台刷脏页，而刷脏页的过程是要将内存页写入磁盘。所以，无论是你的查询语句在需要内存的时候可能要求淘汰一个脏页，还是由于刷脏页的逻辑会占用 IO 资源并可能影响到了你的更新语句，都可能是造成你从业务端感知MySQL突然慢的原因。</p><h3 id="什么是脏页"><a href="#什么是脏页" class="headerlink" title="什么是脏页"></a>什么是脏页</h3><p>当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。</p><p>按照这个定义感觉脏页是不可避免的，写的时候总会先写内存再写磁盘</p><h3 id="什么是干净页"><a href="#什么是干净页" class="headerlink" title="什么是干净页"></a>什么是干净页</h3><p>内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。</p><h3 id="怎么让MySQL查询性能稳定"><a href="#怎么让MySQL查询性能稳定" class="headerlink" title="怎么让MySQL查询性能稳定"></a>怎么让MySQL查询性能稳定</h3><p>设置合理参数配配置，尤其是设置好innodb_io_capacity 的值，并且平时要多关注脏页比例，不要让它经常接近 75%</p><h2 id="删数据但表文件大小不变"><a href="#删数据但表文件大小不变" class="headerlink" title="删数据但表文件大小不变"></a>删数据但表文件大小不变</h2><ul><li><p>delete 命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的。也就是说，通过 delete 命令是不能回收表空间的。</p></li><li><p>如果要收缩一个表，只是 delete 掉表里面不用的数据的话，表文件的大小是不会变的，你还要通过 alter table A engine=InnoDB 命令重建表，才能达到表文件变小的目的。</p></li></ul><h2 id="join语法"><a href="#join语法" class="headerlink" title="join语法"></a>join语法</h2><p><img src="/2021/06/03/2021-06-03-chong-xue-mysql/Image231.png" alt></p><h2 id="join"><a href="#join" class="headerlink" title="join"></a>join</h2><h3 id="NLJ（Index-Nested-Loop-Join）"><a href="#NLJ（Index-Nested-Loop-Join）" class="headerlink" title="NLJ（Index Nested-Loop Join）"></a>NLJ（Index Nested-Loop Join）</h3><p>驱动表选出一行行数据到被驱动表中查找（被驱动表上有可用的索引）</p><h3 id="BNL（Block-Nested-Loop-Join）"><a href="#BNL（Block-Nested-Loop-Join）" class="headerlink" title="BNL（Block Nested-Loop Join）"></a>BNL（Block Nested-Loop Join）</h3><p>把驱动表的数据分段读入join buffer中，然后和被驱动表join（被驱动表上没有可用的索引）</p><h3 id="能不能使用-join-语句？"><a href="#能不能使用-join-语句？" class="headerlink" title="能不能使用 join 语句？"></a>能不能使用 join 语句？</h3><ul><li>如果可以使用 Index Nested-Loop Join 算法，也就是说可以用上被驱动表上的索引，其实是没问题的；</li><li>如果使用 Block Nested-Loop Join 算法，扫描行数就会过多。尤其是在大表上的 join 操作，这样可能要扫描被驱动表很多次，会占用大量的系统资源。所以这种 join 尽量不要用。</li></ul><h3 id="应该选择大表做驱动表还是选择小表做驱动表？"><a href="#应该选择大表做驱动表还是选择小表做驱动表？" class="headerlink" title="应该选择大表做驱动表还是选择小表做驱动表？"></a>应该选择大表做驱动表还是选择小表做驱动表？</h3><ul><li><p>如果是 Index Nested-Loop Join 算法，应该选择小表做驱动表；</p></li><li><p>如果是 Block Nested-Loop Join 算法：</p><ul><li>在 join_buffer_size 足够大的时候，是一样的；</li><li>在 join_buffer_size 不够大的时候（这种情况更常见），应该选择小表做驱动表。</li></ul></li></ul><h2 id="drop、delete与truncate的区别"><a href="#drop、delete与truncate的区别" class="headerlink" title="drop、delete与truncate的区别"></a>drop、delete与truncate的区别</h2><p>delete和truncate只删除表的数据不删除表的结构</p><p>速度,一般来说: drop&gt; truncate &gt;delete </p><p>delete语句是dml,这个操作会放到rollback segement中,事务提交之后才生效;</p><h2 id="count"><a href="#count" class="headerlink" title="count()"></a>count()</h2><ul><li><p>按照效率排序的话，count(字段)&lt;count(主键 id)&lt;count(1)≈count(<em>)，建议尽量使用 count(</em>)</p></li><li><p>MyISAM 引擎把一个表的总行数存在了磁盘上，因此执行 count(*) 的时候会直接返回这个数，效率很高；</p></li><li><p>而 InnoDB 引擎就麻烦了，它执行 count(*) 的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。</p></li><li><p>count() 是一个聚合函数，对于返回的结果集，一行行地判断，如果 count 函数的参数不是 NULL，累计值就加 1，否则不加，最后返回累计值。</p></li><li><p>如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加；</p></li><li><p>如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。</p></li><li><p>从引擎返回的字段会涉及到解析数据行，以及拷贝字段值的操作。</p></li><li><p>对于 count(主键 id) 来说，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加。从引擎返回的 主键id 会涉及到解析数据行，以及拷贝字段值的操作。</p></li><li><p>对于 count(1) 来说，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。</p></li><li><p>对于count(<em>)来说，并不会把全部字段取出来，而是专门做了优化，不取值。count(</em>) 肯定不是 null，按行累加。</p></li></ul><h2 id="order-by"><a href="#order-by" class="headerlink" title="order by"></a>order by</h2><ul><li><p>MySQL会为每个线程分配一个内存（sort_buffer）用于排序该内存大小为sort_buffer_size</p><ul><li>如果排序的数据量小于sort_buffer_size，排序将会在内存中完成</li><li>如果排序数据量很大，内存中无法存下这么多数据，则会使用磁盘临时文件来辅助排序，也称外部排序</li><li>在使用外部排序时，MySQL会分成好几份单独的临时文件用来存放排序后的数据，然后在将这些文件合并成一个大文件</li><li>按照情况建立联合索引来避免排序所带来的性能损耗，允许的情况下也可以建立覆盖索引来避免回表</li></ul></li><li><p>全字段排序</p><ul><li>通过索引将所查询的字段全部读取到sort_buffer中</li><li>按照排序字段进行排序</li><li>将结果集返回给客户端<br>缺点：</li><li>造成sort_buffer中存放不下很多数据，因为除了排序字段还存放其他字段，对sort_buffer的利用效率不高</li><li>当所需排序数据量很大时，会有很多的临时文件，排序性能也会很差<br>优点：MySQL认为内存足够大时会优先选择全字段排序，因为这种方式比rowid 排序避免了一次回表操作</li></ul></li><li><p>rowid排序</p><ul><li>通过控制排序的行数据的长度来让sort_buffer中尽可能多的存放数据，max_length_for_sort_data</li><li>只将需要排序的字段和主键读取到sort_buffer中，并按照排序字段进行排序</li><li>按照排序后的顺序，取id进行回表取出想要获取的数据</li><li>将结果集返回给客户端<br>优点：更好的利用内存的sort_buffer进行排序操作，尽量减少对磁盘的访问<br>缺点：回表的操作是随机IO，会造成大量的随机读，不一定就比全字段排序减少对磁盘的访问</li></ul></li><li><p>全字段排序 VS rowid 排序</p><ul><li>如果 MySQL 实在是担心排序内存太小，会影响排序效率，才会采用 rowid 排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据。</li><li>如果 MySQL 认为内存足够大，会优先选择全字段排序，把需要的字段都放到 sort_buffer 中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据。</li></ul></li><li><p>如果遇到order by，尽量使用索引，因为索引本来就已经排好序了的。甚至可以利用覆盖索引，减少回表查询，提高查询效率。</p></li><li><p>创建的表没有主键，或者把一个表的主键删掉了，那么 InnoDB 会自己生成一个长度为 6 字节的 rowid 来作为主键。这也就是排序模式里面，rowid 名字的来历。实际上它表示的是：每个引擎用来唯一标识数据行的信息。</p></li><li><p>order by rand() 使用了内存临时表，内存临时表排序的时候使用了 rowid 排序方法。</p></li></ul><h2 id="char和varchar"><a href="#char和varchar" class="headerlink" title="char和varchar"></a>char和varchar</h2><p>varchar 类型的长度是可变的，而 char 类型的长度是固定的</p><p>char 类型是一个定长的字段，以 char(10) 为例，不管真实的存储内容多大或者是占了多少空间，都会消耗掉 10 个字符的空间。坦通俗来讲，当定义为 char(10) 时，即使插入的内容是 ‘abc’ 3 个字符，它依然会占用 10 个字节，其中包含了 7 个空字节。</p><p>char 长度最大为 255 个字符，varchar 长度最大为 65535 个字符</p><p>总结: 可变长度使用 varchar，固定长度使用 char</p><h2 id="主备一致"><a href="#主备一致" class="headerlink" title="主备一致"></a>主备一致</h2><h3 id="主从复制"><a href="#主从复制" class="headerlink" title="主从复制"></a>主从复制</h3><p><img src="/2021/06/03/2021-06-03-chong-xue-mysql/fd75a2b37ae6ca709b7f16fe060c2c10.png" alt="MySQL 主备切换流程"></p><p><img src="/2021/06/03/2021-06-03-chong-xue-mysql/a66c154c1bc51e071dd2cc8c1d6ca6a3.png" alt="主备流程图"></p><p>备库 B 跟主库 A 之间维持了一个长连接。主库 A 内部有一个线程，专门用于服务备库 B 的这个长连接。一个事务日志同步的完整过程是这样的：</p><ol><li>在备库 B 上通过 change master 命令，设置主库 A 的 IP、端口、用户名、密码，以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量。</li><li>在备库 B 上执行 start slave 命令，这时候备库会启动两个线程，就是图中的 io_thread 和 sql_thread。其中 io_thread 负责与主库建立连接。</li><li>主库 A 校验完用户名、密码后，开始按照备库 B 传过来的位置，从本地读取 binlog，发给 B。</li><li>备库 B 拿到 binlog 后，写到本地文件，称为中转日志（relay log）。</li><li>sql_thread 读取中转日志，解析出日志里的命令，并执行。</li></ol><h3 id="binlog有三种格式："><a href="#binlog有三种格式：" class="headerlink" title="binlog有三种格式："></a>binlog有三种格式：</h3><ul><li>statement，记录数据操作的命令，<br>  优点：对人友好，直接记录原命令，数据量小<br>  缺点：可能会导导致主备数据不一致</li><li>row 记录数据的操作，推荐的格式<br>  优点：会明确操作那一行，不会出现主备数据不一致的情况；会记录原数据的整行信息，数据恢复小能手。<br>  缺点：数据量大，传输带宽和性能有损。例如删除10W行数据，statement数据仅仅记录命令，而row就要把这 10 万条记录都写到 binlog 中。这样做，不仅会占用更大的空间，同时写 binlog 也要耗费 IO 资源，影响执行速度。</li><li>mixed 自行判断使用statement格式还是row格式<br>  优点：MySQL 自己会判断这条 SQL 语句是否可能引起主备不一致，如果有可能，就用 row 格式，否则就用 statement 格式。<br>  缺点：有些情况和常识相冲，其日志不能简单的拿来用于数据恢复</li></ul><h3 id="双-M-结构"><a href="#双-M-结构" class="headerlink" title="双 M 结构"></a>双 M 结构</h3><p><img src="/2021/06/03/2021-06-03-chong-xue-mysql/20ad4e163115198dc6cf372d5116c956.png" alt="主备切换流程 -- 双 M 结构"></p><p>根据server id来解决循环复制问题</p><ul><li>规定两个库的 server id 必须不同，如果相同，则它们之间不能设定为主备关系；</li><li>一个备库接到 binlog 并在重放的过程中，生成与原 binlog 的 server id 相同的新的 binlog；</li><li>每个库在收到从自己的主库发过来的日志后，先判断 server id，如果跟自己的相同，表示这个日志是自己生成的，就直接丢弃这个日志。</li></ul><h2 id="高可用"><a href="#高可用" class="headerlink" title="高可用"></a>高可用</h2><h3 id="主备延迟"><a href="#主备延迟" class="headerlink" title="主备延迟"></a>主备延迟</h3><p>在同一个事务在备库执行完成的时间和主库执行完成的时间之间的差值，包括主库事务执行完成时间和将binlog发送给备库，备库事务的执行完成时间的差值。每个事务的seconds_behind_master延迟时间，每个事务的 binlog 里面都有一个时间字段，用于记录主库上的写入时间，备库取出当前正在执行的事务的时间字段的值，计算它与当前系统时的差值。在备库上执行 show slave status，采集 seconds_behind_master 的值。</p><h3 id="主备延迟的来源"><a href="#主备延迟的来源" class="headerlink" title="主备延迟的来源"></a>主备延迟的来源</h3><ul><li>备库所在机器的性能要比主库所在的机器性能差：有些部署条件下，备库所在机器的性能要比主库所在的机器性能差，原因多个备库部署在同一台机器上，大量的查询会导致io资源的竞争，解决办法是配置”双1“，redo log和binlog都只write fs page cache</li><li>备库的压力大：产生的原因大量的查询操作在备库操作，耗费了大量的cpu，导致同步延迟，解决办法，使用一主多从，多个从减少备的查询压力</li><li>大事务：因为如果一个大的事务的dml操作导致执行时间过长，将其事务binlog发送给备库，备库也需执行那么长时间，导致主备延迟，解决办法尽量减少大事务，比如delete操作，使用limit分批删除，可以防止大事务也可以减少锁的范围。</li><li>大表的ddl：会导致主库将其ddl binlog发送给备库，备库解析中转日志，同步，后续的dml binlog发送过来，需等待ddl的mdl写锁释放，导致主备延迟。</li></ul><h3 id="可靠性优先策略"><a href="#可靠性优先策略" class="headerlink" title="可靠性优先策略"></a>可靠性优先策略</h3><ul><li>判断备库 B 现在的 seconds_behind_master如果小于某个值（比如 5 秒）继续下一步，否则持续重试这一步</li><li>把主库 A 改成只读状态，即把 readonly 设置为 true</li><li>判断备库 B 的 seconds_behind_master的值，直到这个值变成 0 为止； </li><li>把备库 B 改成可读写也就是把 readonly 设置为 false；</li><li>把业务请求切换到备库.</li></ul><p>个人理解如果发送过来的binlog在中转日志中有多个事务，只能readonly，业务不能更新的时间，就是多个事务被运用的总时间，因为要等到seconds_behind_master等于0才能切换主备。如果非正常情况下，主库掉电，会导致出现的问题，如果备库和主库的延迟时间短，在中转日志运用完成，业务才能正常使用（造成一定时间内不可用，不能操作数据库）；如果在中转日志还未运用完成，备库切换为主库会导致之前完成的事务还没同步完成，造成查询数据丢失，随着中转日志的继续应用，这些数据会恢复回来，再次查询又有数据让人感觉很诡异。</p><h3 id="可用性策略"><a href="#可用性策略" class="headerlink" title="可用性策略"></a>可用性策略</h3><p>出现的问题：在双m，且binlog_format=mixed，会导致主备数据不一致。使用使用 row 格式的 binlog 时，数据不一致的问题更容易发现，因为binlog row会记录字段的所有值。</p><p>使用 row 格式的 binlog 时，数据不一致的问题更容易被发现。而使用 mixed 或者 statement 格式的 binlog 时，数据很可能悄悄地就不一致了。如果你过了很久才发现数据不一致的问题，很可能这时的数据不一致已经不可查，或者连带造成了更多的数据逻辑不一致。</p><h3 id="用哪个策略"><a href="#用哪个策略" class="headerlink" title="用哪个策略"></a>用哪个策略</h3><p>主备切换的可用性优先策略会导致数据不一致。因此，大多数情况下，我都建议你使用可靠性优先策略。毕竟对数据服务来说的话，数据的可靠性一般还是要优于可用性的。</p><h2 id="大表优化"><a href="#大表优化" class="headerlink" title="大表优化"></a>大表优化</h2><h3 id="限定数据的范围"><a href="#限定数据的范围" class="headerlink" title="限定数据的范围"></a>限定数据的范围</h3><p>添加查询条件或者范围查询。</p><h3 id="垂直分区"><a href="#垂直分区" class="headerlink" title="垂直分区"></a>垂直分区</h3><p>根据数据库里面数据表的相关性进行拆分。 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。</p><p><img src="/2021/06/03/2021-06-03-chong-xue-mysql/w1pft649.bmp" alt></p><ul><li>垂直拆分的优点： 可以使得列数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。</li><li>垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂；</li></ul><h3 id="水平分区"><a href="#水平分区" class="headerlink" title="水平分区"></a>水平分区</h3><p>保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。</p><p><img src="/2021/06/03/2021-06-03-chong-xue-mysql/gv5r1ghp.bmp" alt></p><p>水平拆分能够 支持非常大的数据量存储，应用端改造也少，但 分片事务难以解决 ，跨节点Join性能较差，逻辑复杂。</p><p>数据库分片的两种常见方案：</p><ul><li>客户端代理： 分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。 当当网的 Sharding-JDBC 、阿里的TDDL是两种比较常用的实现。</li><li>中间件代理： 在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。 我们现在谈的 <strong>Mycat</strong> 、360的Atlas、网易的DDB等等都是这种架构的实现。</li></ul>]]></content>
      
      
      <categories>
          
          <category> review </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis</title>
      <link href="/2021/05/30/2021-05-30-chong-xue-redis/"/>
      <url>/2021/05/30/2021-05-30-chong-xue-redis/</url>
      
        <content type="html"><![CDATA[<h2 id="为什么要用缓存？"><a href="#为什么要用缓存？" class="headerlink" title="为什么要用缓存？"></a>为什么要用缓存？</h2><p>用缓存，主要有两个用途：高性能、高并发。</p><h3 id="高性能"><a href="#高性能" class="headerlink" title="高性能"></a>高性能</h3><p>直接查询Redis缓存比查询数据库要快得多</p><h3 id="高并发"><a href="#高并发" class="headerlink" title="高并发"></a>高并发</h3><p>高并发的时候，可能数据库顶不住会宕机，但Redis承载并发量是 mysql 单机的几十倍。</p><p><strong>缓存是走内存的，内存天然就支撑高并发。</strong></p><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><p>目前支持 6 种数据类型：string、hash、list、set、zset、HyperLogLog。</p><h3 id="string-类型"><a href="#string-类型" class="headerlink" title="string 类型"></a>string 类型</h3><p>常规 KV 结构，其中 V 可以是 string 或 number 类型。</p><p>容量：string 的一个 key 最大能存 512MB。</p><p>应用场景：</p><ul><li>简单值缓存：token，验证码。</li><li>计数器：登录错误次数计数，产品库存。</li></ul><p>底层：简单动态字符串（SDS）。</p><h4 id="简单动态字符串（SDS）"><a href="#简单动态字符串（SDS）" class="headerlink" title="简单动态字符串（SDS）"></a>简单动态字符串（SDS）</h4><ul><li>O（1）复杂度获取字符串长度: 结构中定义len，记录了字符串长度。O（1）复杂度获取字符串长度。</li><li>减少内存分配次数: 结构中定义alloc，分配给字符数组的空间长度。这样在修改字符串的时候，可以通过 alloc - len 计算出剩余的空间大小，可以用来判断空间是否满足修改需求，如果不满足的话，就会自动将 SDS 的空间扩展至执行修改所需的大小，还会给 SDS 分配额外的「未使用空间」。这样的好处是，下次在操作 SDS 时，如果 SDS 空间够的话，API 就会直接使用「未使用空间」，而无须执行内存分配，有效的减少内存分配次数。</li><li>节省内存空间: 结构中定义flags 成员变量，表示的是 SDS 类型。根据不同大小的字符串保存对应的SDS类型，从而有效节省内存空间。</li><li>二进制安全: C语言用 “\0” 字符来标识字符串结尾。SDS有个专门的 len 成员变量来记录长度，所以可存储包含 “\0” 的数据。</li></ul><h3 id="hash-类型"><a href="#hash-类型" class="headerlink" title="hash 类型"></a>hash 类型</h3><p>hash 是一个的键值对集合，其中 V 相当于一个 HashMap，适用于存储对象数据。</p><p>容量：hash 的一个 key 最大可以存储 2^32 -1 个键值对（40 多亿）。</p><p>应用场景：对象信息，如用户信息，产品信息。</p><p>底层：哈希表或压缩列表，包含的元素数量较少，或者元素值不大的情况才会使用压缩列表，否则用哈希表。</p><h4 id="压缩列表"><a href="#压缩列表" class="headerlink" title="压缩列表"></a>压缩列表</h4><p>压缩列表的最大特点，就是它被设计成一种内存紧凑型的数据结构，占用一块连续的内存空间，不仅可以利用 CPU 缓存，而且会针对不同长度的数据，进行相应编码，这种方法可以有效地节省内存开销。不能保存过多的元素，否则查询效率就会降低。</p><h4 id="哈希表"><a href="#哈希表" class="headerlink" title="哈希表"></a>哈希表</h4><ul><li>查询数据 O(1) 的复杂度：哈希表优点在于，它能以 O(1) 的复杂度快速查询数据。将 key 通过 Hash 函数的计算，就能定位数据在表中的位置，因为哈希表实际上是数组，所以可以通过索引值快速查询到数据。</li><li>链式哈希来解决哈希冲突：每个哈希表节点都有一个 next 指针，用于指向下一个哈希表节点，因此多个哈希表节点可以用 next 指针构成一个单项链表，被分配到同一个哈希桶上的多个节点可以用这个单项链表连接起来，这样就解决了哈希冲突。</li><li>rehash：如果哈希冲突严重，随着链表长度的增加，在查询这一位置上的数据的耗时就会增加，毕竟链表的查询的时间复杂度是 O(n)。通过rehash解决，对哈希表的大小进行扩展。结构定义了 2 个哈希表，是因为进行 rehash 的时候，需要用上 2 个哈希表了。在正常服务请求阶段，插入的数据，都会写入到「哈希表 1」，此时的「哈希表 2 」 并没有被分配空间。执行rehash的时候，给「哈希表 2」 分配空间，一般会比「哈希表 1」 大 2 倍；将「哈希表 1」的数据迁移到「哈希表 2」 中。</li><li>渐进式 rehash：如果「哈希表 1 」的数据量非常大，那么在迁移至「哈希表 2 」的时候，因为会涉及大量的数据拷贝，此时可能会对 Redis 造成阻塞，无法服务其他请求。Redis 采用了渐进式 rehash，也就是将数据的迁移的工作不再是一次性迁移完成，而是分多次迁移。渐进式 rehash 步骤：给「哈希表 2」 分配空间；在 rehash 进行期间，每次哈希表元素进行新增、删除、查找或者更新操作时，Redis 除了会执行对应的操作之外，还会顺序将「哈希表 1 」中索引位置上的所有 key-value 迁移到「哈希表 2」 上；随着处理客户端发起的哈希表操作请求数量越多，最终在某个时间点会把「哈希表 1 」的所有 key-value 迁移到「哈希表 2」，从而完成 rehash 操作。查找一个 key 的值的话，先会在「哈希表 1」 里面进行查找，如果没找到，就会继续到哈希表 2 里面进行找到。另外，在渐进式 rehash 进行期间，新增一个 key-value 时，会被保存到「哈希表 2 」里面，而「哈希表 1」 则不再进行任何添加操作，这样保证了「哈希表 1 」的 key-value 数量只会减少，随着 rehash 操作的完成，最终「哈希表 1 」就会变成空表。</li></ul><h3 id="list-类型"><a href="#list-类型" class="headerlink" title="list 类型"></a>list 类型</h3><p>list 是有序列表，它可以压入头部和尾部，也可以弹出头部和尾部。</p><p>容量：list最多可存储 2^32 - 1 个元素元素（40 多亿）。</p><p>应用场景：微博的关注列表，粉丝列表，消息列表。</p><p>底层：快速列表（QuickList）。</p><ul><li>quicklist 就是「双向链表 + 压缩列表」组合，因为一个 quicklist 就是一个链表，而链表中的每个元素又是一个压缩列表。</li><li>在向 quicklist 添加一个元素的时候，不会像普通的链表那样，直接新建一个链表节点。而是会检查插入位置的压缩列表是否能容纳该元素，如果能容纳就直接保存到 quicklistNode 结构里的压缩列表，如果不能容纳，才会新建一个新的 quicklistNode 结构。</li></ul><h3 id="set-类型"><a href="#set-类型" class="headerlink" title="set 类型"></a>set 类型</h3><p>set 是无序集合，其中 value 相当于 HashSet。适用于数据去重、排重、交集，并集，差集等场景。</p><p>容量：集合中最多可以存储 2^32 - 1 个元素元素（40 多亿）。</p><p>应用场景：共同爱好等。</p><p>底层：底层是整数集合或哈希表。如果集合对象的所有元素都是整数值，并且保存元素小于 512 个时，底层将使用整数集合。否则用哈希表。</p><h4 id="整数集合"><a href="#整数集合" class="headerlink" title="整数集合"></a>整数集合</h4><ul><li>当一个 Set 对象只包含整数值元素，并且元素数量不大时，就会使用整数集这个数据结构作为底层实现。</li><li>整数集合升级的好处是节省内存资源</li><li>整数集合会有一个升级规则，就是当我们将一个新元素加入到整数集合里面，如果新元素的类型（int32_t）比整数集合现有所有元素的类型（int16_t）都要长时，整数集合需要先进行升级，也就是按新元素的类型（int32_t）扩展 contents 数组的空间大小，然后才能将新元素加入到整数集合里，当然升级的过程中，也要维持整数集合的有序性。</li><li>整数集合升级的过程不会重新分配一个新类型的数组，而是在原本的数组上扩展空间，然后在将每个元素按间隔类型大小分割，如果 encoding 属性值为 INTSET_ENC_INT16，则每个元素的间隔就是 16 位。</li></ul><h3 id="zset-类型"><a href="#zset-类型" class="headerlink" title="zset 类型"></a>zset 类型</h3><p>zset 是排序集合，其中 value 相当于 TreeSet。适用场景和 set 相似，但数据是有序的。增加了一个权重参数score，使得集合中的元素能够按score进行有序排列。</p><p>容量：集合中最多可以存储 2^32 - 1 个元素元素（40 多亿）。</p><p>应用场景：金额最多的前几位。</p><p>底层：底层是压缩列表或跳表+哈希表，如果元素少于 128 个，且每个元素长度小于 64 字节，使用压缩列表。否则用跳表+哈希表。</p><h4 id="跳表"><a href="#跳表" class="headerlink" title="跳表"></a>跳表</h4><ul><li>支持平均 O(logN) 复杂度的节点查找</li><li>zset 结构体里有两个数据结构：一个是跳表，一个是哈希表。这样的好处是既能进行高效的范围查询，也能进行高效单点查询。</li><li>Zset 对象在执行数据插入或是数据更新的过程中，会依次在跳表和哈希表中插入或更新相应的数据，从而保证了跳表和哈希表中记录的信息一致。</li><li>Zset 对象能支持范围查询（如 ZRANGEBYSCORE 操作），这是因为它的数据结构设计采用了跳表，而又能以常数复杂度获取元素权重（如 ZSCORE 操作），这是因为它同时采用了哈希表进行索引。</li><li>哈希表只是用于以常数复杂度获取元素权重，大部分操作都是跳表实现的。</li><li>查找过程就是在多个层级上跳来跳去，最后定位到元素。当数据量很大时，跳表的查找复杂度就是 O(logN)。</li><li>查找一个跳表节点的过程时，跳表会从头节点的最高层开始，逐一遍历每一层。在遍历某一层的跳表节点时，会用跳表节点中的 SDS 类型的元素和元素的权重来进行判断，共有两个判断条件：- 如果当前节点的权重「小于」要查找的权重时，跳表就会访问该层上的下一个节点。- 如果当前节点的权重「等于」要查找的权重时，并且当前节点的 SDS 类型数据「小于」要查找的数据时，跳表就会访问该层上的下一个节点。如果上面两个条件都不满足，或者下一个节点为空时，跳表就会使用目前遍历到的节点的 level 数组里的下一层指针，然后沿着下一层指针继续查找，这就相当于跳到了下一层接着查找。</li></ul><p><img src="/2021/05/30/2021-05-30-chong-xue-redis/3%E5%B1%82%E8%B7%B3%E8%A1%A8-%E8%B7%A8%E5%BA%A6.drawio.webp" alt></p><p>如果要查找「元素：abcd，权重：4」的节点，查找的过程是这样的：</p><ul><li>先从头节点的最高层开始，L2 指向了「元素：abc，权重：3」节点，这个节点的权重比要查找节点的小，所以要访问该层上的下一个节点；</li><li>但是该层的下一个节点是空节点（ leve[2]指向的是空节点），于是就会跳到「元素：abc，权重：3」节点的下一层去找，也就是 leve[1];</li><li>「元素：abc，权重：3」节点的 leve[1] 的下一个指针指向了「元素：abcde，权重：4」的节点，然后将其和要查找的节点比较。虽然「元素：abcde，权重：4」的节点的权重和要查找的权重相同，但是当前节点的 SDS 类型数据「大于」要查找的数据，所以会继续跳到「元素：abc，权重：3」节点的下一层去找，也就是 leve[0]；</li><li>「元素：abc，权重：3」节点的 leve[0] 的下一个指针指向了「元素：abcd，权重：4」的节点，该节点正是要查找的节点，查询结束。</li></ul><h2 id="Redis-的线程模型-重要，灵魂之处"><a href="#Redis-的线程模型-重要，灵魂之处" class="headerlink" title="Redis 的线程模型(重要，灵魂之处)"></a>Redis 的线程模型(重要，灵魂之处)</h2><h3 id="线程模型"><a href="#线程模型" class="headerlink" title="线程模型"></a>线程模型</h3><p><strong>Redis内部使用文件事件处理器，这个文件事件处理器是单线程的，所以 Redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 Socket，将产生文件事件的 Socket 压入内存队列中，同时文件事件分派器每次从队列中取出一个 Socket，根据 Socket 上的文件事件类型来选择对应的事件处理器进行处理。</strong>。</p><p><img src="/2021/05/30/2021-05-30-chong-xue-redis/image.png" alt></p><p>文件事件处理器的结构包含 4 个部分：</p><ul><li>多个 socket</li><li>IO 多路复用程序</li><li>文件事件分派器</li><li>事件处理器（连接应答处理器、命令请求处理器、命令回复处理器）</li></ul><h3 id="通信过程"><a href="#通信过程" class="headerlink" title="通信过程"></a>通信过程</h3><p><img src="/2021/05/30/2021-05-30-chong-xue-redis/redis-single-thread-model.png" alt></p><p>首先，Redis 服务端进程初始化的时候，会将 server socket 的 AE_READABLE 事件与连接应答处理器关联。</p><p>客户端 socket01 向 Redis 进程的 server socket 请求建立连接，此时 server socket 会产生一个 AE_READABLE 事件，IO 多路复用程序监听到 server socket 产生的事件后，将该 socket 压入队列中。文件事件分派器从队列中获取 socket，交给连接应答处理器。连接应答处理器会创建一个能与客户端通信的 socket01，并将该 socket01 后面产生的 AE_READABLE 事件与命令请求处理器关联。</p><p>假设此时客户端发送了一个 set key value 请求，此时 Redis 中的 socket01 会产生 AE_READABLE 事件，IO 多路复用程序将 socket01 压入队列，此时事件分派器从队列中获取到 socket01 产生的 AE_READABLE 事件，由于前面 socket01 的 AE_READABLE 事件已经与命令请求处理器关联，因此事件分派器将事件交给命令请求处理器来处理。命令请求处理器读取 socket01 的 key value 并在自己内存中完成 key value 的设置。操作完成后，它会将 socket01 的 AE_WRITABLE 事件与命令回复处理器关联。</p><p>如果此时客户端准备好接收返回结果了，那么 Redis 中的 socket01 会产生一个 AE_WRITABLE 事件，同样压入队列中，事件分派器找到相关联的命令回复处理器，由命令回复处理器对 socket01 输入本次操作的一个结果，比如 ok ，之后解除 socket01 的 AE_WRITABLE 事件与命令回复处理器的关联。</p><h2 id="Redis6-0引入多线程"><a href="#Redis6-0引入多线程" class="headerlink" title="Redis6.0引入多线程"></a>Redis6.0引入多线程</h2><p>Redis在处理客户端的请求时，包括获取 (socket 读)、解析、执行、内容返回 (socket 写) 等都由一个顺序串行的主线程处理，这就是所谓的“单线程”。</p><p>使用Redis时，几乎不存在CPU成为瓶颈的情况， Redis主要受限于内存和网络。因为读写网络的read/write系统调用占用了Redis执行期间大部分CPU时间，瓶颈主要在于网络的 IO 消耗, 所以Redis6.0引入多线程。</p><p><img src="/2021/05/30/2021-05-30-chong-xue-redis/da584410975a483bbb989841ae1803ee.png" alt="Redis6.0多线程的实现机制"></p><p>1、主线程负责接收建立连接请求，获取 socket 放入全局等待读处理队列<br>2、主线程通过 RR(Round Robin) 将这些连接分配给这些 IO 线程<br>3、主线程阻塞等待 IO 线程读取 socket 完毕<br>4、主线程通过单线程的方式执行请求命令，请求数据读取并解析完成，但并不执行<br>5、主线程阻塞等待 IO 线程将数据回写 socket 完毕<br>6、解除绑定，清空等待队列</p><p><img src="/2021/05/30/2021-05-30-chong-xue-redis/3dfcd72591994630a76244927945fef6.png" alt></p><ul><li>IO 线程要么同时在读 socket，要么同时在写，不会同时读或写</li><li>IO 线程只负责读写 socket 解析命令，不负责命令处理</li></ul><p>Redis的多线程部分只是用来处理网络数据的读写和协议解析，执行命令仍然是单线程顺序执行，不会存在线程并发安全问题。</p><p>就是任务过来判断等待队列是否满，如果不满就加入等待队列，如果等待队列满了，就平均分给IO线程组处理，主线程阻塞等待IO线程组读socket并解析请求完成，主线程再执行所有请求命令。然后主线程再阻塞等待 IO 线程将数据回写 socket 完毕。</p><h2 id="为什么-Redis-单线程却能支撑高并发"><a href="#为什么-Redis-单线程却能支撑高并发" class="headerlink" title="为什么 Redis 单线程却能支撑高并发"></a>为什么 Redis 单线程却能支撑高并发</h2><ul><li>纯内存操作。</li><li>核心是基于非阻塞的 IO 多路复用机制。</li><li>C 语言实现，一般来说，C 语言实现的程序“距离”操作系统更近，执行速度相对会更快。</li><li>单线程反而避免了多线程的频繁上下文切换问题，预防了多线程可能产生的竞争问题。</li></ul><h2 id="过期策略"><a href="#过期策略" class="headerlink" title="过期策略"></a>过期策略</h2><p><strong>Redis 过期策略是：定期删除+惰性删除。</strong></p><p>所谓定期删除，指的是 Redis 默认是每隔 100ms 就<strong>随机</strong>抽取一些设置了过期时间的 key，检查其是否过期，如果过期就删除。</p><p>定期删除可能会导致很多过期 key 到了时间并没有被删除掉，那咋办呢？</p><p>惰性删除，在获取某个 key 的时候，会检查下这个 key 是否过期了，如果过期就删除。</p><blockquote><p>获取 key 的时候，如果此时 key 已经过期，就删除，不会返回任何东西。</p></blockquote><p>如果定期删除漏掉了很多过期 key，然后你又没去获取 key（没走惰性删除），此时就会有大量过期 key 堆积在内存里，就可能导致 Redis 内存块耗尽。如果出现这种问题，就需要走内存淘汰机制。</p><h2 id="内存淘汰机制"><a href="#内存淘汰机制" class="headerlink" title="内存淘汰机制"></a>内存淘汰机制</h2><ul><li>noeviction: 当内存不足以容纳新写入数据时，新写入操作会报错。（一般没人用）</li><li>allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个最常用）。</li><li>allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个 key。（一般没人用）</li><li>volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 key（一般没人用）。</li><li>volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个 key。（一般没人用）</li><li>volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 key 优先移除。（一般没人用）</li></ul><h2 id="手写一个-LRU-算法"><a href="#手写一个-LRU-算法" class="headerlink" title="手写一个 LRU 算法"></a>手写一个 LRU 算法</h2><p>LRU是Least Recently Used的缩写，即最近最少使用，是一种常用的页面置换算法，选择最近最久未使用的数据予以淘汰。</p><p>查找快、插入快、删除快，（O(1)时间复杂度）且还需要先后排序———-&gt;什么样的数据结构可以满足这个问题？</p><p>LRU的算法核心是哈希链表</p><pre><code>public class LRUDemo extends LinkedHashMap&lt;Integer, Integer&gt; {    private int capacity;    public LRUDemo(int initialCapacity) {        // true 表示让 linkedHashMap 按照访问顺序来进行排序，最近访问的放在头部，最老访问的放在尾部。        super(initialCapacity, 0.75F, true);        this.capacity = initialCapacity;    }    @Override    protected boolean removeEldestEntry(Map.Entry&lt;Integer, Integer&gt; eldest) {        // 当 map中的数据量大于指定的缓存个数的时候，就自动删除最老的数据。        return super.size() &gt; capacity;    }    // 测试    public static void main(String[] args) {        LRUDemo lruDemo = new LRUDemo(3);        lruDemo.put(1, 1);        lruDemo.put(2, 2);        lruDemo.put(3, 3);        System.out.println(lruDemo.keySet());        lruDemo.put(4, 4);        System.out.println(lruDemo.keySet());        lruDemo.put(3, 3);        lruDemo.put(3, 3);        System.out.println(lruDemo.keySet());    }}</code></pre><h2 id="持久化机制"><a href="#持久化机制" class="headerlink" title="持久化机制"></a>持久化机制</h2><ul><li>RDB：对数据执行周期性的持久化。</li><li>AOF：对每条写命令都作为日志写入日志文件中，以后可以通过回放日志中的写命令来重新构建整个数据集。</li></ul><h3 id="RDB-优缺点"><a href="#RDB-优缺点" class="headerlink" title="RDB 优缺点"></a>RDB 优缺点</h3><p>优点：</p><ul><li>可以通过配置生成多个时间段的数据文件进行保存，这种方式很适合做冷备。</li><li>对 Redis 对外提供读写服务影响非常小，只需要 fork 一个子线程来单独进行持久化即可。</li><li>直接基于 RDB 数据文件来重启恢复 Redis 进程比 AOF 文件更快。</li></ul><p>缺点：</p><ul><li>因为是周期备份，所以在 Redis 故障时会丢失一部分时间的数据。</li><li>如果生成快照文件的数据特别大，可能会导致客户端提供的服务暂停数毫秒，甚至数秒。</li></ul><h3 id="AOF优缺点"><a href="#AOF优缺点" class="headerlink" title="AOF优缺点"></a>AOF优缺点</h3><p>优点：</p><ul><li>可以更好的保护数据，每隔 1 秒就会执行一次 fsync 操作，最多只会丢失 1 秒钟的数据。</li><li>日志文件以 Append-Only 模式写入，没有磁盘寻址开销，写入性能高，文件不容易破损。</li><li>日志文件即使过大的时候，出现后台重写操作时，也不会影响客户端的读写。因为在 rewrite log 的时候，会对其中的指导进行压缩，创建出一份需要恢复数据的最小日志出来。在创建新日志文件的时候，老的日志文件还是照常写入。当新的 merge 后的日志文件 ready 的时候，再交换新老日志文件即可。</li></ul><p>缺点：</p><ul><li>对于同一份数据来说，AOF 日志文件通常比 RDB 数据快照文件更大。</li><li>AOF 开启后，支持的写 QPS 会比 RDB 支持的写 QPS 低，因为 AOF 一般会配置成每秒 fsync 一次日志文件，当然，每秒一次 fsync，性能也还是很高的。</li></ul><h3 id="RDB-和-AOF-到底该如何选择"><a href="#RDB-和-AOF-到底该如何选择" class="headerlink" title="RDB 和 AOF 到底该如何选择"></a>RDB 和 AOF 到底该如何选择</h3><p>两个都要用，用 AOF 来保证数据不丢失，作为数据恢复首选项。用 RDB 做多种冷备，在 AOF 文件丢失或损坏的时候，还可以恢复大部分数据。</p><h2 id="主从架构"><a href="#主从架构" class="headerlink" title="主从架构"></a>主从架构</h2><h3 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h3><p>Redis 虽然读写很快，但是也会产生性能瓶颈（单机最多几万 QPS），特别是在读的压力上（通常写请求是比较少的），为了分担压力，就需要用到主从架构。</p><p>其特点是一主多从，由主节点负责写，并且将数据同步到从节点上，由从节点负责读。如果 QPS 暴增，只要增加从节点就可以了。</p><p><img src="/2021/05/30/2021-05-30-chong-xue-redis/image1.png" alt></p><h3 id="Redis-replication-的核心机制"><a href="#Redis-replication-的核心机制" class="headerlink" title="Redis replication 的核心机制"></a>Redis replication 的核心机制</h3><ul><li>Redis 采用异步方式复制数据到 slave 节点，slave 复制时，不会阻塞 master 的正常工作。</li><li>一个 master node 是可以配置多个 slave node 的；同时slave node 也可以连接其他的 slave node；</li><li>slave node 在做复制的时候，也不会 block 对自己的查询操作，它会用旧的数据集来提供服务；但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了；</li><li>slave node 主要用来进行横向扩容，做读写分离，扩容的 slave node 可以提高读的吞吐量。</li></ul><h3 id="Redis-主从复制的核心原理"><a href="#Redis-主从复制的核心原理" class="headerlink" title="Redis 主从复制的核心原理"></a>Redis 主从复制的核心原理</h3><p>当启动一个 slave node 的时候，它会发送一个 PSYNC 命令给 master node。</p><p>如果 slave 节点是首次连接 master 节点，那么会触发一次全量复制。</p><p>如果 slave 节点是重新连接 master 节点，那么会做增量更新（断点续传）。</p><p><img src="/2021/05/30/2021-05-30-chong-xue-redis/redis-master-slave-replication.png" alt></p><h4 id="全量复制"><a href="#全量复制" class="headerlink" title="全量复制"></a>全量复制</h4><ol><li>master 会异步 fork 一个子线程生成 RDB 快照文件，同时还会将新收到的所有写命令保存在内存中。</li><li>RDB 文件生成完毕后，master 会把这个文件发给 slave。</li><li>slave 会先写入本地磁盘，再从本地磁盘加载到内存中。</li><li>接着 master 会把保存在内存中的所有写命令发送到 slave，salve 也会同步这些数据。</li></ol><h4 id="断点续传"><a href="#断点续传" class="headerlink" title="断点续传"></a>断点续传</h4><p>网络连接断了，连接之后 master node 仅会复制给 slave 部分缺少的数据。</p><ol><li>master 在接受数据写入后，会写到数据缓冲区，同时也会积压到 backlog 中。</li><li>master 和 slave 两端都会维护一个 offset 记录当前已经同步过的命令。</li><li>如果 slave 断开重连，会发送 psync runid offset 指令，让 master 从 上次的 replication offset 开始复制。</li><li>如果没有找到对应的 offset，那么就会执行一次全量复制。</li></ol><h3 id="slave过期key处理"><a href="#slave过期key处理" class="headerlink" title="slave过期key处理"></a>slave过期key处理</h3><p>slave 不会过期 key，只会等待 master 过期 key。如果 master 过期了一个 key，或者通过 LRU 淘汰了一个 key，那么会模拟一条 del 命令发送给 slave。</p><h2 id="哨兵架构"><a href="#哨兵架构" class="headerlink" title="哨兵架构"></a>哨兵架构</h2><h3 id="哨兵的介绍"><a href="#哨兵的介绍" class="headerlink" title="哨兵的介绍"></a>哨兵的介绍</h3><p>sentinel，中文名是哨兵。哨兵是 Redis 集群架构中非常重要的一个组件，主要有以下功能：</p><ul><li>集群监控：负责监控 Redis master 和 slave 进程是否正常工作。</li><li>消息通知：如果某个 Redis 实例有故障，那么哨兵负责发送消息作为报警通知给管理员。</li><li>故障转移：如果 master node 挂掉了，会自动转移到 slave node 上。</li><li>配置中心：如果故障转移发生了，通知 client 客户端新的 master 地址。</li></ul><p>哨兵用于实现 Redis 集群的高可用，本身也是分布式的，作为一个哨兵集群去运行，互相协同工作。</p><ul><li>故障转移时，判断一个 master node 是否宕机了，需要大部分的哨兵都同意才行，涉及到了分布式选举的问题。</li><li>即使部分哨兵节点挂掉了，哨兵集群还是能正常工作的，保证自身高可用。</li></ul><h3 id="哨兵的核心知识"><a href="#哨兵的核心知识" class="headerlink" title="哨兵的核心知识"></a>哨兵的核心知识</h3><ul><li>哨兵至少需要 3 个实例，来保证自己的健壮性。</li><li>哨兵 + Redis 主从的部署架构，是不保证数据零丢失的，只能保证 Redis 集群的高可用性。</li><li>对于哨兵 + Redis 主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练。</li></ul><h3 id="哨兵至少要-3-个实例"><a href="#哨兵至少要-3-个实例" class="headerlink" title="哨兵至少要 3 个实例"></a>哨兵至少要 3 个实例</h3><p>哨兵集群如果要做主备切换，至少需要2数量的哨兵认为 odown，才能选举出一个哨兵来做切换。如果你配置2个哨兵实例，如果宕机一个，就只剩1数量的哨兵了。</p><h3 id="哨兵主备切换的数据丢失问题"><a href="#哨兵主备切换的数据丢失问题" class="headerlink" title="哨兵主备切换的数据丢失问题"></a>哨兵主备切换的数据丢失问题</h3><p>Redis 主备切换会有两种数据丢失的情况：异步复制 &amp; 集群脑裂。</p><h4 id="异步复制导致的数据丢失"><a href="#异步复制导致的数据丢失" class="headerlink" title="异步复制导致的数据丢失"></a>异步复制导致的数据丢失</h4><p>因为 master-&gt;slave 的复制是异步的，所以可能有部分数据还没复制到 slave，master 就宕机了，此时这部分数据就丢失了。</p><p><img src="/2021/05/30/2021-05-30-chong-xue-redis/async-replication-data-lose-case.png" alt></p><h4 id="脑裂导致的数据丢失"><a href="#脑裂导致的数据丢失" class="headerlink" title="脑裂导致的数据丢失"></a>脑裂导致的数据丢失</h4><p>脑裂，也就是说，某个 master 所在机器突然脱离了正常的网络，跟其他 slave 机器不能连接，但是实际上 master 还运行着。此时哨兵可能就会认为 master 宕机了，然后开启选举，将其他 slave 切换成了 master。这个时候，集群里就会有两个 master ，也就是所谓的脑裂。</p><p>此时虽然某个 slave 被切换成了 master，但是可能 client 还没来得及切换到新的 master，还继续向旧 master 写数据。因此旧 master 再次恢复的时候，会被作为一个 slave 挂到新的 master 上去，自己的数据会清空，重新从新的 master 复制数据。而新的 master 并没有后来 client 写入的数据，因此，这部分数据也就丢失了。</p><p><img src="/2021/05/30/2021-05-30-chong-xue-redis/redis-cluster-split-brain.png" alt></p><h4 id="数据丢失问题的解决方案"><a href="#数据丢失问题的解决方案" class="headerlink" title="数据丢失问题的解决方案"></a>数据丢失问题的解决方案</h4><p>加两个配置，核心思想是，如果跟任何一个 slave 丢了连接，在 10 秒后发现没有 slave 给自己 ack，那么就拒绝新的写请求。</p><pre><code>min-slaves-to-write 1min-slaves-max-lag 10</code></pre><ul><li><p>减少异步复制数据的丢失</p><p>  有了 min-slaves-max-lag 这个配置，就可以确保说，一旦 slave 复制数据和 ack 延时太长，就认为可能 master 宕机后损失的数据太多了，那么就拒绝写请求，这样可以把 master 宕机时由于部分数据未同步到 slave 导致的数据丢失降低的可控范围内。</p></li><li><p>减少脑裂的数据丢失</p><p>  如果一个 master 出现了脑裂，跟其他 slave 丢了连接，那么上面两个配置可以确保说，如果不能继续给指定数量的 slave 发送数据，而且 slave 超过 10 秒没有给自己 ack 消息，那么就直接拒绝客户端的写请求。因此在脑裂场景下，最多就丢失 10 秒的数据。</p></li></ul><h3 id="sdown-和-odown-转换机制"><a href="#sdown-和-odown-转换机制" class="headerlink" title="sdown 和 odown 转换机制"></a>sdown 和 odown 转换机制</h3><ul><li>sdown 是主观宕机，就一个哨兵如果自己觉得一个 master 宕机了，那么就是主观宕机</li><li>odown 是客观宕机，如果 quorum 数量的哨兵都觉得一个 master 宕机了，那么就是客观宕机</li></ul><h3 id="slave-gt-master-选举算法"><a href="#slave-gt-master-选举算法" class="headerlink" title="slave-&gt;master 选举算法"></a>slave-&gt;master 选举算法</h3><p>如果一个 master 被认为 odown 了，而且 majority 数量的哨兵都允许主备切换，那么某个哨兵就会执行主备切换操作，此时首先要选举一个 slave 来，会考虑 slave 的一些信息：</p><ul><li>跟 master 断开连接的时长</li><li>slave 优先级</li><li>复制 offset</li><li>run id</li></ul><p>如果一个 slave 跟 master 断开连接的时间已经超过了 down-after-milliseconds 的 10 倍，外加 master 宕机的时长，那么 slave 就被认为不适合选举为 master。</p><p>接下来会对 slave 进行排序：</p><ul><li>按照 slave 优先级进行排序，slave priority 越低，优先级就越高。</li><li>如果 slave priority 相同，那么看 replica offset，哪个 slave 复制了越多的数据，offset 越靠后，优先级就越高。</li><li>如果上面两个条件都相同，那么选择一个 run id 比较小的那个 slave。</li></ul><h2 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h2><p>Redis-cluster 是官方 3.0 版本后的集群方案，它具有以下特点：</p><ul><li>多主多从：能支撑多个 master，且每个 master 都可以挂载多个 slave。</li><li>读写分离：写操作会用 master，读操作会从 master 对应的 slave 去读。</li><li>横向扩容：自动将数据进行分片，每个 master 都放一部分数据，只要增加 master 节点就能存放更多的数据。</li><li>高可用：如果 master 挂掉，Redis cluster 这套机制，就会自动将某个 slave 切换成 master。</li></ul><h2 id="分布式寻址算法"><a href="#分布式寻址算法" class="headerlink" title="分布式寻址算法"></a>分布式寻址算法</h2><h3 id="最老土的-Hash-算法"><a href="#最老土的-Hash-算法" class="headerlink" title="最老土的 Hash 算法"></a>最老土的 Hash 算法</h3><p>设节点数为 N，写一个数据的时候，根据 hash(key) % N 计算出哈希值，用来决定数据映射到哪一个节点上。</p><p><img src="/2021/05/30/2021-05-30-chong-xue-redis/image10.png" alt></p><p>一旦某一个 master 节点宕机，所有请求过来，都会基于最新的剩余 master 节点数去取模，尝试去取数据。这会导致大部分的请求过来，全部无法拿到有效的缓存，导致大量的流量涌入数据库。</p><h3 id="一致性哈希分区"><a href="#一致性哈希分区" class="headerlink" title="一致性哈希分区"></a>一致性哈希分区</h3><p>一致性 hash 算法将整个 hash 值空间组织成一个虚拟的圆环，整个空间按顺时针方向组织，下一步将各个 master 节点（使用服务器的 ip 或主机名）进行 hash。这样就能确定每个节点在其哈希环上的位置。</p><p>来了一个 key，首先计算 hash 值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，遇到的第一个 master 节点就是 key 所在位置。</p><p>在一致性哈希算法中，如果一个节点挂了，受影响的数据仅仅是此节点到环空间前一个节点（沿着逆时针方向行走遇到的第一个节点）之间的数据，其它不受影响。增加一个节点也同理。</p><p>当一致性哈希算法在节点太少时，容易因为节点分布不均匀而造成缓存热点的问题。为了解决这种热点问题，一致性 hash 算法引入了虚拟节点机制，即对每一个节点计算多个 hash，每个计算结果位置都放置一个虚拟节点。这样就实现了数据的均匀分布，负载均衡。</p><p><img src="/2021/05/30/2021-05-30-chong-xue-redis/consistent-hashing-algorithm.png" alt></p><h3 id="Redis-cluster-的-hash-slot-算法"><a href="#Redis-cluster-的-hash-slot-算法" class="headerlink" title="Redis cluster 的 hash slot 算法"></a>Redis cluster 的 hash slot 算法</h3><p>Redis cluster 有固定的 16384 个 hash slot，对每个 key 计算 CRC16 值，然后对 16384 取模，可以获取 key 对应的 hash slot。</p><p>Redis cluster 中每个 master 都会持有部分 slot，比如有 3 个 master，那么可能每个 master 持有 5000 多个 hash slot。hash slot 让 node 的增加和移除很简单，增加一个 master，就将其他 master 的 hash slot 移动部分过去，减少一个 master，就将它的 hash slot 移动到其他 master 上去。</p><p>任何一台机器宕机，另外两个节点，不影响的。因为 key 找的是 hash slot，不是机器。</p><p><img src="/2021/05/30/2021-05-30-chong-xue-redis/hash-slot.png" alt></p><h2 id="缓存雪崩"><a href="#缓存雪崩" class="headerlink" title="缓存雪崩"></a>缓存雪崩</h2><p>缓存挂了，造成 DB 短时间承受大量请求而崩掉。</p><ul><li>事前：Redis 高可用，主从+哨兵，Redis cluster，避免全盘崩溃。</li><li>事中：本地 ehcache 缓存 + hystrix 限流&amp;降级，避免 MySQL 被打死。</li><li>事后：Redis 持久化，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。</li></ul><p><img src="/2021/05/30/2021-05-30-chong-xue-redis/redis-caching-avalanche-solution.png" alt></p><h2 id="缓存穿透"><a href="#缓存穿透" class="headerlink" title="缓存穿透"></a>缓存穿透</h2><p>恶意查询缓存和 DB 都没有的数据，造成数据库短时间内承受大量请求而崩掉。</p><p>布隆过滤器：布隆过滤器可以非常方便地判断一个给定数据是否存在于海量数据中。具体是这样做的：把所有可能存在的请求的值都存放在布隆过滤器中，当用户请求过来，我会先判断用户发来的请求的值是否存在于布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端。</p><p><img src="/2021/05/30/2021-05-30-chong-xue-redis/k0ml0thj.bmp" alt></p><h2 id="缓存击穿"><a href="#缓存击穿" class="headerlink" title="缓存击穿"></a>缓存击穿</h2><p>热点数据突然失效，造成 DB  短时间承受大量请求而崩掉。</p><ul><li>若缓存的数据是基本不会发生更新的，则可尝试将该热点数据设置为永不过期</li><li>加Redis、zookeeper 等分布式中间件的分布式互斥锁</li><li>最好在代码中也再上限流，防止DB挂了</li></ul><h2 id="缓存与数据库的双写一致性"><a href="#缓存与数据库的双写一致性" class="headerlink" title="缓存与数据库的双写一致性"></a>缓存与数据库的双写一致性</h2><h3 id="初级情况的解决方法"><a href="#初级情况的解决方法" class="headerlink" title="初级情况的解决方法"></a>初级情况的解决方法</h3><ul><li>读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。</li><li>更新的时候先删除缓存，再更新数据库</li></ul><h3 id="比较复杂的数据不一致问题"><a href="#比较复杂的数据不一致问题" class="headerlink" title="比较复杂的数据不一致问题"></a>比较复杂的数据不一致问题</h3><p>问题是说数据库修改还没提交，新的请求过来读缓存发现为空，于是查了旧的数据，又放回缓存里，随后数据库修改提交，造成缓存中数据不一致。</p><ul><li>同一个主键操作放到RocketMQ的同一队列中，一个队列对应一个消费者，可以保证顺序性</li></ul><h2 id="Redis分布式锁"><a href="#Redis分布式锁" class="headerlink" title="Redis分布式锁"></a>Redis分布式锁</h2><p>redis支持nx和ex操作是同一原子操作。</p><pre><code>set resourceName value ex 5 nx</code></pre><ul><li>NX：表示只有 key 不存在的时候才会设置成功，如果此时 redis 中存在这个 key，那么设置失败，返回 nil。</li><li>expire()，设置过期时间，防止死锁，假设，如果一个锁set后，一直不删掉，那这个锁相当于一直存在，产生死锁。</li><li>value要唯一，防止删了其他线程的锁</li></ul><p>删除锁的时候，先要判断是否是当前锁，再删除，这两个过程涉及原子性问题，所以也只能使用lua脚本</p><p>当机器A申请到一把锁之后，如果Redis主宕机了，这个时候从机并没有同步到这一把锁，那么机器B再次申请的时候就会再次申请到这把锁，为了解决这个问题Redis作者提出了RedLock红锁的算法,在Redission中也对RedLock进行了实现。</p><p>RedLock基本原理是利用多个Redis集群，用多数的集群加锁成功，减少Redis某个集群出故障，造成分布式锁出现问题的概率。</p><h2 id="Redis事务"><a href="#Redis事务" class="headerlink" title="Redis事务"></a>Redis事务</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>Redis 事务的本质是一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。</p><p>总结说：redis事务就是一次性、顺序性、排他性的执行一个队列中的一系列命令。</p><h3 id="Redis不保证原子性"><a href="#Redis不保证原子性" class="headerlink" title="Redis不保证原子性"></a>Redis不保证原子性</h3><p>Redis中，单条命令是原子性执行的，但事务不保证原子性，且没有回滚。事务中任意命令执行失败，其余的命令仍会被执行。</p><h3 id="Redis事务相关命令"><a href="#Redis事务相关命令" class="headerlink" title="Redis事务相关命令"></a>Redis事务相关命令</h3><pre><code>watch key1 key2 ... : 监视一或多个key,如果在事务执行之前，                被监视的key被其他命令改动，则事务被打断 （ 类似乐观锁 ）multi :     标记一个事务块的开始（ queued ）exec :         执行所有事务块的命令 （ 一旦执行exec后，之前加的监控锁都会被取消掉 ）　discard :     取消事务，放弃事务块中的所有命令unwatch :    取消watch对所有key的监控</code></pre><h3 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a>使用案例</h3><ul><li>正常执行</li></ul><p><img src="/2021/05/30/2021-05-30-chong-xue-redis/0ceb3495583e4da3a8a6bee897b92379.png" alt></p><ul><li>放弃事务</li></ul><p><img src="/2021/05/30/2021-05-30-chong-xue-redis/2bb0875a89984d0ab694011049492622.png" alt></p><ul><li>命令性错误</li></ul><p>若在事务队列中存在命令性错误（类似于java编译性错误），则执行EXEC命令时，所有命令都不会执行</p><p><img src="/2021/05/30/2021-05-30-chong-xue-redis/1bf77ab946f24444b2476765218a2de4.png" alt></p><ul><li>执行性错误</li></ul><p>若在事务队列中存在执行性错误（类似于java的1/0的运行时异常），则执行EXEC命令时，其他正确命令会被执行，错误命令抛出异常。</p><p><img src="/2021/05/30/2021-05-30-chong-xue-redis/1b9b58e4cc03418096553b487ec7eb7f.png" alt></p><ul><li>使用watch</li></ul><p>使用watch检测balance，事务期间balance数据未变动，事务执行成功</p><p><img src="/2021/05/30/2021-05-30-chong-xue-redis/d5246a2f02ca4ec38f9ad240c8c2e31e.png" alt></p><p>使用watch检测balance，在开启事务后（标注1处），在新窗口执行标注2中的操作，更改balance的值，模拟其他客户端在事务执行期间更改watch监控的数据，然后再执行标注1后命令，执行EXEC后，事务未成功执行。</p><p><img src="/2021/05/30/2021-05-30-chong-xue-redis/e8369f65d4e6439e8f9ef4abb3a5e976.png" alt></p><p><img src="/2021/05/30/2021-05-30-chong-xue-redis/b3a3dcb25a484c60938a896889d60ea2.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> review </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dubbo</title>
      <link href="/2021/05/27/2021-05-27-chong-xue-dubbo/"/>
      <url>/2021/05/27/2021-05-27-chong-xue-dubbo/</url>
      
        <content type="html"><![CDATA[<h2 id="架构与工作原理"><a href="#架构与工作原理" class="headerlink" title="架构与工作原理"></a>架构与工作原理</h2><ul><li>第一层：service 层，接口层，给服务提供者和消费者来实现的</li><li>第二层：config 层，配置层，主要是对 dubbo 进行各种配置的</li><li>第三层：proxy 层，服务代理层，无论是 consumer 还是 provider，dubbo 都会给你生成代理，代理之间进行网络通信</li><li>第四层：registry 层，服务注册层，负责服务的注册与发现</li><li>第五层：cluster 层，集群层，封装多个服务提供者的路由以及负载均衡，将多个实例组合成一个服务</li><li>第六层：monitor 层，监控层，对 rpc 接口的调用次数和调用时间进行监控</li><li>第七层：protocal 层，远程调用层，封装 rpc 调用</li><li>第八层：exchange 层，信息交换层，封装请求响应模式，同步转异步</li><li>第九层：transport 层，网络传输层，抽象 mina 和 netty 为统一接口</li><li>第十层：serialize 层，数据序列化层</li></ul><ol><li>第一步：provider 向注册中心去注册</li><li>第二步：consumer 从注册中心订阅服务，注册中心会通知 consumer 注册好的服务</li><li>第三步：consumer 调用 provider</li><li>第四步：consumer 和 provider 都异步通知监控中心</li></ol><p><img src="/2021/05/27/2021-05-27-chong-xue-dubbo/dubbo-operating-principle.png" alt></p><h2 id="dubbo和springcloud的区别"><a href="#dubbo和springcloud的区别" class="headerlink" title="dubbo和springcloud的区别"></a>dubbo和springcloud的区别</h2><p>Springboot可以离开SpringCloud独立使用开发项目,但是SpringCloud离不开Springboot,属于依赖的关系.</p><p>Springboot专注于快速、方便的开发单个微服务个体,SpringCloud关注全局的服务治理框架.</p><p>最大区别:的是RPC通信,SpringCloudDubbo采用的是基于HTTP的REST方式.</p><h2 id="注册中心挂了可以继续通信吗"><a href="#注册中心挂了可以继续通信吗" class="headerlink" title="注册中心挂了可以继续通信吗"></a>注册中心挂了可以继续通信吗</h2><p>可以的，消费者本地会缓存从 Zookeeper 拉取的生产者的列表信息，他会按照列表继续工作，但是无法从注册中心去同步最新的服务列表，短期的注册中心挂掉是不要紧的，但一定要尽快修复。</p><h2 id="支持的通信协议"><a href="#支持的通信协议" class="headerlink" title="支持的通信协议"></a>支持的通信协议</h2><p>支持的协议有：Dubbo，RMI，Hessian，Http，WebService，Redis，Rest等。</p><p>默认就是走 dubbo 协议，单一长连接，进行的是 NIO 异步通信，基于 hessian 作为序列化协议。使用的场景是：传输数据量小（每次请求在 100kb 以内），但是并发量很高。以及消费者数量远大于提供者数量的情况。不适合传送大数据量的服务，比如传文件，传视频等，除非请求量很低。</p><h3 id="为什么-Dubbo-协议采用异步单一长链接"><a href="#为什么-Dubbo-协议采用异步单一长链接" class="headerlink" title="为什么 Dubbo 协议采用异步单一长链接"></a>为什么 Dubbo 协议采用异步单一长链接</h3><p>是为了支持小数据量高并发场景，一般业务场景是服务提供者就几台机器，但是服务消费者有上百台，可能每天调用量达到上亿次。如果采用短连接，服务提供者很容易就被压跨，通过单一连接，保证单一消费者不会压死提供者，长连接可以减少连接握手验证，并使用异步 IO，复用线程池，防止 C10K 问题。</p><h2 id="支持的序列化协议"><a href="#支持的序列化协议" class="headerlink" title="支持的序列化协议"></a>支持的序列化协议</h2><p>dubbo 支持 hession、Java 二进制序列化、json、SOAP 文本序列化多种序列化协议。但是 <strong>hessian</strong> 是其默认的序列化协议。</p><h2 id="负载均衡策略"><a href="#负载均衡策略" class="headerlink" title="负载均衡策略"></a>负载均衡策略</h2><p>默认情况下，dubbo 是 RandomLoadBalance ，即随机调用实现负载均衡。</p><h3 id="Random（随机负载均衡）"><a href="#Random（随机负载均衡）" class="headerlink" title="Random（随机负载均衡）"></a>Random（随机负载均衡）</h3><p>就是从多个 Provider 中随机选择一个。但有一个权重的概念，即按照权重设置随机概率。比如说，有10个 Provider，并不是说，每个 Provider 的概率都是一样的，而是要结合这10个 Provider 的权重来分配概率。</p><p>算法：例如有 4 台机器，权重分别是 ABCD。通过 Random.nextInt(A+B+C+D)，从中随机选择一个数。然后再判断该数分布在哪个区域。</p><h3 id="RoundRobin（轮询负载均衡）"><a href="#RoundRobin（轮询负载均衡）" class="headerlink" title="RoundRobin（轮询负载均衡）"></a>RoundRobin（轮询负载均衡）</h3><p>就是依次的调用所有的 Provider。也有权重的概念。</p><p>这个策略可以让 RPC 调用严格按照我们设置的比例来分配。不管是少量的调用还是大量的调用。但是存在慢的 Provider 累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上，导致整个系统变慢。</p><h3 id="LeastActive（最少活跃调用数负载均衡）"><a href="#LeastActive（最少活跃调用数负载均衡）" class="headerlink" title="LeastActive（最少活跃调用数负载均衡）"></a>LeastActive（最少活跃调用数负载均衡）</h3><p>最少活跃调用数，相同活跃数的随机。目的是让更慢的机器收到更少的请求。</p><p>举个例子：每个服务维护一个活跃数计数器。当 A 机器开始处理请求，该计数器加 1，此时 A 还未处理完成。若处理完毕则计数器减 1。而 B 机器接受到请求后很快处理完毕。那么 A，B 的活跃数分别是 1，0。当又产生了一个新的请求，则选择 B 机器去执行（B 活跃数最小），这样使慢的机器 A 收到少的请求。</p><p>算法：处理一个新的请求时，Consumer 会检查所有 Provider 的活跃数，如果具有最小活跃数的 Invoker 只有一个，直接返回该 Invoker。如果最小活跃数的 Invoker 有多个，且权重不相等同时总权重大于 0，这时随机生成一个权重，范围在 (0，totalWeight) 间内。最后根据随机生成的权重，来选择 Invoker。</p><h3 id="ConsistentHash（一致性哈希负载均衡）"><a href="#ConsistentHash（一致性哈希负载均衡）" class="headerlink" title="ConsistentHash（一致性哈希负载均衡）"></a>ConsistentHash（一致性哈希负载均衡）</h3><p>相同参数的请求总是落在同一台机器上。</p><h2 id="集群容错策略"><a href="#集群容错策略" class="headerlink" title="集群容错策略"></a>集群容错策略</h2><p>默认的容错方案是配置：Failover Cluster，优先使用消费端配置。</p><h3 id="Failover（失败自动切换）"><a href="#Failover（失败自动切换）" class="headerlink" title="Failover（失败自动切换）"></a>Failover（失败自动切换）</h3><p>调用失败时，根据配置的重试次数，自动重试其他机器，默认是重试 2 次。</p><h3 id="Failfast（快速失败）"><a href="#Failfast（快速失败）" class="headerlink" title="Failfast（快速失败）"></a>Failfast（快速失败）</h3><p>只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。</p><h3 id="Failsafe（失败安全）"><a href="#Failsafe（失败安全）" class="headerlink" title="Failsafe（失败安全）"></a>Failsafe（失败安全）</h3><p>出现异常时，直接忽略，并记录一条日志，同时返回一个空结果，在上游看来调用是成功的。</p><h3 id="Failback（失败自动恢复）"><a href="#Failback（失败自动恢复）" class="headerlink" title="Failback（失败自动恢复）"></a>Failback（失败自动恢复）</h3><p>如果调用失败，则此次失败相当于 Failsafe，将返回一个空结果。同时会将这次调用加入内存中的失败列表中，对于这个列表中的失败调用，会有另外的线程进行异步重试。重试如果再发生失败，则会忽略，但即使重试调用成功，原来的调用方也感知不到了。</p><h3 id="Forking（并行调用）"><a href="#Forking（并行调用）" class="headerlink" title="Forking（并行调用）"></a>Forking（并行调用）</h3><p>第一次调用的时候就同时发起多个调用，只要其中一个调用成功，就认为成功。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks=”2” 来设置最大并行数。</p><h3 id="Broadcast（广播调用）"><a href="#Broadcast（广播调用）" class="headerlink" title="Broadcast（广播调用）"></a>Broadcast（广播调用）</h3><p>广播调用所有提供者，逐个调用，任意一台报错则报错 。通常用于通知所有提供者更新缓存或日志等本地资源信息。</p><h2 id="配置的优先级"><a href="#配置的优先级" class="headerlink" title="配置的优先级"></a>配置的优先级</h2><ul><li><p>方法级别优于接口级别，接口级别优于全局配置，即小 Scope 优先。</p></li><li><p>Consumer 配置优于 Provider 配置。</p><p>  如果 Consumer 超时，而 Provider 未超时，则 Provider 依旧会继续执行。</p><p>  建议在 Provider 设置超时时间，因为一个方法需要执行多长时间，Provider 更清楚，如果一个 Consumer 同时引用多个服务，就不需要关心每个服务的超时设置。</p></li></ul><p><img src="/2021/05/27/2021-05-27-chong-xue-dubbo/dubbo-config-override.jpg" alt></p><h2 id="SPI"><a href="#SPI" class="headerlink" title="SPI"></a>SPI</h2><p>SPI 就是通过动态加载机制实现面向接口编程，提高了框架和底层实现的分离。</p><p>例如你有一个接口 A，A1/A2/A3 分别是接口 A 的不同实现，你通过配置指定接口 A 的实现是 A2，那么在系统实际运行时，就会加载你的配置，实例化 A2 来提供服务。</p><p>SPI 机制通常用在插件拓展的场景，比如说你开发了一个给别人使用的开源框架，如果你想让别人自己写个插件，插到你的开源框架里面，从而扩展某个功能，这个时候 SPI 就能用上了。</p><blockquote><p>可以描述自己的日志输出和语言过滤器</p></blockquote><p>语言过滤器：</p><p>src/main/resources/META-INF/dubbo/com.alibaba.dubbo.rpc.Filter</p><pre><code>DubboContextLanguageFilter=com.xxx.infrastructure.filter.DubboContextLanguageFilter</code></pre><pre><code>@Activate(group = Constants.PROVIDER)public class DubboContextLanguageFilter implements Filter {    private final Logger log = LoggerFactory.getLogger(this.getClass());    @Override    public Result invoke(Invoker&lt;?&gt; invoker, Invocation invocation) throws RpcException {        try {            String language = RpcContext.getContext().getAttachment(&quot;session:org.springframework.web.servlet.i18n.SessionLocaleResolver.LOCALE&quot;);            if (&quot;en&quot;.equals(language)) {                LocaleContextHolder.setDefaultLocale(new Locale(&quot;en&quot;, &quot;US&quot;));            } else {                LocaleContextHolder.setDefaultLocale(new Locale(&quot;zh&quot;, &quot;CN&quot;));            }        } catch (Throwable t) {            log.error(&quot;[DubboContextEnterFilter - invoke - language fail] 获取并设置language失败！&quot;, t);        }        return invoker.invoke(invocation);    }}</code></pre><h2 id="RPC"><a href="#RPC" class="headerlink" title="RPC"></a>RPC</h2><p>RPC 使得程序能够像访问本地系统资源一样，去访问远端系统资源。</p><h3 id="RPC-的调用过程"><a href="#RPC-的调用过程" class="headerlink" title="RPC 的调用过程"></a>RPC 的调用过程</h3><ul><li>Client（客户端）：服务调用方。</li><li>Client Stub（客户端存根）：存放服务端地址信息，将客户端的请求信息打包成网络信息，再通过网络传输给发送端。</li><li>Server Stub（服务端存根）：接收客户端发送的请求并解包，然后调用本地服务进行处理。</li><li>Server（服务端）：服务提供方。</li></ul><p><img src="/2021/05/27/2021-05-27-chong-xue-dubbo/05uOa8vFDQou0jyDmnkecA.png" alt></p><h3 id="RPC-和-HTTP-比有啥优势"><a href="#RPC-和-HTTP-比有啥优势" class="headerlink" title="RPC 和 HTTP 比有啥优势"></a>RPC 和 HTTP 比有啥优势</h3><ul><li><p>长链接</p><p>  不必每次通信都要像 HTTP 一样去 3 次握手什么的，减少了网络开销。</p></li><li><p>屏蔽了底层细节</p><p>  开发人员不需要关心底层细节，比如序列化和反序列化，网络传输协议等细节，让远程调用像调用本地方法一样.</p></li></ul><h3 id="设计一个-RPC-框架需要解决的问题"><a href="#设计一个-RPC-框架需要解决的问题" class="headerlink" title="设计一个 RPC 框架需要解决的问题"></a>设计一个 RPC 框架需要解决的问题</h3><ul><li>应用都基于微服务化，实现资源调用离不开远程调用。</li><li>一个服务有多个实例，在调用时如何获取这些实例地址？这就需要注册中心，从注册中心获取服务的实例列表，再从中选择一个进行调用。</li><li>选哪个调用好呢？这时候就需要负载均衡了，于是又得考虑如何实现复杂均衡。</li><li>总不能每次调用时都去注册中心查询实例列表吧，于是有有了缓存，有了缓存就得考虑缓存更新问题。</li><li>客户端总不能每次调用完都干等着服务端返回数据吧，于是就要支持异步调用。</li><li>服务端的接口修改了，老的接口还有人在用怎么办？这就需要版本控制了。</li><li>服务端总不能每次接到请求都马上启动一个线程去处理吧？于是就需要线程池。</li></ul><h2 id="服务降级"><a href="#服务降级" class="headerlink" title="服务降级"></a>服务降级</h2><p>比如说服务 A 调用服务 B，结果服务 B 挂掉了，服务 A 重试几次调用服务 B，还是不行，那么直接降级，走一个备用的逻辑，给用户返回响应。</p><p>服务降级：可以通过 <a href="dubbo:reference" target="_blank" rel="noopener">dubbo:reference</a> 中设置 mock 值。</p><p>可以通过 mock 统一返回 null:</p><pre><code>&lt;dubbo:reference id=&quot;fooService&quot; interface=&quot;com.test.service.FooService&quot;  timeout=&quot;10000&quot; check=&quot;false&quot; mock=&quot;return null&quot;&gt;&lt;/dubbo:reference&gt;</code></pre><p>mock 的值也可以修改为 true，然后再跟接口同一个路径下实现一个 Mock 类，命名规则是 “接口名称+ Mock ” 后缀。然后在 Mock 类里实现自己的降级逻辑。</p><pre><code>public class HelloServiceMock implements HelloService {    public void sayHello() {        // 降级逻辑    }}</code></pre><h2 id="失败重试和超时重试"><a href="#失败重试和超时重试" class="headerlink" title="失败重试和超时重试"></a>失败重试和超时重试</h2><p>所谓失败重试，就是 consumer 调用 provider 要是失败了，比如抛异常了，此时应该是可以重试的，或者调用超时了也可以重试。配置如下：</p><pre><code>&lt;dubbo:reference id=&quot;xxxx&quot; interface=&quot;xx&quot; check=&quot;true&quot; async=&quot;false&quot; retries=&quot;3&quot; timeout=&quot;2000&quot;/&gt;</code></pre><ul><li>timeout ：一般设置为 200ms ，我们认为不能超过 200ms 还没返回。</li><li>retries ：设置 retries，一般是在读请求的时候，比如你要查询个数据，你可以设置个 retries，如果第一次没读到，报错，重试指定的次数，尝试再次读取。</li></ul><h2 id="分布式服务接口请求的顺序性如何保证"><a href="#分布式服务接口请求的顺序性如何保证" class="headerlink" title="分布式服务接口请求的顺序性如何保证"></a>分布式服务接口请求的顺序性如何保证</h2><p>设置Dubbo 的一致性 hash 负载均衡策略，将同一唯一性的操作扔一个内存队列里去（如RocketMQ等），强制排队，这样来确保他们的顺序性。</p><h2 id="Dubbo-支持分布式事务"><a href="#Dubbo-支持分布式事务" class="headerlink" title="Dubbo 支持分布式事务"></a>Dubbo 支持分布式事务</h2><p>目前暂时不支持，可以自己整合分布式事务框架，例如用 Seata。</p><h2 id="Dubbo-服务调用是阻塞的吗"><a href="#Dubbo-服务调用是阻塞的吗" class="headerlink" title="Dubbo 服务调用是阻塞的吗"></a>Dubbo 服务调用是阻塞的吗</h2><p>默认是阻塞的，可以异步调用，没有返回值的可以这么做。</p><p>Dubbo 是基于 NIO 的非阻塞实现并行调用，客户端不需要启动多线程即可完成并行调用多个远程服务，相对多线程开销较小，异步调用会返回一个 Future 对象。</p><h2 id="同一个服务多个注册的情况下可以直连某一个服务吗"><a href="#同一个服务多个注册的情况下可以直连某一个服务吗" class="headerlink" title="同一个服务多个注册的情况下可以直连某一个服务吗"></a>同一个服务多个注册的情况下可以直连某一个服务吗</h2><p>可以点对点直连，修改 url 参数配置即可。</p>]]></content>
      
      
      <categories>
          
          <category> review </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Dubbo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>消息队列</title>
      <link href="/2021/05/25/2021-05-25-chong-xue-mq/"/>
      <url>/2021/05/25/2021-05-25-chong-xue-mq/</url>
      
        <content type="html"><![CDATA[<h2 id="消息队列的用处"><a href="#消息队列的用处" class="headerlink" title="* 消息队列的用处"></a>* 消息队列的用处</h2><h3 id="异步"><a href="#异步" class="headerlink" title="异步"></a>异步</h3><p>例如发短信或邮件，直接发个短信或邮件mq，让消费者统一去消费发短信或邮件。异步发送消息，这样可以减少主流程的耗时，让用户更快的接收到响应。</p><h3 id="解耦"><a href="#解耦" class="headerlink" title="解耦"></a>解耦</h3><p>例如用户修改了信息，需要上报到各出款渠道，调用各渠道的修改信息接口。这个时候就可以发个修改用户信息的mq，让各渠道自己去消费调用修改信息的接口。主流程中就不仅不用写各种渠道冗余的RPC调用接口代码，而且也不用处理各渠道调用接口的异常，一切让各渠道自己去处理。</p><h3 id="削峰"><a href="#削峰" class="headerlink" title="削峰"></a>削峰</h3><p>例如入账的时候，在某个时间段拉取到很多入账信息，数据量众多数据库可能处理不过来，甚至可能导致数据库崩。可以在每拉取到一笔入账信息时，就发个入账的mq，然后消费者可以慢慢的从mq中消费入账信息，可能会导致短时间内的mq堆积，但当其他空闲的时间段时候，没有那么多入账信息拉取，就会逐步消费完这些mq，逐渐恢复正常。</p><h2 id="Kafka、ActiveMQ、RabbitMQ、RocketMQ-对比"><a href="#Kafka、ActiveMQ、RabbitMQ、RocketMQ-对比" class="headerlink" title="* Kafka、ActiveMQ、RabbitMQ、RocketMQ 对比"></a>* Kafka、ActiveMQ、RabbitMQ、RocketMQ 对比</h2><p><img src="/2021/05/25/2021-05-25-chong-xue-mq/1595136879.jpg" alt></p><p>反正我现在一直用的是RocketMQ，吞吐量10万级别，消息基本0丢失，时效性ms级别。</p><h2 id="如何保证消息队列的高可用？"><a href="#如何保证消息队列的高可用？" class="headerlink" title="* 如何保证消息队列的高可用？"></a>* 如何保证消息队列的高可用？</h2><h3 id="RabbitMQ"><a href="#RabbitMQ" class="headerlink" title="RabbitMQ"></a>RabbitMQ</h3><p>镜像集群模式（高可用性）</p><p><img src="/2021/05/25/2021-05-25-chong-xue-mq/mq-8.png" alt></p><p>在镜像集群模式下，你创建的 queue，无论元数据还是 queue 里的消息都会存在于多个实例上，就是说，每个 RabbitMQ 节点都有这个 queue 的一个完整镜像，包含 queue 的全部数据的意思。然后每次你写消息到 queue 的时候，都会自动把消息同步到多个实例的 queue 上。</p><ul><li>好处：任何一个机器宕机了，没事，其它机器（节点）还包含了这个 queue 的完整数据，别的 consumer 都可以到其它节点上去消费数据。</li><li>坏处：第一，性能开销也太大了吧，消息需要同步到所有机器上，导致网络带宽压力和消耗很重！第二，不是分布式的，就没有扩展性可言了，如果某个 queue 负载很重，你加机器，新增的机器也包含了这个 queue 的所有数据，并没有办法线性扩展你的 queue。</li></ul><h3 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h3><p>Kafka 一个最基本的架构认识：由多个 broker 组成，每个 broker 是一个节点；你创建一个 topic，这个 topic 可以划分为多个 partition，每个 partition 可以存在于不同的 broker 上，每个 partition 就放一部分数据。</p><p><strong>这就是天然的分布式消息队列，就是说一个 topic 的数据，是分散放在多个机器上的，每个机器就放一部分数据。</strong></p><p>Kafka 0.8 以后，提供了 HA 机制，就是 replica（复制品） 副本机制。每个 partition 的数据都会同步到其它机器上，形成自己的多个 replica 副本。所有 replica 会选举一个 leader 出来，那么生产和消费都跟这个 leader 打交道，然后其他 replica 就是 follower。写的时候，leader 会负责把数据同步到所有 follower 上去，读的时候就直接读 leader 上的数据即可。只能读写 leader？很简单，要是你可以随意读写每个 follower，那么就要 care 数据一致性的问题，系统复杂度太高，很容易出问题。Kafka 会均匀地将一个 partition 的所有 replica 分布在不同的机器上，这样才可以提高容错性。</p><p><img src="/2021/05/25/2021-05-25-chong-xue-mq/kafka-after.png" alt></p><p>写数据的时候，生产者就写 leader，然后 leader 将数据落地写本地磁盘，接着其他 follower 自己主动从 leader 来 pull 数据。一旦所有 follower 同步好数据了，就会发送 ack 给 leader，leader 收到所有 follower 的 ack 之后，就会返回写成功的消息给生产者。</p><p>消费的时候，只会从 leader 去读，但是只有当一个消息已经被所有 follower 都同步成功返回 ack 的时候，这个消息才会被消费者读到。</p><p>这么搞，就有所谓的高可用性了，因为如果某个 broker 宕机了，没事儿，那个 broker上面的 partition 在其他机器上都有副本的。如果这个宕机的 broker 上面有某个 partition 的 leader，那么此时会从 follower 中重新选举一个新的 leader 出来，大家继续读写那个新的 leader 即可。这就有所谓的高可用性了。</p><h2 id="RocketMQ"><a href="#RocketMQ" class="headerlink" title="* RocketMQ"></a>* RocketMQ</h2><h3 id="RocketMQ的消息模型"><a href="#RocketMQ的消息模型" class="headerlink" title="RocketMQ的消息模型"></a>RocketMQ的消息模型</h3><p><img src="/2021/05/25/2021-05-25-chong-xue-mq/16ef383d3e8c9788.jpg" alt></p><ul><li>Producer Group 生产者组： 代表某一类的生产者，比如我们有多个秒杀系统作为生产者，这多个合在一起就是一个 Producer Group 生产者组，它们一般生产相同的消息。</li><li>Consumer Group 消费者组： 代表某一类的消费者，比如我们有多个短信系统作为消费者，这多个合在一起就是一个 Consumer Group 消费者组，它们一般消费相同的消息。</li><li>Topic 主题： 代表一类消息，比如订单消息，物流消息等等。</li></ul><p>主题中存在多个队列，生产者每次生产消息之后是指定主题中的某个队列发送消息的。</p><p>每个主题中都有多个队列(这里还不涉及到 Broker)，集群消费模式下，一个消费者集群多台机器共同消费一个 topic 的多个队列，一个队列只会被一个消费者消费。如果某个消费者挂掉，分组内其它消费者会接替挂掉的消费者继续消费。就像上图中 Consumer1 和 Consumer2 分别对应着两个队列，而 Consuer3 是没有队列对应的，所以一般来讲要控制 消费者组中的消费者个数和主题中队列个数相同 。</p><h3 id="RocketMQ的架构图"><a href="#RocketMQ的架构图" class="headerlink" title="RocketMQ的架构图"></a>RocketMQ的架构图</h3><p>RocketMQ 技术架构中有四大角色 NameServer 、Broker 、Producer 、Consumer 。</p><h4 id="Broker"><a href="#Broker" class="headerlink" title="Broker"></a>Broker</h4><p>Broker： 主要负责消息的存储、投递和查询以及服务高可用保证。说白了就是消息队列服务器嘛，生产者生产消息到 Broker ，消费者从 Broker 拉取消息并消费。</p><p>一个 Topic 分布在多个 Broker上，一个 Broker 可以配置多个 Topic ，它们是多对多的关系。 </p><p>如果某个 Topic 消息量很大，应该给它多配置几个队列(提高并发能力)，并且 尽量多分布在不同 Broker 上，以减轻某个 Broker 的压力 。</p><p>Topic 消息量都比较均匀的情况下，如果某个 broker 上的队列越多，则该 broker 压力越大。</p><p><img src="/2021/05/25/2021-05-25-chong-xue-mq/16ef38687488a5a4.jpg" alt></p><blockquote><p>所以说我们需要配置多个Broker。</p></blockquote><h4 id="NameServer"><a href="#NameServer" class="headerlink" title="NameServer"></a>NameServer</h4><p>NameServer： 就像 ZooKeeper 和 Spring Cloud 中的 Eureka ，它其实也是一个 注册中心 ，主要提供两个功能：Broker管理 和 路由信息管理 。说白了就是 Broker 会将自己的信息注册到 NameServer 中，此时 NameServer 就存放了很多 Broker 的信息(Broker的路由表)，消费者和生产者就从 NameServer 中获取路由表然后照着路由表的信息和对应的 Broker 进行通信(生产者和消费者定期会向 NameServer 去查询相关的 Broker 的信息)。</p><h4 id="Producer"><a href="#Producer" class="headerlink" title="Producer"></a>Producer</h4><p>Producer： 消息发布的角色，支持分布式集群方式部署。说白了就是生产者。</p><h4 id="Consumer"><a href="#Consumer" class="headerlink" title="Consumer"></a>Consumer</h4><p>Consumer： 消息消费的角色，支持分布式集群方式部署。支持以push推，pull拉两种模式对消息进行消费。同时也支持集群方式和广播方式的消费，它提供实时消息订阅机制。说白了就是消费者。</p><p><img src="/2021/05/25/2021-05-25-chong-xue-mq/16ef386fa3be1e53.jpg" alt></p><h2 id="保证消息消费的幂等性"><a href="#保证消息消费的幂等性" class="headerlink" title="* 保证消息消费的幂等性"></a>* 保证消息消费的幂等性</h2><ul><li>例如创建订单的时候，根据订单的唯一性，先去SQL查询一下是否存在，如果存在，直接return，或者唯一键约束；</li><li>比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性；</li><li>如果你的场景还复杂一点，那就需要在Redis中判断唯一字段是否消费过。</li></ul><p>反正这个没有绝对的做法，主要看你的场景吧~</p><h2 id="保证消息不丢失"><a href="#保证消息不丢失" class="headerlink" title="* 保证消息不丢失"></a>* 保证消息不丢失</h2><p>这个主要从三个方面分别保证：生产者、MQ自身、消费者</p><h3 id="RabbitMQ-1"><a href="#RabbitMQ-1" class="headerlink" title="RabbitMQ"></a>RabbitMQ</h3><h4 id="生产者方面"><a href="#生产者方面" class="headerlink" title="生产者方面"></a>生产者方面</h4><p>可以开启 confirm 模式，在生产者那里设置开启 confirm 模式之后，你每次写的消息都会分配一个唯一的 id，然后如果写入了 RabbitMQ 中，RabbitMQ 会给你回传一个 ack 消息，告诉你说这个消息 ok 了。如果 RabbitMQ 没能处理这个消息，会回调你的一个 nack 接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息 id 的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。</p><h4 id="RabbitMQ自身"><a href="#RabbitMQ自身" class="headerlink" title="RabbitMQ自身"></a>RabbitMQ自身</h4><p>就是 RabbitMQ 自己弄丢了数据，这个你必须开启 RabbitMQ 的持久化，就是消息写入之后会持久化到磁盘，哪怕是 RabbitMQ 自己挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。</p><ul><li><p>创建 queue 的时候将其设置为持久化</p><p>  这样就可以保证 RabbitMQ 持久化 queue 的元数据，但是它是不会持久化 queue 里的数据的。</p></li><li><p>第二个是发送消息的时候将消息的 deliveryMode 设置为 2</p><p>  就是将消息设置为持久化的，此时 RabbitMQ 就会将消息持久化到磁盘上去。</p></li></ul><p>必须要同时设置这两个持久化才行，RabbitMQ 哪怕是挂了，再次重启，也会从磁盘上重启恢复 queue，恢复这个 queue 里的数据。</p><h4 id="消费者方面"><a href="#消费者方面" class="headerlink" title="消费者方面"></a>消费者方面</h4><p>你消费的时候，刚消费到，还没处理，结果进程挂了，比如重启了，那么就尴尬了，RabbitMQ 认为你都消费了，这数据就丢了。</p><p>这个时候得用 RabbitMQ 提供的 ack 机制，简单来说，就是你必须关闭 RabbitMQ 的自动 ack ，然后每次你自己代码里确保处理完的时候，再在程序里 ack 一把。这样的话，如果你还没处理完，不就没有 ack 了？那 RabbitMQ 就认为你还没处理完，这个时候 RabbitMQ 会把这个消费分配给别的 consumer 去处理，消息是不会丢的。</p><h3 id="Kafka-1"><a href="#Kafka-1" class="headerlink" title="Kafka"></a>Kafka</h3><h4 id="消费者方面-1"><a href="#消费者方面-1" class="headerlink" title="消费者方面"></a>消费者方面</h4><p>你消费到了这个消息，然后消费者那边自动提交了 offset，让 Kafka 以为你已经消费好了这个消息，但其实你才刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。</p><p>只要关闭自动提交 offset，在处理完之后自己手动提交 offset，就可以保证数据不会丢。但是此时确实还是可能会有重复消费，比如你刚处理完，还没提交 offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。</p><h4 id="Kafka自身"><a href="#Kafka自身" class="headerlink" title="Kafka自身"></a>Kafka自身</h4><p>这块比较常见的一个场景，就是 Kafka 某个 broker 宕机，然后重新选举 partition 的 leader。大家想想，要是此时其他的 follower 刚好还有些数据没有同步，结果此时 leader 挂了，然后选举某个 follower 成 leader 之后，不就少了一些数据？这就丢了一些数据啊。</p><ul><li>给 topic 设置 replication.factor 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。</li><li>在 Kafka 服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。</li><li>在 producer 端设置 acks=all ：这个是要求每条数据，必须是写入所有 replica 之后，才能认为是写成功了。</li><li>在 producer 端设置 retries=MAX （很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了。</li></ul><h4 id="生产者方面-1"><a href="#生产者方面-1" class="headerlink" title="生产者方面"></a>生产者方面</h4><p>设置了 acks=all ，一定不会丢，要求是，你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。</p><h2 id="保证消息的顺序性"><a href="#保证消息的顺序性" class="headerlink" title="* 保证消息的顺序性"></a>* 保证消息的顺序性</h2><h3 id="RocketMQ有序性"><a href="#RocketMQ有序性" class="headerlink" title="RocketMQ有序性"></a>RocketMQ有序性</h3><p>RocketMQ 在主题上是无序的(多个队列)、它只有在队列层面才是保证有序 的。</p><p>解决其实很简单，我们需要处理的仅仅是将同一语义下的消息放入同一个队列(比如这里是同一个订单)，那我们就可以使用 Hash取模法 来保证同一个订单在同一个队列中就行了。</p><h2 id="消息堆积"><a href="#消息堆积" class="headerlink" title="* 消息堆积"></a>* 消息堆积</h2><ul><li>先修复 consumer 的问题，确保其恢复消费速度，然后将现有 consumer 都停掉。</li><li>新建一个 topic，临时建立好原先 10 倍的 queue 数量。</li><li>然后写一个临时的分发数据的 consumer 程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的 10 倍数量的 queue。</li><li>接着临时征用 10 倍的机器来部署 consumer，每一批 consumer 消费一个临时 queue 的数据。这种做法相当于是临时将 queue 资源和 consumer 资源扩大 10 倍，以正常的 10 倍速度来消费数据。</li><li>等快速消费完积压数据之后，得恢复原先部署的架构，重新用原先的 consumer 机器来消费消息。</li></ul><h2 id="重试队列与死信队列"><a href="#重试队列与死信队列" class="headerlink" title="重试队列与死信队列"></a>重试队列与死信队列</h2><h3 id="重试队列"><a href="#重试队列" class="headerlink" title="重试队列"></a>重试队列</h3><p>消息消费失败了，总不能一直卡着后面的消息也等着吧，那么消费失败的消息肯定需要放到另一个Topic中，让它一个人等着被再次消费。</p><p>所以这时会有一个重试队列，用于暂时保存因为各种异常而导致Consumer端无法消费的消息，重试队列的名称是在原队列的名称前加上%RETRY%（这个Topic的重试队列是针对消费组，而不是针对每个Topic设置的）。</p><p>RocketMQ对于重试消息的处理是先保存至Topic名称为“SCHEDULE_TOPIC_XXXX”的延迟队列中，后台定时任务按照对应的时间进行Delay后重新保存至“%RETRY%+consumerGroup”的重试队列中。</p><p>在RocketMQ的console控制台上可以看到重试队列的信息</p><p><img src="/2021/05/25/2021-05-25-chong-xue-mq/20200213202101318.png" alt></p><p>现在我们已经知道消费失败的消息会进入重试队列，那么多久重试一次呢？能进行多少次的重试呢？</p><p>考虑到异常恢复起来需要一些时间，会为重试队列设置多个重试级别，每个重试级别都有与之对应的重新投递延时间，重试次数越多投递延时就越大。有一个参数messageDelayLevel，这个参数是在服务器端的Broker上配置的，默认是</p><pre><code>messageDelayLevel=1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h</code></pre><p>默认是最多可以重试16次</p><h3 id="死信队列"><a href="#死信队列" class="headerlink" title="死信队列"></a>死信队列</h3><p>如果重试了16次之后，这条消息还是没有被成功消费，那么就认为这条消息是抢救不过来了，此时，消息队列不会立刻将消息丢弃，于是它被放入了死信队列中，上面重试队列的图中你也可以看到死信队列，死信队列的名称是在原队列名称前加%DLQ%。如果你还是不死心的话，觉得这条消息还能抢救一下，可以开启一个后台线程不断扫描死信队列然后继续重试，也可以通过使用console控制台对死信队列中的消息进行重发来使得消费者实例再次进行消费</p><p><img src="/2021/05/25/2021-05-25-chong-xue-mq/20200213210003126.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> review </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 消息队列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM</title>
      <link href="/2021/05/20/2021-05-20-chong-xue-jvm/"/>
      <url>/2021/05/20/2021-05-20-chong-xue-jvm/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Java-内存区域-运行时数据区"><a href="#1-Java-内存区域-运行时数据区" class="headerlink" title="1 Java 内存区域(运行时数据区)"></a>1 Java 内存区域(运行时数据区)</h2><h3 id="JDK-1-8之前："><a href="#JDK-1-8之前：" class="headerlink" title="JDK 1.8之前："></a>JDK 1.8之前：</h3><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/hlwi08xk.bmp" alt></p><h3 id="JDK-1-8-："><a href="#JDK-1-8-：" class="headerlink" title="JDK 1.8 ："></a>JDK 1.8 ：</h3><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/j141wm5o.bmp" alt></p><h3 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h3><ul><li>程序计数器是唯一一个不会出现 OutOfMemoryError 的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。</li><li>程序计数器可以实现代码的流程控制，如：顺序执行、选择、循环、异常处理。</li><li>在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。</li></ul><h3 id="Java-虚拟机栈"><a href="#Java-虚拟机栈" class="headerlink" title="Java 虚拟机栈"></a>Java 虚拟机栈</h3><p>Java 内存可以粗糙的区分为堆内存（Heap）和栈内存(Stack),其中栈就是现在说的虚拟机栈，或者说是虚拟机栈中局部变量表部分。 </p><p>局部变量表主要存放了编译器可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。</p><p>Java 虚拟机栈会出现两种异常：StackOverFlowError 和 OutOfMemoryError。</p><ul><li>StackOverFlowError： 当线程请求栈的深度超过当前Java虚拟机栈的最大深度的时候，就抛出StackOverFlowError异常。例如没完没了的递归（炸了吧）</li><li>OutOfMemoryError： 若 Java 虚拟机栈的内存用完了，无法再动态扩展了，此时抛出OutOfMemoryError异常。</li></ul><p>Java 虚拟机栈也是线程私有的，每个线程都有各自的Java虚拟机栈，而且随着线程的创建而创建，随着线程的死亡而死亡。</p><p>Java 栈可类比数据结构中的栈，Java 栈中保存的主要内容是栈帧，每一次函数调用都会有一个对应的栈帧被压入Java栈，每一个函数调用结束后，都会有一个栈帧被弹出。Java方法有两种返回方式：</p><ul><li>return 语句</li><li>抛出异常</li></ul><h3 id="本地方法栈"><a href="#本地方法栈" class="headerlink" title="本地方法栈"></a>本地方法栈</h3><p>和虚拟机栈所发挥的作用非常相似，区别是：虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务，底层是调用C/C++方法。 </p><p>本地方法栈也会出现 StackOverFlowError 和 OutOfMemoryError 两种异常。</p><h3 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h3><p><strong>Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。</strong></p><p><strong>Java 堆是垃圾收集器管理的主要区域</strong>，因此也被称作GC堆。从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以Java堆还可以细分为：新生代和老年代：再细致一点有：Eden空间、From Survivor、To Survivor空间等。<strong>进一步划分的目的是更好地回收内存，或者更快地分配内存。</strong></p><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/0md51ltn.bmp" alt></p><p><strong>eden区、s0区、s1区都属于新生代，tentired 区属于老年代。大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden区-&gt;Survivor 区后对象的初始年龄变为1)，当它的年龄增加到一定程度（默认为15岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。</strong></p><h3 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h3><p>方法区与 Java 堆一样，是各个线程共享的内存区域。它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。</p><p><strong>方法区也被称为永久代。</strong> 相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。</p><p>JDK 1.8 的时候，方法区被彻底移除了（JDK1.7就已经开始了），取而代之是元空间，元空间使用的是直接内存。</p><h4 id="为什么要将永久代-PermGen-替换为元空间-MetaSpace-呢"><a href="#为什么要将永久代-PermGen-替换为元空间-MetaSpace-呢" class="headerlink" title="为什么要将永久代(PermGen)替换为元空间(MetaSpace)呢?"></a>为什么要将永久代(PermGen)替换为元空间(MetaSpace)呢?</h4><p>整个永久代有一个 JVM 本身设置固定大小上线，无法进行调整，而元空间使用的是直接内存，受本机可用内存的限制，并且永远不会得到java.lang.OutOfMemoryError。你可以使用 -XX：MaxMetaspaceSize 标志设置最大元空间大小，默认值为 unlimited，这意味着它只受系统内存的限制。-XX：MetaspaceSize 调整标志定义元空间的初始大小如果未指定此标志，则 Metaspace 将根据运行时的应用程序需求动态地重新调整大小。</p><h3 id="运行时常量池"><a href="#运行时常量池" class="headerlink" title="运行时常量池"></a>运行时常量池</h3><p>运行时常量池是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池信息（用于存放编译期生成的各种字面量和符号引用）</p><p>既然运行时常量池时方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 异常。</p><p><strong>DK1.7及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。</strong></p><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/7p80rev4.bmp" alt></p><h3 id="直接内存"><a href="#直接内存" class="headerlink" title="直接内存"></a>直接内存</h3><p>直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用。而且也可能导致 OutOfMemoryError 异常出现。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>线程私有的是JAVA 虚拟机栈、本地方法栈、程序计数器；线程共享的是堆和方法区。</li><li>Java 虚拟机栈：虚拟机栈中主要保存了局部变量，包含各种基本数据类型（boolean、byte、char、short、int、float、long、double）和对象引用（reference类型）。Java 虚拟机栈中主要内容是栈帧，每一次函数调用都会有一个对应的栈帧被压入Java栈，每一个函数调用结束后，都会有一个栈帧被弹出。栈帧里面又主要包含了：局部变量表、操作数栈、动态链接、方法出口。Java 虚拟机栈会出现两种异常：StackOverFlowError 和 OutOfMemoryError。</li><li>本地方法栈：本地方法栈和和虚拟机栈所发挥的作用非常相似。两者的区别在于：虚拟机栈执行的是JAVA方法，而本地方法栈调用的是nvtice方法，底层调用的是C/C++方法。也会出现 StackOverFlowError 和 OutOfMemoryError 两种异常。</li><li>程序计数器：程序计数器实现代码的流程控制，如：顺序执行、选择、循环、异常处理。在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。程序计数器是唯一一个不会出现 OutOfMemoryError 的内存区域.</li><li>堆：堆存放的是对象实例，几乎所有的对象实例以及数组都在这里分配内存。堆是垃圾收集器管理的主要区域，因此也被称作GC堆。堆还可以细分为：新生代和老年代，再细致一点有：Eden空间、From Survivor、To Survivor空间等。大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden区-&gt;Survivor 区后对象的初始年龄变为1)，当它的年龄增加到一定程度（默认为15岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。</li><li>方法区：方法区主要存储加载的类信息、常量、静态变量等。方法区也被称为永久代。在JDK 1.8 的时候，方法区被取代成元空间，元空间使用的是直接内存。因为方法区受jvm的大小配置限制，而元空间直接受机器的内存管理，永远不会得到java.lang.OutOfMemoryError。</li><li>运行时常量池：常量池是方法区的一部分，主要存放字面量和符号引用，会抛出 OutOfMemoryError 异常。DK1.7及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。</li></ul><h2 id="2-Java对象的创建过程"><a href="#2-Java对象的创建过程" class="headerlink" title="2 Java对象的创建过程"></a>2 Java对象的创建过程</h2><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/1rfud9wf.bmp" alt></p><h3 id="类加载检查"><a href="#类加载检查" class="headerlink" title="类加载检查"></a>类加载检查</h3><p>虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。</p><h3 id="分配内存"><a href="#分配内存" class="headerlink" title="分配内存"></a>分配内存</h3><p>在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。分配方式有 “指针碰撞” 和 “空闲列表” 两种，选择那种分配方式由 Java 堆是否规整决定，而Java堆是否规整又取决于 GC 收集器的算法是”标记-清除”，还是”标记-整理”（也称作”标记-压缩”），值得注意的是，复制算法内存也是规整的</p><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/y2du1php.bmp" alt></p><h3 id="初始化零值"><a href="#初始化零值" class="headerlink" title="初始化零值"></a>初始化零值</h3><p>内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。</p><h3 id="设置对象头"><a href="#设置对象头" class="headerlink" title="设置对象头"></a>设置对象头</h3><p>初始化零值完成之后，虚拟机要对对象进行必要的设置，例如这个对象是那个类的实例、如何才能找到类的元数据信息、对象的哈希吗、对象的 GC 分代年龄等信息存放在对象头中。另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。</p><h3 id="执行-init-方法"><a href="#执行-init-方法" class="headerlink" title="执行 init 方法"></a>执行 init 方法</h3><p>在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，<init> 方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 <init> 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。</init></init></p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><ol><li>类加载检查：虚拟机遇到一条new指令时，会先在常量池中定位是否有类的符号引用，并且检查这个类是否已经被加载过，如果没有被加载过，就先执行类的加载过程。</li><li>分配内存：当类加载检查完成后，对象所需的内存大小也就确定了。分配内存主要有两种方式：“指针碰撞” 和 “空闲列表” 。用哪种方式主要看内存的规整来决定。如果内存规整，也就是用指针碰撞的形式，如serial，parlnew垃圾收集器，回收算法是标志-整理和复制算法。如果内存不规整，用空闲列表的形式，如CMS垃圾收集器，回收算法是标志-清除。</li><li>初始化零值：这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用。</li><li>设置对象头：主要把类的信息，hash码以及分代年龄等信息存放在对象头中。</li><li>执行init方法：把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。</li></ol><h2 id="3-类的加载过程"><a href="#3-类的加载过程" class="headerlink" title="3 类的加载过程"></a>3 类的加载过程</h2><p>类的加载过程为：加载、链接和初始化，其中链接可以细分为：验证、准备和解析。</p><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/image-20200705082601441.png" alt></p><h3 id="加载阶段"><a href="#加载阶段" class="headerlink" title="加载阶段"></a>加载阶段</h3><ul><li>通过类的全限定名读取此类二进制字节流。</li><li>将字节流转化为方法区的运行时数据结构。</li><li>在堆内存中生成该类的 Class 对象，作为该类的访问入口。</li></ul><h3 id="链接阶段"><a href="#链接阶段" class="headerlink" title="链接阶段"></a>链接阶段</h3><ul><li>验证：确保被加载类的正确性。</li><li>准备：为类变量分配内存并设置默认初始值。</li><li>解析：将常量池内的符号引用转换为直接引用。(符号引用就是描述目标，直接引用就是指向目标的地址。举个例子来说，现在调用方法 hello，这个方法的地址是 1234567，那么 hello 就是符号引用，1234567 就是直接引用)</li></ul><h3 id="初始化阶段"><a href="#初始化阶段" class="headerlink" title="初始化阶段"></a>初始化阶段</h3><p>执行类的初始化方法 clinit，执行内容包括静态变量初始化和静态块的执行。</p><h2 id="4-对象的访问定位两种方式"><a href="#4-对象的访问定位两种方式" class="headerlink" title="4 对象的访问定位两种方式"></a>4 对象的访问定位两种方式</h2><p>建立对象就是为了使用对象，我们的Java程序通过栈上的 reference 数据来操作堆上的具体对象。对象的访问方式有虚拟机实现而定，目前主流的访问方式有使用句柄和直接指针两种。</p><h3 id="使用句柄"><a href="#使用句柄" class="headerlink" title="使用句柄"></a>使用句柄</h3><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/dllw5s27.bmp" alt></p><p>如果使用句柄的话，那么Java堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息。</p><h3 id="直接指针"><a href="#直接指针" class="headerlink" title="直接指针"></a>直接指针</h3><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/aqodd113.bmp" alt></p><p>如果使用直接指针访问，那么 Java 堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而reference 中存储的直接就是对象的地址。</p><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><p>这两种对象访问方式各有优势。使用句柄来访问的最大好处是 reference 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而 reference 本身不需要修改。使用直接指针访问方式最大的好处就是速度快，它节省了一次指针定位的时间开销。</p><h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><ol><li>Java程序通过栈上的 reference 数据来操作堆上的具体对象，访问对象有使用句柄和直接指针两种方式。</li><li>使用句柄：Java堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息。</li><li>直接指针：reference 中存储的直接就是对象实列数据的地址，对象实例数据中包含了类型数据的具体地址信息。</li><li>优缺点：使用句柄来访问的最大好处是 reference 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而 reference 本身不需要修改。使用直接指针访问方式最大的好处就是速度快，reference 中存储的直接就是对象实列数据的地址，节省了一次指针定位的时间开销。</li></ol><h2 id="5-堆内存中对象的分配的基本策略"><a href="#5-堆内存中对象的分配的基本策略" class="headerlink" title="5 堆内存中对象的分配的基本策略"></a>5 堆内存中对象的分配的基本策略</h2><p>堆空间的基本结构：</p><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/al16mkpf.bmp" alt></p><p>eden区、s0区、s1区都属于新生代，tentired 区属于老年代。大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden区-&gt;Survivor 区后对象的初始年龄变为1)，当它的年龄增加到一定程度（默认为15岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。</p><p>另外，大对象和长期存活的对象会直接进入老年代。</p><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/40cpg52o.bmp" alt></p><h2 id="6-Minor-Gc和Full-GC区别"><a href="#6-Minor-Gc和Full-GC区别" class="headerlink" title="6 Minor Gc和Full GC区别"></a>6 Minor Gc和Full GC区别</h2><p>大多数情况下，对象在新生代中 eden 区分配。当 eden 区没有足够空间进行分配时，虚拟机将发起一次Minor GC。</p><h3 id="新生代GC（Minor-GC）"><a href="#新生代GC（Minor-GC）" class="headerlink" title="新生代GC（Minor GC）"></a>新生代GC（Minor GC）</h3><p>指发生新生代的的垃圾收集动作，Minor GC非常频繁，回收速度一般也比较快。</p><h3 id="老年代GC（Major-GC-Full-GC）"><a href="#老年代GC（Major-GC-Full-GC）" class="headerlink" title="老年代GC（Major GC/Full GC）"></a>老年代GC（Major GC/Full GC）</h3><p>指发生在老年代的GC，出现了Major GC经常会伴随至少一次的Minor GC（并非绝对），Major GC的速度一般会比Minor GC的慢10倍以上。</p><h2 id="7-判断对象是否死亡"><a href="#7-判断对象是否死亡" class="headerlink" title="7 判断对象是否死亡"></a>7 判断对象是否死亡</h2><p>堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断哪些对象已经死亡（即不能再被任何途径使用的对象）。</p><h3 id="引用计数法"><a href="#引用计数法" class="headerlink" title="引用计数法"></a>引用计数法</h3><p>给对象中添加一个引用计数器，每当有一个地方引用它，计数器就加1；当引用失效，计数器就减1；任何时候计数器为0的对象就是不可能再被使用的。</p><ul><li>优点：实现简单，垃圾对象便于辨识；判定效率高，回收没有延迟性。</li><li>缺点：无法解决循环引用问题</li></ul><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/image.png" alt></p><h3 id="可达性分析算法"><a href="#可达性分析算法" class="headerlink" title="可达性分析算法"></a>可达性分析算法</h3><p>这个算法的基本思想就是通过一系列的称为 “GC Roots” 的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的。</p><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/image1.png" alt></p><p>Java中可以作为GC Roots的对象：</p><ul><li>栈帧中的局部变量引用的对象</li><li>方法区中的类静态属性引用的对象</li><li>方法区中常量引用的对象</li><li>本地方法栈中JNI(Native方法)引用的对象</li></ul><p><a href="https://blog.csdn.net/summer_fish/article/details/115399628" target="_blank" rel="noopener">参考例子</a></p><h2 id="8-强引用-软引用-弱引用-虚引用"><a href="#8-强引用-软引用-弱引用-虚引用" class="headerlink" title="8 强引用,软引用,弱引用,虚引用"></a>8 强引用,软引用,弱引用,虚引用</h2><p>无论是通过引用计数法判断对象引用数量，还是通过可达性分析法判断对象的引用链是否可达，判定对象的存活都与“引用”有关。</p><h3 id="强引用-StrongReference"><a href="#强引用-StrongReference" class="headerlink" title="强引用(StrongReference)"></a>强引用(StrongReference)</h3><p>以前我们使用的大部分引用实际上都是强引用，这是使用最普遍的引用。如果一个对象具有强引用，那就类似于必不可少的生活用品，垃圾回收器绝不会回收它。当内存空 间不足，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。</p><h3 id="软引用-SoftReference"><a href="#软引用-SoftReference" class="headerlink" title="软引用(SoftReference)"></a>软引用(SoftReference)</h3><p>如果一个对象只具有软引用，那就类似于可有可无的生活用品。如果内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。</p><p>软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收，JAVA虚拟机就会把这个软引用加入到与之关联的引用队列中。</p><h3 id="弱引用-WeakReference"><a href="#弱引用-WeakReference" class="headerlink" title="弱引用(WeakReference)"></a>弱引用(WeakReference)</h3><p>如果一个对象只具有弱引用，那就类似于可有可无的生活用品。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它 所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。 </p><p>弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。</p><h3 id="虚引用（PhantomReference）"><a href="#虚引用（PhantomReference）" class="headerlink" title="虚引用（PhantomReference）"></a>虚引用（PhantomReference）</h3><p>“虚引用”顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。</p><p>虚引用主要用来跟踪对象被垃圾回收的活动。</p><p>虚引用与软引用和弱引用的一个区别在于： 虚引用必须和引用队列（ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 </p><p>特别注意，在程序设计中一般很少使用弱引用与虚引用，使用软引用的情况较多，这是因为软引用可以加速JVM对垃圾内存的回收速度，可以维护系统的运行安全，防止内存溢出（OutOfMemory）等问题的产生。</p><h2 id="9-判断一个常量是废弃常量"><a href="#9-判断一个常量是废弃常量" class="headerlink" title="9 判断一个常量是废弃常量"></a>9 判断一个常量是废弃常量</h2><p>假如在常量池中存在字符串 “abc”，如果当前没有任何String对象引用该字符串常量的话，就说明常量 “abc” 就是废弃常量，如果这时发生内存回收的话而且有必要的话，”abc” 就会被系统清理出常量池。</p><h2 id="10-判断一个类是无用的类"><a href="#10-判断一个类是无用的类" class="headerlink" title="10 判断一个类是无用的类"></a>10 判断一个类是无用的类</h2><ul><li>该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。</li><li>加载该类的 ClassLoader 已经被回收。</li><li>该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。</li></ul><h2 id="11-方法区与其他内存结构关系"><a href="#11-方法区与其他内存结构关系" class="headerlink" title="11 方法区与其他内存结构关系"></a>11 方法区与其他内存结构关系</h2><ul><li>Person：存放在元空间，也可以说方法区。</li><li>person：存放在Java栈的局部变量表中。</li><li>new Person()：存放在Java堆中。</li></ul><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/image-20200708094747667.png" alt></p><h2 id="12-堆"><a href="#12-堆" class="headerlink" title="12 堆"></a>12 堆</h2><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/20210401230845268.png" alt></p><h3 id="默认新生代和老年代的占比"><a href="#默认新生代和老年代的占比" class="headerlink" title="默认新生代和老年代的占比"></a>默认新生代和老年代的占比</h3><p>默认 -XX:NewRatio=2，代表新生代占 1，老年代占 2，即新生代占整个堆的 1/3。</p><h3 id="默认-Eden-和-Survivor-的占比"><a href="#默认-Eden-和-Survivor-的占比" class="headerlink" title="默认 Eden 和 Survivor 的占比"></a>默认 Eden 和 Survivor 的占比</h3><p>默认 -xx:SurvivorRatio=8，代表 Eden 和 From Survivor，To Survivor 占比是 8：1：1。</p><h3 id="堆对象流转过程"><a href="#堆对象流转过程" class="headerlink" title="堆对象流转过程"></a>堆对象流转过程</h3><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/20210401230851499.png" alt></p><ol><li>创建的对象优先在 Eden 区分配（较长的字符串，数组，大对象直接进老年代）。</li><li>如果发现 Eden 满了，会触发第一次GC，把还活着的对象拷贝到from区。而当Eden Space再次触发GC时，会扫描Eden Space和from，对这两个区进行垃圾回收，经过此次回收后依旧存活的对象，则直接复制到to区（如果对象的年龄已经达到老年的标准，则移动至老年代区），同时把这些对象的年龄+1。然后，清空Eden Space和from中的对象，此时的from是空的。最后，from和to进行互换，原from成为下一次GC时的to，原to成为下一次GC时的from。</li><li>部分对象会在from和to中来回进行交换复制，如果交换15次（由JVM参数MaxTenuringThreshold决定，默认15），最终依旧存活的对象就会移动至老年代。</li><li>当老年代内存不足时，会触发 Major GC 对老年代进行回收。</li></ol><h2 id="13-类文件结构"><a href="#13-类文件结构" class="headerlink" title="13 类文件结构"></a>13 类文件结构</h2><p>根据 Java 虚拟机规范，类文件由单个 ClassFile 结构组成：</p><pre><code>ClassFile {    u4             magic; //Class 文件的标志    u2             minor_version;//Class 的小版本号    u2             major_version;//Class 的大版本号    u2             constant_pool_count;//常量池的数量    cp_info        constant_pool[constant_pool_count-1];//常量池    u2             access_flags;//Class 的访问标记    u2             this_class;//当前类    u2             super_class;//父类    u2             interfaces_count;//接口    u2             interfaces[interfaces_count];//一个类可以实现多个接口    u2             fields_count;//Class 文件的字段属性    field_info     fields[fields_count];//一个类会可以有个字段    u2             methods_count;//Class 文件的方法数量    method_info    methods[methods_count];//一个类可以有个多个方法    u2             attributes_count;//此类的属性表中的属性数    attribute_info attributes[attributes_count];//属性表集合}</code></pre><p>Class文件字节码结构组织示意图</p><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/n30hvpi9.bmp" alt></p><h2 id="14-双亲委派机制"><a href="#14-双亲委派机制" class="headerlink" title="14 双亲委派机制"></a>14 双亲委派机制</h2><h3 id="类加载器"><a href="#类加载器" class="headerlink" title="类加载器"></a>类加载器</h3><ul><li><p>启动类加载器属于虚拟机的一部分，是用 C++ 写的，看不到源码。</p><p>  启动类加载器：加载的是 jre/lib 目录下的核心库。</p></li><li><p>其他类加载器是用 Java 写的。</p><ul><li><p>扩展类加载器：加载的是 jre/lib/ext 目录下的扩展包。</p></li><li><p>应用类加载器：加载的是 我们自己的 Java 代码编译成的 Class 文件的目录（ClassPath）。</p></li><li><p>自定义类加载器。</p></li></ul></li></ul><h3 id="双亲委派机制的工作原理"><a href="#双亲委派机制的工作原理" class="headerlink" title="双亲委派机制的工作原理"></a>双亲委派机制的工作原理</h3><ol><li>当一个类加载器接收到类加载任务时，先查缓存里有没有，如果没有，将任务委托给它的父加载器去执行。</li><li>父加载器也做同样的事情，一层一层往上委托，直到最顶层的启动类加载器为止。</li><li>如果启动类加载器没有找到所需加载的类，便将此加载任务退回给下一级类加载器去执行，而下一级的类加载器也做同样的事情。</li><li>如果最底层类加载器仍然没有找到所需要的 class 文件，则抛出异常。</li></ol><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/image-20200705105151258.png" alt></p><h3 id="双亲委派机制的优势"><a href="#双亲委派机制的优势" class="headerlink" title="双亲委派机制的优势"></a>双亲委派机制的优势</h3><p>避免类被重复加载 + 避免核心 API 被篡改。</p><p>如果没有双亲委托机制来确保类的全局唯一性，谁都可以编写一个 java.lang.String 类放在 classpath 下，那应用程序就乱套了。所以自定义一个与核心类库重名的类，会发现这个类可以被正常编译，但永远无法被加载运行。因为这个类不会被应用类加载器加载，而是被委托到顶层，被启动类加载器在核心类库中找到了。</p><h3 id="如何破坏双亲委派模型"><a href="#如何破坏双亲委派模型" class="headerlink" title="如何破坏双亲委派模型"></a>如何破坏双亲委派模型</h3><p>自定义类加载器，重写 loadClass 方法。</p><h2 id="15-并发的可达性分析（三色标记）"><a href="#15-并发的可达性分析（三色标记）" class="headerlink" title="15 并发的可达性分析（三色标记）"></a>15 并发的可达性分析（三色标记）</h2><p>首先需要引入三种颜色：</p><ul><li>白色：对象没被 GC 访问过。若分析结束还是白色的对象，则代表不可达。</li><li>黑色：对象已经被 GC 访问过，且这个对象的所有引用都扫描过了，他是安全存活的。</li><li>灰色：对象已经被 GC 访问过，但这个对象上还存在没有被扫描过的引用。</li></ul><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/image2.png" alt></p><p>并发标记阶段，会出现两种问题：</p><ul><li>把白色对象错标成了黑色对象（问题不大，只是产生了浮动垃圾，下次再清理）。</li><li>把黑色对象错标成白色对象（裂了啊，我的对象怎么不见了）。并发时，灰色对象的一个引用被切断，然后和已扫描过的黑色对象建立了引用关系。</li></ul><p>解决这个问题的方案有两个：增量更新和原始快照。</p><ul><li>增量更新（CMS）：当黑色对象和白色对象建立引用关系时，会记录下来黑色对象的引用，等到最终标记时，再将这些黑色对象作为根，再重新扫描。</li><li>原始快照（G1）：当灰色对象删除对白色对象的引用关系时，会记录下来白色对象的引用，再以这些白色对象作为根，再重新扫描。</li></ul><h2 id="16-垃圾收集算法"><a href="#16-垃圾收集算法" class="headerlink" title="16 垃圾收集算法"></a>16 垃圾收集算法</h2><h3 id="标记-清除算法"><a href="#标记-清除算法" class="headerlink" title="标记-清除算法"></a>标记-清除算法</h3><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/tpi6cpml.bmp" alt></p><p>算法分为“标记”和“清除”阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。它是最基础的收集算法，后续的算法都是对其不足进行改进得到。</p><h4 id="适用场景："><a href="#适用场景：" class="headerlink" title="适用场景："></a>适用场景：</h4><ul><li>存活对象较多的情况下比较高效（老年代）。</li></ul><h4 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h4><ul><li>使用内存少</li></ul><h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h4><ul><li>效率低。扫描了整个空间两次。</li><li>会产生大量不连续的碎片，导致大对象无法在空闲列表找到可分配的空间。</li></ul><h3 id="复制算法"><a href="#复制算法" class="headerlink" title="复制算法"></a>复制算法</h3><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/dknyft70.bmp" alt></p><p>为了解决效率问题，“复制”收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。</p><h4 id="适用场合："><a href="#适用场合：" class="headerlink" title="适用场合："></a>适用场合：</h4><ul><li>存活对象较少的情况下比较高效（新生代）。</li><li>只扫描了整个空间一次（标记存活对象并复制移动）。</li></ul><h4 id="优点：-1"><a href="#优点：-1" class="headerlink" title="优点："></a>优点：</h4><ul><li>效率高</li><li>内存规整</li></ul><h4 id="缺点：-1"><a href="#缺点：-1" class="headerlink" title="缺点："></a>缺点：</h4><ul><li>需要两倍的内存空间。</li><li>像老年代那种回收完还有大量的对象存活的区域，需要复制的对象将会有很多，效率会很低。</li></ul><h3 id="标记-整理算法"><a href="#标记-整理算法" class="headerlink" title="标记-整理算法"></a>标记-整理算法</h3><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/qish34yq.bmp" alt></p><p>首先也需要从根节点开始对所有可达对象做一次标记，但之后，它并不简单地清理未标记的对象，而是将所有的存活对象压缩到内存的一端。之后，清理边界外所有的空间。这种方法既避免了碎片的产生，又不需要两块相同的内存空间，因此，其性价比比较高。</p><ul><li>效率最低</li></ul><h3 id="分代收集算法"><a href="#分代收集算法" class="headerlink" title="分代收集算法"></a>分代收集算法</h3><p>当前虚拟机的垃圾收集都采用分代收集算法，这种算法没有什么新的思想，只是根据对象存活周期的不同将内存分为几块。一般将java堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。</p><p><strong>在新生代中，每次收集都会有大量对象死去，所以可以选择复制算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。</strong></p><h2 id="17-分为新生代和老年代的好处"><a href="#17-分为新生代和老年代的好处" class="headerlink" title="17 分为新生代和老年代的好处"></a>17 分为新生代和老年代的好处</h2><p>主要是为了提升GC效率。</p><h2 id="18-垃圾收集器"><a href="#18-垃圾收集器" class="headerlink" title="18 垃圾收集器"></a>18 垃圾收集器</h2><h3 id="Serial收集器"><a href="#Serial收集器" class="headerlink" title="Serial收集器"></a>Serial收集器</h3><p>Serial（串行）收集器收集器是最基本、历史最悠久的垃圾收集器了。大家看名字就知道这个收集器是一个单线程收集器了。它的 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ “Stop The World” ），直到它收集结束。</p><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/w2jtzc95.bmp" alt></p><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/image7.png" alt></p><p><strong>新生代采用复制算法，老年代采用标记-整理算法</strong></p><p>虚拟机的设计者们当然知道Stop The World带来的不良用户体验，所以在后续的垃圾收集器设计中停顿时间在不断缩短（仍然还有停顿，寻找最优秀的垃圾收集器的过程仍然在继续）。</p><p>但是Serial收集器有没有优于其他垃圾收集器的地方呢？当然有，它简单而高效（与其他收集器的单线程相比）。Serial收集器由于没有线程交互的开销，自然可以获得很高的单线程收集效率。Serial收集器对于运行在Client模式下的虚拟机来说是个不错的选择。</p><h3 id="ParNew收集器"><a href="#ParNew收集器" class="headerlink" title="ParNew收集器"></a>ParNew收集器</h3><p>ParNew收集器其实就是Serial收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和Serial收集器完全一样。</p><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/ngopf1fj.bmp" alt></p><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/image10.png" alt></p><p><strong>新生代采用复制算法，老年代采用标记-整理算法。</strong></p><p>它是许多运行在Server模式下的虚拟机的首要选择，除了Serial收集器外，只有它能与CMS收集器（真正意义上的并发收集器）配合工作。</p><h3 id="Parallel-Scavenge收集器"><a href="#Parallel-Scavenge收集器" class="headerlink" title="Parallel Scavenge收集器"></a>Parallel Scavenge收集器</h3><p>Parallel Scavenge 收集器类似于ParNew 收集器。</p><p>Parallel Scavenge收集器关注点是吞吐量（高效率的利用CPU）。CMS等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是CPU中用于运行用户代码的时间与CPU总消耗时间的比值。 Parallel Scavenge收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解的话，手工优化存在的话可以选择把内存管理优化交给虚拟机去完成也是一个不错的选择。</p><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/2tlrkyi5.bmp" alt></p><p><strong>新生代采用复制算法，老年代采用标记-整理算法。</strong></p><h3 id="Serial-Old收集器"><a href="#Serial-Old收集器" class="headerlink" title="Serial Old收集器"></a>Serial Old收集器</h3><p>Serial收集器的老年代版本，它同样是一个单线程收集器。它主要有两大用途：一种用途是在JDK1.5以及以前的版本中与Parallel Scavenge收集器搭配使用，另一种用途是作为CMS收集器的后备方案。</p><h3 id="Parallel-Old收集器"><a href="#Parallel-Old收集器" class="headerlink" title="Parallel Old收集器"></a>Parallel Old收集器</h3><p>Parallel Scavenge收集器的老年代版本。使用多线程和“标记-整理”算法。在注重吞吐量以及CPU资源的场合，都可以优先考虑 Parallel Scavenge收集器和Parallel Old收集器。</p><h3 id="CMS收集器"><a href="#CMS收集器" class="headerlink" title="CMS收集器"></a>CMS收集器</h3><p>CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它而非常符合在注重用户体验的应用上使用。</p><p>CMS（Concurrent Mark Sweep）收集器是HotSpot虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。</p><p>从名字中的Mark Sweep这两个词可以看出，CMS收集器是一种 “标记-清除”算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤：</p><ul><li>初始标记： 暂停所有的其他线程，并记录下直接与root相连的对象，速度很快 ；</li><li>并发标记： 同时开启GC和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以GC线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。</li><li>重新标记： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短</li><li>并发清除： 开启用户线程，同时GC线程开始对为标记的区域做清扫。</li></ul><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/yjdb4a3c.bmp" alt></p><p>从它的名字就可以看出它是一款优秀的垃圾收集器，主要优点：并发收集、低停顿。但是它有下面三个明显的缺点：</p><ul><li>对CPU资源敏感；</li><li>无法处理浮动垃圾；</li><li>它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。</li></ul><p>由于 CMS 的并发标记和并发清除阶段，用户线程还在进行运行，所以也会不断产生垃圾，而 CMS 无法在当次收集中处理它们，只能等到下一次垃圾收集时再清理掉，这部分垃圾就称为浮动垃圾。</p><h3 id="G1收集器"><a href="#G1收集器" class="headerlink" title="G1收集器"></a>G1收集器</h3><p>G1 (Garbage-First)是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足GC停顿时间要求的同时,还具备高吞吐量性能特征.</p><p>被视为JDK1.7中HotSpot虚拟机的一个重要进化特征。它具备一下特点：</p><ul><li>并行与并发：G1能充分利用CPU、多核环境下的硬件优势，使用多个CPU（CPU或者CPU核心）来缩短Stop-The-World停顿时间。部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让java程序继续执行。</li><li>分代收集：虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但是还是保留了分代的概念。</li><li>空间整合：与CMS的“标记–清理”算法不同，G1从整体来看是基于“标记整理”算法实现的收集器；从局部上来看是基于“复制”算法实现的。</li><li>可预测的停顿：这是G1相对于CMS的另一个大优势，降低停顿时间是G1 和 CMS 共同的关注点，但G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内。</li></ul><p>G1收集器的运作大致分为以下几个步骤：</p><ul><li>初始标记</li><li>并发标记</li><li>最终标记</li><li>筛选回收</li></ul><p>G1收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的Region(这也就是它的名字Garbage-First的由来)。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了GF收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。</p><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/r0psxic5.bmp" alt></p><h3 id="垃圾收集器分类"><a href="#垃圾收集器分类" class="headerlink" title="垃圾收集器分类"></a>垃圾收集器分类</h3><ul><li>串行回收器：Serial、Serial Old。</li><li>并行回收器：ParNew、Parallel Scavenge、Parallel Old。</li><li>并发回收器：CMS、G1。</li></ul><h3 id="垃圾收集器与分代之间的关系："><a href="#垃圾收集器与分代之间的关系：" class="headerlink" title="垃圾收集器与分代之间的关系："></a>垃圾收集器与分代之间的关系：</h3><ul><li>新生代收集器：Serial、ParNew、Parallel Scavenge。</li><li>老年代收集器：Serial Old、Parallel Old、CMS。</li><li>整堆收集器：G1。</li></ul><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/image-20200713093757644.png" alt></p><h3 id="垃圾收集器的组合关系："><a href="#垃圾收集器的组合关系：" class="headerlink" title="垃圾收集器的组合关系："></a>垃圾收集器的组合关系：</h3><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/image-20200713094745366.png" alt></p><ul><li><p>Serial + Serial Old，Serial + CMS</p></li><li><p>ParNew + Serial Old，ParNew + CMS</p></li><li><p>Parallel Scavenge + Serial Old，Parallel Scavenge + Parallel Old。</p></li><li><p>其中 Serial Old 还会作为 CMS 出现 “Concurrent Mode Failure” 失败的后备预案。</p></li><li><p>红色虚线：在 JDK 8 时将 Serial + CMS，ParNew + Serial Old 声明为废弃，并在 JDK9 中彻底取消了对这些组合的支持。</p></li><li><p>绿色虚线：在 JDK14 中，弃用 Parallel Scavenge + Serial Old 的组合。</p></li><li><p>青色虚线：在 JDK14 中，删除 CMS 垃圾收集器。</p></li></ul><h3 id="查看默认收集器"><a href="#查看默认收集器" class="headerlink" title="查看默认收集器"></a>查看默认收集器</h3><p>Java -XX:+PrintCommandLineFlags -version</p><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/image5.png" alt></p><h2 id="19-遇到过哪些-OOM-问题"><a href="#19-遇到过哪些-OOM-问题" class="headerlink" title="19 遇到过哪些 OOM 问题"></a>19 遇到过哪些 OOM 问题</h2><ul><li>内存泄漏memory leak :是指程序在申请内存后，无法释放已申请的内存空间，一次内存泄漏似乎不会有大的影响，但内存泄漏堆积后的后果就是内存溢出。 </li><li>内存溢出out of memory :指程序申请内存时，没有足够的内存供申请者使用，或者说，给了你一块存储int类型数据的存储空间，但是你却存储long类型的数据，那么结果就是内存不够用，此时就会报错OOM,即所谓的内存溢出。</li></ul><h3 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h3><ol><li><p>一般首先考虑内存泄露/内存溢出的情况，需要配置 JVM 参数输出 dump 文件。</p><p> -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/heapdump.hprof。</p><p> -XX:+HeapDumpOnOutOfMemoryError 设置当首次遭遇内存溢出时导出此时堆中相关信息。</p><p> -XX:HeapDumpPath=/tmp/heapdump.hprof 指定导出堆信息时的路径或文件名。</p></li><li><p>接着通过内存映像分析工具（如 JProfiler，MAT）对 dump 出来的堆转储快照进行分析，确认内存中的对象是否是必要的，也就是要先分清楚到底是出现了内存泄漏还是内存溢出。</p></li><li><p>如果是内存泄漏，可进一步通过工具查看泄漏对象到 GC Roots 的引用链。于是就能找到泄漏对象是通过怎样的路径与 GCRoots 相关联并导致垃圾收集器无法自动回收它们的。掌握了泄漏对象的类型信息，以及 GCRoots 引用链的信息，就可以比较准确地定位出泄漏代码的位置。</p></li><li><p>如果不存在内存泄漏，换句话说就是内存中的对象确实都还必须存活着，那就应当检查虚拟机的堆参数（-Xmx与-Xms），与机器物理内存对比看是否还可以调大，从代码上检查是否存在某些对象生命周期过长、持有状态时间过长的情况，尝试减少程序运行期的内存消耗。</p></li></ol><h3 id="溢出场景"><a href="#溢出场景" class="headerlink" title="溢出场景"></a>溢出场景</h3><h4 id="内存溢出"><a href="#内存溢出" class="headerlink" title="内存溢出"></a>内存溢出</h4><p>如突发高峰期时，内存存活对象使用空间的量超出最大堆，并且无法回收。</p><ul><li>小公司没这个可能，可以说是某个时间段导出特大报表导致。</li><li>想下项目有哪些垃圾代码使用了大内存耗时操作，且长时间不能被回收，什么全表查数据计算，循环做 String 拼接操作等等。</li></ul><h4 id="内存泄漏"><a href="#内存泄漏" class="headerlink" title="内存泄漏"></a>内存泄漏</h4><p>重新不在使用的对象仍然存在引用导致无法回收，随着时间推移，泄漏内存对象占用了所有可用堆。</p><ul><li>比如 dubbo 泛化调用没缓存 ReferenceConfig 实例导致了内存泄漏。</li></ul><h3 id="OOM-原因"><a href="#OOM-原因" class="headerlink" title="OOM 原因"></a>OOM 原因</h3><h4 id="Java-heap-space（应用尝试从堆申请一个区域时，堆没有可分配的空间）"><a href="#Java-heap-space（应用尝试从堆申请一个区域时，堆没有可分配的空间）" class="headerlink" title="Java heap space（应用尝试从堆申请一个区域时，堆没有可分配的空间）"></a>Java heap space（应用尝试从堆申请一个区域时，堆没有可分配的空间）</h4><p>原因：内存泄漏、内存溢出</p><h4 id="GC-overhead-limit-exceeded（超过-98-的时间在-GC，且回收了不到-2-的堆内存）"><a href="#GC-overhead-limit-exceeded（超过-98-的时间在-GC，且回收了不到-2-的堆内存）" class="headerlink" title="GC overhead limit exceeded（超过 98% 的时间在 GC，且回收了不到 2% 的堆内存）"></a>GC overhead limit exceeded（超过 98% 的时间在 GC，且回收了不到 2% 的堆内存）</h4><p>原因：内存泄漏、内存溢出</p><h4 id="unable-to-create-new-native-thread（无法创建新的本地线程）"><a href="#unable-to-create-new-native-thread（无法创建新的本地线程）" class="headerlink" title="unable to create new native thread（无法创建新的本地线程）"></a>unable to create new native thread（无法创建新的本地线程）</h4><ul><li><p>原因</p><ul><li>系统内存耗尽，无法为新线程分配内存</li><li>创建线程数超过了操作系统的限制</li></ul></li><li><p>解决</p><ul><li>排查应用线程数：通过 jstack 排查应用哪个地方创建了过多的线程。</li><li>调整操作系统线程数阈值：通过 ulimit -u 查看操作系统的最大进程数是否需要修改。</li><li>减小堆内存：JVM 是否占满了系统总内存，导致系统线程没有足够的内存分配，如果是这种，就考虑降低堆内存，否则就增加系统内存。</li></ul></li></ul><h4 id="Metaspace（元空间不足，加载到内存中的-class-数量太多或者体积太大）"><a href="#Metaspace（元空间不足，加载到内存中的-class-数量太多或者体积太大）" class="headerlink" title="Metaspace（元空间不足，加载到内存中的 class 数量太多或者体积太大）"></a>Metaspace（元空间不足，加载到内存中的 class 数量太多或者体积太大）</h4><p>解决：修改 -XX:MetaspaceSize 和 -XX:MaxMetaspaceSize。</p><h4 id="Direct-buffer-memory（直接内存不足，通常是-NIO-引起的）"><a href="#Direct-buffer-memory（直接内存不足，通常是-NIO-引起的）" class="headerlink" title="Direct buffer memory（直接内存不足，通常是 NIO 引起的）"></a>Direct buffer memory（直接内存不足，通常是 NIO 引起的）</h4><ul><li><p>原因</p><p>  ByteBuffer.allocteDirect(capability) 是直接分配 OS 本地内存，如何不断分配本地内存，堆内存很少使用，那么 JVM 就不需要执行 GC，DirectByteBuffer 就不会被回收，这时候堆内存充足，但本地内存很快就给耗光了。</p></li><li><p>解决</p><p>  加大 -XX:MaxDirectMemorySize。</p></li></ul><h4 id="Requested-array-size-exceeds-VM-limit（数组太大，超过了平台限制值）"><a href="#Requested-array-size-exceeds-VM-limit（数组太大，超过了平台限制值）" class="headerlink" title="Requested array size exceeds VM limit（数组太大，超过了平台限制值）"></a>Requested array size exceeds VM limit（数组太大，超过了平台限制值）</h4><p>解决：减小数组长度，拆分大数组等。</p><h4 id="Out-of-swap-space（交换空间空间不足，表明请求分配内存的操作失败了）"><a href="#Out-of-swap-space（交换空间空间不足，表明请求分配内存的操作失败了）" class="headerlink" title="Out of swap space（交换空间空间不足，表明请求分配内存的操作失败了）"></a>Out of swap space（交换空间空间不足，表明请求分配内存的操作失败了）</h4><p>解决：java 进程使用了虚拟内存才会发生，这时得增加虚拟内存的大小。</p><h3 id="堆是分配对象的唯一选择"><a href="#堆是分配对象的唯一选择" class="headerlink" title="堆是分配对象的唯一选择"></a>堆是分配对象的唯一选择</h3><p>不一定，在 JDK 1.7 版本之后，HotSpot 中默认就已经开启了逃逸分析。</p><p>如果经过逃逸分析后发现，一个对象没有逃逸出方法的话，就会被优化成栈上分配，以此达到降低 GC 的回收频率和提升 GC 的回收效率的目的。</p><p>逃逸分析的基本行为就是分析对象动态作用域：</p><ul><li>当一个对象在方法中被定义后，对象只在方法内部使用，则认为没有发生逃逸。</li><li>当一个对象在方法中被定义后，它被外部方法所引用，则认为发生逃逸。例如作为调用参数传递到其他地方中。</li></ul><h2 id="20-分析-GC-日志"><a href="#20-分析-GC-日志" class="headerlink" title="20 分析 GC 日志"></a>20 分析 GC 日志</h2><h3 id="Young-GC-参数图"><a href="#Young-GC-参数图" class="headerlink" title="Young GC 参数图"></a>Young GC 参数图</h3><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/image-20200714082555688.png" alt></p><h3 id="Full-GC-参数图"><a href="#Full-GC-参数图" class="headerlink" title="Full GC 参数图"></a>Full GC 参数图</h3><p><img src="/2021/05/20/2021-05-20-chong-xue-jvm/image-20200714082714690.png" alt></p><h2 id="21-JVM参数"><a href="#21-JVM参数" class="headerlink" title="21 JVM参数"></a>21 JVM参数</h2><h3 id="标准参数"><a href="#标准参数" class="headerlink" title="标准参数"></a>标准参数</h3><p>标准参数以-开头，比较稳定，后续版本基本不会变化。</p><p>在控制台输入java -help，可以看到所有的标准参数</p><h3 id="X参数"><a href="#X参数" class="headerlink" title="-X参数"></a>-X参数</h3><p>非标准化参数，功能比较稳定，后续版本可能会变更，以-X开头</p><p>在控制台输入java -X，可以看到相关参数</p><ul><li>-Xinit：禁用JIT，所有的字节码都被解释执行，速度最慢</li><li>-Xcomp，所有的字节码第一次就被编译，然后再执行</li><li>-Xmixed，混合模式，默认，让JIT根据程序运行情况，动态编译</li><li>-Xms，初始化java堆的大小，相当于-XX:InitialHeapSize，如-Xms2g</li><li>-Xmx，设置最大java堆大小，相当于-XX:MaxHeapSize</li><li>-Xss，设置java线程栈的大小，相当于-XX:ThreadStackSize</li></ul><h3 id="XX参数"><a href="#XX参数" class="headerlink" title="-XX参数"></a>-XX参数</h3><p>使用最多的参数类型，比较不稳定，以-XX开头</p><p>-XX参数分为Boolean类型和非Boolean类型</p><h4 id="Boolean类型"><a href="#Boolean类型" class="headerlink" title="Boolean类型"></a>Boolean类型</h4><p>-XX:+，表示开启option</p><p>-XX:-，表示关闭option，有些参数是默认开启的，可以通过-进行关闭</p><p>如，-XX:+UseParallelGC，-XX:+UseAdaptiveSizePolicy</p><h4 id="非Boolean"><a href="#非Boolean" class="headerlink" title="非Boolean"></a>非Boolean</h4><p>非Boolean，分为数值类型和非数值类型，如果是数值类型，就进行赋值</p><p>-XX:=<option>=<string></string></option></p><p>如，-XX:NewRatio=2，-XX:NewSize=2M</p><p>如果是非数值类型，则填写对应的值</p><p>-XX:=<option>=<string></string></option></p><p>如，-XX:HeapDumpPath=/xxx/xxx/duml.hprof</p><h4 id="PrintFlagsFinal"><a href="#PrintFlagsFinal" class="headerlink" title="PrintFlagsFinal"></a>PrintFlagsFinal</h4><p>-XX:+PrintFlagsFinal，输出所有参数的名称和默认值，默认不包括诊断和实验的参数</p><h2 id="生产配置垃圾回收器"><a href="#生产配置垃圾回收器" class="headerlink" title="生产配置垃圾回收器"></a>生产配置垃圾回收器</h2><p>JDK8默认的是Parallel Scavenge（复制） + Parallel Old（标记-整理）。</p><p>但我们的是金融支付系统，如果垃圾回收的可能会造成用户线程较长停顿，进而可能造成一些订单超时。</p><p>所以我们线上配置的是ParNew(复制) + CMS(标志清除)。CMS是并发收集器，可以进一步缩短回收的停顿时间。</p><ul><li>初始标记： 暂停所有的其他线程，并记录下直接与root相连的对象，速度很快 ；</li><li>并发标记： 同时开启GC和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以GC线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。</li><li>重新标记： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短</li><li>并发清除： 开启用户线程，同时GC线程开始对为标记的区域做清扫。</li></ul><h2 id="开发中遇到OOM问题"><a href="#开发中遇到OOM问题" class="headerlink" title="开发中遇到OOM问题"></a>开发中遇到OOM问题</h2><ol><li>获取dump文件；</li><li>用Eclipse Memory Analyzer(mat)导入dump文件分析；</li><li>发现多数据源中某个链接信息错误，导致不断重连；</li><li>修改代码或链接信息。</li></ol><h2 id="大厂面试题"><a href="#大厂面试题" class="headerlink" title="大厂面试题"></a>大厂面试题</h2><h3 id="阿里"><a href="#阿里" class="headerlink" title="阿里"></a>阿里</h3><ul><li>你知道哪几种垃圾回收器，各自的优缺点，重点讲一下 CMS 和 G1？</li><li>JVM GC 算法有哪些，目前的 JDK 版本采用什么回收算法？可以说用的 JDK8，Parallel Scavenge（复制） + Parallel Old（标记-整理）。</li><li>G1 回收器讲下回收过程？GC 是什么？为什么要有 GC？</li><li>CMS 收集器与 G1 收集器的特点？</li><li>CMS 特点，垃圾回收算法有哪些？各自的优缺点，他们共同的缺点是什么？</li><li>JDK8 的内存分代改进？JVM 内存分哪几个区，每个区的作用是什么？</li><li>JVM 内存分布/内存结构？</li><li>栈和堆的区别？</li><li>堆的结构？</li><li>为什么两个 Survivor 区？ </li><li>Eden 和 Survior 的比例分配？</li><li>讲一讲垃圾回收算法。</li><li>什么情况下触发垃圾回收？</li><li>如何选择合适的垃圾收集算法？</li><li>JVM有哪三种垃圾回收器？</li></ul><h3 id="字节"><a href="#字节" class="headerlink" title="字节"></a>字节</h3><ul><li>常见的垃圾回收器算法有哪些，各有什么优劣？</li><li>System.gc() 和 Runtime.gc() 会做什么事情？</li><li>Java GC 机制？GC Roots 有哪些？</li><li>Java对象的回收方式，回收算法。</li><li>CMS 和 G1 了解么，CMS 解决什么问题，说一下回收的过程。</li><li>CMS 回收停顿了几次，为什么要停顿两次？</li><li>什么时候对象会进入老年代？</li></ul><h3 id="百度"><a href="#百度" class="headerlink" title="百度"></a>百度</h3><ul><li>说一下 JVM 内存模型吧，有哪些区？分别干什么的？</li><li>说一下 GC 算法，分代回收说下。</li></ul><h3 id="京东"><a href="#京东" class="headerlink" title="京东"></a>京东</h3><ul><li>你知道哪几种垃圾收集器，各自的优缺点，重点讲下 CMS 和 G1。 </li></ul><h3 id="小米"><a href="#小米" class="headerlink" title="小米"></a>小米</h3><ul><li>JVM 内存分区，为什么要有新生代和老年代？</li></ul><h3 id="美团"><a href="#美团" class="headerlink" title="美团"></a>美团</h3><ul><li>JVM 的永久代中会发生垃圾回收吗？</li></ul><h3 id="滴滴"><a href="#滴滴" class="headerlink" title="滴滴"></a>滴滴</h3><ul><li>Java 的垃圾回收器都有哪些，说下 G1 的应用场景，平时你是如何搭配使用垃圾回收器的？</li></ul>]]></content>
      
      
      <categories>
          
          <category> review </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>手写LRU</title>
      <link href="/2021/05/13/2021-5-13-shou-xie-lru/"/>
      <url>/2021/05/13/2021-5-13-shou-xie-lru/</url>
      
        <content type="html"><![CDATA[<h2 id="LRU算法简介"><a href="#LRU算法简介" class="headerlink" title="LRU算法简介"></a>LRU算法简介</h2><p>LRU是Least Recently Used的缩写，即最近最少使用，是一种常用的页面置换算法，选择最近最久未使用的数据予以淘汰。</p><h2 id="LRU的思想"><a href="#LRU的思想" class="headerlink" title="LRU的思想"></a>LRU的思想</h2><ul><li>写操作+读操作时间复杂度都需要为O(1)</li><li>必须要有顺序之分，区分最近使用的和很久没有使用的数据排序</li></ul><p>查找快、插入快、删除快，（O(1)时间复杂度）且还需要先后排序———-&gt;什么样的数据结构可以满足这个问题？</p><p>LRU的算法核心是哈希链表</p><h2 id="巧用LinkedHashMap完成LRU算法"><a href="#巧用LinkedHashMap完成LRU算法" class="headerlink" title="巧用LinkedHashMap完成LRU算法"></a>巧用LinkedHashMap完成LRU算法</h2><pre><code>import java.util.LinkedHashMap;import java.util.Map;public class LRUDemo extends LinkedHashMap&lt;Integer, Integer&gt; {    private int capacity;    public LRUDemo(int initialCapacity) {        super(initialCapacity, 0.75F, true);        this.capacity = initialCapacity;    }    @Override    protected boolean removeEldestEntry(Map.Entry&lt;Integer, Integer&gt; eldest) {        return super.size() &gt; capacity;    }    public static void main(String[] args) {        LRUDemo lruDemo = new LRUDemo(3);        lruDemo.put(1, 1);        lruDemo.put(2, 2);        lruDemo.put(3, 3);        System.out.println(lruDemo.keySet());        lruDemo.put(4, 4);        System.out.println(lruDemo.keySet());        lruDemo.put(3, 3);        lruDemo.put(3, 3);        System.out.println(lruDemo.keySet());    }}</code></pre><p>输出结果：</p><pre><code>[1, 2, 3][2, 3, 4][2, 4, 3]</code></pre>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis分布式锁</title>
      <link href="/2021/05/12/2021-05-12-redis-fen-bu-shi-suo/"/>
      <url>/2021/05/12/2021-05-12-redis-fen-bu-shi-suo/</url>
      
        <content type="html"><![CDATA[<h2 id="redis分布式锁01"><a href="#redis分布式锁01" class="headerlink" title="redis分布式锁01"></a>redis分布式锁01</h2><p>JVM层面的加锁，单机版的锁</p><pre><code>class X {    private final ReentrantLock lock = new ReentrantLock();    // ...    public void m() {        lock.lock();  // block until condition holds//不见不散        try {            // ... method body        } finally {            lock.unlock()        }    }    public void m2() {           if(lock.tryLock(timeout, unit)){//过时不候            try {            // ... method body            } finally {                lock.unlock()            }           }else{            // perform alternative actions        }   } }</code></pre><h2 id="redis分布式锁02"><a href="#redis分布式锁02" class="headerlink" title="redis分布式锁02"></a>redis分布式锁02</h2><p>如果分布式部署下，会有问题。</p><p>Redis具有极高的性能，且其命令对分布式锁支持友好，借助SET命令即可实现加锁处理。</p><ul><li>EX seconds – Set the specified expire time, in seconds.</li><li>PX milliseconds – Set the specified expire time, in milliseconds.</li><li>NX – Only set the key if it does not already exist.</li><li>XX – Only set the key if it already exist.</li></ul><pre><code>public static final String REDIS_LOCK = &quot;redis_lock&quot;;@Autowiredprivate StringRedisTemplate stringRedisTemplate;public void m(){    String value = UUID.randomUUID().toString() + Thread.currentThread().getName();    Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(REDIS_LOCK, value);    if(!flag) {        return &quot;抢锁失败&quot;;    }    ...//业务逻辑    stringRedisTemplate.delete(REDIS_LOCK);}</code></pre><h2 id="redis分布式锁03"><a href="#redis分布式锁03" class="headerlink" title="redis分布式锁03"></a>redis分布式锁03</h2><p>出现异常的话，可能无法释放锁，必须要在代码层面finally释放锁。</p><pre><code>public static final String REDIS_LOCK = &quot;redis_lock&quot;;@Autowiredprivate StringRedisTemplate stringRedisTemplate;public void m(){    String value = UUID.randomUUID().toString() + Thread.currentThread().getName();    try{        Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(REDIS_LOCK, value);           if(!flag) {            return &quot;抢锁失败&quot;;        }        ...//业务逻辑    }finally{        stringRedisTemplate.delete(REDIS_LOCK);       }}</code></pre><p>另一个问题：分布式中获取锁的机器挂了，代码层面根本没有走到finally这块，没办法保证解锁，这个key没有被删除，需要加入一个过期时间限定key。</p><pre><code>public static final String REDIS_LOCK = &quot;redis_lock&quot;;@Autowiredprivate StringRedisTemplate stringRedisTemplate;public void m(){    String value = UUID.randomUUID().toString() + Thread.currentThread().getName();    try{        Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(REDIS_LOCK, value);        //设定时间        stringRedisTemplate.expire(REDIS_LOCK, 10L, TimeUnit.SECONDS);           if(!flag) {            return &quot;抢锁失败&quot;;        }        ...//业务逻辑    }finally{        stringRedisTemplate.delete(REDIS_LOCK);       }}</code></pre><h2 id="redis分布式锁04"><a href="#redis分布式锁04" class="headerlink" title="redis分布式锁04"></a>redis分布式锁04</h2><p>设置key+过期时间分开了，必须要合并成一行具备原子性。</p><pre><code>public static final String REDIS_LOCK = &quot;redis_lock&quot;;@Autowiredprivate StringRedisTemplate stringRedisTemplate;public void m(){    String value = UUID.randomUUID().toString() + Thread.currentThread().getName();    try{        Boolean flag = stringRedisTemplate.opsForValue()//使用另一个带有设置超时操作的方法            .setIfAbsent(REDIS_LOCK, value, 10L, TimeUnit.SECONDS);        //设定时间        //stringRedisTemplate.expire(REDIS_LOCK, 10L, TimeUnit.SECONDS);           if(!flag) {            return &quot;抢锁失败&quot;;        }        ...//业务逻辑    }finally{        stringRedisTemplate.delete(REDIS_LOCK);       }}</code></pre><p>另一个新问题：张冠李戴，删除了别人的锁。可能A线程业务复杂，耗时比较长，然后超时释放锁了，这时B线程获取到了锁，A线程执行到delete锁过程，然后把B的lock删除了，这时其他线程又可以获取锁了，B又删除其他线程的lock…炸了啊</p><p><img src="/2021/05/12/2021-05-12-redis-fen-bu-shi-suo/8491f7f7a87dcc888d60141f6d662e1b.png" alt></p><p>解决方法：只能自己删除自己的，不许动别人的。</p><pre><code>public static final String REDIS_LOCK = &quot;redis_lock&quot;;@Autowiredprivate StringRedisTemplate stringRedisTemplate;public void m(){    String value = UUID.randomUUID().toString() + Thread.currentThread().getName();    try{        Boolean flag = stringRedisTemplate.opsForValue()//使用另一个带有设置超时操作的方法            .setIfAbsent(REDIS_LOCK, value, 10L, TimeUnit.SECONDS);        //设定时间        //stringRedisTemplate.expire(REDIS_LOCK, 10L, TimeUnit.SECONDS);           if(!flag) {            return &quot;抢锁失败&quot;;        }        ...//业务逻辑    }finally{        if(stringRedisTemplate.opsForValue().get(REDIS_LOCK).equals(value)) {            stringRedisTemplate.delete(REDIS_LOCK);        }    }}</code></pre><h2 id="redis分布式锁05"><a href="#redis分布式锁05" class="headerlink" title="redis分布式锁05"></a>redis分布式锁05</h2><p>finally块的判断 + del删除操作不是原子性的</p><p>解决办法：</p><ul><li>用lua脚本</li><li>用redis自身的事务</li></ul><p>Redis事务介绍：</p><ul><li>Redis的事条是通过MULTI，EXEC，DISCARD和WATCH这四个命令来完成。</li><li>Redis的单个命令都是原子性的，所以这里确保事务性的对象是命令集合。</li><li>Redis将命令集合序列化并确保处于一事务的命令集合连续且不被打断的执行。</li><li>Redis不支持回滚的操作。</li></ul><p><img src="/2021/05/12/2021-05-12-redis-fen-bu-shi-suo/Image1.png" alt></p><pre><code>public static final String REDIS_LOCK = &quot;redis_lock&quot;;@Autowiredprivate StringRedisTemplate stringRedisTemplate;public void m(){    String value = UUID.randomUUID().toString() + Thread.currentThread().getName();    try{        Boolean flag = stringRedisTemplate.opsForValue()//使用另一个带有设置超时操作的方法            .setIfAbsent(REDIS_LOCK, value, 10L, TimeUnit.SECONDS);        //设定时间        //stringRedisTemplate.expire(REDIS_LOCK, 10L, TimeUnit.SECONDS);           if(!flag) {            return &quot;抢锁失败&quot;;        }        ...//业务逻辑    }finally{        while(true){            stringRedisTemplate.watch(REDIS_LOCK);            if(stringRedisTemplate.opsForValue().get(REDIS_LOCK).equalsIgnoreCase(value)){                stringRedisTemplate.setEnableTransactionSupport(true);                stringRedisTemplate.multi();                stringRedisTemplate.delete(REDIS_LOCK);                List&lt;Object&gt; list = stringRedisTemplate.exec();                if (list == null) {                    continue;                }            }            stringRedisTemplate.unwatch();            break;        }     }}</code></pre><h2 id="redis分布式锁06"><a href="#redis分布式锁06" class="headerlink" title="redis分布式锁06"></a>redis分布式锁06</h2><p>确保RedisLock过期时间大于业务执行时间的问题</p><p>Redission有看门狗机制（watch dog）,每隔10秒钟看一下，如果还持有锁，延长生存时间。</p><h2 id="redis分布式锁07"><a href="#redis分布式锁07" class="headerlink" title="redis分布式锁07"></a>redis分布式锁07</h2><pre><code>public static final String REDIS_LOCK = &quot;REDIS_LOCK&quot;;@Autowiredprivate Redisson redisson;@GetMapping(&quot;/doSomething&quot;)public String doSomething(){    RLock redissonLock = redisson.getLock(REDIS_LOCK);    redissonLock.lock();    try {        //doSomething    }finally {        //添加后，更保险        if(redissonLock.isLocked() &amp;&amp; redissonLock.isHeldByCurrentThread()) {            redissonLock.unlock();        }    }}</code></pre><h2 id="redis分布式锁总结"><a href="#redis分布式锁总结" class="headerlink" title="redis分布式锁总结"></a>redis分布式锁总结</h2><ul><li>synchronized, reentranLock单机版oK，上分布式不行</li><li>取消单机锁，上Redis分布式锁setnx，如果不存在才能设置</li><li>只加了锁，没有释放锁，出异常的话，可能无法释放锁,必须要在代码层面finally释放锁</li><li>分布式中获取锁的机器挂了，代码层面根本没有走到finally这块，没办法保证解锁，这个key没有被删除，需要加入一个过期时间限定key。</li><li>设置key+过期时间分开了，必须要合并成一行具备原子性。nx ex</li><li>必须规定只能自己删除自己的锁,不能把别人的锁删除了，防止张冠李戴，1删2，2删3</li><li>finally块的判断 + del删除操作不是原子性的</li><li>确保RedisLock过期时间大于业务执行时间的问题，看门狗机制</li></ul>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式锁 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring-循环依赖</title>
      <link href="/2021/05/11/2021-05-11-spring-xun-huan-yi-lai/"/>
      <url>/2021/05/11/2021-05-11-spring-xun-huan-yi-lai/</url>
      
        <content type="html"><![CDATA[<h2 id="spring循环依赖前置知识"><a href="#spring循环依赖前置知识" class="headerlink" title="spring循环依赖前置知识"></a>spring循环依赖前置知识</h2><p>3个Map和四大方法，总体相关对象</p><p><img src="/2021/05/11/2021-05-11-spring-xun-huan-yi-lai/fe2c0b589930bbf2988a374c2644d941.png" alt></p><ul><li>第一层singletonObjects存放的是已经初始化好了的Bean</li><li>第二层earlySingletonObjects存放的是实例化了，但是未初始化的Bean</li><li>第三层singletonFactories存放的是FactoryBean。假如A类实现了FactoryBean,那么依赖注入的时候不是A类，而是A类产生的Bean</li></ul><pre><code>package org.springframework.beans.factory.support;...public class DefaultSingletonBeanRegistry extends SimpleAliasRegistry implements SingletonBeanRegistry {    ...    /**     单例对象的缓存:bean名称—bean实例，即:所谓的单例池。    表示已经经历了完整生命周期的Bean对象    第一级缓存    */    private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;&gt;(256);    /**    早期的单例对象的高速缓存: bean名称—bean实例。    表示 Bean的生命周期还没走完（Bean的属性还未填充）就把这个 Bean存入该缓存中也就是实例化但未初始化的 bean放入该缓存里    第二级缓存    */    private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories = new HashMap&lt;&gt;(16);    /**    单例工厂的高速缓存:bean名称—ObjectFactory    表示存放生成 bean的工厂    第三级缓存    */    private final Map&lt;String, Object&gt; earlySingletonObjects = new HashMap&lt;&gt;(16);    ...}</code></pre><p>A / B两对象在三级缓存中的迁移说明</p><ol><li>A创建过程中需要B，于是A将自己放到三级缓里面，去实例化B。</li><li>B实例化的时候发现需要A，于是B先查一级缓存，没有，再查二级缓存，还是没有，再查三级缓存，找到了A然后把三级缓存里面的这个A放到二级缓存里面，并删除三级缓存里面的A。</li><li>B顺利初始化完毕，将自己放到一级缓存里面（此时B里面的A依然是创建中状态)，然后回来接着创建A，此时B已经创建结束，直接从一级缓存里面拿到B，然后完成创建，并将A自己放到一级缓存里面。</li></ol><h2 id="Spring解决循环依赖过程："><a href="#Spring解决循环依赖过程：" class="headerlink" title="Spring解决循环依赖过程："></a>Spring解决循环依赖过程：</h2><ol><li>调用doGetBean()方法，想要获取beanA，于是调用getSingleton()方法从缓存中查找beanA</li><li>在getSingleton()方法中，从一级缓存中查找，没有，返回null</li><li>doGetBean()方法中获取到的beanA为null，于是走对应的处理逻辑，调用getSingleton()的重载方法（参数为ObjectFactory的)</li><li>在getSingleton()方法中，先将beanA_name添加到一个集合中，用于标记该bean正在创建中。然后回调匿名内部类的creatBean方法</li><li>进入AbstractAutowireCapableBeanFactory#ndoCreateBean，先反射调用构造器创建出beanA的实例，然后判断:是否为单例、是否允许提前暴露引用(对于单例一般为true)、是否正在创建中（即是否在第四步的集合中）。判断为true则将beanA添加到【三级缓存】中</li><li>对beanA进行属性填充，此时检测到beanA依赖于beanB，于是开始查找beanB</li><li>调用doGetBean()方法，和上面beanA的过程一样，到缓存中查找beanB，没有则创建，然后给beanB填充属性</li><li>此时 beanB依赖于beanA，调用getSingleton()获取beanA，依次从一级、二级、三级缓存中找，此时从三级缓存中获取到beanA的创建工厂，通过创建工厂获取到singletonObject，此时这个singletonObject指向的就是上面在doCreateBean()方法中实例化的beanA</li><li>这样beanB就获取到了beanA的依赖，于是beanB顺利完成实例化，并将beanA从三级缓存移动到二级缓存中</li><li>随后beanA继续他的属性填充工作，此时也获取到了beanB，beanA也随之完成了创建，回到getsingleton()方法中继续向下执行，将beanA从二级缓存移动到一级缓存中</li></ol><h2 id="spring循环依赖结论"><a href="#spring循环依赖结论" class="headerlink" title="spring循环依赖结论"></a>spring循环依赖结论</h2><p>Spring创建 bean主要分为两个步骤，创建原始bean对象，接着去填充对象属性和初始化</p><p>每次创建 bean之前，我们都会从缓存中查下有没有该bean，因为是单例，只能有一个</p><p>当我们创建 beanA的原始对象后，并把它放到三级缓存中，接下来就该填充对象属性了，这时候发现依赖了beanB，接着就又去创建beanB，同样的流程，创建完beanB填充属性时又发现它依赖了beanA又是同样的流程.</p><p>不同的是：这时候可以在三级缓存中查到刚放进去的原始对象beanA.所以不需要继续创建，用它注入 beanB，完成 beanB的创建</p><p>既然 beanB创建好了，所以 beanA就可以完成填充属性的步骤了，接着执行剩下的逻辑，闭环完成</p><p><img src="/2021/05/11/2021-05-11-spring-xun-huan-yi-lai/b34a877c4363380c0243acbb69b4b834.png" alt></p><p>Spring为了解决单例的循坏依赖问题，使用了三级缓存：</p><ul><li>其中一级缓存为单例池(singletonObjects)。</li><li>二级缓存为提前曝光对象(earlySingletonObjects)。</li><li>三级缓存为提前曝光对象工厂(singletonFactories) 。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring-AOP</title>
      <link href="/2021/05/10/2021-05-10-spring-aop/"/>
      <url>/2021/05/10/2021-05-10-spring-aop/</url>
      
        <content type="html"><![CDATA[<h2 id="AOP常用注解："><a href="#AOP常用注解：" class="headerlink" title="AOP常用注解："></a>AOP常用注解：</h2><ul><li>@Before 前置通知：目标方法之前执行</li><li>@After 后置通知：目标方法之后执行（始终执行)</li><li>@AfterReturning 返回后通知：执行方法结束前执行(异常不执行)</li><li>@AfterThrowing 异常通知：出现异常时候执行</li><li>@Around 环绕通知：环绕目标方法执行</li></ul><h2 id="Spring的AOP顺序"><a href="#Spring的AOP顺序" class="headerlink" title="Spring的AOP顺序"></a>Spring的AOP顺序</h2><h3 id="spring4下的aop测试案例"><a href="#spring4下的aop测试案例" class="headerlink" title="spring4下的aop测试案例"></a>spring4下的aop测试案例</h3><p>spring4的springboot版本为1.X version</p><pre><code>import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.After;import org.aspectj.lang.annotation.AfterReturning;import org.aspectj.lang.annotation.AfterThrowing;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;import org.springframework.stereotype.Component;@Aspect@Componentpublic class MyAspect {    @Before(&quot;execution(public int com.lun.interview.service.CalcServiceImpl.*(..))&quot;)    public void beforeNotify() {        System.out.println(&quot;********@Before我是前置通知&quot;);    }    @After(&quot;execution(public int com.lun.interview.service.CalcServiceImpl.*(..))&quot;)    public void afterNotify() {        System.out.println(&quot;********@After我是后置通知&quot;);    }    @AfterReturning(&quot;execution(public int com.lun.interview.service.CalcServiceImpl.*(..))&quot;)    public void afterReturningNotify() {        System.out.println(&quot;********@AfterReturning我是返回后通知&quot;);    }    @AfterThrowing(&quot; execution(public int com.lun.interview.service.CalcServiceImpl.*(..))&quot;)    public void afterThrowingNotify() {        System.out.println(&quot;********@AfterThrowing我是异常通知&quot;);    }    @Around(&quot; execution(public int com.lun.interview.service.CalcServiceImpl.*(..))&quot;)    public Object around(ProceedingJoinPoint proceedingJoinPoint) throws Throwable {        Object retvalue = null;        System.out.println(&quot;我是环绕通知之前AAA&quot;);        retvalue = proceedingJoinPoint.proceed();        System.out.println(&quot;我是环绕通知之后BBB&quot;);        return retvalue ;    }}</code></pre><pre><code>import org.springframework.stereotype.Service;@Servicepublic class CalcServiceImpl implements CalcService {    @Override    public int div(int x, int y) {        int result = x / y;        System.out.println(&quot;===&gt;CalcServiceImpl被调用，计算结果为：&quot; + result);        return result;    }}</code></pre><p>正常执行输出结果：</p><pre><code>Spring Verision : 4.3.13.RELEASE, Sring Boot Version : 1.5.9.RELEASE.我是环绕通知之前AAA********@Before我是前置通知===&gt;CalcServiceImpl被调用，计算结果为：5我是环绕通知之后BBB********@After我是后置通知********@AfterReturning我是返回后通知</code></pre><p>如果接口中抛出异常，输出结果：</p><pre><code>Spring Verision : 4.3.13.RELEASE, Sring Boot Version : 1.5.9.RELEASE.我是环绕通知之前AAA********@Before我是前置通知********@After我是后置通知********@AfterThrowing我是异常通知java.lang.ArithmeticException: / by zero    at com.lun.interview.service.CalcServiceImpl.div(CalcServiceImpl.java:10)    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    at java.lang.reflect.Method.invoke(Method.java:498)    ...</code></pre><p>AOP执行顺序：</p><ul><li>正常情况下：@Before前置通知—–&gt;@After后置通知—–&gt;@AfterRunning正常返回</li><li>异常情况下：@Before前置通知—–&gt;@After后置通知—–&gt;@AfterThrowing方法异常</li></ul><h3 id="spring5下的aop测试"><a href="#spring5下的aop测试" class="headerlink" title="spring5下的aop测试"></a>spring5下的aop测试</h3><p>spring5的springboot版本为2.X version</p><p>正常执行输出结果:</p><pre><code>Spring Verision : 5.2.8.RELEASE, Sring Boot Version : 2.3.3.RELEASE.我是环绕通知之前AAA********@Before我是前置通知===&gt;CalcServiceImpl被调用，计算结果为：5********@AfterReturning我是返回后通知********@After我是后置通知我是环绕通知之后BBB</code></pre><p>异常执行输出结果:</p><pre><code>Spring Verision : 5.2.8.RELEASE, Sring Boot Version : 2.3.3.RELEASE.我是环绕通知之前AAA********@Before我是前置通知********@AfterThrowing我是异常通知********@After我是后置通知java.lang.ArithmeticException: / by zero    at com.lun.interview.service.CalcServiceImpl.div(CalcServiceImpl.java:10)    at com.lun.interview.service.CalcServiceImpl$$FastClassBySpringCGLIB$$355acbc4.invoke(&lt;generated&gt;)    at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)    at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:771)</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="Spring4"><a href="#Spring4" class="headerlink" title="Spring4"></a>Spring4</h3><p>正常执行输出结果：</p><pre><code>Spring Verision : 4.3.13.RELEASE, Sring Boot Version : 1.5.9.RELEASE.我是环绕通知之前AAA********@Before我是前置通知===&gt;CalcServiceImpl被调用，计算结果为：5我是环绕通知之后BBB********@After我是后置通知********@AfterReturning我是返回后通知</code></pre><p>如果接口中抛出异常，输出结果：</p><pre><code>Spring Verision : 4.3.13.RELEASE, Sring Boot Version : 1.5.9.RELEASE.我是环绕通知之前AAA********@Before我是前置通知********@After我是后置通知********@AfterThrowing我是异常通知java.lang.ArithmeticException: / by zero    at com.lun.interview.service.CalcServiceImpl.div(CalcServiceImpl.java:10)    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    at java.lang.reflect.Method.invoke(Method.java:498)    ...</code></pre><h3 id="Spring5"><a href="#Spring5" class="headerlink" title="Spring5"></a>Spring5</h3><p>spring5的springboot版本为2.X version</p><p>正常执行输出结果:</p><pre><code>Spring Verision : 5.2.8.RELEASE, Sring Boot Version : 2.3.3.RELEASE.我是环绕通知之前AAA********@Before我是前置通知===&gt;CalcServiceImpl被调用，计算结果为：5********@AfterReturning我是返回后通知********@After我是后置通知我是环绕通知之后BBB</code></pre><p>异常执行输出结果:</p><pre><code>Spring Verision : 5.2.8.RELEASE, Sring Boot Version : 2.3.3.RELEASE.我是环绕通知之前AAA********@Before我是前置通知********@AfterThrowing我是异常通知********@After我是后置通知java.lang.ArithmeticException: / by zero    at com.lun.interview.service.CalcServiceImpl.div(CalcServiceImpl.java:10)    at com.lun.interview.service.CalcServiceImpl$$FastClassBySpringCGLIB$$355acbc4.invoke(&lt;generated&gt;)    at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)    at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:771)</code></pre>]]></content>
      
      
      <categories>
          
          <category> Spring </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入并发学习-AQS</title>
      <link href="/2021/05/09/2021-05-09-bing-fa-aqs/"/>
      <url>/2021/05/09/2021-05-09-bing-fa-aqs/</url>
      
        <content type="html"><![CDATA[<h2 id="AQS理论初步"><a href="#AQS理论初步" class="headerlink" title="AQS理论初步"></a>AQS理论初步</h2><p>AbstractQueuedSynchronizer 是一个抽象队列同步器。</p><p>AQS是用来构建锁或者其它同步器组件的重量级基础框架及整个JUC体系的基石，通过内置的FIFO队列来完成资源获取线程的排队工作，并通过一个int类型变量表示持有锁的状态。</p><p><img src="/2021/05/09/2021-05-09-bing-fa-aqs/47fad3563d427ffe5058343de85e3e05.png" alt></p><p>CLH：Craig、Landin and Hagersten队列，是一个单向链表，AQS中的队列是CLH变体的虚拟双向队列FIFO。</p><h2 id="AQS能干嘛"><a href="#AQS能干嘛" class="headerlink" title="AQS能干嘛"></a>AQS能干嘛</h2><p><img src="/2021/05/09/2021-05-09-bing-fa-aqs/a7434c955af6273241cd44746d19db00.png" alt></p><p><img src="/2021/05/09/2021-05-09-bing-fa-aqs/7ecbe7fbeecd5d5e20b2d8de59e8a033.png" alt></p><p>进一步理解锁和同步器的关系</p><ul><li>锁:面向锁的使用者 - 定义了程序员和锁交互的使用层APl，隐藏了实现细节，你调用即可</li><li>同步器:面向锁的实现者 - 简化了锁的实现，屏蔽了同步状态管理、阻塞线程排队和通知、唤醒机制等。</li></ul><p>加锁会导致阻塞 - 有阻塞就需要排队，实现排队必然需要有某种形式的队列来进行管理</p><p>抢到资源的线程直接使用处理业务逻辑，抢不到资源的必然涉及一种排队等候机制。抢占资源失败的线程继续去等待(类似银行业务办理窗口都满了，暂时没有受理窗口的顾客只能去候客区排队等候)，但等候线程仍然保留获取锁的可能且获取锁流程仍在继续(候客区的顾客也在等着叫号，轮到了再去受理窗口办理业务)。</p><p>如果共享资源被占用，就需要一定的阻塞等待唤醒机制来保证锁分配。这个机制主要用的是CLH队列的变体实现的，将暂时获取不到锁的线程加入到队列中，这个队列就是AQS的抽象表现。它将请求共享资源的线程封装成队列的结点(Node)，通过CAS、自旋以及LockSupportpark)的方式，维护state变量的状态，使并发达到同步的控制效果。</p><h2 id="AQS源码"><a href="#AQS源码" class="headerlink" title="AQS源码"></a>AQS源码</h2><p>有阻塞就需要排队，实现排队必然需要队列</p><p>AQS使用一个volatile的int类型的成员变量来表示同步状态，通过内置的FIFO队列来完成资源获取的排队工作将每条要去抢占资源的线程封装成一个Node，节点来实现锁的分配，通过CAS完成对State值的修改。</p><pre><code>public abstract class AbstractQueuedSynchronizer    extends AbstractOwnableSynchronizer    implements java.io.Serializable {    private static final long serialVersionUID = 7373984972572414691L;     * Creates a new {@code AbstractQueuedSynchronizer} instance    protected AbstractQueuedSynchronizer() { }     * Wait queue node class.    static final class Node {     * Head of the wait queue, lazily initialized.  Except for    private transient volatile Node head;     * Tail of the wait queue, lazily initialized.  Modified only via    private transient volatile Node tail;     * The synchronization state.    private volatile int state;     * Returns the current value of synchronization state.    protected final int getState() {     * Sets the value of synchronization state.    protected final void setState(int newState) {     * Atomically sets synchronization state to the given updated    protected final boolean compareAndSetState(int expect, int update) {    ...}         </code></pre><h3 id="AQS的int变量-AQS的同步状态state成员变量"><a href="#AQS的int变量-AQS的同步状态state成员变量" class="headerlink" title="AQS的int变量 - AQS的同步状态state成员变量"></a>AQS的int变量 - AQS的同步状态state成员变量</h3><pre><code>public abstract class AbstractQueuedSynchronizer    extends AbstractOwnableSynchronizer    implements java.io.Serializable {    ...     * The synchronization state.    private volatile int state;    ...}</code></pre><p>state成员变量可以理解为银行办理业务的受理窗口状态。零就是没人，自由状态可以办理，大于等于1，有人占用窗口，等着去。</p><h3 id="AQS的CLH队列"><a href="#AQS的CLH队列" class="headerlink" title="AQS的CLH队列"></a>AQS的CLH队列</h3><p>FIFO双向队列</p><h3 id="AbstractQueuedSynchronizer内部类Node源码"><a href="#AbstractQueuedSynchronizer内部类Node源码" class="headerlink" title="AbstractQueuedSynchronizer内部类Node源码"></a>AbstractQueuedSynchronizer内部类Node源码</h3><pre><code>public abstract class AbstractQueuedSynchronizer    extends AbstractOwnableSynchronizer    implements java.io.Serializable {    ...     * Creates a new {@code AbstractQueuedSynchronizer} instance    protected AbstractQueuedSynchronizer() { }     * Wait queue node class.    static final class Node {        //表示线程以共享的模式等待锁        /** Marker to indicate a node is waiting in shared mode */        static final Node SHARED = new Node();        //表示线程正在以独占的方式等待锁        /** Marker to indicate a node is waiting in exclusive mode */        static final Node EXCLUSIVE = null;        //线程被取消了        /** waitStatus value to indicate thread has cancelled */        static final int CANCELLED =  1;        //后继线程需要唤醒        /** waitStatus value to indicate successor&#39;s thread needs unparking */        static final int SIGNAL    = -1;        //等待condition唤醒        /** waitStatus value to indicate thread is waiting on condition */        static final int CONDITION = -2;        //共享式同步状态获取将会无条件地传播下去        * waitStatus value to indicate the next acquireShared should             static final int PROPAGATE = -3;        //当前节点在队列中的状态（重点）        //队列中每个排队的个体就是一个Node        //初始为0，状态上面的几种         * Status field, taking on only the values:        volatile int waitStatus;        //前驱节点（重点）         * Link to predecessor node that current node/thread relies on        volatile Node prev;        //后继节点（重点）         * Link to the successor node that the current node/thread        volatile Node next;        //表示处于该节点的线程         * The thread that enqueued this node.  Initialized on        volatile Thread thread;        //指向下一个处于CONDITION状态的节点         * Link to next node waiting on condition, or the special        Node nextWaiter;         * Returns true if node is waiting in shared mode.        final boolean isShared() {        //返回前驱节点，没有的话抛出npe         * Returns previous node, or throws NullPointerException if null.        final Node predecessor() throws NullPointerException {        Node() {    // Used to establish initial head or SHARED marker        Node(Thread thread, Node mode) {     // Used by addWaiter        Node(Thread thread, int waitStatus) { // Used by Condition    }    ...}</code></pre><p>AQS同步队列的基本结构</p><p><img src="/2021/05/09/2021-05-09-bing-fa-aqs/0efad5e335d52c8487af4e80680d251e.png" alt></p><h2 id="AQS源码深度解读"><a href="#AQS源码深度解读" class="headerlink" title="AQS源码深度解读"></a>AQS源码深度解读</h2><p>从ReentrantLock开始解读AQS,Lock接口的实现类，基本都是通过聚合了一个队列同步器的子类完成线程访问控制的。</p><p><img src="/2021/05/09/2021-05-09-bing-fa-aqs/f991a259579532ca2528a66aa5c8ac60.png" alt></p><p><img src="/2021/05/09/2021-05-09-bing-fa-aqs/Image1.png" alt></p><pre><code> * A reentrant mutual exclusion {@link Lock} with the same basicpublic class ReentrantLock implements Lock, java.io.Serializable {    private static final long serialVersionUID = 7373984872572414699L;    /** Synchronizer providing all implementation mechanics */    private final Sync sync;     * Base of synchronization control for this lock. Subclassed    abstract static class Sync extends AbstractQueuedSynchronizer {     * Sync object for non-fair locks    static final class NonfairSync extends Sync {     * Sync object for fair locks    static final class FairSync extends Sync {     * Creates an instance of {@code ReentrantLock}.    public ReentrantLock() {        sync = new NonfairSync();    }     * Creates an instance of {@code ReentrantLock} with the    public ReentrantLock(boolean fair) {        sync = fair ? new FairSync() : new NonfairSync();    }     * Acquires the lock.    public void lock() {        sync.lock();//&lt;------------------------注意，我们从这里入手    }    * Attempts to release this lock.    public void unlock() {        sync.release(1);    }    ...}</code></pre><p>从最简单的lock方法开始看看公平和非公平，先浏览下AbstractQueuedSynchronizer，FairSync，NonfairSync类的源码。</p><pre><code>public abstract class AbstractQueuedSynchronizer    extends AbstractOwnableSynchronizer    implements java.io.Serializable {    ...     * Acquires in exclusive mode, ignoring interrupts.  Implemented    public final void acquire(int arg) {//公平锁或非公平锁都会调用这方法        if (!tryAcquire(arg) &amp;&amp;//0.            acquireQueued(addWaiter(Node.EXCLUSIVE), arg))//1. 2.            selfInterrupt();//3.    }    //0.    * Attempts to acquire in exclusive mode. This method should query    protected boolean tryAcquire(int arg) {//取决于公平锁或非公平锁的实现        throw new UnsupportedOperationException();    }    //1.    * Acquires in exclusive uninterruptible mode for thread already in    final boolean acquireQueued(final Node node, int arg) {        boolean failed = true;        try {            boolean interrupted = false;            for (;;) {                final Node p = node.predecessor();                if (p == head &amp;&amp; tryAcquire(arg)) {                    setHead(node);                    p.next = null; // help GC                    failed = false;                    return interrupted;                }                if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;                    parkAndCheckInterrupt())                    interrupted = true;            }        } finally {            if (failed)                cancelAcquire(node);        }    }    //2.    * Creates and enqueues node for current thread and given mode.    private Node addWaiter(Node mode) {        Node node = new Node(Thread.currentThread(), mode);        // Try the fast path of enq; backup to full enq on failure        Node pred = tail;        if (pred != null) {            node.prev = pred;            if (compareAndSetTail(pred, node)) {                pred.next = node;                return node;            }        }        enq(node);        return node;    }    //3.    static void selfInterrupt() {        Thread.currentThread().interrupt();    }    //这个方法将会被公平锁的tryAcquire()调用    * Queries whether any threads have been waiting to acquire longer    public final boolean hasQueuedPredecessors() {        // The correctness of this depends on head being initialized        // before tail and on head.next being accurate if the current        // thread is first in queue.        Node t = tail; // Read fields in reverse initialization order        Node h = head;        Node s;        return h != t &amp;&amp;            ((s = h.next) == null || s.thread != Thread.currentThread());    }    ...         }</code></pre><p><img src="/2021/05/09/2021-05-09-bing-fa-aqs/15641ff20adcaf0e8d72b5dcec7d2da1.png" alt></p><p>可以明显看出公平锁与非公平锁的lock()方法唯一的区别就在于tryAcquire公平锁在获取同步状态时多了一个限制条件：hasQueuedPredecessors()</p><p>hasQueuedPredecessors()中判断了是否需要排队，导致公平锁和非公平锁的差异如下：</p><ul><li>公平锁：公平锁讲究先来先到，线程在获取锁时，如果这个锁的等待队列中已经有线程在等待，那么当前线程就会进入等待队列中;</li><li>非公平锁：不管是否有等待队列，如果可以获取锁，则立刻占有锁对象。也就是说队列的第一个排队线程在unpark()，之后还是需要竞争锁（存在线程竞争的情况下)</li></ul><p>整个ReentrantLock 的加锁lock()过程，可以分为三个阶段：</p><ul><li>尝试加锁；</li><li>加锁失败，线程入队列；</li><li>线程入队列后，进入阻赛状态。</li></ul><h2 id="结合例子看ReentranLock源码比较容易理解"><a href="#结合例子看ReentranLock源码比较容易理解" class="headerlink" title="结合例子看ReentranLock源码比较容易理解"></a>结合例子看ReentranLock源码比较容易理解</h2><pre><code>import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.ReentrantLock;public class AQSDemo {    public static void main(String[] args) {        ReentrantLock lock = new ReentrantLock();        //带入一个银行办理业务的案例来模拟我们的AQs 如何进行线程的管理和通知唤醒机制        //3个线程模拟3个来银行网点，受理窗口办理业务的顾客        //A顾客就是第一个顾客，此时受理窗口没有任何人，A可以直接去办理        new Thread(()-&gt;{            lock.lock();            try {                System.out.println(Thread.currentThread().getName() + &quot; come in.&quot;);                try {                    TimeUnit.SECONDS.sleep(5);//模拟办理业务时间                } catch (InterruptedException e) {                    e.printStackTrace();                }            } finally {                lock.unlock();            }        }, &quot;Thread A&quot;).start();        //第2个顾客，第2个线程----&gt;，由于受理业务的窗口只有一个(只能一个线程持有锁)，此代B只能等待，        //进入候客区        new Thread(()-&gt;{            lock.lock();            try {                System.out.println(Thread.currentThread().getName() + &quot; come in.&quot;);            } finally {                lock.unlock();            }        }, &quot;Thread B&quot;).start();        //第3个顾客，第3个线程----&gt;，由于受理业务的窗口只有一个(只能一个线程持有锁)，此代C只能等待，        //进入候客区        new Thread(()-&gt;{            lock.lock();            try {                System.out.println(Thread.currentThread().getName() + &quot; come in.&quot;);            } finally {                lock.unlock();            }        }, &quot;Thread C&quot;).start();    }}</code></pre><p>程序初始状态方便理解图</p><p><img src="/2021/05/09/2021-05-09-bing-fa-aqs/c43af638d95a7c6d45219b9da17fad64.png" alt></p><h3 id="线程A先执行"><a href="#线程A先执行" class="headerlink" title="线程A先执行"></a>线程A先执行</h3><p>启动程序，首先是运行线程A，ReentrantLock默认是选用非公平锁。</p><pre><code>public class ReentrantLock implements Lock, java.io.Serializable {    ...    * Acquires the lock.    public void lock() {        sync.lock();//&lt;------------------------注意，我们从这里入手,一开始将线程A的    }    abstract static class Sync extends AbstractQueuedSynchronizer {        ...        //被NonfairSync的tryAcquire()调用        final boolean nonfairTryAcquire(int acquires) {            final Thread current = Thread.currentThread();            int c = getState();            if (c == 0) {                if (compareAndSetState(0, acquires)) {                    setExclusiveOwnerThread(current);                    return true;                }            }            else if (current == getExclusiveOwnerThread()) {                int nextc = c + acquires;                if (nextc &lt; 0) // overflow                    throw new Error(&quot;Maximum lock count exceeded&quot;);                setState(nextc);                return true;            }            return false;        }        ...    }    //非公平锁    static final class NonfairSync extends Sync {        private static final long serialVersionUID = 7316153563782823691L;        /**         * Performs lock.  Try immediate barge, backing up to normal         * acquire on failure.         */        final void lock() {//&lt;----线程A的lock.lock()调用该方法            if (compareAndSetState(0, 1))//AbstractQueuedSynchronizer的方法,刚开始这方法返回true                setExclusiveOwnerThread(Thread.currentThread());//设置独占的所有者线程，显然一开始是线程A            else                acquire(1);//稍后紧接着的线程B将会调用该方法。        }        //acquire()将会间接调用该方法        protected final boolean tryAcquire(int acquires) {            return nonfairTryAcquire(acquires);//调用父类Sync的nonfairTryAcquire()        }    }    ...}</code></pre><pre><code>public abstract class AbstractQueuedSynchronizer    extends AbstractOwnableSynchronizer    implements java.io.Serializable {    /**     * The synchronization state.     */    private volatile int state;    //线程A将state设为1，下图红色椭圆区    /*Atomically sets synchronization state to the given updated value     if the current state value equals the expected value.    This operation has memory semantics of a volatile read and write.*/    protected final boolean compareAndSetState(int expect, int update) {        // See below for intrinsics setup to support this        return unsafe.compareAndSwapInt(this, stateOffset, expect, update);    }}</code></pre><p>线程A开始办业务了。</p><p><img src="/2021/05/09/2021-05-09-bing-fa-aqs/096f574353f3965eed5996e8a6962f94.png" alt></p><h3 id="轮到线程B运行"><a href="#轮到线程B运行" class="headerlink" title="轮到线程B运行"></a>轮到线程B运行</h3><pre><code>public class ReentrantLock implements Lock, java.io.Serializable {    ...    * Acquires the lock.    public void lock() {        sync.lock();//&lt;------------------------注意，我们从这里入手,线程B的执行这    }    //非公平锁    static final class NonfairSync extends Sync {        private static final long serialVersionUID = 7316153563782823691L;        /**         * Performs lock.  Try immediate barge, backing up to normal         * acquire on failure.         */        final void lock() {//&lt;-------------------------线程B的lock.lock()调用该方法            if (compareAndSetState(0, 1))//这是预定线程A还在工作，这里返回false                setExclusiveOwnerThread(Thread.currentThread());//            else                acquire(1);//线程B将会调用该方法，该方法在AbstractQueuedSynchronizer，                           //它会调用本类的tryAcquire()方法        }        //acquire()将会间接调用该方法        protected final boolean tryAcquire(int acquires) {            return nonfairTryAcquire(acquires);//调用父类Sync的nonfairTryAcquire()        }    }    //非公平锁与公平锁的公共父类     * Base of synchronization control for this lock. Subclassed    abstract static class Sync extends AbstractQueuedSynchronizer {        //acquire()将会间接调用该方法        ...        final boolean nonfairTryAcquire(int acquires) {            final Thread current = Thread.currentThread();//这里是线程B            int c = getState();//线程A还在工作，c=&gt;1            if (c == 0) {//false                if (compareAndSetState(0, acquires)) {                    setExclusiveOwnerThread(current);                    return true;                }            }            else if (current == getExclusiveOwnerThread()) {//(线程B == 线程A) =&gt; false                int nextc = c + acquires;//+1                if (nextc &lt; 0) // overflow                    throw new Error(&quot;Maximum lock count exceeded&quot;);                setState(nextc);                return true;            }            return false;//最终返回false        }         ...    }    ...}</code></pre><pre><code>public abstract class AbstractQueuedSynchronizer    extends AbstractOwnableSynchronizer    implements java.io.Serializable {    ...     * Acquires in exclusive mode, ignoring interrupts.  Implemented    public final void acquire(int arg) {        if (!tryAcquire(arg) &amp;&amp;//线程B调用非公平锁的tryAcquire(), 最终返回false，加上!,也就是true,也就是还要执行下面两行语句            acquireQueued(addWaiter(Node.EXCLUSIVE), arg))            selfInterrupt();    }    ...}</code></pre><pre><code>public abstract class AbstractQueuedSynchronizer    extends AbstractOwnableSynchronizer    implements java.io.Serializable {    ...     * Acquires in exclusive mode, ignoring interrupts.  Implemented    public final void acquire(int arg) {        if (!tryAcquire(arg) &amp;&amp;//线程B调用非公平锁的tryAcquire(), 最终返回false，加上!,也就是true,也就是还要执行下面两行语句            acquireQueued(addWaiter(Node.EXCLUSIVE), arg))//线程B加入等待队列            selfInterrupt();//    }    private Node addWaiter(Node mode) {        Node node = new Node(Thread.currentThread(), mode);        // Try the fast path of enq; backup to full enq on failure        Node pred = tail;        if (pred != null) {//根据上面一句注释，本语句块的意义是将新节点快速添加至队尾            node.prev = pred;            if (compareAndSetTail(pred, node)) {                pred.next = node;                return node;            }        }        enq(node);//快速添加至队尾失败，则用这方法调用（可能链表为空，才调用该方法）        return node;    }    //Inserts node into queue, initializing if necessary.    private Node enq(final Node node) {        for (;;) {            Node t = tail;            if (t == null) { // Must initialize                if (compareAndSetHead(new Node()))//插入一个哨兵节点（或称傀儡节点）                    tail = head;            } else {                node.prev = t;                if (compareAndSetTail(t, node)) {//真正插入我们需要的节点，也就是包含线程B引用的节点                    t.next = node;                    return t;                }            }        }    }    //CAS head field. Used only by enq.    private final boolean compareAndSetHead(Node update) {        return unsafe.compareAndSwapObject(this, headOffset, null, update);    }    //CAS tail field. Used only by enq.    private final boolean compareAndSetTail(Node expect, Node update) {        return unsafe.compareAndSwapObject(this, tailOffset, expect, update);    }    ...}</code></pre><p><img src="/2021/05/09/2021-05-09-bing-fa-aqs/5b6e78a76dbef77aa015d3f2b5baf927.png" alt></p><p>线程B加入等待队列。</p><h3 id="假设线程A第二次lock-lock"><a href="#假设线程A第二次lock-lock" class="headerlink" title="假设线程A第二次lock.lock()"></a>假设线程A第二次lock.lock()</h3><p>假设线程B，C还没启动，正在工作线程A重新尝试获得锁，也就是调用lock.lock()多一次</p><pre><code>    //非公平锁与公平锁的公共父类fa     * Base of synchronization control for this lock. Subclassed    abstract static class Sync extends AbstractQueuedSynchronizer {        ...        final boolean nonfairTryAcquire(int acquires) {            final Thread current = Thread.currentThread();//这里是线程A            int c = getState();//线程A还在工作，c=&gt;1；如果线程A恰好运行到在这工作完了，c=&gt;0，这时它又要申请锁的话            if (c == 0) {//线程A正在工作为false;如果线程A恰好工作完，c=&gt;0，这时它又要申请锁的话,则为true                if (compareAndSetState(0, acquires)) {//线程A重新获得锁                    setExclusiveOwnerThread(current);//这里相当于NonfairSync.lock()另一重设置吧！                    return true;                }            }            else if (current == getExclusiveOwnerThread()) {//(线程A == 线程A) =&gt; true                int nextc = c + acquires;//1+1=&gt;nextc=2                if (nextc &lt; 0) // overflow                    throw new Error(&quot;Maximum lock count exceeded&quot;);                setState(nextc);//state=2,说明要unlock多两次吧（现在盲猜）                return true;//返回true            }            return false;        }         ...    }</code></pre><h3 id="线程C执行"><a href="#线程C执行" class="headerlink" title="线程C执行"></a>线程C执行</h3><p>线程A依然工作，线程C如线程B那样炮制加入等待队列。</p><p><img src="/2021/05/09/2021-05-09-bing-fa-aqs/e1bf33ccbf81dacd7601677bda136f02.png" alt></p><p>双向链表中，第一个节点为虚节点(也叫哨兵节点)，其实并不存储任何信息，只是占位。真正的第一个有数据的节点，是从第二个节点开始的。</p><h3 id="线程B加入等待队列后，继续执行"><a href="#线程B加入等待队列后，继续执行" class="headerlink" title="线程B加入等待队列后，继续执行"></a>线程B加入等待队列后，继续执行</h3><pre><code>public abstract class AbstractQueuedSynchronizer    extends AbstractOwnableSynchronizer    implements java.io.Serializable {    ...     * Acquires in exclusive mode, ignoring interrupts.  Implemented    public final void acquire(int arg) {        if (!tryAcquire(arg) &amp;&amp;//线程B调用非公平锁的tryAcquire(), 最终返回false，加上!,也就是true,也就是还要执行下面两行语句            //线程B加入等待队列，acquireQueued现在论述&lt;--------------------------            acquireQueued(addWaiter(Node.EXCLUSIVE), arg))            selfInterrupt();//    }    //Acquires in exclusive uninterruptible mode for thread already inqueue.     //Used by condition wait methods as well as acquire.    //    //return true if interrupted while waiting    final boolean acquireQueued(final Node node, int arg) {        boolean failed = true;        try {            boolean interrupted = false;            for (;;) {                final Node p = node.predecessor();//1.返回前一节点，对与线程B来说，p也就是傀儡节点                //p==head为true，tryAcquire()方法                //假设线程A正在工作,现在线程B只能等待，所以tryAcquire(arg)返回false，下面的if语块不执行                //                //第二次循环，假设线程A继续正在工作，下面的if语块还是不执行                if (p == head &amp;&amp; tryAcquire(arg)) {                    setHead(node);                    p.next = null; // help GC                    failed = false;                    return interrupted;                }                //请移步到2.处的shouldParkAfterFailedAcquire()解说。第一次返回false, 下一次（第二次）循环                //第二次循环，shouldParkAfterFailedAcquire()返回true，执行parkAndCheckInterrupt()                if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;                     //4.                     parkAndCheckInterrupt())                    interrupted = true;            }        } finally {            if (failed)                cancelAcquire(node);        }    }    static final class Node {        ...        //1.返回前一节点        final Node predecessor() throws NullPointerException {            Node p = prev;            if (p == null)                throw new NullPointerException();            else                return p;        }        ...    }    //2.     private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) {        int ws = pred.waitStatus;//此时pred指向傀儡节点，它的waitStatus为0        //Node.SIGNAL为-1，跳过        //第二次调用，ws为-1，条件成立，返回true        if (ws == Node.SIGNAL)//-1            /*             * This node has already set status asking a release             * to signal it, so it can safely park.             */            return true;        if (ws &gt; 0) {//跳过            /*             * Predecessor was cancelled. Skip over predecessors and             * indicate retry.             */            do {                node.prev = pred = pred.prev;            } while (pred.waitStatus &gt; 0);            pred.next = node;        } else {            /*             * waitStatus must be 0 or PROPAGATE.  Indicate that we             * need a signal, but don&#39;t park yet.  Caller will need to             * retry to make sure it cannot acquire before parking.             */            //3. 傀儡节点的WaitStatus设置为-1//下图红圈            compareAndSetWaitStatus(pred, ws, Node.SIGNAL);        }        return false;//第一次返回    }    /**     * CAS waitStatus field of a node.     */    //3.    private static final boolean compareAndSetWaitStatus(Node node,                                                         int expect,                                                         int update) {        return unsafe.compareAndSwapInt(node, waitStatusOffset,                                        expect, update);    }    /**     * Convenience method to park and then check if interrupted     *     * @return {@code true} if interrupted     */    //4.    private final boolean parkAndCheckInterrupt() {        //前段章节讲述的LockSupport，this指的是NonfairSync对象，        //这意味着真正阻塞线程B，同样地阻塞了线程C        LockSupport.park(this);//线程B,C在此处暂停了运行&lt;-------------------------        return Thread.interrupted();    }}</code></pre><p><img src="/2021/05/09/2021-05-09-bing-fa-aqs/741c9ead0810b5b6d9c48915e5355aaa.png" alt></p><p>图中的傀儡节点的waitStatus由0变为-1（Node.SIGNAL）。</p><h3 id="线程A执行完毕"><a href="#线程A执行完毕" class="headerlink" title="线程A执行完毕"></a>线程A执行完毕</h3><p>假设线程A工作结束，调用unLock()，释放锁占用。</p><pre><code>public class ReentrantLock implements Lock, java.io.Serializable {    private final Sync sync;    abstract static class Sync extends AbstractQueuedSynchronizer {        ...        //2.unlock()间接调用本方法，releases传入1        protected final boolean tryRelease(int releases) {            //3.            int c = getState() - releases;//c为0            //4.            if (Thread.currentThread() != getExclusiveOwnerThread())                throw new IllegalMonitorStateException();            boolean free = false;            if (c == 0) {//c为0，条件为ture，执行if语句块                free = true;                //5.                setExclusiveOwnerThread(null);            }            //6.            setState(c);            return free;//最后返回true        }        ...    }    static final class NonfairSync extends Sync {...}    public ReentrantLock() {        sync = new NonfairSync();//我们使用的非公平锁    }                        //注意！注意！注意！    public void unlock() {//&lt;----------从这开始，假设线程A工作结束，调用unLock()，释放锁占用        //1.        sync.release(1);//在AbstractQueuedSynchronizer类定义    }    ...}</code></pre><pre><code>public abstract class AbstractQueuedSynchronizer    extends AbstractOwnableSynchronizer    implements java.io.Serializable {    ...    //1.    public final boolean release(int arg) {        //2.        if (tryRelease(arg)) {//该方法看子类NonfairSync实现，最后返回true            Node h = head;//返回傀儡节点            if (h != null &amp;&amp; h.waitStatus != 0)//傀儡节点非空，且状态为-1，条件为true，执行if语句                //7.                unparkSuccessor(h);            return true;        }        return false;//返回true,false都无所谓了，unlock方法只是简单调用release方法，对返回结果没要求    }    /**     * The synchronization state.     */    private volatile int state;    //3.    protected final int getState() {        return state;    }    //6.    protected final void setState(int newState) {        state = newState;    }    //7. Wakes up node&#39;s successor, if one exists.    //传入傀儡节点    private void unparkSuccessor(Node node) {        /*         * If status is negative (i.e., possibly needing signal) try         * to clear in anticipation of signalling.  It is OK if this         * fails or if status is changed by waiting thread.         */        int ws = node.waitStatus;//傀儡节点waitStatus为-1        if (ws &lt; 0)//ws为-1，条件成立，执行if语块            compareAndSetWaitStatus(node, ws, 0);//8.将傀儡节点waitStatus由-1变为0        /*         * Thread to unpark is held in successor, which is normally         * just the next node.  But if cancelled or apparently null,         * traverse backwards from tail to find the actual         * non-cancelled successor.         */        Node s = node.next;//傀儡节点的下一节点,也就是带有线程B的节点        if (s == null || s.waitStatus &gt; 0) {//s非空，s.waitStatus非0，条件为false，不执行if语块            s = null;            for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev)                if (t.waitStatus &lt;= 0)                    s = t;        }        if (s != null)//s非空，条件为true，不执行if语块            LockSupport.unpark(s.thread);//唤醒线程B。运行到这里，线程A的工作基本告一段落了。    }    //8.    private static final boolean compareAndSetWaitStatus(Node node,                                                         int expect,                                                         int update) {        return unsafe.compareAndSwapInt(node, waitStatusOffset,                                        expect, update);    }}</code></pre><pre><code>public abstract class AbstractOwnableSynchronizer    implements java.io.Serializable {    ...    protected AbstractOwnableSynchronizer() { }    private transient Thread exclusiveOwnerThread;    //5.    protected final void setExclusiveOwnerThread(Thread thread) {        exclusiveOwnerThread = thread;    }    //4.    protected final Thread getExclusiveOwnerThread() {        return exclusiveOwnerThread;    }}</code></pre><p>线程A结束工作，调用unlock()的tryRelease()后的状态，state由1变为0，exclusiveOwnerThread由线程A变为null。</p><p><img src="/2021/05/09/2021-05-09-bing-fa-aqs/4834c2b2372914e35d9e6a40d8618b25.png" alt></p><h3 id="线程B被唤醒"><a href="#线程B被唤醒" class="headerlink" title="线程B被唤醒"></a>线程B被唤醒</h3><p>线程B被唤醒，即从原先park()的方法继续运行</p><pre><code>public abstract class AbstractQueuedSynchronizer    extends AbstractOwnableSynchronizer    implements java.io.Serializable {     private final boolean parkAndCheckInterrupt() {        LockSupport.park(this);//线程B从阻塞到非阻塞，继续执行(这里第一步)        return Thread.interrupted();//线程B没有被中断，返回false    }    ...    //Acquires in exclusive uninterruptible mode for thread already inqueue.     //Used by condition wait methods as well as acquire.    //    //return true if interrupted while waiting    final boolean acquireQueued(final Node node, int arg) {        boolean failed = true;        try {            boolean interrupted = false;            for (;;) {                final Node p = node.predecessor();//线程B所在的节点的是傀儡节点                //傀儡节点是头节点                //tryAcquire()返回true,线程B成功上位(这里第三步)                if (p == head &amp;&amp; tryAcquire(arg)) {                    setHead(node);//1.将附带线程B的节点的变成新的傀儡节点                    p.next = null; // help GC//置空原傀儡指针与新的傀儡节点之间的前后驱指针，方便GC回收                    failed = false;                    return interrupted;//返回false，跳到2.acquire()                }                if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;                     //唤醒线程B继续工作，parkAndCheckInterrupt()返回false                    //if语块不执行，跳到下一循环(这里第二步)                    parkAndCheckInterrupt())//&lt;---------------------------------唤醒线程在这里继续运行(这里第二步)                    interrupted = true;            }        } finally {            if (failed)                cancelAcquire(node);        }    }    //1.     private void setHead(Node node) {        head = node;        node.thread = null;        node.prev = null;    }    //2.    * Acquires in exclusive mode, ignoring interrupts.  Implemented    public final void acquire(int arg) {        if (!tryAcquire(arg) &amp;&amp;            //acquireQueued()返回fasle,条件为false，if语块不执行，acquire()返回            //也就是说，线程B成功获得锁，可以展开线程B自己的工作了。(这里第四步)            acquireQueued(addWaiter(Node.EXCLUSIVE), arg))            selfInterrupt();//    }}</code></pre><p>最后，线程B成功获取锁。</p><p><img src="/2021/05/09/2021-05-09-bing-fa-aqs/aa60bf0eb603162facee9e8503f59c9a.png" alt></p><h2 id="总结辣，太复杂了"><a href="#总结辣，太复杂了" class="headerlink" title="总结辣，太复杂了"></a>总结辣，太复杂了</h2><h3 id="加锁和挂起线程流程："><a href="#加锁和挂起线程流程：" class="headerlink" title="加锁和挂起线程流程："></a>加锁和挂起线程流程：</h3><ol><li><p>首先调用nonfairTryAcquire()获取<code>state</code>的值。如果为0，利用<code>CAS</code>尝试抢占锁compareAndSetState(0, 1)，且设置对象独占锁线程为当前线程setExclusiveOwnerThread(Thread.currentThread())；如果不为0则说明当前对象的锁已经被其他线程所占有，接着判断占有锁的线程是否为当前线程，如果是则累加<code>state</code>值。获取锁成功。</p></li><li><p>如果获取锁失败：首先执行<code>addWaiter(Node.EXCLUSIVE)</code>，加入到一个<code>FIFO</code>   CLH等待队列中。<code>addWaiter()</code>方法执行完后，会返回当前线程创建的节点信息。继续往后执行<code>acquireQueued(addWaiter(Node.EXCLUSIVE), arg)</code>逻辑。<code>acquireQueued()</code>这个方法会先判断当前传入的<code>Node</code>对应的前置节点是否为<code>head</code>，如果是则尝试加锁。加锁成功过则将当前节点设置为<code>head</code>节点，然后空置之前的<code>head</code>节点，方便后续被垃圾回收掉。如果加锁失败或者<code>Node</code>的前置节点不是<code>head</code>节点，就会通过<code>shouldParkAfterFailedAcquire</code>方法 将前一个节点的<code>waitStatus</code>变为了<code>SIGNAL=-1</code>，最后执行<code>parkAndChecknIterrupt</code>方法，调用<code>LockSupport.park()</code>挂起当前线程，等着其他线程释放锁来唤醒它。</p></li></ol><h3 id="释放锁和唤醒线程流程："><a href="#释放锁和唤醒线程流程：" class="headerlink" title="释放锁和唤醒线程流程："></a>释放锁和唤醒线程流程：</h3><ol><li>首先是拥有锁的线程释放锁，释放锁后会唤醒<code>head</code>节点的后置节点。</li><li>首先会执行<code>tryRelease()</code>方法，执行完<code>ReentrantLock.tryRelease()</code>后，<code>state</code>被设置成0，Lock对象的独占锁被设置为null。如果<code>tryRelease</code>执行成功，则继续判断<code>head</code>节点的<code>waitStatus</code>是否为0，就会执行<code>unparkSuccessor()</code>方法来唤醒<code>head</code>的后置节点。<code>unparkSuccessor()</code>主要是将<code>head</code>节点的<code>waitStatus</code>设置为0，然后解除<code>head</code>节点<code>next</code>的指向，使<code>head</code>节点空置，等待着被垃圾回收。<code>LockSupport.unpark(傀儡节点后的下一个节点)</code>，然后唤醒的节点就可以执行加锁流程了。</li></ol><h3 id="总的来说"><a href="#总的来说" class="headerlink" title="总的来说"></a>总的来说</h3><ul><li>CountDownLatch、CyclicBarriar、Semaphore信号量、ReentrantLock、ReentrantReadWriteLock读写锁底层都有Sync类继承AQS类，都属于AQS框架。</li><li>AQS中 维护了一个volatile int state（代表共享资源）和一个FIFO双向等待CLH队列（多线程争用资源被阻塞时会进入此队列）。</li><li>另外state的操作都是通过CAS来保证其并发修改的安全性。</li><li>这里volatile能够保证多线程下的可见性，当state=1则代表当前对象锁已经被占有，其他线程来加锁时则会失败，加锁失败的线程会被放入一个FIFO的等待队列中，比列会被UNSAFE.park()操作挂起，等待其他获取锁的线程释放锁才能够被唤醒。</li><li>当线程执行完逻辑后，会释放同步状态，释放后会唤醒其后继节点，使其再次尝试获取同步状态。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA并发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入并发学习-LockSupport</title>
      <link href="/2021/05/08/2021-05-08-bing-fa-locksupport/"/>
      <url>/2021/05/08/2021-05-08-bing-fa-locksupport/</url>
      
        <content type="html"><![CDATA[<h2 id="LockSupport"><a href="#LockSupport" class="headerlink" title="LockSupport"></a>LockSupport</h2><p>LockSupport是用来创建锁和其他同步类的基本线程阻塞原语。</p><p>LockSupport中的park()和 unpark()的作用分别是阻塞线程和解除阻塞线程。</p><p>总之，比wait/notify，await/signal更强。</p><p>3种让线程等待和唤醒的方法</p><ul><li>使用Object中的wait()方法让线程等待，使用object中的notify()方法唤醒线程</li><li>使用JUC包中Condition的await()方法让线程等待，使用signal()方法唤醒线程</li><li>LockSupport类可以阻塞当前线程以及唤醒指定被阻塞的线程</li></ul><h2 id="waitNotify限制"><a href="#waitNotify限制" class="headerlink" title="waitNotify限制"></a>waitNotify限制</h2><p>Object类中的wait和notify方法实现线程等待和唤醒</p><pre><code>public class WaitDemo {    static Object lock = new Object();    public static void main(String[] args) throws InterruptedException {        new Thread(()-&gt;{            synchronized (lock) {                System.out.println(Thread.currentThread().getName()+&quot; come in.&quot;);                try {                    lock.wait();                } catch (Exception e) {                    e.printStackTrace();                }            }            System.out.println(Thread.currentThread().getName()+&quot; 换醒.&quot;);        }, &quot;Thread A&quot;).start();        Thread.sleep(2000);        new Thread(()-&gt;{            synchronized (lock) {                lock.notify();                System.out.println(Thread.currentThread().getName()+&quot; 通知.&quot;);                try {                    Thread.sleep(3000);                } catch (InterruptedException e) {                    e.printStackTrace();                }            }        }, &quot;Thread B&quot;).start();    }}</code></pre><p>wait和notify方法必须要在同步块或者方法里面且成对出现使用，否则会抛出java.lang.IllegalMonitorStateException。</p><p>调用顺序要先wait后notify才OK。</p><h2 id="awaitSignal限制"><a href="#awaitSignal限制" class="headerlink" title="awaitSignal限制"></a>awaitSignal限制</h2><p>Condition接口中的await后signal方法实现线程的等待和唤醒，与Object类中的wait和notify方法实现线程等待和唤醒类似。</p><pre><code>public class ConditionAwaitSignalDemo {    public static void main(String[] args) {        ReentrantLock lock = new ReentrantLock();        Condition condition = lock.newCondition();        new Thread(()-&gt;{            try {                System.out.println(Thread.currentThread().getName()+&quot; come in.&quot;);                lock.lock();                condition.await();            } catch (InterruptedException e) {                e.printStackTrace();            } finally {                lock.unlock();            }            System.out.println(Thread.currentThread().getName()+&quot; 换醒.&quot;);        },&quot;Thread A&quot;).start();        new Thread(()-&gt;{            try {                lock.lock();                condition.signal();                System.out.println(Thread.currentThread().getName()+&quot; 通知.&quot;);            }finally {                lock.unlock();            }        },&quot;Thread B&quot;).start();    }} </code></pre><p>await和signal方法必须要在同步块或者方法里面且成对出现使用，否则会抛出java.lang.IllegalMonitorStateException。</p><p>调用顺序要先await后signal才OK。</p><h2 id="LockSupport方法介绍"><a href="#LockSupport方法介绍" class="headerlink" title="LockSupport方法介绍"></a>LockSupport方法介绍</h2><p>传统的synchronized和Lock实现等待唤醒通知的约束</p><ul><li>线程先要获得并持有锁，必须在锁块(synchronized或lock)中</li><li>必须要先等待后唤醒，线程才能够被唤醒</li></ul><p>LockSupport是用来创建锁和其他同步类的基本线程阻塞原语。</p><p>LockSupport类使用了一种名为Permit（许可）的概念来做到阻塞和唤醒线程的功能，每个线程都有一个许可（permit），permit只有两个值1和零，默认是零。</p><p>可以把许可看成是一种(0.1)信号量（Semaphore），但与Semaphore不同的是，许可的累加上限是1。</p><p>通过park()和unpark(thread)方法来实现阻塞和唤醒线程的操作</p><p>park()/park(Object blocker) - 阻塞当前线程阻塞传入的具体线程</p><pre><code>public class LockSupport {    ...    public static void park() {        UNSAFE.park(false, 0L);    }    public static void park(Object blocker) {        Thread t = Thread.currentThread();        setBlocker(t, blocker);        UNSAFE.park(false, 0L);        setBlocker(t, null);    }    ...}</code></pre><p>permit默认是0，所以一开始调用park()方法，当前线程就会阻塞，直到别的线程将当前线程的permit设置为1时，park方法会被唤醒，然后会将permit再次设置为0并返回。</p><p>unpark(Thread thread) - 唤醒处于阻塞状态的指定线程</p><pre><code>public class LockSupport {    ...    public static void unpark(Thread thread) {        if (thread != null)            UNSAFE.unpark(thread);    }    ...}</code></pre><p>调用unpark(thread)方法后，就会将thread线程的许可permit设置成1（注意多次调用unpark方法，不会累加，pemit值还是1）会自动唤醒thead线程，即之前阻塞中的LockSupport.park()方法会立即返回。</p><h2 id="LockSupport案例解析"><a href="#LockSupport案例解析" class="headerlink" title="LockSupport案例解析"></a>LockSupport案例解析</h2><pre><code>public class LockSupportDemo {    public static void main(String[] args) throws InterruptedException {        Thread a = new Thread(() -&gt; {            System.out.println(Thread.currentThread().getName() + &quot;come in&quot;);            LockSupport.park();            System.out.println(Thread.currentThread().getName() + &quot;unpark&quot;);        }, &quot;a&quot;);        a.start();        Thread.sleep(3000);        new Thread(() -&gt; {            System.out.println(Thread.currentThread().getName() + &quot;come in&quot;);            LockSupport.unpark(a);        }, &quot;b&quot;).start();    }}</code></pre><p>LockSupport是用来创建锁和共他同步类的基本线程阻塞原语。</p><p>LockSuport是一个线程阻塞工具类，所有的方法都是静态方法，可以让线程在任意位置阻塞，阻寨之后也有对应的唤醒方法。归根结底，LockSupport调用的Unsafe中的native代码。</p><p>LockSupport提供park()和unpark()方法实现阻塞线程和解除线程阻塞的过程</p><p>LockSupport和每个使用它的线程都有一个许可(permit)关联。permit相当于1，0的开关，默认是0，调用一次unpark就加1变成1，调用一次park会消费permit，也就是将1变成0，同时park立即返回。</p><p>如再次调用park会变成阻塞(因为permit为零了会阻塞在这里，一直到permit变为1)，这时调用unpark会把permit置为1。每个线程都有一个相关的permit, permit最多只有一个，重复调用unpark也不会积累凭证。</p><p>形象的理解: 线程阻塞需要消耗凭证(permit)，这个凭证最多只有1个。当调用park方法时,如果有凭证，则会直接消耗掉这个凭证然后正常退出。如果无凭证，就必须阻塞等待凭证可用。而unpark则相反，它会增加一个凭证，但凭证最多只能有1个，累加无放。</p><h3 id="为什么可以先唤醒线程后阻塞线程？"><a href="#为什么可以先唤醒线程后阻塞线程？" class="headerlink" title="为什么可以先唤醒线程后阻塞线程？"></a>为什么可以先唤醒线程后阻塞线程？</h3><p>因为unpark获得了一个凭证，之后再调用park方法，就可以名正言顺的凭证消费，故不会阻塞。</p><h3 id="为什么唤醒两次后阻塞两次，但最终结果还会阻塞线程？"><a href="#为什么唤醒两次后阻塞两次，但最终结果还会阻塞线程？" class="headerlink" title="为什么唤醒两次后阻塞两次，但最终结果还会阻塞线程？"></a>为什么唤醒两次后阻塞两次，但最终结果还会阻塞线程？</h3><p>因为凭证的数量最多为1（不能累加），连续调用两次 unpark和调用一次 unpark效果一样，只会增加一个凭证；而调用两次park却需要消费两个凭证，证不够，不能放行。</p>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA并发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>字符串-intern</title>
      <link href="/2021/05/07/2021-05-07-zi-fu-chuan-intern/"/>
      <url>/2021/05/07/2021-05-07-zi-fu-chuan-intern/</url>
      
        <content type="html"><![CDATA[<p>String:intern()是一个本地方法，它的作用是如果字符串常量池中已经包含一个等于此String对象的字符串，则返回代表池中这个字符串的String对象的引用；否则，会将此String对象包含的字符串添加到常量池中，并且返回此String对象的引用。</p><pre><code>public class InternDemo {    public static void main(String[] args) {        String a = new StringBuffer(&quot;chen&quot;).append(&quot;fei&quot;).toString();        System.out.println(a);        System.out.println(a.intern());        System.out.println(a == a.intern());        System.out.println(&quot;----------&quot;);        String b = new StringBuffer(&quot;ja&quot;).append(&quot;va&quot;).toString();        System.out.println(b);        System.out.println(b.intern());        System.out.println(b == b.intern());    }}</code></pre><p>输出结果：</p><pre><code>chenfeichenfeitrue----------javajavafalse</code></pre><p>按照代码结果，Java字符串答案为false必然是两个不同的java，那另外一个java字符串如何加载进来的?</p><p>有一个初始化的Java字符串（JDK出娘胎自带的），在加载sun.misc.Version这个类的时候进入常量池。</p><p>System代码解析 System -&gt; initializeSystemClass() -&gt; Version</p><pre><code>package java.lang;public final class System {    /* register the natives via the static initializer.     *     * VM will invoke the initializeSystemClass method to complete     * the initialization for this class separated from clinit.     * Note that to use properties set by the VM, see the constraints     * described in the initializeSystemClass method.     */    private static native void registerNatives();    static {        registerNatives();    }    //本地方法registerNatives()将会调用initializeSystemClass()    private static void initializeSystemClass() {        ...        sun.misc.Version.init();        ...    }    ...}</code></pre><pre><code>package sun.misc;//反编译后的代码public class Version {    private static final String launcher_name = &quot;java&quot;;    ...}</code></pre><p>类加载器和rt.jar - 根加载器提前部署加载rt.jar</p><p>sun.misc.Version类会在JDK类库的初始化过程中被加载并初始化，而在初始化时它需要对静态常量字段根据指定的常量值(ConstantValue〉做默认初始化，此时被sun.misc.Version.launcher静态常量字段所引用的”java”字符串字面量就被intern到HotSpot VM的字符串常量池——StringTable里了。</p>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入并发学习-线程池</title>
      <link href="/2021/05/06/2021-05-06-bing-fa-xian-cheng-chi/"/>
      <url>/2021/05/06/2021-05-06-bing-fa-xian-cheng-chi/</url>
      
        <content type="html"><![CDATA[<h2 id="Callable接口"><a href="#Callable接口" class="headerlink" title="Callable接口"></a>Callable接口</h2><p>Callable接口，是一种让线程执行完成后，能够返回结果的。</p><pre><code>import java.util.concurrent.Callable;import java.util.concurrent.ExecutionException;import java.util.concurrent.FutureTask;import java.util.concurrent.TimeUnit;class MyThread implements Callable&lt;Integer&gt; {    @Override    public Integer call() throws Exception {        System.out.println(Thread.currentThread().getName() + &quot; come in Callable&quot;);        TimeUnit.SECONDS.sleep(2);        return 1024;    }}public class CallableDemo {    public static void main(String[] args) throws InterruptedException, ExecutionException {        FutureTask&lt;Integer&gt; futureTask = new FutureTask&lt;&gt;(new MyThread());        new Thread(futureTask, &quot;A&quot;).start();        new Thread(futureTask, &quot;B&quot;).start();//多个线程执行 一个FutureTask的时候，只会计算一次        // 输出FutureTask的返回值        System.out.println(&quot;result FutureTask &quot; + futureTask.get());    }}</code></pre><h2 id="线程池使用及优势"><a href="#线程池使用及优势" class="headerlink" title="线程池使用及优势"></a>线程池使用及优势</h2><p>线程池做的工作主要是控制运行的线程的数量，处理过程中将任务放入队列，然后在线程创建后启动这些任务，如果线程数量超过了最大数量超出数量的线程排队等候，等其它线程执行完毕，再从队列中取出任务来执行。</p><p>它的主要特点为：线程复用，控制最大并发数，管理线程。</p><ul><li>降低资源消耗。通过重复利用己创建的线程降低线程创建和销毁造成的消耗。</li><li>提高响应速度。当任务到达时，任务可以不需要的等到线程创建就能立即执行。</li><li>提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。</li></ul><h2 id="线程池3个常用方式"><a href="#线程池3个常用方式" class="headerlink" title="线程池3个常用方式"></a>线程池3个常用方式</h2><p>Java中的线程池是通过Executor框架实现的，该框架中用到了Executor，Executors，ExecutorService，ThreadPoolExecutor这几个类。</p><p><img src="/2021/05/06/2021-05-06-bing-fa-xian-cheng-chi/947b9a063ddd04eaa276b03b38c45ec6.png" alt></p><h3 id="Executors-newSingleThreadExecutor"><a href="#Executors-newSingleThreadExecutor" class="headerlink" title="Executors.newSingleThreadExecutor()"></a>Executors.newSingleThreadExecutor()</h3><pre><code>public static ExecutorService newSingleThreadExecutor() {    return new FinalizableDelegatedExecutorService        (new ThreadPoolExecutor(1, 1,                                0L, TimeUnit.MILLISECONDS,                                new LinkedBlockingQueue&lt;Runnable&gt;()));}</code></pre><ul><li>创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序执行。</li><li>newSingleThreadExecutor将corePoolSize和maximumPoolSize都设置为1，它使用的LinkedBlockingQueue。</li></ul><h3 id="Executors-newFixedThreadPool-int"><a href="#Executors-newFixedThreadPool-int" class="headerlink" title="Executors.newFixedThreadPool(int)"></a>Executors.newFixedThreadPool(int)</h3><pre><code>public static ExecutorService newFixedThreadPool(int nThreads) {    return new ThreadPoolExecutor(nThreads, nThreads,                                  0L, TimeUnit.MILLISECONDS,                                  new LinkedBlockingQueue&lt;Runnable&gt;());}</code></pre><ul><li>创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。</li><li>newFixedThreadPool创建的线程池corePoolSize和maximumPoolSize值是相等的，它使用的LinkedBlockingQueue。</li></ul><h3 id="Executors-newCachedThreadPool"><a href="#Executors-newCachedThreadPool" class="headerlink" title="Executors.newCachedThreadPool()"></a>Executors.newCachedThreadPool()</h3><pre><code>public static ExecutorService newCachedThreadPool() {    return new ThreadPoolExecutor(0, Integer.MAX_VALUE,                                  60L, TimeUnit.SECONDS,                                  new SynchronousQueue&lt;Runnable&gt;());}</code></pre><ul><li>创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。</li><li>newCachedThreadPool将corePoolSize设置为0，将maximumPoolSize设置为Integer.MAX_VALUE，使用的SynchronousQueue，也就是说来了任务就创建线程运行，当线程空闲超过60秒，就销毁线程。</li></ul><pre><code>import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class ThreadPoolDemo {    public static void main(String[] args) {        // 一池5个处理线程（用池化技术，一定要记得关闭）//        ExecutorService threadPool = Executors.newFixedThreadPool(5);        // 创建一个只有一个线程的线程池//        ExecutorService threadPool = Executors.newSingleThreadExecutor();        // 创建一个拥有N个线程的线程池，根据调度创建合适的线程        ExecutorService threadPool = Executors.newCachedThreadPool();        // 模拟10个用户来办理业务，每个用户就是一个来自外部请求线程        try {            // 循环十次，模拟业务办理，让5个线程处理这10个请求            for (int i = 0; i &lt; 10; i++) {                final int tempInt = i;                threadPool.execute(() -&gt; {                    System.out.println(Thread.currentThread().getName() + &quot;\t 给用户:&quot; + tempInt + &quot; 办理业务&quot;);                });            }        } catch (Exception e) {            e.printStackTrace();        } finally {            threadPool.shutdown();        }    }}</code></pre><h2 id="线程池7大参数入门简介"><a href="#线程池7大参数入门简介" class="headerlink" title="线程池7大参数入门简介"></a>线程池7大参数入门简介</h2><pre><code>public class ThreadPoolExecutor extends AbstractExecutorService {    ...    public ThreadPoolExecutor(int corePoolSize,                              int maximumPoolSize,                              long keepAliveTime,                              TimeUnit unit,                              BlockingQueue&lt;Runnable&gt; workQueue,                              ThreadFactory threadFactory,                              RejectedExecutionHandler handler) {        if (corePoolSize &lt; 0 ||            maximumPoolSize &lt;= 0 ||            maximumPoolSize &lt; corePoolSize ||            keepAliveTime &lt; 0)            throw new IllegalArgumentException();        if (workQueue == null || threadFactory == null || handler == null)            throw new NullPointerException();        this.acc = System.getSecurityManager() == null ?                null :                AccessController.getContext();        this.corePoolSize = corePoolSize;        this.maximumPoolSize = maximumPoolSize;        this.workQueue = workQueue;        this.keepAliveTime = unit.toNanos(keepAliveTime);        this.threadFactory = threadFactory;        this.handler = handler;    }    ...}</code></pre><h2 id="线程池7大参数深入介绍"><a href="#线程池7大参数深入介绍" class="headerlink" title="线程池7大参数深入介绍"></a>线程池7大参数深入介绍</h2><h3 id="corePoolSize：线程池中的常驻核心线程数"><a href="#corePoolSize：线程池中的常驻核心线程数" class="headerlink" title="corePoolSize：线程池中的常驻核心线程数"></a>corePoolSize：线程池中的常驻核心线程数</h3><ul><li>在创建了线程池后，当有请求任务来之后，就会安排池中的线程去执行请求任务。</li><li>当线程池中的线程数目达到corePoolSize后，就会把到达的任务放到缓存队列当中。</li></ul><h3 id="maximumPoolSize："><a href="#maximumPoolSize：" class="headerlink" title="maximumPoolSize："></a>maximumPoolSize：</h3><pre><code>线程池能够容纳同时执行的最大线程数，此值必须大于等于1</code></pre><h3 id="keepAliveTime："><a href="#keepAliveTime：" class="headerlink" title="keepAliveTime："></a>keepAliveTime：</h3><pre><code>多余的空闲线程的存活时间。当前线程池数量超过corePoolSize时，当空闲时间达到keepAliveTime值时，多余空闲线程会被销毁直到只剩下corePoolSize个线程为止</code></pre><h3 id="unit："><a href="#unit：" class="headerlink" title="unit："></a>unit：</h3><pre><code>keepAliveTime的单位。</code></pre><h3 id="workQueue："><a href="#workQueue：" class="headerlink" title="workQueue："></a>workQueue：</h3><pre><code>任务队列，被提交但尚未被执行的任务。</code></pre><h3 id="threadFactory："><a href="#threadFactory：" class="headerlink" title="threadFactory："></a>threadFactory：</h3><pre><code>表示生成线程池中工作线程的线程工厂，用于创建线程一般用默认的即可。</code></pre><h3 id="handler："><a href="#handler：" class="headerlink" title="handler："></a>handler：</h3><pre><code>拒绝策略，表示当队列满了并且工作线程大于等于线程池的最大线程数（ maximumPoolSize)。</code></pre><h2 id="线程池底层工作原理"><a href="#线程池底层工作原理" class="headerlink" title="线程池底层工作原理"></a>线程池底层工作原理</h2><p><img src="/2021/05/06/2021-05-06-bing-fa-xian-cheng-chi/e409c2155477e1a58733372caefee96f.png" alt></p><p><img src="/2021/05/06/2021-05-06-bing-fa-xian-cheng-chi/90c6fb12f14ffe1a7e2d695ece27c94e.png" alt></p><ul><li><p>在创建了线程池后，等待提交过来的任务请求。</p></li><li><p>当调用execute()方法添加一个请求任务时，线程池会做如下判断：</p><ul><li>如果正在运行的线程数量小于corePoolSize，那么马上创建线程运行这个任务；</li><li>如果正在运行的线程数量大于或等于corePoolSize，那么将这个任务放入队列；</li><li>如果这时候队列满了且正在运行的线程数量还小于maximumPoolSize，那么还是要创建非核心线程立刻运行这个任务;</li><li>如果队列满了且正在运行的线程数量大于或等于maximumPoolSize，那么线程池会启动饱和拒绝策略来执行。</li></ul></li><li><p>当一个线程完成任务时，它会从队列中取下一个任务来执行。</p></li><li><p>当一个线程无事可做超过一定的时间（keepAliveTime）时，线程池会判断:如果当前运行的线程数大于corePoolSize，那么这个线程就被停掉，所以线程池的所有任务完成后它最终会收缩到corePoolSize的大小。</p></li></ul><h2 id="线程池的4种拒绝策略理论简介"><a href="#线程池的4种拒绝策略理论简介" class="headerlink" title="线程池的4种拒绝策略理论简介"></a>线程池的4种拒绝策略理论简介</h2><p>等待队列也已经排满了，再也塞不下新任务了同时，线程池中的max线程也达到了，无法继续为新任务服务。这时候我们就需要拒绝策略机制合理的处理这个问题。</p><p>JDK拒绝策略：</p><ul><li>AbortPolicy（默认）：直接抛出 RejectedExecutionException异常阻止系统正常运知。</li><li>CallerRunsPolicy：”调用者运行”一种调节机制，该策略既不会抛弃任务，也不会抛出异常，而是将某些任务回退到调用者，从而降低新任务的流量。</li><li>DiscardOldestPolicy：抛弃队列中等待最久的任务，然后把当前任务加入队列中尝试再次提交当前任务。</li><li>DiscardPolicy：直接丢弃任务，不予任何处理也不抛出异常。如果允许任务丢失，这是最好的一种方案。</li></ul><p>以上内置拒绝策略均实现了RejectedExecutionHandler接口。</p><h2 id="线程池实际中使用哪一种拒绝策略"><a href="#线程池实际中使用哪一种拒绝策略" class="headerlink" title="线程池实际中使用哪一种拒绝策略"></a>线程池实际中使用哪一种拒绝策略</h2><p>工作中单一的/固定数的/可变的三种创建线程池的方法都不用，用自定义的，自己定义7个参数构建。</p><h2 id="线程池的手写改造和拒绝策略"><a href="#线程池的手写改造和拒绝策略" class="headerlink" title="线程池的手写改造和拒绝策略"></a>线程池的手写改造和拒绝策略</h2><pre><code>import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.LinkedBlockingQueue;import java.util.concurrent.RejectedExecutionHandler;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class MyThreadPoolExecutorDemo {    public static void doSomething(ExecutorService executorService, int numOfRequest) {        try {            System.out.println(((ThreadPoolExecutor)executorService).getRejectedExecutionHandler().getClass() + &quot;:&quot;);            TimeUnit.SECONDS.sleep(1);            for (int i = 0; i &lt; numOfRequest; i++) {                final int tempInt = i;                executorService.execute(() -&gt; {                    System.out.println(Thread.currentThread().getName() + &quot;\t 给用户:&quot; + tempInt + &quot; 办理业务&quot;);                });            }            TimeUnit.SECONDS.sleep(1);            System.out.println(&quot;\n\n&quot;);        } catch (Exception e) {            System.err.println(e);        } finally {            executorService.shutdown();        }    }    public static ExecutorService newMyThreadPoolExecutor(int corePoolSize,           int maximumPoolSize, int blockingQueueSize, RejectedExecutionHandler handler){        return new ThreadPoolExecutor(                corePoolSize,                maximumPoolSize,                1,//keepAliveTime                TimeUnit.SECONDS,                new LinkedBlockingQueue&lt;&gt;(blockingQueueSize),                Executors.defaultThreadFactory(),                handler);    }    public static void main(String[] args) {        doSomething(newMyThreadPoolExecutor(2, 5, 3, new ThreadPoolExecutor.AbortPolicy()), 10);        doSomething(newMyThreadPoolExecutor(2, 5, 3, new ThreadPoolExecutor.CallerRunsPolicy()), 20);        doSomething(newMyThreadPoolExecutor(2, 5, 3, new ThreadPoolExecutor.DiscardOldestPolicy()), 10);        doSomething(newMyThreadPoolExecutor(2, 5, 3, new ThreadPoolExecutor.DiscardPolicy()), 10);    }}</code></pre><h2 id="线程池配置合理线程数"><a href="#线程池配置合理线程数" class="headerlink" title="线程池配置合理线程数"></a>线程池配置合理线程数</h2><h3 id="CPU密集型"><a href="#CPU密集型" class="headerlink" title="CPU密集型"></a>CPU密集型</h3><p>CPU密集的意思是该任务需要大量的运算，而没有阻塞，CPU一直全速运行。</p><p>CPU密集任务只有在真正的多核CPU上才可能得到加速(通过多线程),而在单核CPU上，无论你开几个模拟的多线程该任务都不可能得到加速，因为CPU总的运算能力就那些。</p><p>CPU密集型任务配置尽可能少的线程数量：</p><p>一般公式：（CPU核数+1）个线程的线程池</p><h3 id="IO密集型"><a href="#IO密集型" class="headerlink" title="IO密集型"></a>IO密集型</h3><p>由于IO密集型任务线程并不是一直在执行任务，则应配置尽可能多的线程，如CPU核数 * 2。</p><p>IO密集型，即该任务需要大量的IO，即大量的阻塞。</p><p>在单线程上运行IO密集型的任务会导致浪费大量的CPU运算能力浪费在等待。</p><p>所以在IO密集型任务中使用多线程可以大大的加速程序运行，即使在单核CPU上，这种加速主要就是利用了被浪费掉的阻塞时间。</p><p>IO密集型时，大部分线程都阻塞，故需要多配置线程数：</p><p>参考公式：CPU核数/ (1-阻塞系数)</p><p>阻塞系数在0.8~0.9之间</p><p>比如8核CPU：8/(1-0.9)=80个线程数</p>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA并发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入并发学习-阻塞队列</title>
      <link href="/2021/05/05/2021-05-05-bing-fa-zu-sai-dui-lie/"/>
      <url>/2021/05/05/2021-05-05-bing-fa-zu-sai-dui-lie/</url>
      
        <content type="html"><![CDATA[<h2 id="阻塞队列接口结构和实现类"><a href="#阻塞队列接口结构和实现类" class="headerlink" title="阻塞队列接口结构和实现类"></a>阻塞队列接口结构和实现类</h2><p>阻塞队列，顾名思义，首先它是一个队列，而一个阻塞队列在数据结构中所起的作用大致如下图所示：</p><p><img src="/2021/05/05/2021-05-05-bing-fa-zu-sai-dui-lie/6ba101e95c6d3027b697cb4ac9af4a82.png" alt></p><p>线程1往阻塞队列中添加元素，而线程2从阻塞队列中移除元素。</p><p>当阻塞队列是空时，从队列中获取元素的操作将会被阻塞。</p><p>当阻塞队列是满时，往队列里添加元素的操作将会被阻塞。</p><p>试图从空的阻塞队列中获取元素的线程将会被阻塞，直到其他的线程往空的队列插入新的元素。</p><p>同样试图往已满的阻塞队列中添加新元素的线程同样也会被阻塞，直到其他的线程从列中移除一个或者多个元素或者完全清空队列后使队列重新变得空闲起来并后续新增</p><h2 id="为什么用阻塞队列？有什么好处？"><a href="#为什么用阻塞队列？有什么好处？" class="headerlink" title="为什么用阻塞队列？有什么好处？"></a>为什么用阻塞队列？有什么好处？</h2><p>在多线程领域：所谓阻塞，在某些情况下余挂起线程（即阻塞），一旦条件满足，被挂起的线程又会自动被唤醒</p><p>为什么需要BlockingQueue:</p><p>好处是我们不需要关心什么时候需要阻塞线程，什么时候需要唤醒线程，因为这一切BlockingQueue都给你一手包办了</p><p>在Concurrent包发布以前，在多线程环境下，我们每个程序员都必须去自己控制这些细节，尤其还要兼顾效率和线程安全，而这会给我们的程序带来不小的复杂度。</p><h2 id="阻塞队列种类"><a href="#阻塞队列种类" class="headerlink" title="阻塞队列种类"></a>阻塞队列种类</h2><p><img src="/2021/05/05/2021-05-05-bing-fa-zu-sai-dui-lie/e71c12243095b80c6a1719c428ed7810.png" alt></p><p>最常用的三种：</p><ul><li>ArrayBlockingQueue：由数组结构组成的有界阻塞队列。</li><li>LinkedBlockingQueue：由链表结构组成的有界（但大小默认值为Integer.MAX_VALUE）阻塞队列。</li><li>SynchronousQueue：不存储元素的阻塞队列。</li></ul><p><img src="/2021/05/05/2021-05-05-bing-fa-zu-sai-dui-lie/Image1.png" alt></p><h2 id="抛出异常组"><a href="#抛出异常组" class="headerlink" title="抛出异常组"></a>抛出异常组</h2><pre><code>import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;public class BlockingQueueExceptionDemo {    public static void main(String[] args) {        BlockingQueue&lt;String&gt; blockingQueue = new ArrayBlockingQueue&lt;&gt;(3);        System.out.println(blockingQueue.add(&quot;a&quot;));        System.out.println(blockingQueue.add(&quot;b&quot;));        System.out.println(blockingQueue.add(&quot;c&quot;));        try {            //抛出 java.lang.IllegalStateException: Queue full            System.out.println(blockingQueue.add(&quot;XXX&quot;));        } catch (Exception e) {            System.err.println(e);        }        System.out.println(blockingQueue.element());        ///        System.out.println(blockingQueue.remove());        System.out.println(blockingQueue.remove());        System.out.println(blockingQueue.remove());        try {            //抛出 java.util.NoSuchElementException            System.out.println(blockingQueue.remove());                    } catch (Exception e) {            System.err.println(e);        }        try {            //element()相当于peek(),但element()会抛NoSuchElementException            System.out.println(blockingQueue.element());        } catch (Exception e) {            System.err.println(e);        }    }}</code></pre><p>输出结果：</p><pre><code>truetruetrueajava.lang.IllegalStateException: Queue fullabcjava.util.NoSuchElementExceptionjava.util.NoSuchElementException</code></pre><h2 id="返回布尔值组"><a href="#返回布尔值组" class="headerlink" title="返回布尔值组"></a>返回布尔值组</h2><pre><code>import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;public class BlockingQueueBooleanDemo {    public static void main(String[] args) {        BlockingQueue&lt;String&gt; blockingQueue = new ArrayBlockingQueue&lt;&gt;(3);        System.out.println(blockingQueue.offer(&quot;a&quot;));        System.out.println(blockingQueue.offer(&quot;b&quot;));        System.out.println(blockingQueue.offer(&quot;c&quot;));        System.out.println(blockingQueue.offer(&quot;d&quot;));        System.out.println(blockingQueue.poll());        System.out.println(blockingQueue.poll());        System.out.println(blockingQueue.poll());        System.out.println(blockingQueue.poll());    }}</code></pre><p>输出结果：</p><pre><code>truetruetruefalseabcnull</code></pre><h2 id="阻塞和超时控制"><a href="#阻塞和超时控制" class="headerlink" title="阻塞和超时控制"></a>阻塞和超时控制</h2><p>队列阻塞演示：</p><pre><code>import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;import java.util.concurrent.TimeUnit;public class BlockingQueueBlockedDemo {    public static void main(String[] args) throws InterruptedException {        BlockingQueue&lt;String&gt; blockingQueue = new ArrayBlockingQueue&lt;&gt;(3);        new Thread(()-&gt;{            try {                blockingQueue.put(&quot;a&quot;);                blockingQueue.put(&quot;b&quot;);                blockingQueue.put(&quot;c&quot;);                blockingQueue.put(&quot;c&quot;);//将会阻塞,直到主线程take()                System.out.println(&quot;it was blocked.&quot;);            } catch (InterruptedException e) {                e.printStackTrace();            }        }).start();        TimeUnit.SECONDS.sleep(2);        try {            blockingQueue.take();            blockingQueue.take();            blockingQueue.take();            blockingQueue.take();            System.out.println(&quot;Blocking...&quot;);            blockingQueue.take();//将会阻塞        } catch (InterruptedException e) {            e.printStackTrace();        }    }}</code></pre><p>阻塞超时放弃演示</p><pre><code>import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;import java.util.concurrent.TimeUnit;public class BlockingQueueTimeoutDemo {    public static void main(String[] args) throws InterruptedException {        BlockingQueue&lt;String&gt; blockingQueue = new ArrayBlockingQueue&lt;&gt;(3);        System.out.println(&quot;Offer.&quot;);        System.out.println(blockingQueue.offer(&quot;a&quot;, 2L, TimeUnit.SECONDS));        System.out.println(blockingQueue.offer(&quot;b&quot;, 2L, TimeUnit.SECONDS));        System.out.println(blockingQueue.offer(&quot;c&quot;, 2L, TimeUnit.SECONDS));        System.out.println(blockingQueue.offer(&quot;d&quot;, 2L, TimeUnit.SECONDS));        System.out.println(&quot;Poll.&quot;);        System.out.println(blockingQueue.poll(2L, TimeUnit.SECONDS));        System.out.println(blockingQueue.poll(2L, TimeUnit.SECONDS));        System.out.println(blockingQueue.poll(2L, TimeUnit.SECONDS));        System.out.println(blockingQueue.poll(2L, TimeUnit.SECONDS));    }}</code></pre><p>输出结果：</p><pre><code>Offer.truetruetruefalsePoll.abcnull</code></pre><h2 id="同步SynchronousQueue队列"><a href="#同步SynchronousQueue队列" class="headerlink" title="同步SynchronousQueue队列"></a>同步SynchronousQueue队列</h2><p>SynchronousQueue没有容量。</p><p>与其他BlockingQueue不同，SynchronousQueue是一个不存储元素的BlockingQueue。</p><p>每一个put操作必须要等待一个take操作，否则不能继续添加元素，反之亦然。</p><h2 id="线程通信之生产者消费者传统版"><a href="#线程通信之生产者消费者传统版" class="headerlink" title="线程通信之生产者消费者传统版"></a>线程通信之生产者消费者传统版</h2><pre><code>import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;class ShareData {    private int number = 0;    private Lock lock = new ReentrantLock();    private Condition condition = lock.newCondition();    public void increment() throws Exception{        // 同步代码块，加锁        lock.lock();        try {            // 判断            while(number != 0) {                // 等待不能生产                condition.await();            }            // 干活            number++;            System.out.println(Thread.currentThread().getName() + &quot;\t &quot; + number);            // 通知 唤醒            condition.signalAll();        } catch (Exception e) {            e.printStackTrace();        } finally {            lock.unlock();        }    }    public void decrement() throws Exception{        // 同步代码块，加锁        lock.lock();        try {            // 判断            while(number == 0) {                // 等待不能消费                condition.await();            }            // 干活            number--;            System.out.println(Thread.currentThread().getName() + &quot;\t &quot; + number);            // 通知 唤醒            condition.signalAll();        } catch (Exception e) {            e.printStackTrace();        } finally {            lock.unlock();        }    }}public class TraditionalProducerConsumerDemo {    public static void main(String[] args) {        ShareData shareData = new ShareData();        // t1线程，生产        new Thread(() -&gt; {            for (int i = 0; i &lt; 5; i++) {                try {                    shareData.increment();                } catch (Exception e) {                    e.printStackTrace();                }            }        }, &quot;t1&quot;).start();        // t2线程，消费        new Thread(() -&gt; {            for (int i = 0; i &lt; 5; i++) {                try {                    shareData.decrement();                } catch (Exception e) {                    e.printStackTrace();                }            }        }, &quot;t2&quot;).start();    }}</code></pre><h2 id="锁绑定多个条件Condition"><a href="#锁绑定多个条件Condition" class="headerlink" title="锁绑定多个条件Condition"></a>锁绑定多个条件Condition</h2><pre><code>import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;class ShareResource {    // A 1   B 2   c 3    private int number = 1;    // 创建一个重入锁    private Lock lock = new ReentrantLock();    // 这三个相当于备用钥匙    private Condition condition1 = lock.newCondition();    private Condition condition2 = lock.newCondition();    private Condition condition3 = lock.newCondition();    public void print5() {        lock.lock();        try {            // 判断            while(number != 1) {                // 不等于1，需要等待                condition1.await();            }            // 干活            for (int i = 0; i &lt; 5; i++) {                System.out.println(Thread.currentThread().getName() + &quot;\t &quot; + number + &quot;\t&quot; + i);            }            // 唤醒 （干完活后，需要通知B线程执行）            number = 2;            // 通知2号去干活了            condition2.signal();        } catch (Exception e) {            e.printStackTrace();        } finally {            lock.unlock();        }    }    public void print10() {        lock.lock();        try {            // 判断            while(number != 2) {                // 不等于1，需要等待                condition2.await();            }            // 干活            for (int i = 0; i &lt; 10; i++) {                System.out.println(Thread.currentThread().getName() + &quot;\t &quot; + number + &quot;\t&quot; + i);            }            // 唤醒 （干完活后，需要通知C线程执行）            number = 3;            // 通知2号去干活了            condition3.signal();        } catch (Exception e) {            e.printStackTrace();        } finally {            lock.unlock();        }    }    public void print15() {        lock.lock();        try {            // 判断            while(number != 3) {                // 不等于1，需要等待                condition3.await();            }            // 干活            for (int i = 0; i &lt; 15; i++) {                System.out.println(Thread.currentThread().getName() + &quot;\t &quot; + number + &quot;\t&quot; + i);            }            // 唤醒 （干完活后，需要通知C线程执行）            number = 1;            // 通知1号去干活了            condition1.signal();        } catch (Exception e) {            e.printStackTrace();        } finally {            lock.unlock();        }    }}public class SynchronizedAndReentrantLockDemo {    public static void main(String[] args) {        ShareResource shareResource = new ShareResource();        int num = 10;        new Thread(() -&gt; {            for (int i = 0; i &lt; num; i++) {                    shareResource.print5();            }        }, &quot;A&quot;).start();        new Thread(() -&gt; {            for (int i = 0; i &lt; num; i++) {                shareResource.print10();            }        }, &quot;B&quot;).start();        new Thread(() -&gt; {            for (int i = 0; i &lt; num; i++) {                shareResource.print15();            }        }, &quot;C&quot;).start();    }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA并发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入并发学习-锁</title>
      <link href="/2021/05/04/2021-05-04-bing-fa-suo/"/>
      <url>/2021/05/04/2021-05-04-bing-fa-suo/</url>
      
        <content type="html"><![CDATA[<h2 id="公平和非公平锁"><a href="#公平和非公平锁" class="headerlink" title="公平和非公平锁"></a>公平和非公平锁</h2><ul><li>公平锁：是指多个线程按照申请锁的顺序来获取锁，类似排队打饭，先来后到。公平锁就是很公平，在并发环境中，每个线程在获取锁时会先查看此锁维护的等待队列，如果为空，或者当前线程是等待队列的第一个，就占有锁，否则就会加入到等待队列中，以后会按照FIFO的规则从队列中取到自己。</li><li>非公平锁：是指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后中请的线程比先中请的线程优先获取锁。在高并发的情况下，有可能会造成优先级反转或者饥饿现象。非公平锁比较粗鲁，上来就直接尝试占有锁，如果尝试失败，就再采用类似公平锁那种方式。</li></ul><p>并发包中ReentrantLock的创建可以指定构造函数的boolean类型来得到公平锁或非公平锁，默认是非公平锁。</p><p>Java ReentrantLock而言，通过构造函数指定该锁是否是公平锁，默认是非公平锁。</p><p>非公平锁的优点在于吞吐量比公平锁大。</p><p>对于Synchronized而言，也是一种非公平锁</p><h2 id="可重入锁理论知识"><a href="#可重入锁理论知识" class="headerlink" title="可重入锁理论知识"></a>可重入锁理论知识</h2><p>指的是同一线程外层函数获得锁之后，内层递归函数仍然能获取该锁的代码，在同一个线程在外层方法获取锁的时候，在进入内层方法会自动获取锁。</p><p>也即是说，<strong>线程可以进入任何一个它已经拥有的锁所同步着的代码块。</strong></p><p><strong>ReentrantLock/synchronized就是一个典型的可重入锁。</strong></p><p>可重入锁最大的作用是避免死锁。</p><h2 id="可重入锁代码验证"><a href="#可重入锁代码验证" class="headerlink" title="可重入锁代码验证"></a>可重入锁代码验证</h2><h3 id="synchronized"><a href="#synchronized" class="headerlink" title="synchronized"></a>synchronized</h3><pre><code>class Phone {    public synchronized void sendSMS() throws Exception{        System.out.println(Thread.currentThread().getName() + &quot;\t invoked sendSMS()&quot;);        // 在同步方法中，调用另外一个同步方法        sendEmail();    }    public synchronized void sendEmail() throws Exception{        System.out.println(Thread.currentThread().getId() + &quot;\t invoked sendEmail()&quot;);    }}public class SynchronizedReentrantLockDemo {    public static void main(String[] args) {        Phone phone = new Phone();        // 两个线程操作资源列        new Thread(() -&gt; {            try {                phone.sendSMS();            } catch (Exception e) {                e.printStackTrace();            }        }, &quot;t1&quot;).start();        new Thread(() -&gt; {            try {                phone.sendSMS();            } catch (Exception e) {                e.printStackTrace();            }        }, &quot;t2&quot;).start();    }}</code></pre><p>输出结果：</p><pre><code>t1     invoked sendSMS()11     invoked sendEmail()t2     invoked sendSMS()12     invoked sendEmail()</code></pre><h3 id="ReentranLock"><a href="#ReentranLock" class="headerlink" title="ReentranLock"></a>ReentranLock</h3><pre><code>import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;class Phone2 implements Runnable{    Lock lock = new ReentrantLock();    /**     * set进去的时候，就加锁，调用set方法的时候，能否访问另外一个加锁的set方法     */    public void getLock() {        lock.lock();        try {            System.out.println(Thread.currentThread().getName() + &quot;\t get Lock&quot;);            setLock();        } finally {            lock.unlock();        }    }    public void setLock() {        lock.lock();        try {            System.out.println(Thread.currentThread().getName() + &quot;\t set Lock&quot;);        } finally {            lock.unlock();        }    }    @Override    public void run() {        getLock();    }}public class ReentrantLockDemo {    public static void main(String[] args) {        Phone2 phone = new Phone2();        /**         * 因为Phone实现了Runnable接口         */        Thread t3 = new Thread(phone, &quot;t3&quot;);        Thread t4 = new Thread(phone, &quot;t4&quot;);        t3.start();        t4.start();    }}</code></pre><p>输出结果</p><pre><code>t3     get Lockt3     set Lockt4     get Lock    t4     set Lock</code></pre><h2 id="自旋锁理论知识"><a href="#自旋锁理论知识" class="headerlink" title="自旋锁理论知识"></a>自旋锁理论知识</h2><p><strong>自旋锁是指尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁，这样的好处是减少线程上下文切换的消耗，缺点是循环会消耗CPU</strong></p><blockquote><p>提到了互斥同步对性能最大的影响阻塞的实现，挂起线程和恢复线程的操作都需要转入内核态完成，这些操作给系统的并发性能带来了很大的压力。同时，虚拟机的开发团队也注意到在许多应用上，共享数据的锁定状态只会持续很短的一段时间，为了这段时间去挂起和恢复线程并不值得。如果物理机器有一个以上的处理器，能让两个或以上的线程同时并行执行，我们就可以让后面请求锁的那个线程 “稍等一下”，但不放弃处理器的执行时间，看看持有锁的线程是否很快就会释放锁。为了让线程等待，我们只需让线程执行一个忙循环（自旋），这项技术就是所谓的自旋锁。</p></blockquote><h2 id="自旋锁代码验证"><a href="#自旋锁代码验证" class="headerlink" title="自旋锁代码验证"></a>自旋锁代码验证</h2><pre><code>import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicReference;public class SpinLockDemo {    // 现在的泛型装的是Thread，原子引用线程    AtomicReference&lt;Thread&gt;  atomicReference = new AtomicReference&lt;&gt;();    public void myLock() {        // 获取当前进来的线程        Thread thread = Thread.currentThread();        System.out.println(Thread.currentThread().getName() + &quot;\t come in &quot;);        // 开始自旋，期望值是null，更新值是当前线程，如果是null，则更新为当前线程，否者自旋        while(!atomicReference.compareAndSet(null, thread)) {            //摸鱼        }    }    public void myUnLock() {        // 获取当前进来的线程        Thread thread = Thread.currentThread();        // 自己用完了后，把atomicReference变成null        atomicReference.compareAndSet(thread, null);        System.out.println(Thread.currentThread().getName() + &quot;\t invoked myUnlock()&quot;);    }    public static void main(String[] args) {        SpinLockDemo spinLockDemo = new SpinLockDemo();        // 启动t1线程，开始操作        new Thread(() -&gt; {            // 开始占有锁            spinLockDemo.myLock();            try {                TimeUnit.SECONDS.sleep(5);            } catch (InterruptedException e) {                e.printStackTrace();            }            // 开始释放锁            spinLockDemo.myUnLock();        }, &quot;t1&quot;).start();        // 让main线程暂停1秒，使得t1线程，先执行        try {            TimeUnit.SECONDS.sleep(1);        } catch (InterruptedException e) {            e.printStackTrace();        }        // 1秒后，启动t2线程，开始占用这个锁        new Thread(() -&gt; {            // 开始占有锁            spinLockDemo.myLock();            // 开始释放锁            spinLockDemo.myUnLock();        }, &quot;t2&quot;).start();    }}</code></pre><p>输出结果</p><pre><code>t1     come in t2     come in t1     invoked myUnlock()t2     invoked myUnlock()</code></pre><h2 id="读写锁理论知识"><a href="#读写锁理论知识" class="headerlink" title="读写锁理论知识"></a>读写锁理论知识</h2><ul><li>独占锁：指该锁一次只能被一个线程所持有。对ReentrantLock和Synchronized而言都是独占锁</li><li>共享锁：指该锁可被多个线程所持有。</li></ul><p>多个线程同时读一个资源类没有任何问题，所以为了满足并发量，读取共享资源应该可以同时进行。但是，如果有一个线程想去写共享资源来，就不应该再有其它线程可以对该资源进行读或写。</p><p>对ReentrantReadWriteLock其读锁是共享锁，其写锁是独占锁。</p><p>读锁的共享锁可保证并发读是非常高效的，读写，写读，写写的过程是互斥的。</p><h2 id="读写锁代码验证"><a href="#读写锁代码验证" class="headerlink" title="读写锁代码验证"></a>读写锁代码验证</h2><pre><code>import java.util.HashMap;import java.util.Map;import java.util.concurrent.TimeUnit;class MyCache {    private volatile Map&lt;String, Object&gt; map = new HashMap&lt;&gt;();    public void put(String key, Object value) {        System.out.println(Thread.currentThread().getName() + &quot;\t 正在写入：&quot; + key);        try {            // 模拟网络拥堵，延迟0.3秒            TimeUnit.MILLISECONDS.sleep(300);        } catch (InterruptedException e) {            e.printStackTrace();        }        map.put(key, value);        System.out.println(Thread.currentThread().getName() + &quot;\t 写入完成&quot;);    }    public void get(String key) {        System.out.println(Thread.currentThread().getName() + &quot;\t 正在读取:&quot;);        try {            // 模拟网络拥堵，延迟0.3秒            TimeUnit.MILLISECONDS.sleep(300);        } catch (InterruptedException e) {            e.printStackTrace();        }        Object value = map.get(key);        System.out.println(Thread.currentThread().getName() + &quot;\t 读取完成：&quot; + value);    }}public class ReadWriteWithoutLockDemo {    public static void main(String[] args) {        MyCache myCache = new MyCache();        // 线程操作资源类，5个线程写        for (int i = 0; i &lt; 5; i++) {            final int tempInt = i;            new Thread(() -&gt; {                myCache.put(tempInt + &quot;&quot;, tempInt +  &quot;&quot;);            }, String.valueOf(i)).start();        }        // 线程操作资源类， 5个线程读        for (int i = 0; i &lt; 5; i++) {            final int tempInt = i;            new Thread(() -&gt; {                myCache.get(tempInt + &quot;&quot;);            }, String.valueOf(i)).start();        }    }}</code></pre><p>输出结果：</p><pre><code>0     正在写入：01     正在写入：13     正在写入：32     正在写入：24     正在写入：40     正在读取:1     正在读取:2     正在读取:4     正在读取:3     正在读取:1     写入完成4     写入完成0     写入完成2     写入完成3     写入完成3     读取完成：30     读取完成：02     读取完成：21     读取完成：null4     读取完成：null</code></pre><p>看到有些线程读取到null，可用ReentrantReadWriteLock解决</p><pre><code>package com.lun.concurrency;import java.util.HashMap;import java.util.Map;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.ReentrantReadWriteLock;class MyCache2 {    private volatile Map&lt;String, Object&gt; map = new HashMap&lt;&gt;();    private ReentrantReadWriteLock rwLock = new ReentrantReadWriteLock();    public void put(String key, Object value) {        // 创建一个写锁        rwLock.writeLock().lock();        try {            System.out.println(Thread.currentThread().getName() + &quot;\t 正在写入：&quot; + key);            try {                // 模拟网络拥堵，延迟0.3秒                TimeUnit.MILLISECONDS.sleep(300);            } catch (InterruptedException e) {                e.printStackTrace();            }            map.put(key, value);            System.out.println(Thread.currentThread().getName() + &quot;\t 写入完成&quot;);        } catch (Exception e) {            e.printStackTrace();        } finally {            // 写锁 释放            rwLock.writeLock().unlock();        }    }    public void get(String key) {        // 读锁        rwLock.readLock().lock();        try {            System.out.println(Thread.currentThread().getName() + &quot;\t 正在读取:&quot;);            try {                // 模拟网络拥堵，延迟0.3秒                TimeUnit.MILLISECONDS.sleep(300);            } catch (InterruptedException e) {                e.printStackTrace();            }            Object value = map.get(key);            System.out.println(Thread.currentThread().getName() + &quot;\t 读取完成：&quot; + value);        } catch (Exception e) {            e.printStackTrace();        } finally {            // 读锁释放            rwLock.readLock().unlock();        }    }    public void clean() {        map.clear();    }}public class ReadWriteWithLockDemo {    public static void main(String[] args) {        MyCache2 myCache = new MyCache2();        // 线程操作资源类，5个线程写        for (int i = 1; i &lt;= 5; i++) {            // lambda表达式内部必须是final            final int tempInt = i;            new Thread(() -&gt; {                myCache.put(tempInt + &quot;&quot;, tempInt +  &quot;&quot;);            }, String.valueOf(i)).start();        }        // 线程操作资源类， 5个线程读        for (int i = 1; i &lt;= 5; i++) {            // lambda表达式内部必须是final            final int tempInt = i;            new Thread(() -&gt; {                myCache.get(tempInt + &quot;&quot;);            }, String.valueOf(i)).start();        }    }}</code></pre><p>输出结果：</p><pre><code>1     正在写入：11     写入完成2     正在写入：22     写入完成3     正在写入：33     写入完成5     正在写入：55     写入完成4     正在写入：44     写入完成2     正在读取:3     正在读取:1     正在读取:5     正在读取:4     正在读取:3     读取完成：32     读取完成：21     读取完成：15     读取完成：54     读取完成：4</code></pre><p>对ReentrantReadWriteLock其读锁是共享锁，其写锁是独占锁。</p><p>读锁的共享锁可保证并发读是非常高效的，读写，写读，写写的过程是互斥的。</p><h2 id="CountDownLatch"><a href="#CountDownLatch" class="headerlink" title="CountDownLatch"></a>CountDownLatch</h2><p>让一线程阻塞直到另一些线程完成一系列操作才被唤醒。</p><p>CountDownLatch主要有两个方法（await()，countDown()）。</p><p>当一个或多个线程调用await()时，调用线程会被阻塞。其它线程调用countDown()会将计数器减1(调用countDown方法的线程不会阻塞)，当计数器的值变为零时，因调用await方法被阻塞的线程会被唤醒，继续执行。</p><pre><code>public class CountDownLatchDemo {    public static void main(String[] args) throws Exception{        CountDownLatch countDownLatch = new CountDownLatch(5);        for (int i = 0; i &lt; 5; i++) {            new Thread(() -&gt; {                countDownLatch.countDown();                System.out.println(&quot;先执行&quot;);            }).start();        }        countDownLatch.await(); // 这里会阻塞，直到为0        System.out.println(&quot;上面执行完，才最后执行&quot;);    }}</code></pre><p>输出结果：</p><pre><code>先执行先执行先执行先执行先执行最后执行</code></pre><h3 id="枚举-CountDownLatch"><a href="#枚举-CountDownLatch" class="headerlink" title="枚举 + CountDownLatch"></a>枚举 + CountDownLatch</h3><pre><code>import java.util.Objects;public enum CountryEnum {    ONE(1, &quot;齐&quot;), TWO(2, &quot;楚&quot;), THREE(3, &quot;燕&quot;), FOUR(4, &quot;赵&quot;), FIVE(5, &quot;魏&quot;), SIX(6, &quot;韩&quot;);    private Integer retcode;    private String retMessage;    CountryEnum(Integer retcode, String retMessage) {        this.retcode = retcode;        this.retMessage = retMessage;    }    public static CountryEnum forEach_countryEnum(int index) {        CountryEnum[] myArray = CountryEnum.values();        for(CountryEnum ce : myArray) {            if(Objects.equals(index, ce.getRetcode())) {                return ce;            }        }        return null;    }    public Integer getRetcode() {        return retcode;    }    public void setRetcode(Integer retcode) {        this.retcode = retcode;    }    public String getRetMessage() {        return retMessage;    }    public void setRetMessage(String retMessage) {        this.retMessage = retMessage;    }}</code></pre><pre><code>import java.util.concurrent.CountDownLatch;public class UnifySixCountriesDemo {    public static void main(String[] args) throws InterruptedException {        // 计数器        CountDownLatch countDownLatch = new CountDownLatch(6);        for (int i = 1; i &lt;= 6; i++) {            new Thread(() -&gt; {                System.out.println(Thread.currentThread().getName() + &quot;国被灭了！&quot;);                countDownLatch.countDown();            }, CountryEnum.forEach_countryEnum(i).getRetMessage()).start();        }        countDownLatch.await();        System.out.println(Thread.currentThread().getName() + &quot; 秦国统一中原。&quot;);    }}</code></pre><p>输出结果：</p><pre><code>齐国被灭了！燕国被灭了！楚国被灭了！魏国被灭了！韩国被灭了！赵国被灭了！main 秦国统一中原。</code></pre><h2 id="CyclicBarrier"><a href="#CyclicBarrier" class="headerlink" title="CyclicBarrier"></a>CyclicBarrier</h2><p>CyclicBarrier的字面意思就是可循环（Cyclic）使用的屏障（Barrier）。它要求做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活，线程进入屏障通过CyclicBarrier的await方法。</p><p>CyclicBarrier与CountDownLatch的区别：CyclicBarrier可重复多次，而CountDownLatch只能是一次。</p><pre><code>public class CyclicBarrierDemo {    public static void main(String[] args) throws Exception{        CyclicBarrier cyclicBarrier = new CyclicBarrier(7, () -&gt; {            System.out.println(&quot;全部执行后才能最后执行&quot;);        });        for (int i = 0; i &lt; 7; i++) {            new Thread(() -&gt; {                System.out.println(&quot;先执行&quot;);                try {                    // 先到的被阻塞，等全部线程完成后，才能执行方法                    cyclicBarrier.await();                } catch (InterruptedException e) {                    e.printStackTrace();                } catch (BrokenBarrierException e) {                    e.printStackTrace();                }            }).start();        }    }}</code></pre><p>输出结果：</p><pre><code>先执行先执行先执行先执行先执行先执行全部执行后才能最后执行</code></pre><h2 id="Semaphore"><a href="#Semaphore" class="headerlink" title="Semaphore"></a>Semaphore</h2><p>信号量主要用于两个目的，一个是用于多个共享资源的互斥使用，另一个用于并发线程数的控制。</p><p>正常的锁(Concurrent.locks或synchronized锁)在任何时刻都只允许一个任务访问一项资源，而 Semaphore允许n个任务同时访问这个资源。</p><p>模拟一个抢车位的场景，假设一共有6个车，3个停车位</p><pre><code>import java.util.concurrent.Semaphore;import java.util.concurrent.TimeUnit;public class SemaphoreDemo {    public static void main(String[] args) {        /**         * 初始化一个信号量为3，默认是false 非公平锁， 模拟3个停车位         */        Semaphore semaphore = new Semaphore(3, false);        // 模拟6部车        for (int i = 0; i &lt; 6; i++) {            new Thread(() -&gt; {                try {                    // 代表一辆车，已经占用了该车位                    semaphore.acquire(); // 抢占                    System.out.println(Thread.currentThread().getName() + &quot;\t 抢到车位&quot;);                    // 每个车停3秒                    try {                        TimeUnit.SECONDS.sleep(3);                    } catch (InterruptedException e) {                        e.printStackTrace();                    }                    System.out.println(Thread.currentThread().getName() + &quot;\t 离开车位&quot;);                } catch (InterruptedException e) {                    e.printStackTrace();                } finally {                    // 释放停车位                    semaphore.release();                }            }, String.valueOf(i)).start();        }    }}</code></pre><p>输出结果：</p><pre><code>1     抢到车位2     抢到车位0     抢到车位0     离开车位2     离开车位1     离开车位5     抢到车位4     抢到车位3     抢到车位5     离开车位4     离开车位3     离开车位</code></pre><h2 id="Synchronized和Lock有什么区别"><a href="#Synchronized和Lock有什么区别" class="headerlink" title="Synchronized和Lock有什么区别"></a>Synchronized和Lock有什么区别</h2><ul><li>synchronized属于JVM层面，属于java的关键字。monitorenter（底层是通过monitor对象来完成，其实wait/notify等方法也依赖于monitor对象 只能在同步块或者方法中才能调用 wait/ notify等方法）。Lock是具体类（java.util.concurrent.locks.Lock）是api层面的锁</li><li>synchronized不需要用户去手动释放锁，当synchronized代码执行后，系统会自动让线程释放对锁的占用。ReentrantLock：则需要用户去手动释放锁，若没有主动释放锁，就有可能出现死锁的现象，需要lock() 和 unlock() 配置try catch语句来完成</li><li>synchronized不可中断，除非抛出异常或者正常运行完成。ReentrantLock可中断，可以设置超时方法，trylock(long timeout, TimeUnit unit)</li><li>synchronized是非公平锁。ReentrantLock默认非公平锁，构造函数可以传递boolean值，true为公平锁，false为非公平锁</li><li>synchronized没有绑定多个条件Condition ，要么随机，要么全部唤醒。ReentrantLock用来实现分组唤醒需要唤醒的线程，可以精确唤醒，而不是像synchronized那样，要么随机，要么全部唤醒。</li></ul><h2 id="死锁编码及定位分析"><a href="#死锁编码及定位分析" class="headerlink" title="死锁编码及定位分析"></a>死锁编码及定位分析</h2><p>死锁是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象,若无外力干涉那它们都将无法推进下去，如果系统资源充足，进程的资源请求都能够碍到满足，死锁出现的可能性就很低，否则就会因争夺有限的资源而陷入死锁。</p><h3 id="发生死锁的四个条件："><a href="#发生死锁的四个条件：" class="headerlink" title="发生死锁的四个条件："></a>发生死锁的四个条件：</h3><ul><li>互斥条件，线程使用的资源至少有一个不能共享的。</li><li>至少有一个线程必须持有一个资源且正在等待获取一个当前被别的线程持有的资源。</li><li>资源不能被抢占。</li><li>循环等待。</li></ul><h3 id="产生死锁的代码例子"><a href="#产生死锁的代码例子" class="headerlink" title="产生死锁的代码例子"></a>产生死锁的代码例子</h3><pre><code>package com.lun.concurrency;import java.util.concurrent.TimeUnit;class MyTask implements Runnable{    private Object resourceA, resourceB;    public MyTask(Object resourceA, Object resourceB) {        this.resourceA = resourceA;        this.resourceB = resourceB;    }    @Override    public void run() {        synchronized (resourceA) {            System.out.println(String.format(&quot;%s 自己持有%s，尝试持有%s&quot;,//                     Thread.currentThread().getName(), resourceA, resourceB));            try {                TimeUnit.SECONDS.sleep(2);            } catch (InterruptedException e) {                e.printStackTrace();            }            synchronized (resourceB) {                System.out.println(String.format(&quot;%s 同时持有%s，%s&quot;,//                         Thread.currentThread().getName(), resourceA, resourceB));            }        }    }}public class DeadLockDemo {    public static void main(String[] args) {        Object resourceA = new Object();        Object resourceB = new Object();        new Thread(new MyTask(resourceA, resourceB),&quot;Thread A&quot;).start();        new Thread(new MyTask(resourceB, resourceA),&quot;Thread B&quot;).start();    }}</code></pre><p>输出结果：</p><pre><code>Thread A 自己持有java.lang.Object@59d8d77，尝试持有java.lang.Object@7a15e6e6Thread B 自己持有java.lang.Object@7a15e6e6，尝试持有java.lang.Object@59d8d77</code></pre><h3 id="查看是否死锁工具："><a href="#查看是否死锁工具：" class="headerlink" title="查看是否死锁工具："></a>查看是否死锁工具：</h3><ol><li><p>jps命令定位进程号</p></li><li><p>jstack找到死锁查看</p></li></ol><pre><code>C:\Users\abc&gt;jps -l11968 com.lun.concurrency.DeadLockDemo6100 jdk.jcmd/sun.tools.jps.Jps6204 EclipseC:\Users\abc&gt;jstack 119682021-03-09 02:42:46Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.251-b08 mixed mode):&quot;DestroyJavaVM&quot; #13 prio=5 os_prio=0 tid=0x00000000004de800 nid=0x2524 waiting on condition [0x0000000000000000]   java.lang.Thread.State: RUNNABLE    .....</code></pre>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA并发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入并发学习-集合类不安全</title>
      <link href="/2021/05/03/2021-05-03-bing-fa-ji-he-lei-bu-an-quan/"/>
      <url>/2021/05/03/2021-05-03-bing-fa-ji-he-lei-bu-an-quan/</url>
      
        <content type="html"><![CDATA[<h2 id="集合类不安全之List"><a href="#集合类不安全之List" class="headerlink" title="集合类不安全之List"></a>集合类不安全之List</h2><p>假设线程A将通过迭代器next()获取下一元素时，从而将其打印出来。但之前，其他某线程添加新元素至list，结构发生了改变，modCount自增。当线程A运行到checkForComodification()，expectedModCount是modCount之前自增的值，判定modCount != expectedModCount为真，继而抛出ConcurrentModificationException。</p><h3 id="解决方法："><a href="#解决方法：" class="headerlink" title="解决方法："></a>解决方法：</h3><ul><li>Vector</li><li>Collections.synchronizedList(new ArrayList&lt;&gt;())</li><li>CopyOnWriteArrayList（推荐）</li></ul><h3 id="CopyOnWriteArrayList（推荐）"><a href="#CopyOnWriteArrayList（推荐）" class="headerlink" title="CopyOnWriteArrayList（推荐）"></a>CopyOnWriteArrayList（推荐）</h3><pre><code>public class CopyOnWriteArrayList&lt;E&gt;    implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable {    /** The array, accessed only via getArray/setArray. */    private transient volatile Object[] array;    final Object[] getArray() {        return array;    }    final void setArray(Object[] a) {        array = a;    }    ...    public boolean add(E e) {        final ReentrantLock lock = this.lock;        lock.lock();        try {            Object[] elements = getArray();            int len = elements.length;            Object[] newElements = Arrays.copyOf(elements, len + 1);            newElements[len] = e;            setArray(newElements);            return true;        } finally {            lock.unlock();        }    }    ...    public String toString() {        return Arrays.toString(getArray());    }    ...}</code></pre><p>CopyOnWrite容器即写时复制的容器。待一个容器添加元素的时候，不直接往当前容器Object[]添加，而是先将当前容器Object[]进行copy，复制出一个新的容器Object[] newELements，然后新的容器Object[ ] newELements里添加元素，添加完元素之后，再将原容器的引用指向新的容器setArray (newELements)。</p><p>这样做的好处是可以对CopyOnWrite容器进行并发的读，而不需要加锁（区别于Vector和Collections.synchronizedList()），因为当前容器不会添加任何元素。所以CopyOnWrite容器也是一种读写分离的思想，读和写不同的容器。</p><h2 id="集合类不安全之Set"><a href="#集合类不安全之Set" class="headerlink" title="集合类不安全之Set"></a>集合类不安全之Set</h2><p>HashSet也是非线性安全的。（HashSet内部是包装了一个HashMap的）</p><pre><code>import java.util.Collections;import java.util.HashSet;import java.util.Set;import java.util.UUID;import java.util.concurrent.CopyOnWriteArraySet;public class SetNotSafeDemo {    public static void main(String[] args) {        Set&lt;String&gt; set = new HashSet&lt;&gt;();        //Set&lt;String&gt; set = Collections.synchronizedSet(new HashSet&lt;&gt;());        //Set&lt;String&gt; set = new CopyOnWriteArraySet&lt;String&gt;();        for (int i = 0; i &lt; 30; i++) {            new Thread(() -&gt; {                set.add(UUID.randomUUID().toString().substring(0, 8));                System.out.println(set);            }, String.valueOf(i)).start();        }    }    }</code></pre><h3 id="解决方法：-1"><a href="#解决方法：-1" class="headerlink" title="解决方法："></a>解决方法：</h3><ul><li>Collections.synchronizedSet(new HashSet&lt;&gt;())</li><li>CopyOnWriteArraySet&lt;&gt;()（推荐）</li></ul><h3 id="CopyOnWriteArraySet源码一览："><a href="#CopyOnWriteArraySet源码一览：" class="headerlink" title="CopyOnWriteArraySet源码一览："></a>CopyOnWriteArraySet源码一览：</h3><pre><code>public class CopyOnWriteArraySet&lt;E&gt; extends AbstractSet&lt;E&gt;        implements java.io.Serializable {    private static final long serialVersionUID = 5457747651344034263L;    private final CopyOnWriteArrayList&lt;E&gt; al;    /**     * Creates an empty set.     */    public CopyOnWriteArraySet() {        al = new CopyOnWriteArrayList&lt;E&gt;();    }    public CopyOnWriteArraySet(Collection&lt;? extends E&gt; c) {        if (c.getClass() == CopyOnWriteArraySet.class) {            @SuppressWarnings(&quot;unchecked&quot;) CopyOnWriteArraySet&lt;E&gt; cc =                (CopyOnWriteArraySet&lt;E&gt;)c;            al = new CopyOnWriteArrayList&lt;E&gt;(cc.al);        }        else {            al = new CopyOnWriteArrayList&lt;E&gt;();            al.addAllAbsent(c);        }    }    //可看出CopyOnWriteArraySet包装了一个CopyOnWriteArrayList    ...    public boolean add(E e) {        return al.addIfAbsent(e);    }    public boolean addIfAbsent(E e) {        Object[] snapshot = getArray();        return indexOf(e, snapshot, 0, snapshot.length) &gt;= 0 ? false :            addIfAbsent(e, snapshot);    }    //暴力查找    private static int indexOf(Object o, Object[] elements,                               int index, int fence) {        if (o == null) {            for (int i = index; i &lt; fence; i++)                if (elements[i] == null)                    return i;        } else {            for (int i = index; i &lt; fence; i++)                if (o.equals(elements[i]))                    return i;        }        return -1;    }    private boolean addIfAbsent(E e, Object[] snapshot) {        final ReentrantLock lock = this.lock;        lock.lock();        try {            Object[] current = getArray();            int len = current.length;            if (snapshot != current) {//还要检查多一次元素存在性，生怕别的线程已经插入了                // Optimize for lost race to another addXXX operation                int common = Math.min(snapshot.length, len);                for (int i = 0; i &lt; common; i++)                    if (current[i] != snapshot[i] &amp;&amp; eq(e, current[i]))                        return false;                if (indexOf(e, current, common, len) &gt;= 0)                        return false;            }            Object[] newElements = Arrays.copyOf(current, len + 1);            newElements[len] = e;            setArray(newElements);            return true;        } finally {            lock.unlock();        }    }    ...}</code></pre><h2 id="集合类不安全之Map"><a href="#集合类不安全之Map" class="headerlink" title="集合类不安全之Map"></a>集合类不安全之Map</h2><pre><code>import java.util.Collections;import java.util.HashMap;import java.util.Hashtable;import java.util.Map;import java.util.UUID;import java.util.concurrent.ConcurrentHashMap;public class MapNotSafeDemo {    public static void main(String[] args) {        Map&lt;String, String&gt; map = new HashMap&lt;&gt;();//        Map&lt;String, String&gt; map = Collections.synchronizedMap(new HashMap&lt;&gt;());//        Map&lt;String, String&gt; map = new ConcurrentHashMap&lt;&gt;();//        Map&lt;String, String&gt; map = new Hashtable&lt;&gt;();        for (int i = 0; i &lt; 30; i++) {            new Thread(() -&gt; {                map.put(Thread.currentThread().getName(), UUID.randomUUID().toString().substring(0, 8));                System.out.println(map);            }, String.valueOf(i)).start();        }    }}</code></pre><h3 id="解决方法：-2"><a href="#解决方法：-2" class="headerlink" title="解决方法："></a>解决方法：</h3><ul><li>HashTable</li><li>Collections.synchronizedMap(new HashMap&lt;&gt;())</li><li>ConcurrencyMap&lt;&gt;()（推荐）</li></ul>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA并发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入并发学习-CAS</title>
      <link href="/2021/05/02/2021-05-02-bing-fa-cas/"/>
      <url>/2021/05/02/2021-05-02-bing-fa-cas/</url>
      
        <content type="html"><![CDATA[<h2 id="CAS是什么"><a href="#CAS是什么" class="headerlink" title="CAS是什么"></a>CAS是什么</h2><p>CAS的全称为Compare-And-Swap，它是一条CPU并发原语。</p><p>它的功能是判断内存某个位置的值是否为预期值，如果是则更改为新的值，这个过程是原子的。</p><p><strong>CAS并发原语体现在JAVA语言中就是sun.misc.Unsafe类中的各个方法。调用UnSafe类中的CAS方法，JVM会帮我们实现出CAS汇编指令。这是一种完全依赖于硬件的功能，通过它实现了原子操作。再次强调，由于CAS是一种系统原语，原语属于操作系统用语范畴，是由若干条指令组成的，用于完成某个功能的一个过程，并且原语的执行必须是连续的，在执行过程中不允许被中断，也就是说CAS是一条CPU的原子指令，不会造成所谓的数据不一致问题。（原子性）</strong></p><h2 id="AtomicInteger"><a href="#AtomicInteger" class="headerlink" title="AtomicInteger"></a>AtomicInteger</h2><p>Compare And Set 比较和设置</p><pre><code>public class CASDemo{    public static void main(string[] args){        AtomicInteger atomicInteger = new AtomicInteger(5);// mian do thing. . . . ..        System.out.println(atomicInteger.compareAndSet(5, 2019)+&quot;\t current data: &quot;+atomicInteger.get());        System.out.println(atomicInteger.compareAndset(5, 1024)+&quot;\t current data: &quot;+atomicInteger.get());    }}</code></pre><p>输出结果为</p><pre><code>true    2019false   2019</code></pre><h2 id="UnSafe类"><a href="#UnSafe类" class="headerlink" title="UnSafe类"></a>UnSafe类</h2><p>atomiclnteger.getAndIncrement()源码</p><pre><code>public class AtomicInteger extends Number implements java.io.Serializable {    private static final long serialVersionUID = 6214790243416807050L;    // setup to use Unsafe.compareAndSwapInt for updates    private static final Unsafe unsafe = Unsafe.getUnsafe();    private static final long valueOffset;    static {        try {            valueOffset = unsafe.objectFieldOffset                (AtomicInteger.class.getDeclaredField(&quot;value&quot;));        } catch (Exception ex) { throw new Error(ex); }    }    private volatile int value;    /**     * Creates a new AtomicInteger with the given initial value.     *     * @param initialValue the initial value     */    public AtomicInteger(int initialValue) {        value = initialValue;    }    /**     * Creates a new AtomicInteger with initial value {@code 0}.     */    public AtomicInteger() {    }    ...    /**     * Atomically increments by one the current value.     *     * @return the previous value     */    public final int getAndIncrement() {        return unsafe.getAndAddInt(this, valueOffset, 1);    }    ...}   </code></pre><pre><code>public final int getAndAddInt(Object var1, long var2, int var4) {    int var5;    do {        var5 = this.getIntVolatile(var1, var2);    } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4));    return var5;}</code></pre><pre><code>public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5);</code></pre><p>Unsafe是CAS的核心类，由于Java方法无法直接访问底层系统，需要通过本地（native）方法来访问，Unsafe相当于一个后门，基于该类可以直接操作特定内存的数据。Unsafe类存在于sun.misc包中，其内部方法操作可以像C的指针一样直接操作内存，因为Java中CAS操作的执行依赖于Unsafe类的方法。</p><p>注意Unsafe类中的所有方法都是native修饰的，也就是说Unsafe类中的方法都直接调用操作系统底层资源执行相应任务。</p><p>变量valueOffset，表示该变量值在内存中的偏移地址，因为Unsafe就是根据内存偏移地址获取数据的。</p><p>变量value用volatile修饰，保证了多线程之间的内存可见性。</p><h2 id="Unsafe源码解释"><a href="#Unsafe源码解释" class="headerlink" title="Unsafe源码解释"></a>Unsafe源码解释</h2><p><img src="/2021/05/02/2021-05-02-bing-fa-cas/de2bc76845ed3724333393f6aee2e62f.png" alt></p><ul><li>var1 AtomicInteger对象本身。</li><li>var2 该对象值得引用地址。</li><li>var4 需要变动的数量。</li><li>var5是用过var1，var2找出的主内存中真实的值。</li><li>用该对象当前的值与var5比较： 如果相同，更新var5+var4并且返回true,如果不同，继续取值然后再比较，直到更新完成。</li></ul><p>假设线程A和线程B两个线程同时执行getAndAddInt操作（分别跑在不同CPU上) ：</p><ul><li>Atomiclnteger里面的value原始值为3，即主内存中Atomiclnteger的value为3，根据JMM模型，线程A和线程B各自持有一份值为3的value的副本分别到各自的工作内存。</li><li>线程A通过getIntVolatile(var1, var2)拿到value值3，这时线程A被挂起。</li><li>线程B也通过getintVolatile(var1, var2)方法获取到value值3，此时刚好线程B没有被挂起并执行compareAndSwapInt方法比较内存值也为3，成功修改内存值为4，线程B打完收工，一切OK。</li><li>这时线程A恢复，执行compareAndSwapInt方法比较，发现自己手里的值数字3和主内存的值数字4不一致，说明该值己经被其它线程抢先一步修改过了，那A线程本次修改失败，只能重新读取重新来一遍了。</li><li>线程A重新获取value值，因为变量value被volatile修饰，所以其它线程对它的修改，线程A总是能够看到，线程A继续执行compareAndSwaplnt进行比较替换，直到成功。</li></ul><p>Unsafe类中的compareAndSwapInt，是一个本地方法，该方法的实现位于unsafe.cpp中。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>CAS有3个操作数，内存值V，旧的预期值A，要修改的更新值B。</p><p>当且仅当预期值A和内存值V相同时，将内存值V修改为B，否则什么都不做。</p><h2 id="CAS缺点"><a href="#CAS缺点" class="headerlink" title="CAS缺点"></a>CAS缺点</h2><h3 id="循环时间长开销很大"><a href="#循环时间长开销很大" class="headerlink" title="循环时间长开销很大"></a>循环时间长开销很大</h3><pre><code>// ursafe.getAndAddIntpublic final int getAndAddInt(Object var1, long var2, int var4){    int var5;    do {        var5 = this.getIntVolatile(var1, var2);    }while(!this.compareAndSwapInt(varl, var2, var5，var5 + var4));    return var5;}</code></pre><p>我们可以看到getAndAddInt方法执行时，有个do while，如果CAS失败，会一直进行尝试。如果CAS长时间一直不成功，可能会给CPU带来很大的开销。</p><p>Java 8 推出了一个新的类，LongAdder，他就是尝试使用分段 CAS 以及自动分段迁移的方式来大幅度提升多线程高并发执行 CAS 操作的性能.</p><p><img src="/2021/05/02/2021-05-02-bing-fa-cas/6.jpg" alt></p><p>在 LongAdder 的底层实现中，首先有一个 base 值，刚开始多线程来不停的累加数值，都是对 base 进行累加的，比如刚开始累加成了 base = 5。</p><p>接着如果发现并发更新的线程数量过多，就会开始施行分段 CAS 的机制，也就是内部会搞一个 Cell 数组，每个数组是一个数值分段。</p><p>这时，让大量的线程分别去对不同 Cell 内部的 value 值进行 CAS 累加操作，这样就把 CAS 计算压力分散到了不同的 Cell 分段数值中了！</p><p>这样就可以大幅度的降低多线程并发更新同一个数值时出现的无限循环的问题，大幅度提升了多线程并发更新数值的性能和效率！</p><p>而且他内部实现了自动分段迁移的机制，也就是如果某个 Cell 的 value 执行 CAS 失败了，那么就会自动去找另外一个 Cell 分段内的 value 值进行 CAS 操作。</p><p>这样也解决了线程空旋转、自旋不停等待执行 CAS 操作的问题，让一个线程过来执行 CAS 时可以尽快的完成这个操作。</p><p>最后，如果你要从 LongAdder 中获取当前累加的总值，就会把 base 值和所有 Cell 分段数值加起来返回给你。</p><h3 id="只能保证一个共享变量的原子操作"><a href="#只能保证一个共享变量的原子操作" class="headerlink" title="只能保证一个共享变量的原子操作"></a>只能保证一个共享变量的原子操作</h3><p>当对一个共享变量执行操作时，我们可以使用循环CAS的方式来保证原子操作，但是，对多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候就可以用锁来保证原子性。</p><h3 id="ABA问题"><a href="#ABA问题" class="headerlink" title="ABA问题"></a>ABA问题</h3><p>CAS算法实现一个重要前提需要取出内存中某时刻的数据并在当下时刻比较并替换，那么在这个时间差类会导致数据的变化。</p><p>比如说一个线程one从内存位置V中取出A，这时候另一个线程two也从内存中取出A，并且线程two进行了一些操作将值变成了B,然后线程two又将V位置的数据变成A，这时候线程one进行CAS操作发现内存中仍然是A，然后线程one操作成功。</p><p>尽管线程one的CAS操作成功，但是不代表这个过程就是没有问题的。</p><h2 id="AtomicReference原子引用"><a href="#AtomicReference原子引用" class="headerlink" title="AtomicReference原子引用"></a>AtomicReference原子引用</h2><pre><code>import java.util.concurrent.atomic.AtomicReference;class User{    String userName;    int age;    public User(String userName, int age) {        this.userName = userName;        this.age = age;    }    @Override    public String toString() {        return String.format(&quot;User [userName=%s, age=%s]&quot;, userName, age);    }}public class AtomicReferenceDemo {    public static void main(String[] args){        User z3 = new User( &quot;z3&quot;,22);        User li4 = new User(&quot;li4&quot; ,25);        AtomicReference&lt;User&gt; atomicReference = new AtomicReference&lt;&gt;();        atomicReference.set(z3);        System.out.println(atomicReference.compareAndSet(z3, li4)+&quot;\t&quot;+atomicReference.get().toString());        System.out.println(atomicReference.compareAndSet(z3, li4)+&quot;\t&quot;+atomicReference.get().toString());    }}</code></pre><pre><code>true    User [userName=li4, age=25]false    User [userName=li4, age=25]</code></pre><h2 id="AtomicStampedReference版本号原子引用-解决ABA问题"><a href="#AtomicStampedReference版本号原子引用-解决ABA问题" class="headerlink" title="AtomicStampedReference版本号原子引用(解决ABA问题)"></a>AtomicStampedReference版本号原子引用(解决ABA问题)</h2><p>原子引用 + 新增一种机制，那就是修改版本号（类似时间戳），它用来解决ABA问题。</p><pre><code>import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicReference;import java.util.concurrent.atomic.AtomicStampedReference;public class ABADemo {    /**     * 普通的原子引用包装类     */    static AtomicReference&lt;Integer&gt; atomicReference = new AtomicReference&lt;&gt;(100);    // 传递两个值，一个是初始值，一个是初始版本号    static AtomicStampedReference&lt;Integer&gt; atomicStampedReference = new AtomicStampedReference&lt;&gt;(100, 1);    public static void main(String[] args) {        System.out.println(&quot;============以下是ABA问题的产生==========&quot;);        new Thread(() -&gt; {            // 把100 改成 101 然后在改成100，也就是ABA            atomicReference.compareAndSet(100, 101);            atomicReference.compareAndSet(101, 100);        }, &quot;t1&quot;).start();        new Thread(() -&gt; {            try {                // 睡眠一秒，保证t1线程，完成了ABA操作                TimeUnit.SECONDS.sleep(1);            } catch (InterruptedException e) {                e.printStackTrace();            }            // 把100 改成 101 然后在改成100，也就是ABA            System.out.println(atomicReference.compareAndSet(100, 2019) + &quot;\t&quot; + atomicReference.get());        }, &quot;t2&quot;).start();        /        try {            TimeUnit.SECONDS.sleep(2);        } catch (Exception e) {            e.printStackTrace();        }        /        System.out.println(&quot;============以下是ABA问题的解决==========&quot;);        new Thread(() -&gt; {            // 获取版本号            int stamp = atomicStampedReference.getStamp();            System.out.println(Thread.currentThread().getName() + &quot;\t 第一次版本号&quot; + stamp);            // 暂停t3一秒钟            try {                TimeUnit.SECONDS.sleep(1);            } catch (InterruptedException e) {                e.printStackTrace();            }            // 传入4个值，期望值，更新值，期望版本号，更新版本号            atomicStampedReference.compareAndSet(100, 101, atomicStampedReference.getStamp(),                    atomicStampedReference.getStamp() + 1);            System.out.println(Thread.currentThread().getName() + &quot;\t 第二次版本号&quot; + atomicStampedReference.getStamp());            atomicStampedReference.compareAndSet(101, 100, atomicStampedReference.getStamp(),                    atomicStampedReference.getStamp() + 1);            System.out.println(Thread.currentThread().getName() + &quot;\t 第三次版本号&quot; + atomicStampedReference.getStamp());        }, &quot;t3&quot;).start();        new Thread(() -&gt; {            // 获取版本号            int stamp = atomicStampedReference.getStamp();            System.out.println(Thread.currentThread().getName() + &quot;\t 第一次版本号&quot; + stamp);            // 暂停t4 3秒钟，保证t3线程也进行一次ABA问题            try {                TimeUnit.SECONDS.sleep(3);            } catch (InterruptedException e) {                e.printStackTrace();            }            boolean result = atomicStampedReference.compareAndSet(100, 2019, stamp, stamp + 1);            System.out.println(Thread.currentThread().getName() + &quot;\t 修改成功否：&quot; + result + &quot;\t 当前最新实际版本号：&quot;                    + atomicStampedReference.getStamp());            System.out.println(Thread.currentThread().getName() + &quot;\t 当前实际最新值&quot; + atomicStampedReference.getReference());        }, &quot;t4&quot;).start();    }}</code></pre><pre><code>============以下是ABA问题的产生==========true    2019============以下是ABA问题的解决==========t3     第一次版本号1t4     第一次版本号1t3     第二次版本号2t3     第三次版本号3t4     修改成功否：false     当前最新实际版本号：3t4     当前实际最新值100</code></pre>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA并发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入并发学习-volatile</title>
      <link href="/2021/05/01/2021-05-01-bing-fa-volatile/"/>
      <url>/2021/05/01/2021-05-01-bing-fa-volatile/</url>
      
        <content type="html"><![CDATA[<h2 id="volatile是什么"><a href="#volatile是什么" class="headerlink" title="volatile是什么"></a>volatile是什么</h2><p>volatile是JVM提供的轻量级的同步机制</p><ul><li>保证可见性</li><li>不保证原子性</li><li>禁止指令重排（保证有序性）</li></ul><h2 id="JMM内存模型之可见性"><a href="#JMM内存模型之可见性" class="headerlink" title="JMM内存模型之可见性"></a>JMM内存模型之可见性</h2><p>JMM（Java内存模型Java Memory Model，简称JMM）本身是一种抽象的概念并不真实存在，它描述的是一组规则或规范，通过这组规范定义了程序中各个变量（包括实例字段，静态字段和构成数组对象的元素）的访问方式。</p><p>JMM关于同步的规定：</p><ul><li>线程解锁前，必须把共享变量的值刷新回主内存</li><li>线程加锁前，必须读取主内存的最新值到自己的工作内存</li><li>加锁解锁是同一把锁</li></ul><p>由于JVM运行程序的实体是线程，而每个线程创建时JVM都会为其创建一个工作内存（有些地方称为栈空间），工作内存是每个线程的私有数据区域，而Java内存模型中规定所有变量都存储在主内存，主内存是共享内存区域，所有线程都可以访问，但线程对变量的操作（读取赋值等）必须在工作内存中进行，首先要将变量从主内存拷贝的自己的工作内存空间，然后对变量进行操作，操作完成后再将变量写回主内存，不能直接操作主内存中的变量，各个线程中的工作内存中存储着主内存中的变量副本拷贝，因此不同的线程间无法访问对方的工作内存，线程间的通信（传值）必须通过主内存来完成，其简要访问过程如下图：</p><p><img src="/2021/05/01/2021-05-01-bing-fa-volatile/086f17f778d19c9be53c65117aaacd87" alt></p><h3 id="可见性"><a href="#可见性" class="headerlink" title="可见性"></a>可见性</h3><p>各个线程对主内存中共享变量的操作,都是各个线程各自拷贝到自己的工作内存进行操作后再写回到主内存中的。</p><p>这就可能存在一个线程AAA修改了共享变量X的值但还未写回主内存时，另外一个线程BBB又对主内存中同一个共享变量X进行操作，但此时A线程工作内存中共享变量x对线程B来说并不可见，这种工作内存与主内存同步延迟现象就造成了可见性问题</p><h2 id="可见性的代码验证说明"><a href="#可见性的代码验证说明" class="headerlink" title="可见性的代码验证说明"></a>可见性的代码验证说明</h2><pre><code>import java.util.concurrent.TimeUnit;/** * 假设是主物理内存 */class MyData {    //volatile int number = 0;    int number = 0;    public void addTo60() {        this.number = 60;    }}/** * 验证volatile的可见性 * 1. 假设int number = 0， number变量之前没有添加volatile关键字修饰 */public class VolatileDemo {    public static void main(String args []) {        // 资源类        MyData myData = new MyData();        // AAA线程 实现了Runnable接口的，lambda表达式        new Thread(() -&gt; {            System.out.println(Thread.currentThread().getName() + &quot;\t come in&quot;);            // 线程睡眠3秒，假设在进行运算            try {                TimeUnit.SECONDS.sleep(3);            } catch (InterruptedException e) {                e.printStackTrace();            }            // 修改number的值            myData.addTo60();            // 输出修改后的值            System.out.println(Thread.currentThread().getName() + &quot;\t update number value:&quot; + myData.number);        }, &quot;AAA&quot;).start();        // main线程就一直在这里等待循环，直到number的值不等于零        while(myData.number == 0) {}        // 按道理这个值是不可能打印出来的，因为主线程运行的时候，number的值为0，所以一直在循环        // 如果能输出这句话，说明AAA线程在睡眠3秒后，更新的number的值，重新写入到主内存，并被main线程感知到了        System.out.println(Thread.currentThread().getName() + &quot;\t mission is over&quot;);    }}</code></pre><p>由于没有volatile修饰MyData类的成员变量number，main线程将会卡在while(myData.number == 0) {}，不能正常结束。若想正确结束，用volatile修饰MyData类的成员变量number吧。</p><h2 id="volatile不保证原子性"><a href="#volatile不保证原子性" class="headerlink" title="volatile不保证原子性"></a>volatile不保证原子性</h2><p>原子性指不可分割，完整性，也即某个线程正在做某个具体业务时，中间不可以被加塞或者被分割。需要整体完整要么同时成功，要么同时失败。</p><p>volatile不保证原子性案例演示：</p><pre><code>class MyData2 {    /**     * volatile 修饰的关键字，是为了增加 主线程和线程之间的可见性，只要有一个线程修改了内存中的值，其它线程也能马上感知     */    volatile int number = 0;    public void addPlusPlus() {        number ++;    }}public class VolatileAtomicityDemo {    public static void main(String[] args) {        MyData2 myData = new MyData2();        // 创建10个线程，线程里面进行1000次循环        for (int i = 0; i &lt; 20; i++) {            new Thread(() -&gt; {                // 里面                for (int j = 0; j &lt; 1000; j++) {                    myData.addPlusPlus();                }            }, String.valueOf(i)).start();        }        // 需要等待上面20个线程都计算完成后，在用main线程取得最终的结果值        // 这里判断线程数是否大于2，为什么是2？因为默认是有两个线程的，一个main线程，一个gc线程        while(Thread.activeCount() &gt; 2) {            // yield表示不执行            Thread.yield();        }        // 查看最终的值        // 假设volatile保证原子性，那么输出的值应该为：  20 * 1000 = 20000        System.out.println(Thread.currentThread().getName() + &quot;\t finally number value: &quot; + myData.number);    }}</code></pre><p>最后的结果总是小于20000。因为i++里面包含有几条语句操作，并发时当A线程写入主内存后，还没来得及通知其他线程时，B线程就覆盖A写入主内存的值了。</p><h2 id="volatile不保证原子性理论解释"><a href="#volatile不保证原子性理论解释" class="headerlink" title="volatile不保证原子性理论解释"></a>volatile不保证原子性理论解释</h2><p>number++在多线程下是非线程安全的。</p><p>我们可以将代码编译成字节码，可看出number++被编译成3条指令。</p><p><img src="/2021/05/01/2021-05-01-bing-fa-volatile/1ea2ba83722d23f1a9ec62b31066ab6d" alt></p><p>假设我们没有加 synchronized那么第一步就可能存在着，三个线程同时通过getfield命令，拿到主存中的 n值，然后三个线程，各自在自己的工作内存中进行加1操作，但他们并发进行 iadd 命令的时候，因为只能一个进行写，所以其它操作会被挂起，假设1线程，先进行了写操作，在写完后，volatile的可见性，应该需要告诉其它两个线程，主内存的值已经被修改了，但是因为太快了，其它两个线程，陆续执行 iadd命令，进行写入操作，这就造成了其他线程没有接受到主内存n的改变，从而覆盖了原来的值，出现写丢失，这样也就让最终的结果少于20000。</p><h2 id="volatile不保证原子性问题解决"><a href="#volatile不保证原子性问题解决" class="headerlink" title="volatile不保证原子性问题解决"></a>volatile不保证原子性问题解决</h2><p>可加synchronized解决，但它是重量级同步机制，性能上有所顾虑。</p><p>如何不加synchronized解决number++在多线程下是非线程安全的问题？使用AtomicInteger。</p><pre><code>import java.util.concurrent.atomic.AtomicInteger;class MyData2 {    /**     * volatile 修饰的关键字，是为了增加 主线程和线程之间的可见性，只要有一个线程修改了内存中的值，其它线程也能马上感知     */    volatile int number = 0;    AtomicInteger number2 = new AtomicInteger();    public void addPlusPlus() {        number ++;    }    public void addPlusPlus2() {        number2.getAndIncrement();    }}public class VolatileAtomicityDemo {    public static void main(String[] args) {        MyData2 myData = new MyData2();        // 创建10个线程，线程里面进行1000次循环        for (int i = 0; i &lt; 20; i++) {            new Thread(() -&gt; {                // 里面                for (int j = 0; j &lt; 1000; j++) {                    myData.addPlusPlus();                    myData.addPlusPlus2();                }            }, String.valueOf(i)).start();        }        // 需要等待上面20个线程都计算完成后，在用main线程取得最终的结果值        // 这里判断线程数是否大于2，为什么是2？因为默认是有两个线程的，一个main线程，一个gc线程        while(Thread.activeCount() &gt; 2) {            // yield表示不执行            Thread.yield();        }        // 查看最终的值        // 假设volatile保证原子性，那么输出的值应该为：  20 * 1000 = 20000        System.out.println(Thread.currentThread().getName() + &quot;\t finally number value: &quot; + myData.number);        System.out.println(Thread.currentThread().getName() + &quot;\t finally number2 value: &quot; + myData.number2);    }}</code></pre><p>输出结果为：</p><pre><code>main     finally number value: 18766main     finally number2 value: 20000</code></pre><h2 id="volatile指令重排案例1"><a href="#volatile指令重排案例1" class="headerlink" title="volatile指令重排案例1"></a>volatile指令重排案例1</h2><p>计算机在执行程序时，为了提高性能，编译器和处理器的常常会对指令做重排，一般分以下3种：</p><p><img src="/2021/05/01/2021-05-01-bing-fa-volatile/bd9649a5795e503000a1fe89e57665a7" alt></p><p>单线程环境里面确保程序最终执行结果和代码顺序执行的结果一致。</p><p>处理器在进行重排序时必须要考虑指令之间的数据依赖性</p><p>多线程环境中线程交替执行，由于编译器优化重排的存在，两个线程中使用的变量能否保证一致性是无法确定的，结果无法预测。</p><pre><code>public void mySort{    int x = 11;//语句1    int y = 12;//语句2    × = × + 5;//语句3    y = x * x;//语句4}</code></pre><p>可重排序列：</p><ul><li>1234</li><li>2134</li><li>1324</li></ul><p>语句4可以重排后变成第一个条吗？答：不能。</p><h2 id="volatile指令重排案例2"><a href="#volatile指令重排案例2" class="headerlink" title="volatile指令重排案例2"></a>volatile指令重排案例2</h2><pre><code>public class ReSortSeqDemo{    int a = 0;    boolean flag = false;    public void method01(){        a = 1;//语句1        flag = true;//语句2    }    public void method02(){        if(flag){            a = a + 5; //语句3        }        System.out.println(&quot;retValue: &quot; + a);//可能是6或1或5或0    }}</code></pre><p>多线程环境中线程交替执行method01()和method02()，由于编译器优化重排的存在，两个线程中使用的变量能否保证一致性是无法确定的，结果无法预测。</p><h2 id="禁止指令重排小总结"><a href="#禁止指令重排小总结" class="headerlink" title="禁止指令重排小总结"></a>禁止指令重排小总结</h2><p>volatile实现禁止指令重排优化，从而避免多线程环境下程序出现乱序执行的现象</p><p>先了解一个概念，内存屏障(Memory Barrier）又称内存栅栏，是一个CPU指令，它的作用有两个:</p><ul><li>保证特定操作的执行顺序</li><li>保证某些变量的内存可见性（利用该特性实现volatile的内存可见性）</li></ul><p>由于编译器和处理器都能执行指令重排优化。如果在指令间插入一条Memory Barrier则会告诉编译器和CPU，不管什么指令都不能和这条Memory Barrier指令重排序，也就是说通过插入内存屏障禁止在内存屏障前后的指令执行重排序优化。内存屏障另外一个作用是强制刷出各种CPU的缓存数据，因此任何CPU上的线程都能读取到这些数据的最新版本。</p><p>对volatile变量进行写操作时，会在写操作后加入一条store屏障指令，将工作内存中的共享变量值刷新回到主内存。</p><p><img src="/2021/05/01/2021-05-01-bing-fa-volatile/d7626eb368ec93d80ed98478dd579401.png" alt></p><p>对Volatile变量进行读操作时，会在读操作前加入一条load屏障指令，从主内存中读取共享变量。</p><p><img src="/2021/05/01/2021-05-01-bing-fa-volatile/c351d47587cbdf0ba57028ec8b894114.png" alt></p><h3 id="线性安全性获得保证"><a href="#线性安全性获得保证" class="headerlink" title="线性安全性获得保证"></a>线性安全性获得保证</h3><p>工作内存与主内存同步延迟现象导致的可见性问题 - 可以使用synchronized或volatile关键字解决，它们都可以使一个线程修改后的变量立即对其他线程可见。</p><p>对于指令重排导致的可见性问题和有序性问题 - 可以利用volatile关键字解决，因为volatile的另外一个作用就是禁止重排序优化。</p><h2 id="单例模式volatile分析"><a href="#单例模式volatile分析" class="headerlink" title="单例模式volatile分析"></a>单例模式volatile分析</h2><p>DCL（Double Check Lock双端检锁机制）和volatile</p><pre><code>public class SingletonDemo{    private SingletonDemo(){}    private volatile static SingletonDemo instance = null;    public static SingletonDemo getInstance() {        if(instance == null) {            synchronized(SingletonDemo.class){                if(instance == null){                    instance = new SingletonDemo();                       }            }        }        return instance;    }}</code></pre><p>DCL中volatile解析</p><p>原因在于某一个线程执行到第一次检测，读取到的instance不为null时，instance的引用对象可能没有完成初始化，返回了空的对象。instance = new SingletonDemo(),可以分为以下3步完成(伪代码)：</p><pre><code>memory = allocate(); //1.分配对象内存空间instance(memory); //2.初始化对象instance = memory; //3.设置instance指向刚分配的内存地址，此时instance != null</code></pre><p>步骤2和步骤3不存在数据依赖关系，而且无论重排前还是重排后程序的执行结果在单线程中并没有改变，因此这种重排优化是允许的。</p><pre><code>memory = allocate(); //1.分配对象内存空间instance = memory;//3.设置instance指向刚分配的内存地址，此时instance! =null，但是对象还没有初始化完成!instance(memory);//2.初始化对象</code></pre><p>但是指令重排只会保证串行语义的执行的一致性(单线程)，但并不会关心多线程间的语义一致性。</p><p>所以当一条线程访问instance不为null时，由于instance实例未必已初始化完成，也就造成了线程安全问题。</p>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA并发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>访问者模式</title>
      <link href="/2021/04/22/2021-04-22-design-fang-wen-zhe/"/>
      <url>/2021/04/22/2021-04-22-design-fang-wen-zhe/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="访问者模式介绍"><a href="#访问者模式介绍" class="headerlink" title="访问者模式介绍"></a>访问者模式介绍</h2><p>访问者要解决的核心事项是，在一个稳定的数据结构下，例如用户信息、雇员信息等，增加易变的业务访问逻辑。为了增强扩展性，将这两部分的业务解耦的一种设计模式。</p><p>为了通俗理解，简单来说，嘿嘿：</p><p><img src="/2021/04/22/2021-04-22-design-fang-wen-zhe/itstack-demo-design-22-02.png" alt></p><p>说白了访问者模式的核心在于同一个事物不同视角下的访问信息不同，比如一个美女手里拿个冰激凌。小朋友会注意冰激凌，大朋友会找自己喜欢的地方观测敌情。</p><h2 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h2><p><img src="/2021/04/22/2021-04-22-design-fang-wen-zhe/itstack-demo-design-22-03.png" alt></p><p>模拟校园中有学生和老师两种身份的用户，那么对于家长和校长关心的角度来看，他们的视角是不同的。家长更关心孩子的成绩和老师的能力，校长更关心老师所在班级学生的人数和升学率.</p><h3 id="访问者模式模型结构"><a href="#访问者模式模型结构" class="headerlink" title="访问者模式模型结构"></a>访问者模式模型结构</h3><p><img src="/2021/04/22/2021-04-22-design-fang-wen-zhe/itstack-demo-design-22-04.png" alt></p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="定义用户抽象类"><a href="#定义用户抽象类" class="headerlink" title="定义用户抽象类"></a>定义用户抽象类</h4><pre><code>// 基础用户信息public abstract class User {    public String name;      // 姓名    public String identity;  // 身份；重点班、普通班 | 特级教师、普通教师、实习教师    public String clazz;     // 班级    public User(String name, String identity, String clazz) {        this.name = name;        this.identity = identity;        this.clazz = clazz;    }    // 核心访问方法    public abstract void accept(Visitor visitor);}</code></pre><h4 id="老师类"><a href="#老师类" class="headerlink" title="老师类"></a>老师类</h4><pre><code>public class Teacher extends User {    public Teacher(String name, String identity, String clazz) {        super(name, identity, clazz);    }    public void accept(Visitor visitor) {        visitor.visit(this);    }    // 升本率    public double entranceRatio() {        return BigDecimal.valueOf(Math.random() * 100).setScale(2, BigDecimal.ROUND_HALF_UP).doubleValue();    }}</code></pre><h4 id="学生类"><a href="#学生类" class="headerlink" title="学生类"></a>学生类</h4><pre><code>public class Student extends User {    public Student(String name, String identity, String clazz) {        super(name, identity, clazz);    }    public void accept(Visitor visitor) {        visitor.visit(this);    }    public int ranking() {        return (int) (Math.random() * 100);    }}</code></pre><h4 id="定义访问数据接口"><a href="#定义访问数据接口" class="headerlink" title="定义访问数据接口"></a>定义访问数据接口</h4><pre><code>public interface Visitor {    // 访问学生信息    void visit(Student student);    // 访问老师信息    void visit(Teacher teacher);}</code></pre><h4 id="访问者-校长"><a href="#访问者-校长" class="headerlink" title="访问者:校长"></a>访问者:校长</h4><pre><code>public class Principal implements Visitor {    private Logger logger = LoggerFactory.getLogger(Principal.class);    public void visit(Student student) {        logger.info(&quot;学生信息 姓名：{} 班级：{}&quot;, student.name, student.clazz);    }    public void visit(Teacher teacher) {        logger.info(&quot;学生信息 姓名：{} 班级：{} 升学率：{}&quot;, teacher.name, teacher.clazz, teacher.entranceRatio());    }}</code></pre><h4 id="访问者-家长"><a href="#访问者-家长" class="headerlink" title="访问者:家长"></a>访问者:家长</h4><pre><code>public class Parent implements Visitor {    private Logger logger = LoggerFactory.getLogger(Parent.class);    public void visit(Student student) {        logger.info(&quot;学生信息 姓名：{} 班级：{} 排名：{}&quot;, student.name, student.clazz, student.ranking());    }    public void visit(Teacher teacher) {        logger.info(&quot;老师信息 姓名：{} 班级：{} 级别：{}&quot;, teacher.name, teacher.clazz, teacher.identity);    }}</code></pre><h4 id="数据看版"><a href="#数据看版" class="headerlink" title="数据看版"></a>数据看版</h4><pre><code>public class DataView {    List&lt;User&gt; userList = new ArrayList&lt;User&gt;();    public DataView() {        userList.add(new Student(&quot;谢飞机&quot;, &quot;重点班&quot;, &quot;一年一班&quot;));        userList.add(new Student(&quot;windy&quot;, &quot;重点班&quot;, &quot;一年一班&quot;));        userList.add(new Student(&quot;大毛&quot;, &quot;普通班&quot;, &quot;二年三班&quot;));        userList.add(new Student(&quot;Shing&quot;, &quot;普通班&quot;, &quot;三年四班&quot;));        userList.add(new Teacher(&quot;BK&quot;, &quot;特级教师&quot;, &quot;一年一班&quot;));        userList.add(new Teacher(&quot;娜娜Goddess&quot;, &quot;特级教师&quot;, &quot;一年一班&quot;));        userList.add(new Teacher(&quot;dangdang&quot;, &quot;普通教师&quot;, &quot;二年三班&quot;));        userList.add(new Teacher(&quot;泽东&quot;, &quot;实习教师&quot;, &quot;三年四班&quot;));    }    // 展示    public void show(Visitor visitor) {        for (User user : userList) {            user.accept(visitor);        }    }}</code></pre><h4 id="测试验证"><a href="#测试验证" class="headerlink" title="测试验证"></a>测试验证</h4><pre><code>@Testpublic void test(){    DataView dataView = new DataView();          logger.info(&quot;\r\n家长视角访问：&quot;);    dataView.show(new Parent());     // 家长    logger.info(&quot;\r\n校长视角访问：&quot;);    dataView.show(new Principal());  // 校长}</code></pre><h4 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a>测试结果</h4><pre><code>23:00:39.726 [main] INFO  org.itstack.demo.design.test.ApiTest - 家长视角访问：23:00:39.730 [main] INFO  o.i.demo.design.visitor.impl.Parent - 学生信息 姓名：谢飞机 班级：一年一班 排名：6223:00:39.730 [main] INFO  o.i.demo.design.visitor.impl.Parent - 学生信息 姓名：windy 班级：一年一班 排名：5123:00:39.730 [main] INFO  o.i.demo.design.visitor.impl.Parent - 学生信息 姓名：大毛 班级：二年三班 排名：1623:00:39.730 [main] INFO  o.i.demo.design.visitor.impl.Parent - 学生信息 姓名：Shing 班级：三年四班 排名：9823:00:39.730 [main] INFO  o.i.demo.design.visitor.impl.Parent - 老师信息 姓名：BK 班级：一年一班 级别：特级教师23:00:39.730 [main] INFO  o.i.demo.design.visitor.impl.Parent - 老师信息 姓名：娜娜Goddess 班级：一年一班 级别：特级教师23:00:39.730 [main] INFO  o.i.demo.design.visitor.impl.Parent - 老师信息 姓名：dangdang 班级：二年三班 级别：普通教师23:00:39.730 [main] INFO  o.i.demo.design.visitor.impl.Parent - 老师信息 姓名：泽东 班级：三年四班 级别：实习教师23:00:39.730 [main] INFO  org.itstack.demo.design.test.ApiTest - 校长视角访问：23:00:39.731 [main] INFO  o.i.d.design.visitor.impl.Principal - 学生信息 姓名：谢飞机 班级：一年一班23:00:39.731 [main] INFO  o.i.d.design.visitor.impl.Principal - 学生信息 姓名：windy 班级：一年一班23:00:39.731 [main] INFO  o.i.d.design.visitor.impl.Principal - 学生信息 姓名：大毛 班级：二年三班23:00:39.731 [main] INFO  o.i.d.design.visitor.impl.Principal - 学生信息 姓名：Shing 班级：三年四班23:00:39.733 [main] INFO  o.i.d.design.visitor.impl.Principal - 学生信息 姓名：BK 班级：一年一班 升学率：70.6223:00:39.733 [main] INFO  o.i.d.design.visitor.impl.Principal - 学生信息 姓名：娜娜Goddess 班级：一年一班 升学率：23.1523:00:39.734 [main] INFO  o.i.d.design.visitor.impl.Principal - 学生信息 姓名：dangdang 班级：二年三班 升学率：70.9823:00:39.734 [main] INFO  o.i.d.design.visitor.impl.Principal - 学生信息 姓名：泽东 班级：三年四班 升学率：90.14Process finished with exit code 0</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>在嵌入访问者模式后，可以让整个工程结构变得容易添加和修改。也就做到了系统服务之间的解耦，不至于为了不同类型信息的访问而增加很多多余的if判断或者类的强制转换。也就是通过这样的设计模式而让代码结构更加清晰。</li><li>说白了访问者模式的核心在于同一个事物不同视角下的访问信息不同。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模板模式</title>
      <link href="/2021/04/21/2021-04-21-design-mo-ban/"/>
      <url>/2021/04/21/2021-04-21-design-mo-ban/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="模版模式介绍"><a href="#模版模式介绍" class="headerlink" title="模版模式介绍"></a>模版模式介绍</h2><p>模板模式的核心设计思路是，通过在抽象类中定义抽象方法的执行顺序，并将抽象方法设定为只有子类实现，但不设计独立访问的方法。</p><p>关于模版模式的核心点在于由抽象类定义抽象方法执行策略，也就是说父类规定了好一系列的执行标准，这些标准的串联成一整套业务流程。</p><p>就像西游记的99八十一难，基本每一关都是；师傅被掳走、打妖怪、妖怪被收走，具体什么妖怪你自己定义，怎么打你想办法，最后收走还是弄死看你本事，我只定义执行顺序和基本策略，具体的每一难由观音来安排。</p><h2 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h2><p><img src="/2021/04/21/2021-04-21-design-mo-ban/itstack-demo-design-21-03.png" alt></p><p>爬取过程分为:模拟登录、爬取信息、生成海报，这三个步骤</p><h3 id="模版模式模型结构"><a href="#模版模式模型结构" class="headerlink" title="模版模式模型结构"></a>模版模式模型结构</h3><p><img src="/2021/04/21/2021-04-21-design-mo-ban/itstack-demo-design-21-04.png" alt></p><p>一个定义了抽象方法执行顺序的核心抽象类，以及三个模拟具体的实现(京东、淘宝、当当)的电商服务。</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="定义执行顺序的抽象类"><a href="#定义执行顺序的抽象类" class="headerlink" title="定义执行顺序的抽象类"></a>定义执行顺序的抽象类</h4><pre><code>/** * 基础电商推广服务 * 1. 生成最优价商品海报 * 2. 海报含带推广邀请码 */public abstract class NetMall {    protected Logger logger = LoggerFactory.getLogger(NetMall.class);    String uId;   // 用户ID    String uPwd;  // 用户密码    public NetMall(String uId, String uPwd) {        this.uId = uId;        this.uPwd = uPwd;    }    /**     * 生成商品推广海报     *     * @param skuUrl 商品地址(京东、淘宝、当当)     * @return 海报图片base64位信息     */    public String generateGoodsPoster(String skuUrl) {        if (!login(uId, uPwd)) return null;             // 1. 验证登录        Map&lt;String, String&gt; reptile = reptile(skuUrl);  // 2. 爬虫商品        return createBase64(reptile);                   // 3. 组装海报    }    // 模拟登录    protected abstract Boolean login(String uId, String uPwd);    // 爬虫提取商品信息(登录后的优惠价格)    protected abstract Map&lt;String, String&gt; reptile(String skuUrl);    // 生成商品海报信息    protected abstract String createBase64(Map&lt;String, String&gt; goodsInfo);}</code></pre><ul><li>定义可被外部访问的方法generateGoodsPoster，用于生成商品推广海报</li><li>generateGoodsPoster 在方法中定义抽象方法的执行顺序 1 2 3 步</li><li>提供三个具体的抽象方法，让外部继承方实现；模拟登录(login)、模拟爬取(reptile)、生成海报(createBase64)</li></ul><h4 id="模拟爬虫京东"><a href="#模拟爬虫京东" class="headerlink" title="模拟爬虫京东"></a>模拟爬虫京东</h4><pre><code>public class JDNetMall extends NetMall {    public JDNetMall(String uId, String uPwd) {        super(uId, uPwd);    }    public Boolean login(String uId, String uPwd) {        logger.info(&quot;模拟京东用户登录 uId：{} uPwd：{}&quot;, uId, uPwd);        return true;    }    public Map&lt;String, String&gt; reptile(String skuUrl) {        String str = HttpClient.doGet(skuUrl);        Pattern p9 = Pattern.compile(&quot;(?&lt;=title\\&gt;).*(?=&lt;/title)&quot;);        Matcher m9 = p9.matcher(str);        Map&lt;String, String&gt; map = new ConcurrentHashMap&lt;String, String&gt;();        if (m9.find()) {            map.put(&quot;name&quot;, m9.group());        }        map.put(&quot;price&quot;, &quot;5999.00&quot;);        logger.info(&quot;模拟京东商品爬虫解析：{} | {} 元 {}&quot;, map.get(&quot;name&quot;), map.get(&quot;price&quot;), skuUrl);        return map;    }    public String createBase64(Map&lt;String, String&gt; goodsInfo) {        BASE64Encoder encoder = new BASE64Encoder();        logger.info(&quot;模拟生成京东商品base64海报&quot;);        return encoder.encode(JSON.toJSONString(goodsInfo).getBytes());    }}</code></pre><h4 id="测试验证"><a href="#测试验证" class="headerlink" title="测试验证"></a>测试验证</h4><pre><code>@Testpublic void test_NetMall() {    NetMall netMall = new JDNetMall(&quot;1000001&quot;,&quot;*******&quot;);    String base64 = netMall.generateGoodsPoster(&quot;https://item.jd.com/100008348542.html&quot;);    logger.info(&quot;测试结果：{}&quot;, base64);}</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li><p>模版模式在定义统一结构也就是执行标准上非常方便，也就很好的控制了后续的实现者不用关心调用逻辑，按照统一方式执行。那么类的继承者只需要关心具体的业务逻辑实现即可。</p></li><li><p>另外模版模式也是为了解决子类通用方法，放到父类中设计的优化。让每一个子类只做子类需要完成的内容，而不需要关心其他逻辑。这样提取公用代码，行为由父类管理，扩展可变部分，也就非常有利于开发拓展和迭代。</p></li><li><p>模板模式的核心设计思路是，通过在抽象类中定义抽象方法的执行顺序，并将抽象方法设定为只有子类实现，但不设计独立访问的方法。</p></li><li><p>关于模版模式的核心点在于由抽象类定义抽象方法执行策略，也就是说父类规定了好一系列的执行标准，这些标准的串联成一整套业务流程。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>策略模式</title>
      <link href="/2021/04/20/2021-04-20-design-ce-lue/"/>
      <url>/2021/04/20/2021-04-20-design-ce-lue/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="策略模式介绍"><a href="#策略模式介绍" class="headerlink" title="策略模式介绍"></a>策略模式介绍</h2><p>策略模式是一种行为模式，也是替代大量ifelse的利器。</p><h2 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h2><p><img src="/2021/04/20/2021-04-20-design-ce-lue/itstack-demo-design-20-03.png" alt></p><h2 id="用一坨坨代码实现"><a href="#用一坨坨代码实现" class="headerlink" title="用一坨坨代码实现"></a>用一坨坨代码实现</h2><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><pre><code>public class CouponDiscountService {    public double discountAmount(int type, double typeContent, double skuPrice, double typeExt) {        // 1. 直减券        if (1 == type) {            return skuPrice - typeContent;        }        // 2. 满减券        if (2 == type) {            if (skuPrice &lt; typeExt) return skuPrice;            return skuPrice - typeContent;        }        // 3. 折扣券        if (3 == type) {            return skuPrice * typeContent;        }        // 4. n元购        if (4 == type) {            return typeContent;        }        return 0D;    }}</code></pre><h2 id="策略模式重构代码"><a href="#策略模式重构代码" class="headerlink" title="策略模式重构代码"></a>策略模式重构代码</h2><h3 id="策略模式模型结构"><a href="#策略模式模型结构" class="headerlink" title="策略模式模型结构"></a>策略模式模型结构</h3><p><img src="/2021/04/20/2021-04-20-design-ce-lue/itstack-demo-design-20-04.png" alt></p><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="优惠券接口"><a href="#优惠券接口" class="headerlink" title="优惠券接口"></a>优惠券接口</h4><pre><code>public interface ICouponDiscount&lt;T&gt; {    /**     * 优惠券金额计算     * @param couponInfo 券折扣信息；直减、满减、折扣、N元购     * @param skuPrice   sku金额     * @return           优惠后金额     */    BigDecimal discountAmount(T couponInfo, BigDecimal skuPrice);}</code></pre><ul><li>定义了优惠券折扣接口，也增加了泛型用于不同类型的接口可以传递不同的类型参数。</li><li>接口中包括商品金额以及出参返回最终折扣后的金额，这里在实际开发中会比现在的接口参数多一些，但核心逻辑是这些。</li></ul><h4 id="满减"><a href="#满减" class="headerlink" title="满减"></a>满减</h4><pre><code>public class MJCouponDiscount implements ICouponDiscount&lt;Map&lt;String,String&gt;&gt;  {    /**     * 满减计算     * 1. 判断满足x元后-n元，否则不减     * 2. 最低支付金额1元     */    public BigDecimal discountAmount(Map&lt;String,String&gt; couponInfo, BigDecimal skuPrice) {        String x = couponInfo.get(&quot;x&quot;);        String o = couponInfo.get(&quot;n&quot;);        // 小于商品金额条件的，直接返回商品原价        if (skuPrice.compareTo(new BigDecimal(x)) &lt; 0) return skuPrice;        // 减去优惠金额判断        BigDecimal discountAmount = skuPrice.subtract(new BigDecimal(o));        if (discountAmount.compareTo(BigDecimal.ZERO) &lt; 1) return BigDecimal.ONE;        return discountAmount;    }}</code></pre><h4 id="直减"><a href="#直减" class="headerlink" title="直减"></a>直减</h4><pre><code>public class ZJCouponDiscount implements ICouponDiscount&lt;Double&gt;  {    /**     * 直减计算     * 1. 使用商品价格减去优惠价格     * 2. 最低支付金额1元     */    public BigDecimal discountAmount(Double couponInfo, BigDecimal skuPrice) {        BigDecimal discountAmount = skuPrice.subtract(new BigDecimal(couponInfo));        if (discountAmount.compareTo(BigDecimal.ZERO) &lt; 1) return BigDecimal.ONE;        return discountAmount;    }}</code></pre><h4 id="折扣"><a href="#折扣" class="headerlink" title="折扣"></a>折扣</h4><pre><code>public class ZKCouponDiscount implements ICouponDiscount&lt;Double&gt; {    /**     * 折扣计算     * 1. 使用商品价格乘以折扣比例，为最后支付金额     * 2. 保留两位小数     * 3. 最低支付金额1元     */    public BigDecimal discountAmount(Double couponInfo, BigDecimal skuPrice) {        BigDecimal discountAmount = skuPrice.multiply(new BigDecimal(couponInfo)).setScale(2, BigDecimal.ROUND_HALF_UP);        if (discountAmount.compareTo(BigDecimal.ZERO) &lt; 1) return BigDecimal.ONE;        return discountAmount;    }}</code></pre><h4 id="N元购"><a href="#N元购" class="headerlink" title="N元购"></a>N元购</h4><pre><code>public class NYGCouponDiscount implements ICouponDiscount&lt;Double&gt; {    /**     * n元购购买     * 1. 无论原价多少钱都固定金额购买     */    public BigDecimal discountAmount(Double couponInfo, BigDecimal skuPrice) {        return new BigDecimal(couponInfo);    }}</code></pre><h4 id="策略控制类"><a href="#策略控制类" class="headerlink" title="策略控制类"></a>策略控制类</h4><pre><code>public class Context&lt;T&gt; {    private ICouponDiscount&lt;T&gt; couponDiscount;    public Context(ICouponDiscount&lt;T&gt; couponDiscount) {        this.couponDiscount = couponDiscount;    }    public BigDecimal discountAmount(T couponInfo, BigDecimal skuPrice) {        return couponDiscount.discountAmount(couponInfo, skuPrice);    }}</code></pre><h4 id="测试验证"><a href="#测试验证" class="headerlink" title="测试验证"></a>测试验证</h4><pre><code>@Testpublic void test_zj() {    // 直减；100-10，商品100元    Context&lt;Double&gt; context = new Context&lt;Double&gt;(new ZJCouponDiscount());    BigDecimal discountAmount = context.discountAmount(10D, new BigDecimal(100));    logger.info(&quot;测试结果：直减优惠后金额 {}&quot;, discountAmount);}</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>通过策略设计模式的使用可以把我们方法中的if语句优化掉，大量的if语句使用会让代码难以扩展，也不好维护，同时在后期遇到各种问题也很难维护。在使用这样的设计模式后可以很好的满足隔离性与和扩展性，对于不断新增的需求也非常方便承接。</li><li>按照个人风格，一般工厂和策略模式一起使用。<a href="https://adbo.gitee.io/2020/08/12/2020-08-12-ce-lue-mo-shi/">工作中结合Spring案例</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>状态模式</title>
      <link href="/2021/04/19/2021-04-19-design-zhuang-tai/"/>
      <url>/2021/04/19/2021-04-19-design-zhuang-tai/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="状态模式介绍"><a href="#状态模式介绍" class="headerlink" title="状态模式介绍"></a>状态模式介绍</h2><p>状态模式描述的是一个行为下的多种状态变更，比如我们最常见的一个网站的页面，在你登录与不登录下展示的内容是略有差异的(不登录不能展示个人信息)，而这种登录与不登录就是我们通过改变状态，而让整个行为发生了变化。</p><h2 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h2><pre><code>itstack-demo-design-19-00└── src    └── main        └── java            └── org.itstack.demo.design                ├── ActivityInfo.java                ├── Status.java                └── ActivityService.java</code></pre><p>在这个模拟工程里我们提供了三个类，包括；状态枚举(Status)、活动对象(ActivityInfo)、活动服务(ActivityService)，三个服务类。</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="基本活动信息"><a href="#基本活动信息" class="headerlink" title="基本活动信息"></a>基本活动信息</h4><pre><code>public class ActivityInfo {    private String activityId;    // 活动ID    private String activityName;  // 活动名称    private Enum&lt;Status&gt; status;  // 活动状态    private Date beginTime;       // 开始时间    private Date endTime;         // 结束时间    // ...get/set}  </code></pre><h4 id="活动枚举状态"><a href="#活动枚举状态" class="headerlink" title="活动枚举状态"></a>活动枚举状态</h4><pre><code>public enum Status {    // 1创建编辑、2待审核、3审核通过(任务扫描成活动中)、4审核拒绝(可以撤审到编辑状态)、5活动中、6活动关闭、7活动开启(任务扫描成活动中)    Editing, Check, Pass, Refuse, Doing, Close, Open}</code></pre><h4 id="活动服务接口"><a href="#活动服务接口" class="headerlink" title="活动服务接口"></a>活动服务接口</h4><pre><code>public class ActivityService {    private static Map&lt;String, Enum&lt;Status&gt;&gt; statusMap = new ConcurrentHashMap&lt;String, Enum&lt;Status&gt;&gt;();    public static void init(String activityId, Enum&lt;Status&gt; status) {        // 模拟查询活动信息        ActivityInfo activityInfo = new ActivityInfo();        activityInfo.setActivityId(activityId);        activityInfo.setActivityName(&quot;早起学习打卡领奖活动&quot;);        activityInfo.setStatus(status);        activityInfo.setBeginTime(new Date());        activityInfo.setEndTime(new Date());        statusMap.put(activityId, status);    }    /**     * 查询活动信息     *     * @param activityId 活动ID     * @return 查询结果     */    public static ActivityInfo queryActivityInfo(String activityId) {        // 模拟查询活动信息        ActivityInfo activityInfo = new ActivityInfo();        activityInfo.setActivityId(activityId);        activityInfo.setActivityName(&quot;早起学习打卡领奖活动&quot;);        activityInfo.setStatus(statusMap.get(activityId));        activityInfo.setBeginTime(new Date());        activityInfo.setEndTime(new Date());        return activityInfo;    }    /**     * 查询活动状态     *     * @param activityId 活动ID     * @return 查询结果     */    public static Enum&lt;Status&gt; queryActivityStatus(String activityId) {        return statusMap.get(activityId);    }    /**     * 执行状态变更     *     * @param activityId   活动ID     * @param beforeStatus 变更前状态     * @param afterStatus  变更后状态 b     */    public static synchronized void execStatus(String activityId, Enum&lt;Status&gt; beforeStatus, Enum&lt;Status&gt; afterStatus) {        if (!beforeStatus.equals(statusMap.get(activityId))) return;        statusMap.put(activityId, afterStatus);    }}</code></pre><h2 id="用一坨坨代码实现"><a href="#用一坨坨代码实现" class="headerlink" title="用一坨坨代码实现"></a>用一坨坨代码实现</h2><h3 id="工程结构"><a href="#工程结构" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-19-01└── src    └── main        └── java            └── org.itstack.demo.design                ├── ActivityExecStatusController.java                └── Result.java</code></pre><p>整个实现的工程结构比较简单，只包括了两个类；ActivityExecStatusController、Result，一个是处理流程状态，另外一个是返回的对象。</p><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><pre><code>public class ActivityExecStatusController {    /**     * 活动状态变更     * 1. 编辑中 -&gt; 提审、关闭     * 2. 审核通过 -&gt; 拒绝、关闭、活动中     * 3. 审核拒绝 -&gt; 撤审、关闭     * 4. 活动中 -&gt; 关闭     * 5. 活动关闭 -&gt; 开启     * 6. 活动开启 -&gt; 关闭     *     * @param activityId   活动ID     * @param beforeStatus 变更前状态     * @param afterStatus  变更后状态     * @return 返回结果     */    public Result execStatus(String activityId, Enum&lt;Status&gt; beforeStatus, Enum&lt;Status&gt; afterStatus) {        // 1. 编辑中 -&gt; 提审、关闭        if (Status.Editing.equals(beforeStatus)) {            if (Status.Check.equals(afterStatus) || Status.Close.equals(afterStatus)) {                ActivityService.execStatus(activityId, beforeStatus, afterStatus);                return new Result(&quot;0000&quot;, &quot;变更状态成功&quot;);            } else {                return new Result(&quot;0001&quot;, &quot;变更状态拒绝&quot;);            }        }        // 2. 审核通过 -&gt; 拒绝、关闭、活动中        if (Status.Pass.equals(beforeStatus)) {            if (Status.Refuse.equals(afterStatus) || Status.Doing.equals(afterStatus) || Status.Close.equals(afterStatus)) {                ActivityService.execStatus(activityId, beforeStatus, afterStatus);                return new Result(&quot;0000&quot;, &quot;变更状态成功&quot;);            } else {                return new Result(&quot;0001&quot;, &quot;变更状态拒绝&quot;);            }        }        // 3. 审核拒绝 -&gt; 撤审、关闭        if (Status.Refuse.equals(beforeStatus)) {            if (Status.Editing.equals(afterStatus) || Status.Close.equals(afterStatus)) {                ActivityService.execStatus(activityId, beforeStatus, afterStatus);                return new Result(&quot;0000&quot;, &quot;变更状态成功&quot;);            } else {                return new Result(&quot;0001&quot;, &quot;变更状态拒绝&quot;);            }        }        // 4. 活动中 -&gt; 关闭        if (Status.Doing.equals(beforeStatus)) {            if (Status.Close.equals(afterStatus)) {                ActivityService.execStatus(activityId, beforeStatus, afterStatus);                return new Result(&quot;0000&quot;, &quot;变更状态成功&quot;);            } else {                return new Result(&quot;0001&quot;, &quot;变更状态拒绝&quot;);            }        }        // 5. 活动关闭 -&gt; 开启        if (Status.Close.equals(beforeStatus)) {            if (Status.Open.equals(afterStatus)) {                ActivityService.execStatus(activityId, beforeStatus, afterStatus);                return new Result(&quot;0000&quot;, &quot;变更状态成功&quot;);            } else {                return new Result(&quot;0001&quot;, &quot;变更状态拒绝&quot;);            }        }        // 6. 活动开启 -&gt; 关闭        if (Status.Open.equals(beforeStatus)) {            if (Status.Close.equals(afterStatus)) {                ActivityService.execStatus(activityId, beforeStatus, afterStatus);                return new Result(&quot;0000&quot;, &quot;变更状态成功&quot;);            } else {                return new Result(&quot;0001&quot;, &quot;变更状态拒绝&quot;);            }        }        return new Result(&quot;0001&quot;, &quot;非可处理的活动状态变更&quot;);    }}</code></pre><h2 id="状态模式重构代码"><a href="#状态模式重构代码" class="headerlink" title="状态模式重构代码"></a>状态模式重构代码</h2><h3 id="工程结构-1"><a href="#工程结构-1" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-19-02└── src    └── main        └── java            └── org.itstack.demo.design                ├── event                │    ├── CheckState.java                │    └── CloseState.java                │    └── DoingState.java                │    └── EditingState.java                │    └── OpenState.java                │    └── PassState.java                │    └── RefuseState.java                ├── Result.java                ├── State.java                └── StateHandler.java</code></pre><p>状态模式模型结构</p><p><img src="/2021/04/19/2021-04-19-design-zhuang-tai/itstack-demo-design-19-04.png" alt></p><ul><li>以上是状态模式的整个工程结构模型，State是一个抽象类，定义了各种操作接口(提审、审核、拒审等)。</li><li>右侧的不同颜色状态与我们场景模拟中的颜色保持一致，是各种状态流程流转的实现操作。这里的实现有一个关键点就是每一种状态到下一个状态，都分配到各个实现方法中控制，也就不需要if语言进行判断了。</li><li>最后是StateHandler对状态流程的统一处理，里面提供Map结构的各项服务接口调用，也就避免了使用if判断各项状态转变的流程。</li></ul><h3 id="代码实现-2"><a href="#代码实现-2" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="定义状态抽象类"><a href="#定义状态抽象类" class="headerlink" title="定义状态抽象类"></a>定义状态抽象类</h4><pre><code>ublic abstract class State {    /**     * 活动提审     *     * @param activityId    活动ID     * @param currentStatus 当前状态     * @return 执行结果     */    public abstract Result arraignment(String activityId, Enum&lt;Status&gt; currentStatus);    /**     * 审核通过     *     * @param activityId    活动ID     * @param currentStatus 当前状态     * @return 执行结果     */    public abstract Result checkPass(String activityId, Enum&lt;Status&gt; currentStatus);    /**     * 审核拒绝     *     * @param activityId    活动ID     * @param currentStatus 当前状态     * @return 执行结果     */    public abstract Result checkRefuse(String activityId, Enum&lt;Status&gt; currentStatus);    /**     * 撤审撤销     *     * @param activityId    活动ID     * @param currentStatus 当前状态     * @return 执行结果     */    public abstract Result checkRevoke(String activityId, Enum&lt;Status&gt; currentStatus);    /**     * 活动关闭     *     * @param activityId    活动ID     * @param currentStatus 当前状态     * @return 执行结果     */    public abstract Result close(String activityId, Enum&lt;Status&gt; currentStatus);    /**     * 活动开启     *     * @param activityId    活动ID     * @param currentStatus 当前状态     * @return 执行结果     */    public abstract Result open(String activityId, Enum&lt;Status&gt; currentStatus);    /**     * 活动执行     *     * @param activityId    活动ID     * @param currentStatus 当前状态     * @return 执行结果     */    public abstract Result doing(String activityId, Enum&lt;Status&gt; currentStatus);}</code></pre><h4 id="编辑"><a href="#编辑" class="headerlink" title="编辑"></a>编辑</h4><pre><code>public class EditingState extends State {    public Result arraignment(String activityId, Enum&lt;Status&gt; currentStatus) {        ActivityService.execStatus(activityId, currentStatus, Status.Check);        return new Result(&quot;0000&quot;, &quot;活动提审成功&quot;);    }    public Result checkPass(String activityId, Enum&lt;Status&gt; currentStatus) {        return new Result(&quot;0001&quot;, &quot;编辑中不可审核通过&quot;);    }    public Result checkRefuse(String activityId, Enum&lt;Status&gt; currentStatus) {        return new Result(&quot;0001&quot;, &quot;编辑中不可审核拒绝&quot;);    }    @Override    public Result checkRevoke(String activityId, Enum&lt;Status&gt; currentStatus) {        return new Result(&quot;0001&quot;, &quot;编辑中不可撤销审核&quot;);    }    public Result close(String activityId, Enum&lt;Status&gt; currentStatus) {        ActivityService.execStatus(activityId, currentStatus, Status.Close);        return new Result(&quot;0000&quot;, &quot;活动关闭成功&quot;);    }    public Result open(String activityId, Enum&lt;Status&gt; currentStatus) {        return new Result(&quot;0001&quot;, &quot;非关闭活动不可开启&quot;);    }    public Result doing(String activityId, Enum&lt;Status&gt; currentStatus) {        return new Result(&quot;0001&quot;, &quot;编辑中活动不可执行活动中变更&quot;);    }}</code></pre><h4 id="提审"><a href="#提审" class="headerlink" title="提审"></a>提审</h4><pre><code>public class CheckState extends State {    public Result arraignment(String activityId, Enum&lt;Status&gt; currentStatus) {        return new Result(&quot;0001&quot;, &quot;待审核状态不可重复提审&quot;);    }    public Result checkPass(String activityId, Enum&lt;Status&gt; currentStatus) {        ActivityService.execStatus(activityId, currentStatus, Status.Pass);        return new Result(&quot;0000&quot;, &quot;活动审核通过完成&quot;);    }    public Result checkRefuse(String activityId, Enum&lt;Status&gt; currentStatus) {        ActivityService.execStatus(activityId, currentStatus, Status.Refuse);        return new Result(&quot;0000&quot;, &quot;活动审核拒绝完成&quot;);    }    @Override    public Result checkRevoke(String activityId, Enum&lt;Status&gt; currentStatus) {        ActivityService.execStatus(activityId, currentStatus, Status.Editing);        return new Result(&quot;0000&quot;, &quot;活动审核撤销回到编辑中&quot;);    }    public Result close(String activityId, Enum&lt;Status&gt; currentStatus) {        ActivityService.execStatus(activityId, currentStatus, Status.Close);        return new Result(&quot;0000&quot;, &quot;活动审核关闭完成&quot;);    }    public Result open(String activityId, Enum&lt;Status&gt; currentStatus) {        return new Result(&quot;0001&quot;, &quot;非关闭活动不可开启&quot;);    }    public Result doing(String activityId, Enum&lt;Status&gt; currentStatus) {        return new Result(&quot;0001&quot;, &quot;待审核活动不可执行活动中变更&quot;);    }}</code></pre><h4 id="状态处理服务"><a href="#状态处理服务" class="headerlink" title="状态处理服务"></a>状态处理服务</h4><pre><code>public class StateHandler {    private Map&lt;Enum&lt;Status&gt;, State&gt; stateMap = new ConcurrentHashMap&lt;Enum&lt;Status&gt;, State&gt;();    public StateHandler() {        stateMap.put(Status.Check, new CheckState());     // 待审核        stateMap.put(Status.Close, new CloseState());     // 已关闭        stateMap.put(Status.Doing, new DoingState());     // 活动中        stateMap.put(Status.Editing, new EditingState()); // 编辑中        stateMap.put(Status.Open, new OpenState());       // 已开启        stateMap.put(Status.Pass, new PassState());       // 审核通过        stateMap.put(Status.Refuse, new RefuseState());   // 审核拒绝    }    public Result arraignment(String activityId, Enum&lt;Status&gt; currentStatus) {        return stateMap.get(currentStatus).arraignment(activityId, currentStatus);    }    public Result checkPass(String activityId, Enum&lt;Status&gt; currentStatus) {        return stateMap.get(currentStatus).checkPass(activityId, currentStatus);    }    public Result checkRefuse(String activityId, Enum&lt;Status&gt; currentStatus) {        return stateMap.get(currentStatus).checkRefuse(activityId, currentStatus);    }    public Result checkRevoke(String activityId, Enum&lt;Status&gt; currentStatus) {        return stateMap.get(currentStatus).checkRevoke(activityId, currentStatus);    }    public Result close(String activityId, Enum&lt;Status&gt; currentStatus) {        return stateMap.get(currentStatus).close(activityId, currentStatus);    }    public Result open(String activityId, Enum&lt;Status&gt; currentStatus) {        return stateMap.get(currentStatus).open(activityId, currentStatus);    }    public Result doing(String activityId, Enum&lt;Status&gt; currentStatus) {        return stateMap.get(currentStatus).doing(activityId, currentStatus);    }}</code></pre><h4 id="测试验证"><a href="#测试验证" class="headerlink" title="测试验证"></a>测试验证</h4><pre><code>@Testpublic void test_Editing2Arraignment() {    String activityId = &quot;100001&quot;;    ActivityService.init(activityId, Status.Editing);    StateHandler stateHandler = new StateHandler();    Result result = stateHandler.arraignment(activityId, Status.Editing);    logger.info(&quot;测试结果(编辑中To提审活动)：{}&quot;, JSON.toJSONString(result));    logger.info(&quot;活动信息：{} 状态：{}&quot;, JSON.toJSONString(ActivityService.queryActivityInfo(activityId)), JSON.toJSONString(ActivityService.queryActivityInfo(activityId).getStatus()));}23:59:20.883 [main] INFO  org.itstack.demo.design.test.ApiTest - 测试结果(编辑中To提审活动)：{&quot;code&quot;:&quot;0000&quot;,&quot;info&quot;:&quot;活动提审成功&quot;}23:59:20.907 [main] INFO  org.itstack.demo.design.test.ApiTest - 活动信息：{&quot;activityId&quot;:&quot;100001&quot;,&quot;activityName&quot;:&quot;早起学习打卡领奖活动&quot;,&quot;beginTime&quot;:1593694760892,&quot;endTime&quot;:1593694760892,&quot;status&quot;:&quot;Check&quot;} 状态：&quot;Check&quot;Process finished with exit code 0</code></pre><pre><code>@Testpublic void test_Editing2Open() {    String activityId = &quot;100001&quot;;    ActivityService.init(activityId, Status.Editing);    StateHandler stateHandler = new StateHandler();    Result result = stateHandler.open(activityId, Status.Editing);    logger.info(&quot;测试结果(编辑中To开启活动)：{}&quot;, JSON.toJSONString(result));    logger.info(&quot;活动信息：{} 状态：{}&quot;, JSON.toJSONString(ActivityService.queryActivityInfo(activityId)), JSON.toJSONString(ActivityService.queryActivityInfo(activityId).getStatus()));}23:59:36.904 [main] INFO  org.itstack.demo.design.test.ApiTest - 测试结果(编辑中To开启活动)：{&quot;code&quot;:&quot;0001&quot;,&quot;info&quot;:&quot;非关闭活动不可开启&quot;}23:59:36.914 [main] INFO  org.itstack.demo.design.test.ApiTest - 活动信息：{&quot;activityId&quot;:&quot;100001&quot;,&quot;activityName&quot;:&quot;早起学习打卡领奖活动&quot;,&quot;beginTime&quot;:1593694776907,&quot;endTime&quot;:1593694776907,&quot;status&quot;:&quot;Editing&quot;} 状态：&quot;Editing&quot;Process finished with exit code 0</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>状态模式描述的是一个行为下的多种状态变更。减少if else判断，非常容易维护。伴随着状态种类越多，会造成多个类的编写。</li><li>非常适用与状态流转的场景，例如用户状态审核，订单状态流转等。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>观察者模式</title>
      <link href="/2021/04/18/2021-04-18-design-guan-cha-zhe/"/>
      <url>/2021/04/18/2021-04-18-design-guan-cha-zhe/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="观察者模式介绍"><a href="#观察者模式介绍" class="headerlink" title="观察者模式介绍"></a>观察者模式介绍</h2><p>简单来讲观察者🕵模式，就是当一个行为发生时传递信息给另外一个用户接收做出相应的处理，两者之间没有直接的耦合关联。</p><p>除了生活中的场景外，在我们编程开发中也会常用到一些观察者的模式或者组件，例如我们经常使用的MQ服务，虽然MQ服务是有一个通知中心并不是每一个类服务进行通知，但整体上也可以算作是观察者模式的思路设计。再比如可能有做过的一些类似事件监听总线，让主线服务与其他辅线业务服务分离，为了使系统降低耦合和增强扩展性，也会使用观察者模式进行处理。</p><h2 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h2><pre><code>itstack-demo-design-18-00└── src    └── main        └── java            └── org.itstack.demo.design                └── MinibusTargetService.java</code></pre><h3 id="场景简述"><a href="#场景简述" class="headerlink" title="场景简述"></a>场景简述</h3><h4 id="摇号服务接口"><a href="#摇号服务接口" class="headerlink" title="摇号服务接口"></a>摇号服务接口</h4><pre><code>public class MinibusTargetService {    /**     * 模拟摇号，但不是摇号算法     *     * @param uId 用户编号     * @return 结果     */    public String lottery(String uId) {        return Math.abs(uId.hashCode()) % 2 == 0 ? &quot;恭喜你，编码&quot;.concat(uId).concat(&quot;在本次摇号中签&quot;) : &quot;很遗憾，编码&quot;.concat(uId).concat(&quot;在本次摇号未中签或摇号资格已过期&quot;);    }}</code></pre><h2 id="用一坨坨代码实现"><a href="#用一坨坨代码实现" class="headerlink" title="用一坨坨代码实现"></a>用一坨坨代码实现</h2><p>按照需求需要在原有的摇号接口中添加MQ消息发送以及短消息通知功能，如果是最直接的方式那么可以直接在方法中补充功能即可。</p><h3 id="工程结构"><a href="#工程结构" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-18-01└── src    └── main        └── java            └── org.itstack.demo.design                ├── LotteryResult.java                ├── LotteryService.java                └── LotteryServiceImpl.java</code></pre><p>这段代码接口中包括了三部分内容；返回对象(LotteryResult)、定义接口(LotteryService)、具体实现(LotteryServiceImpl)。</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><pre><code>public class LotteryServiceImpl implements LotteryService {    private Logger logger = LoggerFactory.getLogger(LotteryServiceImpl.class);    private MinibusTargetService minibusTargetService = new MinibusTargetService();    public LotteryResult doDraw(String uId) {        // 摇号        String lottery = minibusTargetService.lottery(uId);        // 发短信        logger.info(&quot;给用户 {} 发送短信通知(短信)：{}&quot;, uId, lottery);        // 发MQ消息        logger.info(&quot;记录用户 {} 摇号结果(MQ)：{}&quot;, uId, lottery);        // 结果        return new LotteryResult(uId, lottery, new Date());    }}</code></pre><h2 id="观察者模式重构代码"><a href="#观察者模式重构代码" class="headerlink" title="观察者模式重构代码"></a>观察者模式重构代码</h2><h3 id="工程结构-1"><a href="#工程结构-1" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-18-02└── src    └── main        └── java            └── org.itstack.demo.design                ├── event                │    ├── listener                │    │    ├── EventListener.java                │    │    ├── MessageEventListener.java                │    │    └── MQEventListener.java                │    └── EventManager.java                ├── LotteryResult.java                ├── LotteryService.java                └── LotteryServiceImpl.java</code></pre><p>观察者模式模型结构</p><p><img src="/2021/04/18/2021-04-18-design-guan-cha-zhe/itstack-demo-design-18-04.png" alt></p><ul><li>从上图可以分为三大块看；事件监听、事件处理、具体的业务流程，另外在业务流程中 LotteryService 定义的是抽象类，因为这样可以通过抽象类将事件功能屏蔽，外部业务流程开发者不需要知道具体的通知操作。</li><li>右下角圆圈图表示的是核心流程与非核心流程的结构，一般在开发中会把主线流程开发完成后，再使用通知的方式处理辅助流程。他们可以是异步的，在MQ以及定时任务的处理下，保证最终一致性。</li></ul><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="事件监听接口定义"><a href="#事件监听接口定义" class="headerlink" title="事件监听接口定义"></a>事件监听接口定义</h4><pre><code>public interface EventListener {    void doEvent(LotteryResult result);}</code></pre><p>接口中定义了基本的事件类，这里如果方法的入参信息类型是变化的可以使用泛型<t></t></p><h4 id="短消息事件"><a href="#短消息事件" class="headerlink" title="短消息事件"></a>短消息事件</h4><pre><code>public class MessageEventListener implements EventListener {    private Logger logger = LoggerFactory.getLogger(MessageEventListener.class);    @Override    public void doEvent(LotteryResult result) {        logger.info(&quot;给用户 {} 发送短信通知(短信)：{}&quot;, result.getuId(), result.getMsg());    }}</code></pre><h4 id="MQ发送事件"><a href="#MQ发送事件" class="headerlink" title="MQ发送事件"></a>MQ发送事件</h4><pre><code>public class MQEventListener implements EventListener {    private Logger logger = LoggerFactory.getLogger(MQEventListener.class);    @Override    public void doEvent(LotteryResult result) {        logger.info(&quot;记录用户 {} 摇号结果(MQ)：{}&quot;, result.getuId(), result.getMsg());    }}</code></pre><h4 id="事件处理类"><a href="#事件处理类" class="headerlink" title="事件处理类"></a>事件处理类</h4><pre><code>public class EventManager {    Map&lt;Enum&lt;EventType&gt;, List&lt;EventListener&gt;&gt; listeners = new HashMap&lt;&gt;();    public EventManager(Enum&lt;EventType&gt;... operations) {        for (Enum&lt;EventType&gt; operation : operations) {            this.listeners.put(operation, new ArrayList&lt;&gt;());        }    }    public enum EventType {        MQ, Message    }    /**     * 订阅     * @param eventType 事件类型     * @param listener  监听     */    public void subscribe(Enum&lt;EventType&gt; eventType, EventListener listener) {        List&lt;EventListener&gt; users = listeners.get(eventType);        users.add(listener);    }    /**     * 取消订阅     * @param eventType 事件类型     * @param listener  监听     */    public void unsubscribe(Enum&lt;EventType&gt; eventType, EventListener listener) {        List&lt;EventListener&gt; users = listeners.get(eventType);        users.remove(listener);    }    /**     * 通知     * @param eventType 事件类型     * @param result    结果     */    public void notify(Enum&lt;EventType&gt; eventType, LotteryResult result) {        List&lt;EventListener&gt; users = listeners.get(eventType);        for (EventListener listener : users) {            listener.doEvent(result);        }    }}</code></pre><ul><li>整个处理的实现上提供了三个主要方法；订阅(subscribe)、取消订阅(unsubscribe)、通知(notify)。这三个方法分别用于对监听时间的添加和使用。</li><li>另外因为事件有不同的类型，这里使用了枚举的方式进行处理，也方便让外部在规定下使用事件，而不至于乱传信息(EventType.MQ、EventType.Message)。</li></ul><h4 id="业务抽象类接口"><a href="#业务抽象类接口" class="headerlink" title="业务抽象类接口"></a>业务抽象类接口</h4><pre><code>public abstract class LotteryService {    private EventManager eventManager;    public LotteryService() {        eventManager = new EventManager(EventManager.EventType.MQ, EventManager.EventType.Message);        eventManager.subscribe(EventManager.EventType.MQ, new MQEventListener());        eventManager.subscribe(EventManager.EventType.Message, new MessageEventListener());    }    public LotteryResult draw(String uId) {        LotteryResult lotteryResult = doDraw(uId);        // 需要什么通知就给调用什么方法        eventManager.notify(EventManager.EventType.MQ, lotteryResult);        eventManager.notify(EventManager.EventType.Message, lotteryResult);        return lotteryResult;    }    protected abstract LotteryResult doDraw(String uId);}</code></pre><h4 id="业务接口实现类"><a href="#业务接口实现类" class="headerlink" title="业务接口实现类"></a>业务接口实现类</h4><pre><code>public class LotteryServiceImpl extends LotteryService {    private MinibusTargetService minibusTargetService = new MinibusTargetService();    @Override    protected LotteryResult doDraw(String uId) {        // 摇号        String lottery = minibusTargetService.lottery(uId);        // 结果        return new LotteryResult(uId, lottery, new Date());    }}</code></pre><h4 id="测试验证"><a href="#测试验证" class="headerlink" title="测试验证"></a>测试验证</h4><pre><code>@Testpublic void test() {    LotteryService lotteryService = new LotteryServiceImpl();    LotteryResult result = lotteryService.draw(&quot;2765789109876&quot;);    logger.info(&quot;测试结果：{}&quot;, JSON.toJSONString(result));}</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>简单来讲观察者🕵模式，就是当一个行为发生时传递信息给另外一个用户接收做出相应的处理，两者之间没有直接的耦合关联。</p>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>备忘录模式</title>
      <link href="/2021/04/17/2021-04-17-design-bei-wang-lu/"/>
      <url>/2021/04/17/2021-04-17-design-bei-wang-lu/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="备忘录模式介绍"><a href="#备忘录模式介绍" class="headerlink" title="备忘录模式介绍"></a>备忘录模式介绍</h2><p>备忘录模式是以可以恢复或者说回滚，配置、版本、悔棋为核心功能的设计模式，而这种设计模式属于行为模式。在功能实现上是以不破坏原对象为基础增加备忘录操作类，记录原对象的行为从而实现备忘录模式。</p><p>这个设计在我们平常的生活或者开发中也是比较常见的，比如：后悔药、孟婆汤(一下回滚到0)，IDEA编辑和撤销、小霸王游戏机存档。</p><h2 id="备忘录模式记录配置文件版本信息"><a href="#备忘录模式记录配置文件版本信息" class="headerlink" title="备忘录模式记录配置文件版本信息"></a>备忘录模式记录配置文件版本信息</h2><p>备忘录的设计模式实现方式，重点在于不更改原有类的基础上，增加备忘录类存放记录。可能平时虽然不一定非得按照这个设计模式的代码结构来实现自己的需求，但是对于功能上可能也完成过类似的功能，记录系统的信息。</p><h2 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h2><p><img src="/2021/04/17/2021-04-17-design-bei-wang-lu/itstack-demo-design-17-03.png" alt></p><h3 id="工程结构"><a href="#工程结构" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-17-00└── src    ├── main    │   └── java    │       └── org.itstack.demo.design    │           ├── Admin.java    │           ├── ConfigFile.java    │           ├── ConfigMemento.java    │           └── ConfigOriginator.java    └── test        └── java            └── org.itstack.demo.design.test                └── ApiTest.java</code></pre><p>备忘录模式模型结构</p><p><img src="/2021/04/17/2021-04-17-design-bei-wang-lu/itstack-demo-design-17-04.png" alt></p><ul><li>以上是工程结构的一个类图，其实相对来说并不复杂，除了原有的配置类(ConfigFile)以外，只新增加了三个类。</li><li>ConfigMemento：备忘录类，相当于是对原有配置类的扩展</li><li>ConfigOriginator：记录者类，获取和返回备忘录类对象信息</li><li>Admin：管理员类，用于操作记录备忘信息，比如你一些列的顺序执行了什么或者某个版本下的内容信息</li></ul><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="配置信息类"><a href="#配置信息类" class="headerlink" title="配置信息类"></a>配置信息类</h4><pre><code>public class ConfigFile {    private String versionNo; // 版本号    private String content;   // 内容    private Date dateTime;    // 时间    private String operator;  // 操作人    // ...get/set}</code></pre><p>配置类可以是任何形式的，这里只是简单的描述了一个基本的配置内容信息。</p><h4 id="备忘录类"><a href="#备忘录类" class="headerlink" title="备忘录类"></a>备忘录类</h4><pre><code>public class ConfigMemento {    private ConfigFile configFile;    public ConfigMemento(ConfigFile configFile) {        this.configFile = configFile;    }    public ConfigFile getConfigFile() {        return configFile;    }    public void setConfigFile(ConfigFile configFile) {        this.configFile = configFile;    }}</code></pre><p>备忘录是对原有配置类的扩展，可以设置和获取配置信息。</p><h4 id="记录者类"><a href="#记录者类" class="headerlink" title="记录者类"></a>记录者类</h4><pre><code>public class ConfigOriginator {    private ConfigFile configFile;    public ConfigFile getConfigFile() {        return configFile;    }    public void setConfigFile(ConfigFile configFile) {        this.configFile = configFile;    }    public ConfigMemento saveMemento(){        return new ConfigMemento(configFile);    }    public void getMemento(ConfigMemento memento){        this.configFile = memento.getConfigFile();    }}</code></pre><ul><li><p>记录者类除了对ConfigFile配置类增加了获取和设置方法外，还增加了保存saveMemento()、获取getMemento(ConfigMemento memento)。</p></li><li><p>saveMemento：保存备忘录的时候会创建一个备忘录信息，并返回回去，交给管理者处理。</p></li><li><p>getMemento：获取的之后并不是直接返回，而是把备忘录的信息交给现在的配置文件this.configFile，这部分需要注意。</p></li></ul><h4 id="管理员类"><a href="#管理员类" class="headerlink" title="管理员类"></a>管理员类</h4><pre><code>public class Admin {    private int cursorIdx = 0;    private List&lt;ConfigMemento&gt; mementoList = new ArrayList&lt;ConfigMemento&gt;();    private Map&lt;String, ConfigMemento&gt; mementoMap = new ConcurrentHashMap&lt;String, ConfigMemento&gt;();    public void append(ConfigMemento memento) {        mementoList.add(memento);        mementoMap.put(memento.getConfigFile().getVersionNo(), memento);        cursorIdx++;    }    public ConfigMemento undo() {        if (--cursorIdx &lt;= 0) return mementoList.get(0);        return mementoList.get(cursorIdx);    }    public ConfigMemento redo() {        if (++cursorIdx &gt; mementoList.size()) return mementoList.get(mementoList.size() - 1);        return mementoList.get(cursorIdx);    }    public ConfigMemento get(String versionNo){        return mementoMap.get(versionNo);    }}</code></pre><ul><li>在这个类中主要实现的核心功能就是记录配置文件信息，也就是备忘录的效果，之后提供可以回滚和获取的方法，拿到备忘录的具体内容。</li><li>同时这里设置了两个数据结构来存放备忘录，实际使用中可以按需设置。List<configmemento>、Map&lt;String, ConfigMemento&gt;。</configmemento></li><li>最后是提供的备忘录操作方法；存放(append)、回滚(undo)、返回(redo)、定向获取(get)，这样四个操作方法。</li></ul><h3 id="测试验证"><a href="#测试验证" class="headerlink" title="测试验证"></a>测试验证</h3><pre><code>@Testpublic void test() {    Admin admin = new Admin();    ConfigOriginator configOriginator = new ConfigOriginator();    configOriginator.setConfigFile(new ConfigFile(&quot;1000001&quot;, &quot;配置内容A=哈哈&quot;, new Date(), &quot;小傅哥&quot;));    admin.append(configOriginator.saveMemento()); // 保存配置    configOriginator.setConfigFile(new ConfigFile(&quot;1000002&quot;, &quot;配置内容A=嘻嘻&quot;, new Date(), &quot;小傅哥&quot;));    admin.append(configOriginator.saveMemento()); // 保存配置    configOriginator.setConfigFile(new ConfigFile(&quot;1000003&quot;, &quot;配置内容A=么么&quot;, new Date(), &quot;小傅哥&quot;));    admin.append(configOriginator.saveMemento()); // 保存配置    configOriginator.setConfigFile(new ConfigFile(&quot;1000004&quot;, &quot;配置内容A=嘿嘿&quot;, new Date(), &quot;小傅哥&quot;));    admin.append(configOriginator.saveMemento()); // 保存配置      // 历史配置(回滚)    configOriginator.getMemento(admin.undo());    logger.info(&quot;历史配置(回滚)undo：{}&quot;, JSON.toJSONString(configOriginator.getConfigFile()));      // 历史配置(回滚)    configOriginator.getMemento(admin.undo());    logger.info(&quot;历史配置(回滚)undo：{}&quot;, JSON.toJSONString(configOriginator.getConfigFile()));      // 历史配置(前进)    configOriginator.getMemento(admin.redo());    logger.info(&quot;历史配置(前进)redo：{}&quot;, JSON.toJSONString(configOriginator.getConfigFile()));       // 历史配置(获取)    configOriginator.getMemento(admin.get(&quot;1000002&quot;));    logger.info(&quot;历史配置(获取)get：{}&quot;, JSON.toJSONString(configOriginator.getConfigFile()));}</code></pre><p>上面添加了四次配置后，下面分别进行操作是；回滚1次、再回滚1次，之后向前进1次，最后是获取指定的版本配置。具体的效果可以参考测试结果。</p><pre><code>23:12:09.512 [main] INFO  org.itstack.demo.design.test.ApiTest - 历史配置(回滚)undo：{&quot;content&quot;:&quot;配置内容A=嘿嘿&quot;,&quot;dateTime&quot;:159209829432,&quot;operator&quot;:&quot;小傅哥&quot;,&quot;versionNo&quot;:&quot;1000004&quot;}23:12:09.514 [main] INFO  org.itstack.demo.design.test.ApiTest - 历史配置(回滚)undo：{&quot;content&quot;:&quot;配置内容A=么么&quot;,&quot;dateTime&quot;:159209829432,&quot;operator&quot;:&quot;小傅哥&quot;,&quot;versionNo&quot;:&quot;1000003&quot;}23:12:09.514 [main] INFO  org.itstack.demo.design.test.ApiTest - 历史配置(前进)redo：{&quot;content&quot;:&quot;配置内容A=嘿嘿&quot;,&quot;dateTime&quot;:159209829432,&quot;operator&quot;:&quot;小傅哥&quot;,&quot;versionNo&quot;:&quot;1000004&quot;}23:12:09.514 [main] INFO  org.itstack.demo.design.test.ApiTest - 历史配置(获取)get：{&quot;content&quot;:&quot;配置内容A=嘻嘻&quot;,&quot;dateTime&quot;:159320989432,&quot;operator&quot;:&quot;小傅哥&quot;,&quot;versionNo&quot;:&quot;1000002&quot;}Process finished with exit code 0</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>备忘录模式是以可以恢复或者说回滚，配置、版本、悔棋为核心功能的设计模式，而这种设计模式属于行为模式。在功能实现上是以不破坏原对象为基础增加备忘录操作类，记录原对象的行为从而实现备忘录模式。</p>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>中介者模式</title>
      <link href="/2021/04/16/2021-04-16-design-zhong-jie-zhe/"/>
      <url>/2021/04/16/2021-04-16-design-zhong-jie-zhe/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="中介者模式介绍"><a href="#中介者模式介绍" class="headerlink" title="中介者模式介绍"></a>中介者模式介绍</h2><p>中介者模式要解决的就是复杂功能应用之间的重复调用，在这中间添加一层中介者包装服务，对外提供简单、通用、易扩展的服务能力。</p><p>这样的设计模式几乎在我们日常生活和实际业务开发中都会见到，例如；飞机🛬降落有小姐姐在塔台喊话、无论哪个方向来的候车都从站台上下、公司的系统中有一个中台专门为你包装所有接口和提供统一的服务等等，这些都运用了中介者模式。除此之外，你用到的一些中间件，他们包装了底层多种数据库的差异化，提供非常简单的方式进行使用。</p><h2 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h2><p><img src="/2021/04/16/2021-04-16-design-zhong-jie-zhe/itstack-demo-design-16-02.png" alt></p><p>把jdbc层进行包装，让用户在使用数据库服务的时候，可以和使用mybatis一样简单方便，通过这样的源码方式学习中介者模式，也方便对源码知识的拓展学习，增强知识栈。</p><h2 id="用一坨坨代码实现"><a href="#用一坨坨代码实现" class="headerlink" title="用一坨坨代码实现"></a>用一坨坨代码实现</h2><h3 id="工程结构"><a href="#工程结构" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-16-01└── src    └── main        └── java            └── org.itstack.demo.design                └── JDBCUtil.java</code></pre><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><pre><code>public class JDBCUtil {    private static Logger logger = LoggerFactory.getLogger(JDBCUtil.class);    public static final String URL = &quot;jdbc:mysql://127.0.0.1:3306/itstack-demo-design&quot;;    public static final String USER = &quot;root&quot;;    public static final String PASSWORD = &quot;123456&quot;;    public static void main(String[] args) throws Exception {        //1. 加载驱动程序        Class.forName(&quot;com.mysql.jdbc.Driver&quot;);        //2. 获得数据库连接        Connection conn = DriverManager.getConnection(URL, USER, PASSWORD);        //3. 操作数据库        Statement stmt = conn.createStatement();        ResultSet resultSet = stmt.executeQuery(&quot;SELECT id, name, age, createTime, updateTime FROM user&quot;);        //4. 如果有数据 resultSet.next() 返回true        while (resultSet.next()) {            logger.info(&quot;测试结果 姓名：{} 年龄：{}&quot;, resultSet.getString(&quot;name&quot;),resultSet.getInt(&quot;age&quot;));        }    }}</code></pre><h2 id="中介模式开发ORM框架"><a href="#中介模式开发ORM框架" class="headerlink" title="中介模式开发ORM框架"></a>中介模式开发ORM框架</h2><h3 id="工程结构-1"><a href="#工程结构-1" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-16-02└── src    ├── main    │   ├── java    │   │   └── org.itstack.demo.design    │   │       ├── dao    │   │       │    ├── ISchool.java    │   │       │    └── IUserDao.java    │   │       ├── mediator    │   │       │    ├── Configuration.java    │   │       │    ├── DefaultSqlSession.java    │   │       │    ├── DefaultSqlSessionFactory.java    │   │       │    ├── Resources.java    │   │       │    ├── SqlSession.java    │   │       │    ├── SqlSessionFactory.java    │   │       │    ├── SqlSessionFactoryBuilder.java    │   │       │    └── SqlSessionFactoryBuilder.java    │   │       └── po    │   │             ├── School.java    │   │             └── User.java    │   └── resources    │       ├── mapper    │       │   ├── School_Mapper.xml    │       │   └── User_Mapper.xml    │       └── mybatis-config-datasource.xml    └── test         └── java             └── org.itstack.demo.design.test                 └── ApiTest.java</code></pre><p>中介者模式模型结构</p><p><img src="/2021/04/16/2021-04-16-design-zhong-jie-zhe/itstack-demo-design-16-03.png" alt></p><ul><li>以上是对ORM框架实现的核心类，包括了；加载配置文件、对xml解析、获取数据库session、操作数据库以及结果返回。</li><li>左上是对数据库的定义和处理，基本包括我们常用的方法；<t> T selectOne、<t> List<t> selectList等。</t></t></t></li><li>右侧蓝色部分是对数据库配置的开启session的工厂处理类，这里的工厂会操作DefaultSqlSession</li><li>之后是红色地方的SqlSessionFactoryBuilder，这个类是对数据库操作的核心类；处理工厂、解析文件、拿到session等。</li></ul><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="定义SqlSession接口"><a href="#定义SqlSession接口" class="headerlink" title="定义SqlSession接口"></a>定义SqlSession接口</h4><pre><code>public interface SqlSession {    &lt;T&gt; T selectOne(String statement);    &lt;T&gt; T selectOne(String statement, Object parameter);    &lt;T&gt; List&lt;T&gt; selectList(String statement);    &lt;T&gt; List&lt;T&gt; selectList(String statement, Object parameter);    void close();}</code></pre><p>这里定义了对数据库操作的查询接口，分为查询一个结果和查询多个结果，同时包括有参数和没有参数的方法。</p><h4 id="SqlSession具体实现类"><a href="#SqlSession具体实现类" class="headerlink" title="SqlSession具体实现类"></a>SqlSession具体实现类</h4><pre><code>public class DefaultSqlSession implements SqlSession {    private Connection connection;    private Map&lt;String, XNode&gt; mapperElement;    public DefaultSqlSession(Connection connection, Map&lt;String, XNode&gt; mapperElement) {        this.connection = connection;        this.mapperElement = mapperElement;    }    @Override    public &lt;T&gt; T selectOne(String statement) {        try {            XNode xNode = mapperElement.get(statement);            PreparedStatement preparedStatement = connection.prepareStatement(xNode.getSql());            ResultSet resultSet = preparedStatement.executeQuery();            List&lt;T&gt; objects = resultSet2Obj(resultSet, Class.forName(xNode.getResultType()));            return objects.get(0);        } catch (Exception e) {            e.printStackTrace();        }        return null;    }    @Override    public &lt;T&gt; List&lt;T&gt; selectList(String statement) {        XNode xNode = mapperElement.get(statement);        try {            PreparedStatement preparedStatement = connection.prepareStatement(xNode.getSql());            ResultSet resultSet = preparedStatement.executeQuery();            return resultSet2Obj(resultSet, Class.forName(xNode.getResultType()));        } catch (Exception e) {            e.printStackTrace();        }        return null;    }    // ...    private &lt;T&gt; List&lt;T&gt; resultSet2Obj(ResultSet resultSet, Class&lt;?&gt; clazz) {        List&lt;T&gt; list = new ArrayList&lt;&gt;();        try {            ResultSetMetaData metaData = resultSet.getMetaData();            int columnCount = metaData.getColumnCount();            // 每次遍历行值            while (resultSet.next()) {                T obj = (T) clazz.newInstance();                for (int i = 1; i &lt;= columnCount; i++) {                    Object value = resultSet.getObject(i);                    String columnName = metaData.getColumnName(i);                    String setMethod = &quot;set&quot; + columnName.substring(0, 1).toUpperCase() + columnName.substring(1);                    Method method;                    if (value instanceof Timestamp) {                        method = clazz.getMethod(setMethod, Date.class);                    } else {                        method = clazz.getMethod(setMethod, value.getClass());                    }                    method.invoke(obj, value);                }                list.add(obj);            }        } catch (Exception e) {            e.printStackTrace();        }        return list;    }    @Override    public void close() {        if (null == connection) return;        try {            connection.close();        } catch (SQLException e) {            e.printStackTrace();        }    }}</code></pre><h4 id="定义SqlSessionFactory接口"><a href="#定义SqlSessionFactory接口" class="headerlink" title="定义SqlSessionFactory接口"></a>定义SqlSessionFactory接口</h4><pre><code>public interface SqlSessionFactory {    SqlSession openSession();}</code></pre><p>开启一个SqlSession， 这几乎是大家在平时的使用中都需要进行操作的内容。虽然你看不见，但是当你有数据库操作的时候都会获取每一次执行的SqlSession。</p><h4 id="SqlSessionFactory具体实现类"><a href="#SqlSessionFactory具体实现类" class="headerlink" title="SqlSessionFactory具体实现类"></a>SqlSessionFactory具体实现类</h4><pre><code>public class DefaultSqlSessionFactory implements SqlSessionFactory {    private final Configuration configuration;    public DefaultSqlSessionFactory(Configuration configuration) {        this.configuration = configuration;    }    @Override    public SqlSession openSession() {        return new DefaultSqlSession(configuration.connection, configuration.mapperElement);    }}</code></pre><ul><li>DefaultSqlSessionFactory，是使用mybatis最常用的类，这里我们简单的实现了一个版本。</li><li>虽然是简单的版本，但是包括了最基本的核心思路。当开启SqlSession时会进行返回一个DefaultSqlSession</li><li>这个构造函数中向下传递了Configuration配置文件，在这个配置文件中包括；Connection connection、Map&lt;String, String&gt; dataSource、Map&lt;String, XNode&gt; mapperElement。如果有你阅读过Mybatis源码，对这个就不会陌生。</li></ul><h4 id="SqlSessionFactoryBuilder实现"><a href="#SqlSessionFactoryBuilder实现" class="headerlink" title="SqlSessionFactoryBuilder实现"></a>SqlSessionFactoryBuilder实现</h4><pre><code>public class SqlSessionFactoryBuilder {    public DefaultSqlSessionFactory build(Reader reader) {        SAXReader saxReader = new SAXReader();        try {            saxReader.setEntityResolver(new XMLMapperEntityResolver());            Document document = saxReader.read(new InputSource(reader));            Configuration configuration = parseConfiguration(document.getRootElement());            return new DefaultSqlSessionFactory(configuration);        } catch (DocumentException e) {            e.printStackTrace();        }        return null;    }    private Configuration parseConfiguration(Element root) {        Configuration configuration = new Configuration();        configuration.setDataSource(dataSource(root.selectNodes(&quot;//dataSource&quot;)));        configuration.setConnection(connection(configuration.dataSource));        configuration.setMapperElement(mapperElement(root.selectNodes(&quot;mappers&quot;)));        return configuration;    }    // 获取数据源配置信息    private Map&lt;String, String&gt; dataSource(List&lt;Element&gt; list) {        Map&lt;String, String&gt; dataSource = new HashMap&lt;&gt;(4);        Element element = list.get(0);        List content = element.content();        for (Object o : content) {            Element e = (Element) o;            String name = e.attributeValue(&quot;name&quot;);            String value = e.attributeValue(&quot;value&quot;);            dataSource.put(name, value);        }        return dataSource;    }    private Connection connection(Map&lt;String, String&gt; dataSource) {        try {            Class.forName(dataSource.get(&quot;driver&quot;));            return DriverManager.getConnection(dataSource.get(&quot;url&quot;), dataSource.get(&quot;username&quot;), dataSource.get(&quot;password&quot;));        } catch (ClassNotFoundException | SQLException e) {            e.printStackTrace();        }        return null;    }    // 获取SQL语句信息    private Map&lt;String, XNode&gt; mapperElement(List&lt;Element&gt; list) {        Map&lt;String, XNode&gt; map = new HashMap&lt;&gt;();        Element element = list.get(0);        List content = element.content();        for (Object o : content) {            Element e = (Element) o;            String resource = e.attributeValue(&quot;resource&quot;);            try {                Reader reader = Resources.getResourceAsReader(resource);                SAXReader saxReader = new SAXReader();                Document document = saxReader.read(new InputSource(reader));                Element root = document.getRootElement();                //命名空间                String namespace = root.attributeValue(&quot;namespace&quot;);                // SELECT                List&lt;Element&gt; selectNodes = root.selectNodes(&quot;select&quot;);                for (Element node : selectNodes) {                    String id = node.attributeValue(&quot;id&quot;);                    String parameterType = node.attributeValue(&quot;parameterType&quot;);                    String resultType = node.attributeValue(&quot;resultType&quot;);                    String sql = node.getText();                    // ? 匹配                    Map&lt;Integer, String&gt; parameter = new HashMap&lt;&gt;();                    Pattern pattern = Pattern.compile(&quot;(#\\{(.*?)})&quot;);                    Matcher matcher = pattern.matcher(sql);                    for (int i = 1; matcher.find(); i++) {                        String g1 = matcher.group(1);                        String g2 = matcher.group(2);                        parameter.put(i, g2);                        sql = sql.replace(g1, &quot;?&quot;);                    }                    XNode xNode = new XNode();                    xNode.setNamespace(namespace);                    xNode.setId(id);                    xNode.setParameterType(parameterType);                    xNode.setResultType(resultType);                    xNode.setSql(sql);                    xNode.setParameter(parameter);                    map.put(namespace + &quot;.&quot; + id, xNode);                }            } catch (Exception ex) {                ex.printStackTrace();            }        }        return map;    }}</code></pre><p>在这个类中包括的核心方法有；build(构建实例化元素)、parseConfiguration(解析配置)、dataSource(获取数据库配置)、connection(Map&lt;String, String&gt; dataSource) (链接数据库)、mapperElement (解析sql语句)</p><ul><li><p>build(构建实例化元素)</p><p>  这个类主要用于创建解析xml文件的类，以及初始化SqlSession工厂类DefaultSqlSessionFactory。另外需要注意这段代码saxReader.setEntityResolver(new XMLMapperEntityResolver());，是为了保证在不联网的时候一样可以解析xml，否则会需要从互联网获取dtd文件。</p></li><li><p>parseConfiguration(解析配置)</p><p>  是对xml中的元素进行获取，这里主要获取了；dataSource、mappers，而这两个配置一个是我们数据库的链接信息，另外一个是对数据库操作语句的解析。</p></li><li><p>connection(Map&lt;String, String&gt; dataSource) (链接数据库)</p><p>  链接数据库的地方和我们常见的方式是一样的；Class.forName(dataSource.get(“driver”));，但是这样包装以后外部是不需要知道具体的操作。同时当我们需要链接多套数据库的时候，也是可以在这里扩展。</p></li><li><p>mapperElement (解析sql语句)</p><p>  这部分代码块内容相对来说比较长，但是核心的点就是为了解析xml中的sql语句配置。在我们平常的使用中基本都会配置一些sql语句，也有一些入参的占位符。在这里我们使用正则表达式的方式进行解析操作。</p><p>  解析完成的sql语句就有了一个名称和sql的映射关系，当我们进行数据库操作的时候，这个组件就可以通过映射关系获取到对应sql语句进行操作。</p></li></ul><h3 id="测试验证"><a href="#测试验证" class="headerlink" title="测试验证"></a>测试验证</h3><pre><code>CREATE TABLE school ( id bigint NOT NULL AUTO_INCREMENT, name varchar(64), address varchar(256), createTime datetime, updateTime datetime, PRIMARY KEY (id) ) ENGINE=InnoDB DEFAULT CHARSET=utf8;insert into school (id, name, address, createTime, updateTime) values (1, &#39;北京大学&#39;, &#39;北京市海淀区颐和园路5号&#39;, &#39;2019-10-18 13:35:57&#39;, &#39;2019-10-18 13:35:57&#39;);insert into school (id, name, address, createTime, updateTime) values (2, &#39;南开大学&#39;, &#39;中国天津市南开区卫津路94号&#39;, &#39;2019-10-18 13:35:57&#39;, &#39;2019-10-18 13:35:57&#39;);insert into school (id, name, address, createTime, updateTime) values (3, &#39;同济大学&#39;, &#39;上海市彰武路1号同济大厦A楼7楼7区&#39;, &#39;2019-10-18 13:35:57&#39;, &#39;2019-10-18 13:35:57&#39;);CREATE TABLE user ( id bigint(11) NOT NULL AUTO_INCREMENT, name varchar(32), age int(4), address varchar(128), entryTime datetime, remark varchar(64), createTime datetime, updateTime datetime, status int(4) DEFAULT &#39;0&#39;, dateTime varchar(64), PRIMARY KEY (id), INDEX idx_name (name) ) ENGINE=InnoDB DEFAULT CHARSET=utf8;insert into user (id, name, age, address, entryTime, remark, createTime, updateTime, status, dateTime) values (1, &#39;水水&#39;, 18, &#39;吉林省榆树市黑林镇尹家村5组&#39;, &#39;2019-12-22 00:00:00&#39;, &#39;无&#39;, &#39;2019-12-22 00:00:00&#39;, &#39;2019-12-22 00:00:00&#39;, 0, &#39;20200309&#39;);insert into user (id, name, age, address, entryTime, remark, createTime, updateTime, status, dateTime) values (2, &#39;豆豆&#39;, 18, &#39;辽宁省大连市清河湾司马道407路&#39;, &#39;2019-12-22 00:00:00&#39;, &#39;无&#39;, &#39;2019-12-22 00:00:00&#39;, &#39;2019-12-22 00:00:00&#39;, 1, null);insert into user (id, name, age, address, entryTime, remark, createTime, updateTime, status, dateTime) values (3, &#39;花花&#39;, 19, &#39;辽宁省大连市清河湾司马道407路&#39;, &#39;2019-12-22 00:00:00&#39;, &#39;无&#39;, &#39;2019-12-22 00:00:00&#39;, &#39;2019-12-22 00:00:00&#39;, 0, &#39;20200310&#39;);</code></pre><h4 id="创建数据库对象类"><a href="#创建数据库对象类" class="headerlink" title="创建数据库对象类"></a>创建数据库对象类</h4><pre><code>public class User {    private Long id;    private String name;    private Integer age;    private Date createTime;    private Date updateTime;    // ... get/set}</code></pre><pre><code>public class School {    private Long id;    private String name;    private String address;    private Date createTime;    private Date updateTime;      // ... get/set}</code></pre><h4 id="创建DAO包"><a href="#创建DAO包" class="headerlink" title="创建DAO包"></a>创建DAO包</h4><pre><code>public interface IUserDao {     User queryUserInfoById(Long id);}</code></pre><pre><code>public interface ISchoolDao {    School querySchoolInfoById(Long treeId);}</code></pre><h4 id="ORM配置文件"><a href="#ORM配置文件" class="headerlink" title="ORM配置文件"></a>ORM配置文件</h4><pre><code>&lt;configuration&gt;    &lt;environments default=&quot;development&quot;&gt;        &lt;environment id=&quot;development&quot;&gt;            &lt;transactionManager type=&quot;JDBC&quot;/&gt;            &lt;dataSource type=&quot;POOLED&quot;&gt;                &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt;                &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://127.0.0.1:3306/itstack_demo_design?useUnicode=true&quot;/&gt;                &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt;                &lt;property name=&quot;password&quot; value=&quot;123456&quot;/&gt;            &lt;/dataSource&gt;        &lt;/environment&gt;    &lt;/environments&gt;    &lt;mappers&gt;        &lt;mapper resource=&quot;mapper/User_Mapper.xml&quot;/&gt;        &lt;mapper resource=&quot;mapper/School_Mapper.xml&quot;/&gt;    &lt;/mappers&gt;&lt;/configuration&gt;</code></pre><h4 id="操作配置-用户"><a href="#操作配置-用户" class="headerlink" title="操作配置(用户)"></a>操作配置(用户)</h4><pre><code>&lt;mapper namespace=&quot;org.itstack.demo.design.dao.IUserDao&quot;&gt;    &lt;select id=&quot;queryUserInfoById&quot; parameterType=&quot;java.lang.Long&quot; resultType=&quot;org.itstack.demo.design.po.User&quot;&gt;        SELECT id, name, age, createTime, updateTime        FROM user        where id = #{id}    &lt;/select&gt;    &lt;select id=&quot;queryUserList&quot; parameterType=&quot;org.itstack.demo.design.po.User&quot; resultType=&quot;org.itstack.demo.design.po.User&quot;&gt;        SELECT id, name, age, createTime, updateTime        FROM user        where age = #{age}    &lt;/select&gt;&lt;/mapper&gt;</code></pre><pre><code>&lt;mapper namespace=&quot;org.itstack.demo.design.dao.ISchoolDao&quot;&gt;    &lt;select id=&quot;querySchoolInfoById&quot; resultType=&quot;org.itstack.demo.design.po.School&quot;&gt;        SELECT id, name, address, createTime, updateTime        FROM school        where id = #{id}    &lt;/select&gt;&lt;/mapper&gt;</code></pre><h4 id="单个结果查询测试"><a href="#单个结果查询测试" class="headerlink" title="单个结果查询测试"></a>单个结果查询测试</h4><pre><code>@Testpublic void test_queryUserInfoById() {    String resource = &quot;mybatis-config-datasource.xml&quot;;    Reader reader;    try {        reader = Resources.getResourceAsReader(resource);        SqlSessionFactory sqlMapper = new SqlSessionFactoryBuilder().build(reader);        SqlSession session = sqlMapper.openSession();        try {            User user = session.selectOne(&quot;org.itstack.demo.design.dao.IUserDao.queryUserInfoById&quot;, 1L);            logger.info(&quot;测试结果：{}&quot;, JSON.toJSONString(user));        } finally {            session.close();            reader.close();        }    } catch (Exception e) {        e.printStackTrace();    }}</code></pre><p>这里的使用方式和Mybatis是一样的，都包括了；资源加载和解析、SqlSession工厂构建、开启SqlSession以及最后执行查询操作selectOne</p><h4 id="集合结果查询测试"><a href="#集合结果查询测试" class="headerlink" title="集合结果查询测试"></a>集合结果查询测试</h4><pre><code>bugstack虫洞栈首页Java/SpringNetty4.x专题架构师专题Archives重学 Java 设计模式：实战中介者模式「按照Mybatis原理手写ORM框架，给JDBC方式操作数据库增加中介者场景」2020/06/27作者：小傅哥博客：https://bugstack.cn    沉淀、分享、成长，让自己和他人都能有所收获！😄一、前言同龄人的差距是从什么时候拉开的同样的幼儿园、同样的小学、一样的书本、一样的课堂，有人学习好、有人学习差。不只是上学，几乎人生处处都是赛道，发令枪响起的时刻，也就把人生的差距拉开。编程开发这条路也是很长很宽，有人跑得快有人跑得慢。那么你是否想起过，这一点点的差距到遥不可及的距离，是从哪一天开始的。摸摸肚子的肉，看看远处的路，别人讲的是故事，你想起的都是事故。思想没有产品高才写出一片的ifelse当你承接一个需求的时候，比如；交易、订单、营销、保险等各类场景。如果你不熟悉这个场景下的业务模式，以及将来的拓展方向，那么很难设计出良好可扩展的系统。再加上产品功能初建，说老板要的急，尽快上线。作为程序员的你更没有时间思考，整体一看现在的需求也不难，直接上手开干(一个方法两个if语句)，这样确实满足了当前需求。但老板的想法多呀，产品也跟着变化快，到你这就是改改改，加加加。当然你也不客气，回首掏就是1024个if语句！日积月累的技术沉淀是为了厚积薄发粗略的估算过，如果从上大学开始每天写200行，一个月是6000行，一年算10个月话，就是6万行，第三年出去实习的是时候就有20万行的代码量。如果你能做到这一点，找工作难？有时候很多事情就是靠时间积累出来的，想走捷径有时候真的没有。你的技术水平、你的业务能力、你身上的肉，都是一点点积累下来的，不要浪费看似很短的时间，一年年坚持下来，留下印刻青春的痕迹，多给自己武装上一些能力。二、开发环境    JDK 1.8    Idea + Maven    mysql 5.1.20    涉及工程一个，可以通过关注公众号：bugstack虫洞栈，回复源码下载获取(打开获取的链接，找到序号18)工程     描述itstack-demo-design-16-01     使用JDBC方式连接数据库itstack-demo-design-16-02     手写ORM框架操作数据库三、中介者模式介绍中介者模式，图片来自 refactoringguru.cn中介者模式要解决的就是复杂功能应用之间的重复调用，在这中间添加一层中介者包装服务，对外提供简单、通用、易扩展的服务能力。这样的设计模式几乎在我们日常生活和实际业务开发中都会见到，例如；飞机🛬降落有小姐姐在塔台喊话、无论哪个方向来的候车都从站台上下、公司的系统中有一个中台专门为你包装所有接口和提供统一的服务等等，这些都运用了中介者模式。除此之外，你用到的一些中间件，他们包装了底层多种数据库的差异化，提供非常简单的方式进行使用。四、案例场景模拟场景模拟；模仿Mybatis手写ORM框架在本案例中我们通过模仿Mybatis手写ORM框架，通过这样操作数据库学习中介者运用场景除了这样的中间件层使用场景外，对于一些外部接口，例如N种奖品服务，可以由中台系统进行统一包装对外提供服务能力。也是中介者模式的一种思想体现。在本案例中我们会把jdbc层进行包装，让用户在使用数据库服务的时候，可以和使用mybatis一样简单方便，通过这样的源码方式学习中介者模式，也方便对源码知识的拓展学习，增强知识栈。五、用一坨坨代码实现这是一种关于数据库操作最初的方式基本上每一个学习开发的人都学习过直接使用jdbc方式连接数据库，进行CRUD操作。以下的例子可以当做回忆。1. 工程结构itstack-demo-design-16-01└── src    └── main        └── java            └── org.itstack.demo.design                └── JDBCUtil.java    这里的类比较简单只包括了一个数据库操作类。2. 代码实现public class JDBCUtil {    private static Logger logger = LoggerFactory.getLogger(JDBCUtil.class);    public static final String URL = &quot;jdbc:mysql://127.0.0.1:3306/itstack-demo-design&quot;;    public static final String USER = &quot;root&quot;;    public static final String PASSWORD = &quot;123456&quot;;    public static void main(String[] args) throws Exception {        //1. 加载驱动程序        Class.forName(&quot;com.mysql.jdbc.Driver&quot;);        //2. 获得数据库连接        Connection conn = DriverManager.getConnection(URL, USER, PASSWORD);        //3. 操作数据库        Statement stmt = conn.createStatement();        ResultSet resultSet = stmt.executeQuery(&quot;SELECT id, name, age, createTime, updateTime FROM user&quot;);        //4. 如果有数据 resultSet.next() 返回true        while (resultSet.next()) {            logger.info(&quot;测试结果 姓名：{} 年龄：{}&quot;, resultSet.getString(&quot;name&quot;),resultSet.getInt(&quot;age&quot;));        }    }}    以上是使用JDBC的方式进行直接操作数据库，几乎大家都使用过这样的方式。3. 测试结果15:38:10.919 [main] INFO  org.itstack.demo.design.JDBCUtil - 测试结果 姓名：水水 年龄：1815:38:10.922 [main] INFO  org.itstack.demo.design.JDBCUtil - 测试结果 姓名：豆豆 年龄：1815:38:10.922 [main] INFO  org.itstack.demo.design.JDBCUtil - 测试结果 姓名：花花 年龄：19Process finished with exit code 0    从测试结果可以看到这里已经查询到了数据库中的数据。只不过如果在全部的业务开发中都这样实现，会非常的麻烦。六、中介模式开发ORM框架`接下来就使用中介模式的思想完成模仿Mybatis的ORM框架开发~1. 工程结构itstack-demo-design-16-02└── src    ├── main    │   ├── java    │   │   └── org.itstack.demo.design    │   │       ├── dao    │   │       │    ├── ISchool.java    │   │       │    └── IUserDao.java    │   │       ├── mediator    │   │       │    ├── Configuration.java    │   │       │    ├── DefaultSqlSession.java    │   │       │    ├── DefaultSqlSessionFactory.java    │   │       │    ├── Resources.java    │   │       │    ├── SqlSession.java    │   │       │    ├── SqlSessionFactory.java    │   │       │    ├── SqlSessionFactoryBuilder.java    │   │       │    └── SqlSessionFactoryBuilder.java    │   │       └── po    │   │             ├── School.java    │   │             └── User.java    │   └── resources    │       ├── mapper    │       │   ├── School_Mapper.xml    │       │   └── User_Mapper.xml    │       └── mybatis-config-datasource.xml    └── test         └── java             └── org.itstack.demo.design.test                 └── ApiTest.java中介者模式模型结构中介者模式模型结构    以上是对ORM框架实现的核心类，包括了；加载配置文件、对xml解析、获取数据库session、操作数据库以及结果返回。    左上是对数据库的定义和处理，基本包括我们常用的方法；&lt;T&gt; T selectOne、&lt;T&gt; List&lt;T&gt; selectList等。    右侧蓝色部分是对数据库配置的开启session的工厂处理类，这里的工厂会操作DefaultSqlSession    之后是红色地方的SqlSessionFactoryBuilder，这个类是对数据库操作的核心类；处理工厂、解析文件、拿到session等。接下来我们就分别介绍各个类的功能实现过程。2. 代码实现2.1 定义SqlSession接口public interface SqlSession {    &lt;T&gt; T selectOne(String statement);    &lt;T&gt; T selectOne(String statement, Object parameter);    &lt;T&gt; List&lt;T&gt; selectList(String statement);    &lt;T&gt; List&lt;T&gt; selectList(String statement, Object parameter);    void close();}    这里定义了对数据库操作的查询接口，分为查询一个结果和查询多个结果，同时包括有参数和没有参数的方法。2.2 SqlSession具体实现类public class DefaultSqlSession implements SqlSession {    private Connection connection;    private Map&lt;String, XNode&gt; mapperElement;    public DefaultSqlSession(Connection connection, Map&lt;String, XNode&gt; mapperElement) {        this.connection = connection;        this.mapperElement = mapperElement;    }    @Override    public &lt;T&gt; T selectOne(String statement) {        try {            XNode xNode = mapperElement.get(statement);            PreparedStatement preparedStatement = connection.prepareStatement(xNode.getSql());            ResultSet resultSet = preparedStatement.executeQuery();            List&lt;T&gt; objects = resultSet2Obj(resultSet, Class.forName(xNode.getResultType()));            return objects.get(0);        } catch (Exception e) {            e.printStackTrace();        }        return null;    }    @Override    public &lt;T&gt; List&lt;T&gt; selectList(String statement) {        XNode xNode = mapperElement.get(statement);        try {            PreparedStatement preparedStatement = connection.prepareStatement(xNode.getSql());            ResultSet resultSet = preparedStatement.executeQuery();            return resultSet2Obj(resultSet, Class.forName(xNode.getResultType()));        } catch (Exception e) {            e.printStackTrace();        }        return null;    }    // ...    private &lt;T&gt; List&lt;T&gt; resultSet2Obj(ResultSet resultSet, Class&lt;?&gt; clazz) {        List&lt;T&gt; list = new ArrayList&lt;&gt;();        try {            ResultSetMetaData metaData = resultSet.getMetaData();            int columnCount = metaData.getColumnCount();            // 每次遍历行值            while (resultSet.next()) {                T obj = (T) clazz.newInstance();                for (int i = 1; i &lt;= columnCount; i++) {                    Object value = resultSet.getObject(i);                    String columnName = metaData.getColumnName(i);                    String setMethod = &quot;set&quot; + columnName.substring(0, 1).toUpperCase() + columnName.substring(1);                    Method method;                    if (value instanceof Timestamp) {                        method = clazz.getMethod(setMethod, Date.class);                    } else {                        method = clazz.getMethod(setMethod, value.getClass());                    }                    method.invoke(obj, value);                }                list.add(obj);            }        } catch (Exception e) {            e.printStackTrace();        }        return list;    }    @Override    public void close() {        if (null == connection) return;        try {            connection.close();        } catch (SQLException e) {            e.printStackTrace();        }    }}    这里包括了接口定义的方法实现，也就是包装了jdbc层。    通过这样的包装可以让对数据库的jdbc操作隐藏起来，外部调用的时候对入参、出参都有内部进行处理。2.3 定义SqlSessionFactory接口public interface SqlSessionFactory {    SqlSession openSession();}    开启一个SqlSession， 这几乎是大家在平时的使用中都需要进行操作的内容。虽然你看不见，但是当你有数据库操作的时候都会获取每一次执行的SqlSession。2.4 SqlSessionFactory具体实现类public class DefaultSqlSessionFactory implements SqlSessionFactory {    private final Configuration configuration;    public DefaultSqlSessionFactory(Configuration configuration) {        this.configuration = configuration;    }    @Override    public SqlSession openSession() {        return new DefaultSqlSession(configuration.connection, configuration.mapperElement);    }}    DefaultSqlSessionFactory，是使用mybatis最常用的类，这里我们简单的实现了一个版本。    虽然是简单的版本，但是包括了最基本的核心思路。当开启SqlSession时会进行返回一个DefaultSqlSession    这个构造函数中向下传递了Configuration配置文件，在这个配置文件中包括；Connection connection、Map&lt;String, String&gt; dataSource、Map&lt;String, XNode&gt; mapperElement。如果有你阅读过Mybatis源码，对这个就不会陌生。2.5 SqlSessionFactoryBuilder实现public class SqlSessionFactoryBuilder {    public DefaultSqlSessionFactory build(Reader reader) {        SAXReader saxReader = new SAXReader();        try {            saxReader.setEntityResolver(new XMLMapperEntityResolver());            Document document = saxReader.read(new InputSource(reader));            Configuration configuration = parseConfiguration(document.getRootElement());            return new DefaultSqlSessionFactory(configuration);        } catch (DocumentException e) {            e.printStackTrace();        }        return null;    }    private Configuration parseConfiguration(Element root) {        Configuration configuration = new Configuration();        configuration.setDataSource(dataSource(root.selectNodes(&quot;//dataSource&quot;)));        configuration.setConnection(connection(configuration.dataSource));        configuration.setMapperElement(mapperElement(root.selectNodes(&quot;mappers&quot;)));        return configuration;    }    // 获取数据源配置信息    private Map&lt;String, String&gt; dataSource(List&lt;Element&gt; list) {        Map&lt;String, String&gt; dataSource = new HashMap&lt;&gt;(4);        Element element = list.get(0);        List content = element.content();        for (Object o : content) {            Element e = (Element) o;            String name = e.attributeValue(&quot;name&quot;);            String value = e.attributeValue(&quot;value&quot;);            dataSource.put(name, value);        }        return dataSource;    }    private Connection connection(Map&lt;String, String&gt; dataSource) {        try {            Class.forName(dataSource.get(&quot;driver&quot;));            return DriverManager.getConnection(dataSource.get(&quot;url&quot;), dataSource.get(&quot;username&quot;), dataSource.get(&quot;password&quot;));        } catch (ClassNotFoundException | SQLException e) {            e.printStackTrace();        }        return null;    }    // 获取SQL语句信息    private Map&lt;String, XNode&gt; mapperElement(List&lt;Element&gt; list) {        Map&lt;String, XNode&gt; map = new HashMap&lt;&gt;();        Element element = list.get(0);        List content = element.content();        for (Object o : content) {            Element e = (Element) o;            String resource = e.attributeValue(&quot;resource&quot;);            try {                Reader reader = Resources.getResourceAsReader(resource);                SAXReader saxReader = new SAXReader();                Document document = saxReader.read(new InputSource(reader));                Element root = document.getRootElement();                //命名空间                String namespace = root.attributeValue(&quot;namespace&quot;);                // SELECT                List&lt;Element&gt; selectNodes = root.selectNodes(&quot;select&quot;);                for (Element node : selectNodes) {                    String id = node.attributeValue(&quot;id&quot;);                    String parameterType = node.attributeValue(&quot;parameterType&quot;);                    String resultType = node.attributeValue(&quot;resultType&quot;);                    String sql = node.getText();                    // ? 匹配                    Map&lt;Integer, String&gt; parameter = new HashMap&lt;&gt;();                    Pattern pattern = Pattern.compile(&quot;(#\\{(.*?)})&quot;);                    Matcher matcher = pattern.matcher(sql);                    for (int i = 1; matcher.find(); i++) {                        String g1 = matcher.group(1);                        String g2 = matcher.group(2);                        parameter.put(i, g2);                        sql = sql.replace(g1, &quot;?&quot;);                    }                    XNode xNode = new XNode();                    xNode.setNamespace(namespace);                    xNode.setId(id);                    xNode.setParameterType(parameterType);                    xNode.setResultType(resultType);                    xNode.setSql(sql);                    xNode.setParameter(parameter);                    map.put(namespace + &quot;.&quot; + id, xNode);                }            } catch (Exception ex) {                ex.printStackTrace();            }        }        return map;    }}    在这个类中包括的核心方法有；build(构建实例化元素)、parseConfiguration(解析配置)、dataSource(获取数据库配置)、connection(Map&lt;String, String&gt; dataSource) (链接数据库)、mapperElement (解析sql语句)    接下来我们分别介绍这样的几个核心方法。build(构建实例化元素)这个类主要用于创建解析xml文件的类，以及初始化SqlSession工厂类DefaultSqlSessionFactory。另外需要注意这段代码saxReader.setEntityResolver(new XMLMapperEntityResolver());，是为了保证在不联网的时候一样可以解析xml，否则会需要从互联网获取dtd文件。parseConfiguration(解析配置)是对xml中的元素进行获取，这里主要获取了；dataSource、mappers，而这两个配置一个是我们数据库的链接信息，另外一个是对数据库操作语句的解析。connection(Map&lt;String, String&gt; dataSource) (链接数据库)链接数据库的地方和我们常见的方式是一样的；Class.forName(dataSource.get(&quot;driver&quot;));，但是这样包装以后外部是不需要知道具体的操作。同时当我们需要链接多套数据库的时候，也是可以在这里扩展。mapperElement (解析sql语句)这部分代码块内容相对来说比较长，但是核心的点就是为了解析xml中的sql语句配置。在我们平常的使用中基本都会配置一些sql语句，也有一些入参的占位符。在这里我们使用正则表达式的方式进行解析操作。解析完成的sql语句就有了一个名称和sql的映射关系，当我们进行数据库操作的时候，这个组件就可以通过映射关系获取到对应sql语句进行操作。3. 测试验证在测试之前需要导入sql语句到数据库中;    库名：itstack-demo-design    表名：user、schoolCREATE TABLE school ( id bigint NOT NULL AUTO_INCREMENT, name varchar(64), address varchar(256), createTime datetime, updateTime datetime, PRIMARY KEY (id) ) ENGINE=InnoDB DEFAULT CHARSET=utf8;insert into school (id, name, address, createTime, updateTime) values (1, &#39;北京大学&#39;, &#39;北京市海淀区颐和园路5号&#39;, &#39;2019-10-18 13:35:57&#39;, &#39;2019-10-18 13:35:57&#39;);insert into school (id, name, address, createTime, updateTime) values (2, &#39;南开大学&#39;, &#39;中国天津市南开区卫津路94号&#39;, &#39;2019-10-18 13:35:57&#39;, &#39;2019-10-18 13:35:57&#39;);insert into school (id, name, address, createTime, updateTime) values (3, &#39;同济大学&#39;, &#39;上海市彰武路1号同济大厦A楼7楼7区&#39;, &#39;2019-10-18 13:35:57&#39;, &#39;2019-10-18 13:35:57&#39;);CREATE TABLE user ( id bigint(11) NOT NULL AUTO_INCREMENT, name varchar(32), age int(4), address varchar(128), entryTime datetime, remark varchar(64), createTime datetime, updateTime datetime, status int(4) DEFAULT &#39;0&#39;, dateTime varchar(64), PRIMARY KEY (id), INDEX idx_name (name) ) ENGINE=InnoDB DEFAULT CHARSET=utf8;insert into user (id, name, age, address, entryTime, remark, createTime, updateTime, status, dateTime) values (1, &#39;水水&#39;, 18, &#39;吉林省榆树市黑林镇尹家村5组&#39;, &#39;2019-12-22 00:00:00&#39;, &#39;无&#39;, &#39;2019-12-22 00:00:00&#39;, &#39;2019-12-22 00:00:00&#39;, 0, &#39;20200309&#39;);insert into user (id, name, age, address, entryTime, remark, createTime, updateTime, status, dateTime) values (2, &#39;豆豆&#39;, 18, &#39;辽宁省大连市清河湾司马道407路&#39;, &#39;2019-12-22 00:00:00&#39;, &#39;无&#39;, &#39;2019-12-22 00:00:00&#39;, &#39;2019-12-22 00:00:00&#39;, 1, null);insert into user (id, name, age, address, entryTime, remark, createTime, updateTime, status, dateTime) values (3, &#39;花花&#39;, 19, &#39;辽宁省大连市清河湾司马道407路&#39;, &#39;2019-12-22 00:00:00&#39;, &#39;无&#39;, &#39;2019-12-22 00:00:00&#39;, &#39;2019-12-22 00:00:00&#39;, 0, &#39;20200310&#39;);3.1 创建数据库对象类用户类public class User {    private Long id;    private String name;    private Integer age;    private Date createTime;    private Date updateTime;    // ... get/set}学校类public class School {    private Long id;    private String name;    private String address;    private Date createTime;    private Date updateTime;      // ... get/set}    这两个类都非常简单，就是基本的数据库信息。3.2 创建DAO包用户Daopublic interface IUserDao {     User queryUserInfoById(Long id);}学校Daopublic interface ISchoolDao {    School querySchoolInfoById(Long treeId);}3.3 ORM配置文件链接配置&lt;configuration&gt;    &lt;environments default=&quot;development&quot;&gt;        &lt;environment id=&quot;development&quot;&gt;            &lt;transactionManager type=&quot;JDBC&quot;/&gt;            &lt;dataSource type=&quot;POOLED&quot;&gt;                &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt;                &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://127.0.0.1:3306/itstack_demo_design?useUnicode=true&quot;/&gt;                &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt;                &lt;property name=&quot;password&quot; value=&quot;123456&quot;/&gt;            &lt;/dataSource&gt;        &lt;/environment&gt;    &lt;/environments&gt;    &lt;mappers&gt;        &lt;mapper resource=&quot;mapper/User_Mapper.xml&quot;/&gt;        &lt;mapper resource=&quot;mapper/School_Mapper.xml&quot;/&gt;    &lt;/mappers&gt;&lt;/configuration&gt;    这个配置与我们平常使用的mybatis基本是一样的，包括了数据库的连接池信息以及需要引入的mapper映射文件。操作配置(用户)&lt;mapper namespace=&quot;org.itstack.demo.design.dao.IUserDao&quot;&gt;    &lt;select id=&quot;queryUserInfoById&quot; parameterType=&quot;java.lang.Long&quot; resultType=&quot;org.itstack.demo.design.po.User&quot;&gt;        SELECT id, name, age, createTime, updateTime        FROM user        where id = #{id}    &lt;/select&gt;    &lt;select id=&quot;queryUserList&quot; parameterType=&quot;org.itstack.demo.design.po.User&quot; resultType=&quot;org.itstack.demo.design.po.User&quot;&gt;        SELECT id, name, age, createTime, updateTime        FROM user        where age = #{age}    &lt;/select&gt;&lt;/mapper&gt;操作配置(学校)&lt;mapper namespace=&quot;org.itstack.demo.design.dao.ISchoolDao&quot;&gt;    &lt;select id=&quot;querySchoolInfoById&quot; resultType=&quot;org.itstack.demo.design.po.School&quot;&gt;        SELECT id, name, address, createTime, updateTime        FROM school        where id = #{id}    &lt;/select&gt;&lt;/mapper&gt;3.4 单个结果查询测试@Testpublic void test_queryUserInfoById() {    String resource = &quot;mybatis-config-datasource.xml&quot;;    Reader reader;    try {        reader = Resources.getResourceAsReader(resource);        SqlSessionFactory sqlMapper = new SqlSessionFactoryBuilder().build(reader);        SqlSession session = sqlMapper.openSession();        try {            User user = session.selectOne(&quot;org.itstack.demo.design.dao.IUserDao.queryUserInfoById&quot;, 1L);            logger.info(&quot;测试结果：{}&quot;, JSON.toJSONString(user));        } finally {            session.close();            reader.close();        }    } catch (Exception e) {        e.printStackTrace();    }}    这里的使用方式和Mybatis是一样的，都包括了；资源加载和解析、SqlSession工厂构建、开启SqlSession以及最后执行查询操作selectOne测试结果16:56:51.831 [main] INFO  org.itstack.demo.design.demo.ApiTest - 测试结果：{&quot;age&quot;:18,&quot;createTime&quot;:1576944000000,&quot;id&quot;:1,&quot;name&quot;:&quot;水水&quot;,&quot;updateTime&quot;:1576944000000}Process finished with exit code 0    从结果上看已经满足了我们的查询需求。3.5 集合结果查询测试@Testpublic void test_queryUserList() {    String resource = &quot;mybatis-config-datasource.xml&quot;;    Reader reader;    try {        reader = Resources.getResourceAsReader(resource);        SqlSessionFactory sqlMapper = new SqlSessionFactoryBuilder().build(reader);        SqlSession session = sqlMapper.openSession();        try {            User req = new User();            req.setAge(18);            List&lt;User&gt; userList = session.selectList(&quot;org.itstack.demo.design.dao.IUserDao.queryUserList&quot;, req);            logger.info(&quot;测试结果：{}&quot;, JSON.toJSONString(userList));        } finally {            session.close();            reader.close();        }    } catch (Exception e) {        e.printStackTrace();    }}</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>在上述的实现和测试使用中可以看到，这种模式的设计满足了；单一职责和开闭原则，也就符合了迪米特原则，即越少人知道越好。外部的人只需要按照需求进行调用，不需要知道具体的是如何实现的，复杂的一面已经有组件合作服务平台处理。</li><li>中介者模式要解决的就是复杂功能应用之间的重复调用，在这中间添加一层中介者包装服务，对外提供简单、通用、易扩展的服务能力。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>迭代器模式</title>
      <link href="/2021/04/15/2021-04-15-design-die-dai-qi/"/>
      <url>/2021/04/15/2021-04-15-design-die-dai-qi/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="迭代器模式介绍"><a href="#迭代器模式介绍" class="headerlink" title="迭代器模式介绍"></a>迭代器模式介绍</h2><p>迭代器模式，常见的就是我们日常使用的iterator遍历。虽然这个设计模式在我们的实际业务开发中的场景并不多，但却几乎每天都要使用jdk为我们提供的list集合遍历。另外增强的for循环虽然是循环输出数据，但是他不是迭代器模式。迭代器模式的特点是实现Iterable接口，通过next的方式获取集合元素，同时具备对元素的删除等操作。而增强的for循环是不可以的。</p><p>这种设计模式的优点是可以让我们以相同的方式，遍历不同的数据结构元素，这些数据结构包括；数组、链表、树等，而用户在使用遍历的时候并不需要去关心每一种数据结构的遍历处理逻辑，从让使用变得统一易用。</p><h2 id="迭代器模式遍历组织结构"><a href="#迭代器模式遍历组织结构" class="headerlink" title="迭代器模式遍历组织结构"></a>迭代器模式遍历组织结构</h2><p>在实现迭代器模式之前可以先阅读下java中list方法关于iterator的实现部分，几乎所有的迭代器开发都会按照这个模式来实现，这个模式主要分为以下几块；</p><ul><li>Collection，集合方法部分用于对自定义的数据结构添加通用方法:add、remove、iterator等核心方法。</li><li>Iterable，提供获取迭代器，这个接口类会被Collection继承。</li><li>Iterator，提供了两个方法的定义:hasNext、next，会在具体的数据结构中写实现方式。</li></ul><h2 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h2><h3 id="工程结构"><a href="#工程结构" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-15-00└── src    ├── main    │   └── java    │       └── org.itstack.demo.design    │           ├── group    │           │    ├── Employee.java    │           │    ├── GroupStructure.java    │           │    └── Link.java    │           └──  lang    │                ├── Collection.java    │                ├── Iterable.java    │                └── Iterator.java    └── test        └── java            └── org.itstack.demo.design.test                └── ApiTest.java</code></pre><h3 id="迭代器模式模型结构"><a href="#迭代器模式模型结构" class="headerlink" title="迭代器模式模型结构"></a>迭代器模式模型结构</h3><p><img src="/2021/04/15/2021-04-15-design-die-dai-qi/itstack-demo-design-15-03.png" alt></p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="雇员实体类"><a href="#雇员实体类" class="headerlink" title="雇员实体类"></a>雇员实体类</h4><pre><code>/** * 雇员 */public class Employee {    private String uId;   // ID    private String name;  // 姓名    private String desc;  // 备注    // ...get/set}</code></pre><h4 id="树节点链路"><a href="#树节点链路" class="headerlink" title="树节点链路"></a>树节点链路</h4><pre><code>/** * 树节点链路 */public class Link {    private String fromId; // 雇员ID    private String toId;   // 雇员ID        // ...get/set}</code></pre><p>这个类用于描述结构树中的各个节点之间的关系链，也就是A to B、B to C、B to D，以此描述出一套完整的树组织结构。</p><h4 id="迭代器定义"><a href="#迭代器定义" class="headerlink" title="迭代器定义"></a>迭代器定义</h4><pre><code>public interface Iterator&lt;E&gt; {    boolean hasNext();    E next();}</code></pre><p>方法描述:hasNext，判断是否有下一个元素、next，获取下一个元素。这个在list的遍历中是经常用到的。</p><h4 id="可迭代接口定义"><a href="#可迭代接口定义" class="headerlink" title="可迭代接口定义"></a>可迭代接口定义</h4><pre><code>public interface Iterable&lt;E&gt; {    Iterator&lt;E&gt; iterator();}</code></pre><p>这个接口中提供了上面迭代器的实现Iterator的获取，也就是后续在自己的数据结构中需要实现迭代器的功能并交给Iterable，由此让外部调用方进行获取使用。</p><h4 id="集合功能接口定义"><a href="#集合功能接口定义" class="headerlink" title="集合功能接口定义"></a>集合功能接口定义</h4><pre><code>public interface Collection&lt;E, L&gt; extends Iterable&lt;E&gt; {    boolean add(E e);    boolean remove(E e);    boolean addLink(String key, L l);    boolean removeLink(String key);    Iterator&lt;E&gt; iterator();}</code></pre><ul><li>定义集合操作接口；Collection，同时继承了另外一个接口Iterable的方法iterator()。这样后续谁来实现这个接口，就需要实现上述定义的一些基本功能；添加元素、删除元素、遍历。</li><li>这里定义了两个泛型&lt;E, L&gt;，因为我们的数据结构一个是用于添加元素，另外一个是用于添加树节点的链路关系。</li></ul><h4 id="核心-迭代器功能实现"><a href="#核心-迭代器功能实现" class="headerlink" title="(核心)迭代器功能实现"></a>(核心)迭代器功能实现</h4><pre><code>public class GroupStructure implements Collection&lt;Employee, Link&gt; {    private String groupId;                                                 // 组织ID，也是一个组织链的头部ID    private String groupName;                                               // 组织名称    private Map&lt;String, Employee&gt; employeeMap = new ConcurrentHashMap&lt;String, Employee&gt;();  // 雇员列表    private Map&lt;String, List&lt;Link&gt;&gt; linkMap = new ConcurrentHashMap&lt;String, List&lt;Link&gt;&gt;();  // 组织架构关系；id-&gt;list    private Map&lt;String, String&gt; invertedMap = new ConcurrentHashMap&lt;String, String&gt;();       // 反向关系链    public GroupStructure(String groupId, String groupName) {        this.groupId = groupId;        this.groupName = groupName;    }    public boolean add(Employee employee) {        return null != employeeMap.put(employee.getuId(), employee);    }    public boolean remove(Employee o) {        return null != employeeMap.remove(o.getuId());    }    public boolean addLink(String key, Link link) {        invertedMap.put(link.getToId(), link.getFromId());        if (linkMap.containsKey(key)) {            return linkMap.get(key).add(link);        } else {            List&lt;Link&gt; links = new LinkedList&lt;Link&gt;();            links.add(link);            linkMap.put(key, links);            return true;        }    }    public boolean removeLink(String key) {        return null != linkMap.remove(key);    }    public Iterator&lt;Employee&gt; iterator() {        return new Iterator&lt;Employee&gt;() {            HashMap&lt;String, Integer&gt; keyMap = new HashMap&lt;String, Integer&gt;();            int totalIdx = 0;            private String fromId = groupId;  // 雇员ID，From            private String toId = groupId;   // 雇员ID，To            public boolean hasNext() {                return totalIdx &lt; employeeMap.size();            }            public Employee next() {                List&lt;Link&gt; links = linkMap.get(toId);                int cursorIdx = getCursorIdx(toId);                // 同级节点扫描                if (null == links) {                    cursorIdx = getCursorIdx(fromId);                    links = linkMap.get(fromId);                }                // 上级节点扫描                while (cursorIdx &gt; links.size() - 1) {                    fromId = invertedMap.get(fromId);                    cursorIdx = getCursorIdx(fromId);                    links = linkMap.get(fromId);                }                // 获取节点                Link link = links.get(cursorIdx);                toId = link.getToId();                fromId = link.getFromId();                totalIdx++;                // 返回结果                return employeeMap.get(link.getToId());            }            // 给每个层级定义宽度遍历进度            public int getCursorIdx(String key) {                int idx = 0;                if (keyMap.containsKey(key)) {                    idx = keyMap.get(key);                    keyMap.put(key, ++idx);                } else {                    keyMap.put(key, idx);                }                return idx;            }        };    }}</code></pre><ul><li>以上的这部分代码稍微有点长，主要包括了对元素的添加和删除。另外最重要的是对遍历的实现 new Iterator<employee>。</employee></li><li>添加和删除元素相对来说比较简单，使用了两个map数组结构进行定义；雇员列表、组织架构关系；id-&gt;list。当元素添加元素的时候，会分别在不同的方法中向map结构中进行填充指向关系(A-&gt;B)，也就构建出了我们的树形组织关系。</li></ul><p>迭代器实现思路</p><ul><li>这里的树形结构我们需要做的是深度遍历，也就是左侧的一直遍历到最深节点。</li><li>当遍历到最深节点后，开始遍历最深节点的横向节点。</li><li>当横向节点遍历完成后则向上寻找横向节点，直至树结构全部遍历完成。</li></ul><h3 id="测试验证"><a href="#测试验证" class="headerlink" title="测试验证"></a>测试验证</h3><pre><code>@Testpublic void test_iterator() {     // 数据填充    GroupStructure groupStructure = new GroupStructure(&quot;1&quot;, &quot;小傅哥&quot;);      // 雇员信息    groupStructure.add(new Employee(&quot;2&quot;, &quot;花花&quot;, &quot;二级部门&quot;));    groupStructure.add(new Employee(&quot;3&quot;, &quot;豆包&quot;, &quot;二级部门&quot;));    groupStructure.add(new Employee(&quot;4&quot;, &quot;蹦蹦&quot;, &quot;三级部门&quot;));    groupStructure.add(new Employee(&quot;5&quot;, &quot;大烧&quot;, &quot;三级部门&quot;));    groupStructure.add(new Employee(&quot;6&quot;, &quot;虎哥&quot;, &quot;四级部门&quot;));    groupStructure.add(new Employee(&quot;7&quot;, &quot;玲姐&quot;, &quot;四级部门&quot;));    groupStructure.add(new Employee(&quot;8&quot;, &quot;秋雅&quot;, &quot;四级部门&quot;));       // 节点关系 1-&gt;(1,2) 2-&gt;(4,5)    groupStructure.addLink(&quot;1&quot;, new Link(&quot;1&quot;, &quot;2&quot;));    groupStructure.addLink(&quot;1&quot;, new Link(&quot;1&quot;, &quot;3&quot;));    groupStructure.addLink(&quot;2&quot;, new Link(&quot;2&quot;, &quot;4&quot;));    groupStructure.addLink(&quot;2&quot;, new Link(&quot;2&quot;, &quot;5&quot;));    groupStructure.addLink(&quot;5&quot;, new Link(&quot;5&quot;, &quot;6&quot;));    groupStructure.addLink(&quot;5&quot;, new Link(&quot;5&quot;, &quot;7&quot;));    groupStructure.addLink(&quot;5&quot;, new Link(&quot;5&quot;, &quot;8&quot;));           Iterator&lt;Employee&gt; iterator = groupStructure.iterator();    while (iterator.hasNext()) {        Employee employee = iterator.next();        logger.info(&quot;{}，雇员 Id：{} Name：{}&quot;, employee.getDesc(), employee.getuId(), employee.getName());    }}</code></pre><pre><code>22:23:37.166 [main] INFO  org.itstack.demo.design.test.ApiTest - 二级部门，雇员 Id：2 Name：花花22:23:37.168 [main] INFO  org.itstack.demo.design.test.ApiTest - 三级部门，雇员 Id：4 Name：蹦蹦22:23:37.169 [main] INFO  org.itstack.demo.design.test.ApiTest - 三级部门，雇员 Id：5 Name：大烧22:23:37.169 [main] INFO  org.itstack.demo.design.test.ApiTest - 四级部门，雇员 Id：6 Name：虎哥22:23:37.169 [main] INFO  org.itstack.demo.design.test.ApiTest - 四级部门，雇员 Id：7 Name：玲姐22:23:37.169 [main] INFO  org.itstack.demo.design.test.ApiTest - 四级部门，雇员 Id：8 Name：秋雅22:23:37.169 [main] INFO  org.itstack.demo.design.test.ApiTest - 二级部门，雇员 Id：3 Name：豆包Process finished with exit code 0</code></pre><p>从遍历的结果可以看到，我们是顺着树形结构的深度开始遍历，一直到右侧的节点3；雇员 Id：2、雇员 Id：4…雇员 Id：3</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>迭代器的设计模式从以上的功能实现可以看到，满足了单一职责和开闭原则，外界的调用方也不需要知道任何一个不同的数据结构在使用上的遍历差异。可以非常方便的扩展，也让整个遍历变得更加干净整洁。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>命令模式</title>
      <link href="/2021/04/14/2021-04-14-design-ming-ling/"/>
      <url>/2021/04/14/2021-04-14-design-ming-ling/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="命令模式介绍"><a href="#命令模式介绍" class="headerlink" title="命令模式介绍"></a>命令模式介绍</h2><p>命令模式是行为模式中的一种，以数据驱动的方式将命令对象，可以使用构造函数的方式传递给调用者。调用者再提供相应的实现为命令执行提供操作方法。</p><p>在这个设计模式的实现过程中有如下几个比较重要的点；</p><ul><li>抽象命令类：声明执行命令的接口和方法</li><li>具体的命令实现类：接口类的具体实现，可以是一组相似的行为逻辑</li><li>实现者：也就是为命令做实现的具体实现类</li><li>调用者：处理命令、实现的具体操作者，负责对外提供命令服务</li></ul><h2 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h2><p><img src="/2021/04/14/2021-04-14-design-ming-ling/itstack-demo-design-14-02.png" alt></p><p>在这个案例中我们模拟在餐厅中点餐交给厨师👨‍🍳烹饪的场景</p><p>命令场景的核心的逻辑是调用方与不需要去关心具体的逻辑实现，在这个场景中也就是点餐人员只需要把需要点的各种菜系交个小二就可以，小二再把各项菜品交给各个厨师进行烹饪。也就是点餐人员不需要跟各个厨师交流，只需要在统一的环境里下达命令就可以。</p><p>在这个场景中可以看到有不同的菜品；山东（鲁菜）、四川（川菜）、江苏（苏菜）、广东（粤菜）、福建（闽菜）、浙江（浙菜）、湖南（湘菜），每种菜品都会有不同的厨师👩‍🍳进行烹饪。而客户并不会去关心具体是谁烹饪，厨师也不会去关心谁点的餐。客户只关心早点上菜，厨师只关心还有多少个菜要做。而这中间的衔接的过程，由小二完成。</p><h2 id="用一坨坨代码实现"><a href="#用一坨坨代码实现" class="headerlink" title="用一坨坨代码实现"></a>用一坨坨代码实现</h2><h3 id="工程结构"><a href="#工程结构" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-14-01└── src    └── main        └── java            └── org.itstack.demo.design                └── XiaoEr.java</code></pre><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><pre><code>public class XiaoEr {    private Logger logger = LoggerFactory.getLogger(XiaoEr.class);    private Map&lt;Integer, String&gt; cuisineMap = new ConcurrentHashMap&lt;Integer, String&gt;();    public void order(int cuisine) {        // 广东（粤菜）        if (1 == cuisine) {            cuisineMap.put(1, &quot;广东厨师，烹饪鲁菜，宫廷最大菜系，以孔府风味为龙头&quot;);        }        // 江苏（苏菜）        if (2 == cuisine) {            cuisineMap.put(2, &quot;江苏厨师，烹饪苏菜，宫廷第二大菜系，古今国宴上最受人欢迎的菜系。&quot;);        }        // 山东（鲁菜）        if (3 == cuisine) {            cuisineMap.put(3, &quot;山东厨师，烹饪鲁菜，宫廷最大菜系，以孔府风味为龙头.&quot;);        }        // 四川（川菜）        if (4 == cuisine) {            cuisineMap.put(4, &quot;四川厨师，烹饪川菜，中国最有特色的菜系，也是民间最大菜系。&quot;);        }    }    public void placeOrder() {        logger.info(&quot;菜单：{}&quot;, JSON.toJSONString(cuisineMap));    }}</code></pre><p>在这个类的实现中提供了两个方法，一个方法用于点单添加菜品order()，另外一个方法展示菜品的信息placeOrder()。</p><h2 id="命令模式重构代码"><a href="#命令模式重构代码" class="headerlink" title="命令模式重构代码"></a>命令模式重构代码</h2><p>命令模式可以将上述的模式拆解三层大块，命令、命令实现者、命令的调用者，当有新的菜品或者厨师扩充时候就可以在指定的类结构下进行实现添加即可，外部的调用也会非常的容易扩展。</p><h3 id="工程结构-1"><a href="#工程结构-1" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-14-02└── src    ├── main    │   └── java    │       └── org.itstack.demo.design    │           ├── cook    │           │    ├── impl    │           │    │   ├── GuangDongCook.java    │           │    │   ├── JiangSuCook.java    │           │    │   ├── ShanDongCook.java    │           │    │   └── SiChuanCook.java    │           │    └── ICook.java    │           ├── cuisine    │           │    ├── impl    │           │    │   ├── GuangDoneCuisine.java    │           │    │   ├── JiangSuCuisine.java    │           │    │   ├── ShanDongCuisine.java    │           │    │   └── SiChuanCuisine.java    │           │    └── ICuisine.java    │           └── XiaoEr.java    └── test        └── java            └── org.itstack.demo.test                └── ApiTest.java</code></pre><h3 id="命令模式模型结构"><a href="#命令模式模型结构" class="headerlink" title="命令模式模型结构"></a>命令模式模型结构</h3><p><img src="/2021/04/14/2021-04-14-design-ming-ling/itstack-demo-design-14-03.png" alt></p><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="抽象命令定义-菜品接口"><a href="#抽象命令定义-菜品接口" class="headerlink" title="抽象命令定义(菜品接口)"></a>抽象命令定义(菜品接口)</h4><pre><code>public interface ICuisine {    void cook(); // 烹调、制作}</code></pre><h4 id="广东（粤菜）"><a href="#广东（粤菜）" class="headerlink" title="广东（粤菜）"></a>广东（粤菜）</h4><pre><code>public class GuangDoneCuisine implements ICuisine {    private ICook cook;    public GuangDoneCuisine(ICook cook) {        this.cook = cook;    }    public void cook() {        cook.doCooking();    }}</code></pre><h4 id="江苏（苏菜）"><a href="#江苏（苏菜）" class="headerlink" title="江苏（苏菜）"></a>江苏（苏菜）</h4><pre><code>public class JiangSuCuisine implements ICuisine {    private ICook cook;    public JiangSuCuisine(ICook cook) {        this.cook = cook;    }    public void cook() {        cook.doCooking();    }}</code></pre><p>在实现的类中都有添加了一个厨师类(ICook)，并通过这个类提供的方法进行操作命令(烹饪菜品)cook.doCooking()。</p><h4 id="抽象实现者定义-厨师接口"><a href="#抽象实现者定义-厨师接口" class="headerlink" title="抽象实现者定义(厨师接口)"></a>抽象实现者定义(厨师接口)</h4><pre><code>public interface ICook {    void doCooking();}</code></pre><p>这里定义的是具体的为命令的实现者，这里也就是菜品对应的厨师烹饪的指令实现。</p><h4 id="粤菜，厨师"><a href="#粤菜，厨师" class="headerlink" title="粤菜，厨师"></a>粤菜，厨师</h4><pre><code>public class GuangDongCook implements ICook {    private Logger logger = LoggerFactory.getLogger(ICook.class);    public void doCooking() {        logger.info(&quot;广东厨师，烹饪鲁菜，宫廷最大菜系，以孔府风味为龙头&quot;);    }}</code></pre><h4 id="苏菜，厨师"><a href="#苏菜，厨师" class="headerlink" title="苏菜，厨师"></a>苏菜，厨师</h4><pre><code>public class JiangSuCook implements ICook {    private Logger logger = LoggerFactory.getLogger(ICook.class);    public void doCooking() {        logger.info(&quot;江苏厨师，烹饪苏菜，宫廷第二大菜系，古今国宴上最受人欢迎的菜系。&quot;);    }}</code></pre><p>在这个实现的过程是模拟打了日志，相当于通知了厨房里具体的厨师进行菜品烹饪。</p><h4 id="调用者-小二"><a href="#调用者-小二" class="headerlink" title="调用者(小二)"></a>调用者(小二)</h4><pre><code>public class XiaoEr {    private Logger logger = LoggerFactory.getLogger(XiaoEr.class);    private List&lt;ICuisine&gt; cuisineList = new ArrayList&lt;ICuisine&gt;();    public void order(ICuisine cuisine) {        cuisineList.add(cuisine);    }    public synchronized void placeOrder() {        for (ICuisine cuisine : cuisineList) {            cuisine.cook();        }        cuisineList.clear();    }}</code></pre><p>在调用者的具体实现中，提供了菜品的添加和菜单执行烹饪。这个过程是命令模式的具体调用，通过外部将菜品和厨师传递进来而进行具体的调用。</p><h4 id="测试验证"><a href="#测试验证" class="headerlink" title="测试验证"></a>测试验证</h4><pre><code>@Testpublic void test(){    // 菜系 + 厨师；广东（粤菜）、江苏（苏菜）、山东（鲁菜）、四川（川菜）    ICuisine guangDoneCuisine = new GuangDoneCuisine(new GuangDongCook());    JiangSuCuisine jiangSuCuisine = new JiangSuCuisine(new JiangSuCook());    ShanDongCuisine shanDongCuisine = new ShanDongCuisine(new ShanDongCook());    SiChuanCuisine siChuanCuisine = new SiChuanCuisine(new SiChuanCook());    // 点单    XiaoEr xiaoEr = new XiaoEr();    xiaoEr.order(guangDoneCuisine);    xiaoEr.order(jiangSuCuisine);    xiaoEr.order(shanDongCuisine);    xiaoEr.order(siChuanCuisine);    // 下单    xiaoEr.placeOrder();}</code></pre><p>这里可以主要观察菜品与厨师的组合；new GuangDoneCuisine(new GuangDongCook());，每一个具体的命令都拥有一个对应的实现类，可以进行组合。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>从以上的内容和例子可以感受到，命令模式的使用场景需要分为三个比较大的块；命令、实现、调用者，而这三块内容的拆分也是选择适合场景的关键因素，经过这样的拆分可以让逻辑具备单一职责的性质，便于扩展。</li><li>通过这样的实现方式与if语句相比，降低了耦合性也方便其他的命令和实现的扩展。但同时这样的设计模式也带来了一点问题，就是在各种命令与实现的组合下，会扩展出很多的实现类，需要进行管理。</li><li>命令模式是行为模式中的一种，以数据驱动的方式将命令对象，可以使用构造函数的方式传递给调用者。调用者再提供相应的实现为命令执行提供操作方法。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>责任链模式</title>
      <link href="/2021/04/13/2021-04-13-design-ze-ren-lian/"/>
      <url>/2021/04/13/2021-04-13-design-ze-ren-lian/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="责任链模式介绍"><a href="#责任链模式介绍" class="headerlink" title="责任链模式介绍"></a>责任链模式介绍</h2><p>责任链模式的核心是解决一组服务中的先后执行处理关系。就有点像你没钱花了，需要家庭财务支出审批，10块钱以下找闺女审批，100块钱先闺女审批在媳妇审批。你可以理解想象成当你要跳槽的时候被安排的明明白白的被各个领导签字放行。</p><h2 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h2><h3 id="场景模拟工程"><a href="#场景模拟工程" class="headerlink" title="场景模拟工程"></a>场景模拟工程</h3><pre><code>itstack-demo-design-13-00└── src    └── main        └── java            └── org.itstack.demo.design                └── AuthService.java</code></pre><h3 id="场景简述"><a href="#场景简述" class="headerlink" title="场景简述"></a>场景简述</h3><h4 id="模拟审核服务"><a href="#模拟审核服务" class="headerlink" title="模拟审核服务"></a>模拟审核服务</h4><pre><code>public class AuthService {    private static Map&lt;String, Date&gt; authMap = new ConcurrentHashMap&lt;String, Date&gt;();    public static Date queryAuthInfo(String uId, String orderId) {        return authMap.get(uId.concat(orderId));    }    public static void auth(String uId, String orderId) {        authMap.put(uId.concat(orderId), new Date());    }}</code></pre><p>这部分是把由谁审核的和审核的单子ID作为唯一key值记录到内存Map结构中</p><h2 id="用一坨坨代码实现"><a href="#用一坨坨代码实现" class="headerlink" title="用一坨坨代码实现"></a>用一坨坨代码实现</h2><p>按照我们的需求审批流程，平常系统上线只需要三级负责人审批就可以，但是到了618大促时间点，就需要由二级负责以及一级负责人一起加入审批系统上线流程。在这里我们使用非常直接的if判断方式来实现这样的需求。</p><h3 id="工程结构"><a href="#工程结构" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-13-01└── src    └── main        └── java            └── org.itstack.demo.design                └── AuthController.java</code></pre><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><pre><code>public class AuthController {    private SimpleDateFormat f = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);// 时间格式化    public AuthInfo doAuth(String uId, String orderId, Date authDate) throws ParseException {        // 三级审批        Date date = AuthService.queryAuthInfo(&quot;1000013&quot;, orderId);        if (null == date) return new AuthInfo(&quot;0001&quot;, &quot;单号：&quot;, orderId, &quot; 状态：待三级审批负责人 &quot;, &quot;王工&quot;);        // 二级审批        if (authDate.after(f.parse(&quot;2020-06-01 00:00:00&quot;)) &amp;&amp; authDate.before(f.parse(&quot;2020-06-25 23:59:59&quot;))) {            date = AuthService.queryAuthInfo(&quot;1000012&quot;, orderId);            if (null == date) return new AuthInfo(&quot;0001&quot;, &quot;单号：&quot;, orderId, &quot; 状态：待二级审批负责人 &quot;, &quot;张经理&quot;);        }        // 一级审批        if (authDate.after(f.parse(&quot;2020-06-11 00:00:00&quot;)) &amp;&amp; authDate.before(f.parse(&quot;2020-06-20 23:59:59&quot;))) {            date = AuthService.queryAuthInfo(&quot;1000011&quot;, orderId);            if (null == date) return new AuthInfo(&quot;0001&quot;, &quot;单号：&quot;, orderId, &quot; 状态：待一级审批负责人 &quot;, &quot;段总&quot;);        }        return new AuthInfo(&quot;0001&quot;, &quot;单号：&quot;, orderId, &quot; 状态：审批完成&quot;);    }}</code></pre><h3 id="测试验证"><a href="#测试验证" class="headerlink" title="测试验证"></a>测试验证</h3><pre><code>@Testpublic void test_AuthController() throws ParseException {    AuthController authController = new AuthController();      // 模拟三级负责人审批    logger.info(&quot;测试结果：{}&quot;, JSON.toJSONString(authController.doAuth(&quot;小傅哥&quot;, &quot;1000998004813441&quot;, new Date())));    logger.info(&quot;测试结果：{}&quot;, &quot;模拟三级负责人审批，王工&quot;);    AuthService.auth(&quot;1000013&quot;, &quot;1000998004813441&quot;);      // 模拟二级负责人审批                                     logger.info(&quot;测试结果：{}&quot;, JSON.toJSONString(authController.doAuth(&quot;小傅哥&quot;, &quot;1000998004813441&quot;, new Date())));    logger.info(&quot;测试结果：{}&quot;, &quot;模拟二级负责人审批，张经理&quot;);    AuthService.auth(&quot;1000012&quot;, &quot;1000998004813441&quot;);        // 模拟一级负责人审批    logger.info(&quot;测试结果：{}&quot;, JSON.toJSONString(authController.doAuth(&quot;小傅哥&quot;, &quot;1000998004813441&quot;, new Date())));    logger.info(&quot;测试结果：{}&quot;, &quot;模拟一级负责人审批，段总&quot;);    AuthService.auth(&quot;1000011&quot;, &quot;1000998004813441&quot;);                logger.info(&quot;测试结果：{}&quot;, &quot;审批完成&quot;);}</code></pre><pre><code>23:25:00.363 [main] INFO  org.itstack.demo.design.test.ApiTest - 测试结果：{&quot;code&quot;:&quot;0001&quot;,&quot;info&quot;:&quot;单号：1000998004813441 状态：待三级审批负责人 王工&quot;}23:25:00.366 [main] INFO  org.itstack.demo.design.test.ApiTest - 测试结果：模拟三级负责人审批，王工23:25:00.367 [main] INFO  org.itstack.demo.design.test.ApiTest - 测试结果：{&quot;code&quot;:&quot;0001&quot;,&quot;info&quot;:&quot;单号：1000998004813441 状态：待二级审批负责人 张经理&quot;}23:25:00.367 [main] INFO  org.itstack.demo.design.test.ApiTest - 测试结果：模拟二级负责人审批，张经理23:25:00.368 [main] INFO  org.itstack.demo.design.test.ApiTest - 测试结果：{&quot;code&quot;:&quot;0001&quot;,&quot;info&quot;:&quot;单号：1000998004813441 状态：待一级审批负责人 段总&quot;}23:25:00.368 [main] INFO  org.itstack.demo.design.test.ApiTest - 测试结果：模拟一级负责人审批，段总23:25:00.368 [main] INFO  org.itstack.demo.design.test.ApiTest - 测试结果：审批完成Process finished with exit code 0</code></pre><h2 id="责任链模式重构代码"><a href="#责任链模式重构代码" class="headerlink" title="责任链模式重构代码"></a>责任链模式重构代码</h2><p>责任链模式可以让各个服务模块更加清晰，而每一个模块间可以通过next的方式进行获取。而每一个next是由继承的统一抽象类实现的。最终所有类的职责可以动态的进行编排使用，编排的过程可以做成可配置化。</p><h3 id="工程结构-1"><a href="#工程结构-1" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-13-02└── src    └── main        └── java            └── org.itstack.demo.design                ├── impl                │    ├── Level1AuthLink.java                │    ├── Level2AuthLink.java                │    └── Level3AuthLink.java                ├── AuthInfo.java                └── AuthLink.java</code></pre><p>责任链模式模型结构</p><p><img src="/2021/04/13/2021-04-13-design-ze-ren-lian/itstack-demo-design-13-03.png" alt></p><ul><li>上图是这个业务模型中责任链结构的核心部分，通过三个实现了统一抽象类AuthLink的不同规则，再进行责任编排模拟出一条链路。这个链路就是业务中的责任链。</li><li>一般在使用责任链时候如果是场景比较固定，可以通过写死到代码中进行初始化。但如果业务场景经常变化可以做成xml配置的方式进行处理，也可以落到库里进行初始化操作。</li></ul><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="责任链中返回对象定义"><a href="#责任链中返回对象定义" class="headerlink" title="责任链中返回对象定义"></a>责任链中返回对象定义</h4><pre><code>public class AuthInfo {    private String code;    private String info = &quot;&quot;;    public AuthInfo(String code, String ...infos) {        this.code = code;        for (String str:infos){            this.info = this.info.concat(str);        }    }    // ...get/set}</code></pre><p>这个类的是包装了责任链处理过程中返回结果的类，方面处理每个责任链的返回信息。</p><h4 id="链路抽象类定义"><a href="#链路抽象类定义" class="headerlink" title="链路抽象类定义"></a>链路抽象类定义</h4><pre><code>public abstract class AuthLink {    protected Logger logger = LoggerFactory.getLogger(AuthLink.class);    protected SimpleDateFormat f = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);// 时间格式化    protected String levelUserId;                           // 级别人员ID    protected String levelUserName;                         // 级别人员姓名     private AuthLink next;                                  // 责任链    public AuthLink(String levelUserId, String levelUserName) {        this.levelUserId = levelUserId;        this.levelUserName = levelUserName;    }    public AuthLink next() {        return next;    }    public AuthLink appendNext(AuthLink next) {        this.next = next;        return this;    }    public abstract AuthInfo doAuth(String uId, String orderId, Date authDate);}</code></pre><ul><li>这部分是责任链，链接起来的核心部分。AuthLink next，重点在于可以通过next方式获取下一个链路需要处理的节点。</li><li>levelUserId、levelUserName，是责任链中的公用信息，标记每一个审核节点的人员信息。</li><li>抽象类中定义了一个抽象方法，abstract AuthInfo doAuth，这是每一个实现者必须实现的类，不同的审核级别处理不同的业务。</li></ul><h4 id="三个审核实现类"><a href="#三个审核实现类" class="headerlink" title="三个审核实现类"></a>三个审核实现类</h4><pre><code>public class Level1AuthLink extends AuthLink {    public Level1AuthLink(String levelUserId, String levelUserName) {        super(levelUserId, levelUserName);    }    public AuthInfo doAuth(String uId, String orderId, Date authDate) {        Date date = AuthService.queryAuthInfo(levelUserId, orderId);        if (null == date) {            return new AuthInfo(&quot;0001&quot;, &quot;单号：&quot;, orderId, &quot; 状态：待一级审批负责人 &quot;, levelUserName);        }        AuthLink next = super.next();        if (null == next) {            return new AuthInfo(&quot;0000&quot;, &quot;单号：&quot;, orderId, &quot; 状态：一级审批完成负责人&quot;, &quot; 时间：&quot;, f.format(date), &quot; 审批人：&quot;, levelUserName);        }        return next.doAuth(uId, orderId, authDate);    }}</code></pre><pre><code>public class Level2AuthLink extends AuthLink {    private Date beginDate = f.parse(&quot;2020-06-11 00:00:00&quot;);    private Date endDate = f.parse(&quot;2020-06-20 23:59:59&quot;);    public Level2AuthLink(String levelUserId, String levelUserName) throws ParseException {        super(levelUserId, levelUserName);    }    public AuthInfo doAuth(String uId, String orderId, Date authDate) {        Date date = AuthService.queryAuthInfo(levelUserId, orderId);        if (null == date) {            return new AuthInfo(&quot;0001&quot;, &quot;单号：&quot;, orderId, &quot; 状态：待二级审批负责人 &quot;, levelUserName);        }        AuthLink next = super.next();        if (null == next) {            return new AuthInfo(&quot;0000&quot;, &quot;单号：&quot;, orderId, &quot; 状态：二级审批完成负责人&quot;, &quot; 时间：&quot;, f.format(date), &quot; 审批人：&quot;, levelUserName);        }        if (authDate.before(beginDate) || authDate.after(endDate)) {            return new AuthInfo(&quot;0000&quot;, &quot;单号：&quot;, orderId, &quot; 状态：二级审批完成负责人&quot;, &quot; 时间：&quot;, f.format(date), &quot; 审批人：&quot;, levelUserName);        }        return next.doAuth(uId, orderId, authDate);    }}</code></pre><pre><code>public class Level3AuthLink extends AuthLink {    private Date beginDate = f.parse(&quot;2020-06-01 00:00:00&quot;);    private Date endDate = f.parse(&quot;2020-06-25 23:59:59&quot;);    public Level3AuthLink(String levelUserId, String levelUserName) throws ParseException {        super(levelUserId, levelUserName);    }    public AuthInfo doAuth(String uId, String orderId, Date authDate) {        Date date = AuthService.queryAuthInfo(levelUserId, orderId);        if (null == date) {            return new AuthInfo(&quot;0001&quot;, &quot;单号：&quot;, orderId, &quot; 状态：待三级审批负责人 &quot;, levelUserName);        }        AuthLink next = super.next();        if (null == next) {            return new AuthInfo(&quot;0000&quot;, &quot;单号：&quot;, orderId, &quot; 状态：三级审批负责人完成&quot;, &quot; 时间：&quot;, f.format(date), &quot; 审批人：&quot;, levelUserName);        }        if (authDate.before(beginDate) || authDate.after(endDate)) {            return new AuthInfo(&quot;0000&quot;, &quot;单号：&quot;, orderId, &quot; 状态：三级审批负责人完成&quot;, &quot; 时间：&quot;, f.format(date), &quot; 审批人：&quot;, levelUserName);        }        return next.doAuth(uId, orderId, authDate);    }}</code></pre><ul><li>例如第一个审核类中会先判断是否审核通过，如果没有审核通过则返回结果给调用方，引导去审核。</li><li>判断完成后获取下一个审核节点；super.next();，如果不存在下一个节点，则直接返回结果。</li><li>之后是根据不同的业务时间段进行判断是否需要，二级和一级的审核。</li><li>最后返回下一个审核结果；next.doAuth(uId, orderId, authDate);，有点像递归调用。</li></ul><h4 id="测试验证-1"><a href="#测试验证-1" class="headerlink" title="测试验证"></a>测试验证</h4><pre><code>@Testpublic void test_AuthLink() throws ParseException {    AuthLink authLink = new Level3AuthLink(&quot;1000013&quot;, &quot;王工&quot;)            .appendNext(new Level2AuthLink(&quot;1000012&quot;, &quot;张经理&quot;)                    .appendNext(new Level1AuthLink(&quot;1000011&quot;, &quot;段总&quot;)));    logger.info(&quot;测试结果：{}&quot;, JSON.toJSONString(authLink.doAuth(&quot;小傅哥&quot;, &quot;1000998004813441&quot;, new Date())));    // 模拟三级负责人审批    AuthService.auth(&quot;1000013&quot;, &quot;1000998004813441&quot;);    logger.info(&quot;测试结果：{}&quot;, &quot;模拟三级负责人审批，王工&quot;);    logger.info(&quot;测试结果：{}&quot;, JSON.toJSONString(authLink.doAuth(&quot;小傅哥&quot;, &quot;1000998004813441&quot;, new Date())));    // 模拟二级负责人审批    AuthService.auth(&quot;1000012&quot;, &quot;1000998004813441&quot;);    logger.info(&quot;测试结果：{}&quot;, &quot;模拟二级负责人审批，张经理&quot;);    logger.info(&quot;测试结果：{}&quot;, JSON.toJSONString(authLink.doAuth(&quot;小傅哥&quot;, &quot;1000998004813441&quot;, new Date())));    // 模拟一级负责人审批    AuthService.auth(&quot;1000011&quot;, &quot;1000998004813441&quot;);    logger.info(&quot;测试结果：{}&quot;, &quot;模拟一级负责人审批，段总&quot;);    logger.info(&quot;测试结果：{}&quot;, JSON.toJSONString(authLink.doAuth(&quot;小傅哥&quot;, &quot;1000998004813441&quot;, new Date())));}</code></pre><ul><li>这里包括最核心的责任链创建，实际的业务中会包装到控制层； AuthLink authLink = new Level3AuthLink(“1000013”, “王工”) .appendNext(new Level2AuthLink(“1000012”, “张经理”) .appendNext(new Level1AuthLink(“1000011”, “段总”))); 通过把不同的责任节点进行组装，构成一条完整业务的责任链。</li><li>接下里不断的执行查看审核链路authLink.doAuth(…)，通过返回结果对数据进行3、2、1级负责人审核，直至最后审核全部完成。</li></ul><pre><code>23:49:46.585 [main] INFO  org.itstack.demo.design.test.ApiTest - 测试结果：{&quot;code&quot;:&quot;0001&quot;,&quot;info&quot;:&quot;单号：1000998004813441 状态：待三级审批负责人 王工&quot;}23:49:46.590 [main] INFO  org.itstack.demo.design.test.ApiTest - 测试结果：模拟三级负责人审批，王工23:49:46.590 [main] INFO  org.itstack.demo.design.test.ApiTest - 测试结果：{&quot;code&quot;:&quot;0001&quot;,&quot;info&quot;:&quot;单号：1000998004813441 状态：待二级审批负责人 张经理&quot;}23:49:46.590 [main] INFO  org.itstack.demo.design.test.ApiTest - 测试结果：模拟二级负责人审批，张经理23:49:46.590 [main] INFO  org.itstack.demo.design.test.ApiTest - 测试结果：{&quot;code&quot;:&quot;0001&quot;,&quot;info&quot;:&quot;单号：1000998004813441 状态：待一级审批负责人 段总&quot;}23:49:46.590 [main] INFO  org.itstack.demo.design.test.ApiTest - 测试结果：模拟一级负责人审批，段总23:49:46.590 [main] INFO  org.itstack.demo.design.test.ApiTest - 测试结果：{&quot;code&quot;:&quot;0000&quot;,&quot;info&quot;:&quot;单号：1000998004813441 状态：一级审批完成负责人 时间：2020-06-18 23:49:46 审批人：段总&quot;}Process finished with exit code 0</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>从上面代码从if语句重构到使用责任链模式开发可以看到，我们的代码结构变得清晰干净了，也解决了大量if语句的使用。并不是if语句不好，只不过if语句并不适合做系统流程设计，但是在做判断和行为逻辑处理中还是非常可以使用的。</li><li>责任链模式很好的处理单一职责和开闭原则，简单了耦合也使对象关系更加清晰，而且外部的调用方并不需要关心责任链是如何进行处理的(以上程序中可以把责任链的组合进行包装，在提供给外部使用)。但除了这些优点外也需要是适当的场景才进行使用，避免造成性能以及编排混乱调试测试疏漏问题。</li><li>责任链模式的核心是解决一组服务中的先后执行处理关系。 </li></ul>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>代理模式</title>
      <link href="/2021/04/12/2021-04-12-design-dai-li/"/>
      <url>/2021/04/12/2021-04-12-design-dai-li/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="代理模式介绍"><a href="#代理模式介绍" class="headerlink" title="代理模式介绍"></a>代理模式介绍</h2><p>代理模式有点像老大和小弟，也有点像分销商。主要解决的是问题是为某些资源的访问、对象的类的易用操作上提供方便使用的代理服务。而这种设计思想的模式经常会出现在我们的系统中，或者你用到过的组件中，它们都提供给你一种非常简单易用的方式控制原本你需要编写很多代码的进行使用的服务类。</p><p>类似这样的场景可以想到；</p><ul><li>你的数据库访问层面经常会提供一个较为基础的应用，以此来减少应用服务扩容时不至于数据库连接数暴增。</li><li>使用过的一些中间件例如；RPC框架，在拿到jar包对接口的描述后，中间件会在服务启动的时候生成对应的代理类，当调用接口的时候，实际是通过代理类发出的socket信息进行通过。</li><li>另外像我们常用的MyBatis，基本是定义接口但是不需要写实现类，就可以对xml或者自定义注解里的sql语句进行增删改查操作。</li></ul><h2 id="代理类模式实现过程"><a href="#代理类模式实现过程" class="headerlink" title="代理类模式实现过程"></a>代理类模式实现过程</h2><p>接下来会使用代理类模式来模拟实现一个Mybatis中对类的代理过程，也就是只需要定义接口，就可以关联到方法注解中的sql语句完成对数据库的操作。</p><p>这里需要注意一些知识点:</p><ul><li>BeanDefinitionRegistryPostProcessor，spring的接口类用于处理对bean的定义注册。</li><li>GenericBeanDefinition，定义bean的信息，在mybatis-spring中使用到的是；ScannedGenericBeanDefinition 略有不同。</li><li>FactoryBean，用于处理bean工厂的类，这个类非常见。</li></ul><h3 id="工程结构"><a href="#工程结构" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-12-00└── src    ├── main    │   ├── java    │   │   └── org.itstack.demo.design    │   │       ├── agent    │   │       │    ├── MapperFactoryBean.java    │   │       │    ├── RegisterBeanFactory.java    │   │       │    └── Select.java    │   │       └── IUserDao.java    │   └── resources        │       └── spring-config.xml    └── test        └── java            └── org.itstack.demo.test                └── ApiTest.java</code></pre><p>代理模式中间件模型结构</p><p><img src="/2021/04/12/2021-04-12-design-dai-li/itstack-demo-design-12-03.png" alt></p><ul><li>此模型中涉及的类并不多，但都是抽离出来的核心处理类。主要的事情就是对类的代理和注册到spring中。</li><li>上图中最上面是关于中间件的实现部分，下面对应的是功能的使用。</li></ul><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="自定义注解"><a href="#自定义注解" class="headerlink" title="自定义注解"></a>自定义注解</h4><pre><code>@Documented@Retention(RetentionPolicy.RUNTIME)@Target({ElementType.METHOD})public @interface Select {    String value() default &quot;&quot;;  // sql语句}</code></pre><p>这里我们定义了一个模拟mybatis-spring中的自定义注解，用于使用在方法层面。</p><h4 id="Dao层接口"><a href="#Dao层接口" class="headerlink" title="Dao层接口"></a>Dao层接口</h4><pre><code>public interface IUserDao {    @Select(&quot;select userName from user where id = #{uId}&quot;)    String queryUserInfo(String uId);}</code></pre><p>这里定义一个Dao层接口，并把自定义注解添加上。这与你使用的mybatis组件是一样的。</p><h4 id="代理类定义"><a href="#代理类定义" class="headerlink" title="代理类定义"></a>代理类定义</h4><pre><code>public class MapperFactoryBean&lt;T&gt; implements FactoryBean&lt;T&gt; {    private Logger logger = LoggerFactory.getLogger(MapperFactoryBean.class);    private Class&lt;T&gt; mapperInterface;    public MapperFactoryBean(Class&lt;T&gt; mapperInterface) {        this.mapperInterface = mapperInterface;    }    @Override    public T getObject() throws Exception {        InvocationHandler handler = (proxy, method, args) -&gt; {            Select select = method.getAnnotation(Select.class);            logger.info(&quot;SQL：{}&quot;, select.value().replace(&quot;#{uId}&quot;, args[0].toString()));            return args[0] + &quot;,沉淀、分享、成长，让自己和他人都能有所收获！&quot;;        };        return (T) Proxy.newProxyInstance(this.getClass().getClassLoader(), new Class[]{mapperInterface}, handler);    }    @Override    public Class&lt;?&gt; getObjectType() {        return mapperInterface;    }    @Override    public boolean isSingleton() {        return true;    }}</code></pre><ul><li>如果你有阅读过mybatis源码，是可以看到这样的一个类；MapperFactoryBean，这里我们也模拟一个这样的类，在里面实现我们对代理类的定义。</li><li>通过继承FactoryBean，提供bean对象，也就是方法；T getObject()。</li><li>在方法getObject()中提供类的代理以及模拟对sql语句的处理，这里包含了用户调用dao层方法时候的处理逻辑。</li><li>还有最上面我们提供构造函数来透传需要被代理类，Class<t> mapperInterface，在mybatis中也是使用这样的方式进行透传。</t></li><li>另外getObjectType()提供对象类型反馈，以及isSingleton()返回类是单例的。</li></ul><h4 id="将Bean定义注册到Spring容器"><a href="#将Bean定义注册到Spring容器" class="headerlink" title="将Bean定义注册到Spring容器"></a>将Bean定义注册到Spring容器</h4><pre><code>public class RegisterBeanFactory implements BeanDefinitionRegistryPostProcessor {    @Override    public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException {        GenericBeanDefinition beanDefinition = new GenericBeanDefinition();        beanDefinition.setBeanClass(MapperFactoryBean.class);        beanDefinition.setScope(&quot;singleton&quot;);        beanDefinition.getConstructorArgumentValues().addGenericArgumentValue(IUserDao.class);        BeanDefinitionHolder definitionHolder = new BeanDefinitionHolder(beanDefinition, &quot;userDao&quot;);        BeanDefinitionReaderUtils.registerBeanDefinition(definitionHolder, registry);    }    @Override    public void postProcessBeanFactory(ConfigurableListableBeanFactory configurableListableBeanFactory) throws BeansException {        // left intentionally blank    }}</code></pre><ul><li>这里我们将代理的bean交给spring容器管理，也就可以非常方便让我们可以获取到代理的bean。这部分是spring中关于一个bean注册过程的源码。</li><li>GenericBeanDefinition，用于定义一个bean的基本信息setBeanClass(MapperFactoryBean.class);，也包括可以透传给构造函数信息addGenericArgumentValue(IUserDao.class);</li><li>最后使用 BeanDefinitionReaderUtils.registerBeanDefinition，进行bean的注册，也就是注册到DefaultListableBeanFactory中。</li></ul><h4 id="配置文件spring-config"><a href="#配置文件spring-config" class="headerlink" title="配置文件spring-config"></a>配置文件spring-config</h4><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;       xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;       default-autowire=&quot;byName&quot;&gt;    &lt;bean id=&quot;userDao&quot; class=&quot;org.itstack.demo.design.agent.RegisterBeanFactory&quot;/&gt;&lt;/beans&gt;</code></pre><p>接下来在配置文件中添加我们的bean配置，在mybatis的使用中一般会配置扫描的dao层包，这样就可以减少这部分的配置。</p><h4 id="测试验证"><a href="#测试验证" class="headerlink" title="测试验证"></a>测试验证</h4><pre><code>@Testpublic void test_IUserDao() {    BeanFactory beanFactory = new ClassPathXmlApplicationContext(&quot;spring-config.xml&quot;);    IUserDao userDao = beanFactory.getBean(&quot;userDao&quot;, IUserDao.class);    String res = userDao.queryUserInfo(&quot;100001&quot;);    logger.info(&quot;测试结果：{}&quot;, res);}</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>代理模式除了开发中间件外还可以是对服务的包装，物联网组件等等，让复杂的各项服务变为轻量级调用、缓存使用。你可以理解为你家里的电灯开关，我们不能操作220v电线的人肉连接，但是可以使用开关，避免触电。</li><li>代理模式主要解决的是问题是为某些资源的访问、对象的类的易用操作上提供方便使用的代理服务。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>享元模式</title>
      <link href="/2021/04/11/2021-04-11-design-xiang-yuan/"/>
      <url>/2021/04/11/2021-04-11-design-xiang-yuan/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="享元模式介绍"><a href="#享元模式介绍" class="headerlink" title="享元模式介绍"></a>享元模式介绍</h2><p>享元模式，主要在于共享通用对象，减少内存的使用，提升系统的访问效率。而这部分共享对象通常比较耗费内存或者需要查询大量接口或者使用数据库资源，因此统一抽离作为共享对象使用。</p><p>另外享元模式可以分为在服务端和客户端，一般互联网H5和Web场景下大部分数据都需要服务端进行处理，比如数据库连接池的使用、多线程线程池的使用，除了这些功能外，还有些需要服务端进行包装后的处理下发给客户端，因为服务端需要做享元处理。但在一些游戏场景下，很多都是客户端需要进行渲染地图效果，比如；树木、花草、鱼虫，通过设置不同元素描述使用享元公用对象，减少内存的占用，让客户端的游戏更加流畅。</p><p>在享元模型的实现中需要使用到享元工厂来进行管理这部分独立的对象和共享的对象，避免出现线程安全的问题。</p><h2 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h2><p>模拟在商品秒杀场景下使用享元模式查询优化</p><p>你是否经历过一个商品下单的项目从最初的日均十几单到一个月后每个时段秒杀量破十万的项目。一般在最初如果没有经验的情况下可能会使用数据库行级锁的方式下保证商品库存的扣减操作，但是随着业务的快速发展秒杀的用户越来越多，这个时候数据库已经扛不住了，一般都会使用redis的分布式锁来控制商品库存。</p><p>同时在查询的时候也不需要每一次对不同的活动查询都从库中获取，因为这里除了库存以外其他的活动商品信息都是固定不变的，以此这里一般大家会缓存到内存中。</p><p>这里我们模拟使用享元模式工厂结构，提供活动商品的查询。活动商品相当于不变的信息，而库存部分属于变化的信息。</p><h2 id="用一坨坨代码实现"><a href="#用一坨坨代码实现" class="headerlink" title="用一坨坨代码实现"></a>用一坨坨代码实现</h2><h3 id="工程结构"><a href="#工程结构" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-11-01└── src    └── main        └── java            └── org.itstack.demo.design                └── ActivityController.java</code></pre><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><pre><code>public class ActivityController {    public Activity queryActivityInfo(Long id) {        // 模拟从实际业务应用从接口中获取活动信息        Activity activity = new Activity();        activity.setId(10001L);        activity.setName(&quot;图书嗨乐&quot;);        activity.setDesc(&quot;图书优惠券分享激励分享活动第二期&quot;);        activity.setStartTime(new Date());        activity.setStopTime(new Date());        activity.setStock(new Stock(1000,1));        return activity;    }}</code></pre><p>这里模拟的是从接口中查询活动信息，基本也就是从数据库中获取所有的商品信息和库存。有点像最开始写的商品销售系统，数据库就可以抗住购物量。</p><h2 id="享元模式重构代码"><a href="#享元模式重构代码" class="headerlink" title="享元模式重构代码"></a>享元模式重构代码</h2><p>享元模式一般情况下使用此结构在平时的开发中并不太多，除了一些线程池、数据库连接池外，再就是游戏场景下的场景渲染。另外这个设计的模式思想是减少内存的使用提升效率，与我们之前使用的原型模式通过克隆对象的方式生成复杂对象，减少rpc的调用，都是此类思想。</p><h3 id="工程结构-1"><a href="#工程结构-1" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-11-02└── src    ├── main    │   └── java    │       └── org.itstack.demo.design    │           ├── util    │           │    └── RedisUtils.java        │           ├── Activity.java    │           ├── ActivityController.java    │           ├── ActivityFactory.java    │           └── Stock.java    └── test        └── java            └── org.itstack.demo.test                └── ApiTest.java</code></pre><p>享元模式模型结构</p><p><img src="/2021/04/11/2021-04-11-design-xiang-yuan/itstack-demo-design-11-03.png" alt></p><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="活动信息"><a href="#活动信息" class="headerlink" title="活动信息"></a>活动信息</h4><pre><code>public class Activity {    private Long id;        // 活动ID    private String name;    // 活动名称    private String desc;    // 活动描述    private Date startTime; // 开始时间    private Date stopTime;  // 结束时间    private Stock stock;    // 活动库存    // ...get/set}</code></pre><h4 id="库存信息"><a href="#库存信息" class="headerlink" title="库存信息"></a>库存信息</h4><pre><code>public class Stock {    private int total; // 库存总量    private int used;  // 库存已用    // ...get/set}</code></pre><h4 id="享元工厂"><a href="#享元工厂" class="headerlink" title="享元工厂"></a>享元工厂</h4><pre><code>public class ActivityFactory {    static Map&lt;Long, Activity&gt; activityMap = new HashMap&lt;Long, Activity&gt;();    public static Activity getActivity(Long id) {        Activity activity = activityMap.get(id);        if (null == activity) {            // 模拟从实际业务应用从接口中获取活动信息            activity = new Activity();            activity.setId(10001L);            activity.setName(&quot;图书嗨乐&quot;);            activity.setDesc(&quot;图书优惠券分享激励分享活动第二期&quot;);            activity.setStartTime(new Date());            activity.setStopTime(new Date());            activityMap.put(id, activity);        }        return activity;    }}</code></pre><p>这里提供的是一个享元工厂🏭，通过map结构存放已经从库表或者接口中查询到的数据，存放到内存中，用于下次可以直接获取。</p><h4 id="模拟Redis类"><a href="#模拟Redis类" class="headerlink" title="模拟Redis类"></a>模拟Redis类</h4><pre><code>public class RedisUtils {    private ScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(1);    private AtomicInteger stock = new AtomicInteger(0);    public RedisUtils() {        scheduledExecutorService.scheduleAtFixedRate(() -&gt; {            // 模拟库存消耗            stock.addAndGet(1);        }, 0, 100000, TimeUnit.MICROSECONDS);    }    public int getStockUsed() {        return stock.get();    }}</code></pre><p>这里处理模拟redis的操作工具类外，还提供了一个定时任务用于模拟库存的使用，这样方面我们在测试的时候可以观察到库存的变化。</p><h4 id="活动控制类"><a href="#活动控制类" class="headerlink" title="活动控制类"></a>活动控制类</h4><pre><code>public class ActivityController {    private RedisUtils redisUtils = new RedisUtils();    public Activity queryActivityInfo(Long id) {        Activity activity = ActivityFactory.getActivity(id);        // 模拟从Redis中获取库存变化信息        Stock stock = new Stock(1000, redisUtils.getStockUsed());        activity.setStock(stock);        return activity;    }}</code></pre><ul><li>在活动控制类中使用了享元工厂获取活动信息，查询后将库存信息在补充上。因为库存信息是变化的，而活动信息是固定不变的。</li><li>在活动控制类中使用了享元工厂获取活动信息，查询后将库存信息在补充上。因为库存信息是变化的，而活动信息是固定不变的。</li></ul><h3 id="测试验证"><a href="#测试验证" class="headerlink" title="测试验证"></a>测试验证</h3><pre><code>public class ApiTest {    private Logger logger = LoggerFactory.getLogger(ApiTest.class);    private ActivityController activityController = new ActivityController();    @Test    public void test_queryActivityInfo() throws InterruptedException {        for (int idx = 0; idx &lt; 10; idx++) {            Long req = 10001L;            Activity activity = activityController.queryActivityInfo(req);            logger.info(&quot;测试结果：{} {}&quot;, req, JSON.toJSONString(activity));            Thread.sleep(1200);        }    }}</code></pre><h2 id="简单例子-1"><a href="#简单例子-1" class="headerlink" title="简单例子"></a>简单例子</h2><p>只有年龄变化</p><pre><code>public class UserInfo {    private Long id;    private String name;    private int age;}</code></pre><pre><code>public class UserFactory {    static Map&lt;Long, UserInfo&gt; map = new HashMap&lt;&gt;();    public static UserInfo getUserInfo(Long id) {        if (Objects.isNull(map.get(id))) {            // 理论上这里应该查询数据库            System.out.println(&quot;==========&quot;);            UserInfo userInfo = new UserInfo();            userInfo.setId(id);            userInfo.setName(&quot;aaa&quot;);            map.put(id, userInfo);            return userInfo;        }        return map.get(id);    }}</code></pre><pre><code>public class Test {    public static void main(String[] args) {        for (int i = 0; i &lt; 10; i++) {            UserInfo userInfo = UserFactory.getUserInfo(10L);            userInfo.setAge(new Random().nextInt(100));            System.out.println(userInfo);        }    }}</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>关于享元模式的设计可以着重学习享元工厂的设计，在一些有大量重复对象可复用的场景下，使用此场景在服务端减少接口的调用，在客户端减少内存的占用。是这个设计模式的主要应用方式。</li><li>另外通过map结构的使用方式也可以看到，使用一个固定id来存放和获取对象，是非常关键的点。而且不只是在享元模式中使用，一些其他工厂模式、适配器模式、组合模式中都可以通过map结构存放服务供外部获取，减少ifelse的判断使用。</li><li>当然除了这种设计的减少内存的使用优点外，也有它带来的缺点，在一些复杂的业务处理场景，很不容易区分出内部和外部状态，就像我们活动信息部分与库存变化部分。如果不能很好的拆分，就会把享元工厂设计的非常混乱，难以维护。</li><li>享元模式，主要在于共享通用对象，减少内存的使用，提升系统的访问效率。而这部分共享对象通常比较耗费内存或者需要查询大量接口或者使用数据库资源，因此统一抽离作为共享对象使用。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>外观模式</title>
      <link href="/2021/04/10/2021-04-10-design-wai-guan/"/>
      <url>/2021/04/10/2021-04-10-design-wai-guan/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="外观模式介绍"><a href="#外观模式介绍" class="headerlink" title="外观模式介绍"></a>外观模式介绍</h2><p>外观模式也叫门面模式，主要解决的是降低调用方的使用接口的复杂逻辑组合。这样调用方与实际的接口提供方提供方提供了一个中间层，用于包装逻辑提供API接口。有些时候外观模式也被用在中间件层，对服务中的通用性复杂逻辑进行中间件层包装，让使用方可以只关心业务开发。</p><p>那么这样的模式在我们的所见产品功能中也经常遇到，就像几年前我们注册一个网站时候往往要添加很多信息，包括；姓名、昵称、手机号、QQ、邮箱、住址、单身等等，但现在注册成为一个网站的用户只需要一步即可，无论是手机号还是微信也都提供了这样的登录服务。而对于服务端应用开发来说以前是提供了一个整套的接口，现在注册的时候并没有这些信息，那么服务端就需要进行接口包装，在前端调用注册的时候服务端获取相应的用户信息(从各个渠道)，如果获取不到会让用户后续进行补全(营销补全信息给奖励)，以此来拉动用户的注册量和活跃度。</p><h2 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h2><p><img src="/2021/04/10/2021-04-10-design-wai-guan/itstack-demo-design-10-02.png" alt></p><p>在本案例中我们模拟一个将所有服务接口添加白名单的场景</p><h3 id="场景模拟工程"><a href="#场景模拟工程" class="headerlink" title="场景模拟工程"></a>场景模拟工程</h3><pre><code>itstack-demo-design-10-00└── src    ├── main    │   ├── java    │   │   └── org.itstack.demo.design    │   │       ├── domain    │   │       │    └── UserInfo.java    │   │       ├── web        │   │       │    └── HelloWorldController.java    │   │       └── HelloWorldApplication.java    │   └── resources        │       └── application.yml        └── test        └── java            └── org.itstack.demo.test                └── ApiTest.java</code></pre><h3 id="场景简述"><a href="#场景简述" class="headerlink" title="场景简述"></a>场景简述</h3><h4 id="定义基础查询接口"><a href="#定义基础查询接口" class="headerlink" title="定义基础查询接口"></a>定义基础查询接口</h4><pre><code>@RestControllerpublic class HelloWorldController {    @Value(&quot;${server.port}&quot;)    private int port;    /**     * key：需要从入参取值的属性字段，如果是对象则从对象中取值，如果是单个值则直接使用     * returnJson：预设拦截时返回值，是返回对象的Json     *     * http://localhost:8080/api/queryUserInfo?userId=1001     * http://localhost:8080/api/queryUserInfo?userId=小团团     */    @RequestMapping(path = &quot;/api/queryUserInfo&quot;, method = RequestMethod.GET)    public UserInfo queryUserInfo(@RequestParam String userId) {        return new UserInfo(&quot;虫虫:&quot; + userId, 19, &quot;天津市南开区旮旯胡同100号&quot;);    }}</code></pre><p>这里提供了一个基本的查询服务，通过入参userId，查询用户信息。后续就需要在这里扩展白名单，只有指定用户才可以查询，其他用户不能查询。</p><h4 id="设置Application启动类"><a href="#设置Application启动类" class="headerlink" title="设置Application启动类"></a>设置Application启动类</h4><pre><code>@SpringBootApplication@Configurationpublic class HelloWorldApplication {    public static void main(String[] args) {        SpringApplication.run(HelloWorldApplication.class, args);    }}</code></pre><h2 id="用一坨坨代码实现"><a href="#用一坨坨代码实现" class="headerlink" title="用一坨坨代码实现"></a>用一坨坨代码实现</h2><pre><code>public class HelloWorldController {    public UserInfo queryUserInfo(@RequestParam String userId) {        // 做白名单拦截        List&lt;String&gt; userList = new ArrayList&lt;String&gt;();        userList.add(&quot;1001&quot;);        userList.add(&quot;aaaa&quot;);        userList.add(&quot;ccc&quot;);        if (!userList.contains(userId)) {            return new UserInfo(&quot;1111&quot;, &quot;非白名单可访问用户拦截！&quot;);        }        return new UserInfo(&quot;虫虫:&quot; + userId, 19, &quot;天津市南开区旮旯胡同100号&quot;);    }}</code></pre><h2 id="外观模式重构代码"><a href="#外观模式重构代码" class="headerlink" title="外观模式重构代码"></a>外观模式重构代码</h2><p>这次重构的核心是使用外观模式也可以说门面模式，结合SpringBoot中的自定义starter中间件开发的方式，统一处理所有需要白名单的地方。</p><p>后续接下来的实现中，会涉及的知识:</p><ul><li>SpringBoot的starter中间件开发方式。</li><li>面向切面编程和自定义注解的使用。</li><li>外部自定义配置信息的透传，SpringBoot与Spring不同，对于此类方式获取白名单配置存在差异。</li></ul><h3 id="工程结构"><a href="#工程结构" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-10-02└── src    ├── main    │   ├── java    │   │   └── org.itstack.demo.design.door    │   │       ├── annotation    │   │       │    └── DoDoor.java        │   │       ├── config    │   │       │    ├── StarterAutoConfigure.java    │   │       │    ├── StarterService.java    │   │       │    └── StarterServiceProperties.java    │   │       └── DoJoinPoint.java    │   └── resources        │       └── META_INF    │           └── spring.factories    └── test        └── java            └── org.itstack.demo.test                └── ApiTest.java</code></pre><p>门面模式模型结构</p><p><img src="/2021/04/10/2021-04-10-design-wai-guan/itstack-demo-design-10-03.png" alt></p><ul><li>以上是外观模式的中间件实现思路，右侧是为了获取配置文件，左侧是对于切面的处理。</li><li>门面模式可以是对接口的包装提供出接口服务，也可以是对逻辑的包装通过自定义注解对接口提供服务能力。</li></ul><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="配置服务类"><a href="#配置服务类" class="headerlink" title="配置服务类"></a>配置服务类</h4><pre><code>public class StarterService {    private String userStr;    public StarterService(String userStr) {        this.userStr = userStr;    }    public String[] split(String separatorChar) {        return StringUtils.split(this.userStr, separatorChar);    }}</code></pre><h4 id="配置类注解定义"><a href="#配置类注解定义" class="headerlink" title="配置类注解定义"></a>配置类注解定义</h4><pre><code>@ConfigurationProperties(&quot;itstack.door&quot;)public class StarterServiceProperties {    private String userStr;    public String getUserStr() {        return userStr;    }    public void setUserStr(String userStr) {        this.userStr = userStr;    }}</code></pre><p>用于定义好后续在 application.yml 中添加 itstack.door 的配置信息。</p><h4 id="自定义配置类信息获取"><a href="#自定义配置类信息获取" class="headerlink" title="自定义配置类信息获取"></a>自定义配置类信息获取</h4><pre><code>@Configuration@ConditionalOnClass(StarterService.class)@EnableConfigurationProperties(StarterServiceProperties.class)public class StarterAutoConfigure {    @Autowired    private StarterServiceProperties properties;    @Bean    @ConditionalOnMissingBean    @ConditionalOnProperty(prefix = &quot;itstack.door&quot;, value = &quot;enabled&quot;, havingValue = &quot;true&quot;)    StarterService starterService() {        return new StarterService(properties.getUserStr());    }}</code></pre><p>以上代码是对配置的获取操作，主要是对注解的定义；@Configuration、@ConditionalOnClass、@EnableConfigurationProperties，这一部分主要是与SpringBoot的结合使用。</p><h4 id="切面注解定义"><a href="#切面注解定义" class="headerlink" title="切面注解定义"></a>切面注解定义</h4><pre><code>@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface DoDoor {    String key() default &quot;&quot;;    String returnJson() default &quot;&quot;;}</code></pre><ul><li>定义了外观模式门面注解，后续就是此注解添加到需要扩展白名单的方法上。</li><li>这里提供了两个入参，key：获取某个字段例如用户ID、returnJson：确定白名单拦截后返回的具体内容。</li></ul><h4 id="白名单切面逻辑"><a href="#白名单切面逻辑" class="headerlink" title="白名单切面逻辑"></a>白名单切面逻辑</h4><pre><code>@Aspect@Componentpublic class DoJoinPoint {    private Logger logger = LoggerFactory.getLogger(DoJoinPoint.class);    @Autowired    private StarterService starterService;    @Pointcut(&quot;@annotation(org.itstack.demo.design.door.annotation.DoDoor)&quot;)    public void aopPoint() {    }    @Around(&quot;aopPoint()&quot;)    public Object doRouter(ProceedingJoinPoint jp) throws Throwable {        //获取内容        Method method = getMethod(jp);        DoDoor door = method.getAnnotation(DoDoor.class);        //获取字段值        String keyValue = getFiledValue(door.key(), jp.getArgs());        logger.info(&quot;itstack door handler method：{} value：{}&quot;, method.getName(), keyValue);        if (null == keyValue || &quot;&quot;.equals(keyValue)) return jp.proceed();        //配置内容        String[] split = starterService.split(&quot;,&quot;);        //白名单过滤        for (String str : split) {            if (keyValue.equals(str)) {                return jp.proceed();            }        }        //拦截        return returnObject(door, method);    }    private Method getMethod(JoinPoint jp) throws NoSuchMethodException {        Signature sig = jp.getSignature();        MethodSignature methodSignature = (MethodSignature) sig;        return getClass(jp).getMethod(methodSignature.getName(), methodSignature.getParameterTypes());    }    private Class&lt;? extends Object&gt; getClass(JoinPoint jp) throws NoSuchMethodException {        return jp.getTarget().getClass();    }    //返回对象    private Object returnObject(DoDoor doGate, Method method) throws IllegalAccessException, InstantiationException {        Class&lt;?&gt; returnType = method.getReturnType();        String returnJson = doGate.returnJson();        if (&quot;&quot;.equals(returnJson)) {            return returnType.newInstance();        }        return JSON.parseObject(returnJson, returnType);    }    //获取属性值    private String getFiledValue(String filed, Object[] args) {        String filedValue = null;        for (Object arg : args) {            try {                if (null == filedValue || &quot;&quot;.equals(filedValue)) {                    filedValue = BeanUtils.getProperty(arg, filed);                } else {                    break;                }            } catch (Exception e) {                if (args.length == 1) {                    return args[0].toString();                }            }        }        return filedValue;    }}</code></pre><p>这里包括的内容较多，核心逻辑主要是；Object doRouter(ProceedingJoinPoint jp)，接下来我们分别介绍:</p><ul><li><p>@Pointcut(“@annotation(org.itstack.demo.design.door.annotation.DoDoor)”)</p><p>  定义切面，这里采用的是注解路径，也就是所有的加入这个注解的方法都会被切面进行管理。</p></li><li><p>getFiledValue</p><p>  获取指定key也就是获取入参中的某个属性，这里主要是获取用户ID，通过ID进行拦截校验。</p></li><li><p>returnObject</p><p>  返回拦截后的转换对象，也就是说当非白名单用户访问时则返回一些提示信息。</p></li><li><p>doRouter</p><p>  切面核心逻辑，这一部分主要是判断当前访问的用户ID是否白名单用户，如果是则放行jp.proceed();，否则返回自定义的拦截提示信息。</p></li></ul><h3 id="测试验证"><a href="#测试验证" class="headerlink" title="测试验证"></a>测试验证</h3><h4 id="引入中间件POM配置"><a href="#引入中间件POM配置" class="headerlink" title="引入中间件POM配置"></a>引入中间件POM配置</h4><pre><code>&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;itstack-demo-design-10-02&lt;/artifactId&gt;&lt;/dependency&gt;</code></pre><h4 id="配置application-yml"><a href="#配置application-yml" class="headerlink" title="配置application.yml"></a>配置application.yml</h4><pre><code># 自定义中间件配置itstack:  door:    enabled: true    userStr: 1001,aaaa,ccc #白名单用户ID，多个逗号隔开</code></pre><h4 id="在Controller中添加自定义注解"><a href="#在Controller中添加自定义注解" class="headerlink" title="在Controller中添加自定义注解"></a>在Controller中添加自定义注解</h4><pre><code>/** * http://localhost:8080/api/queryUserInfo?userId=1001 * http://localhost:8080/api/queryUserInfo?userId=小团团 */@DoDoor(key = &quot;userId&quot;, returnJson = &quot;{\&quot;code\&quot;:\&quot;1111\&quot;,\&quot;info\&quot;:\&quot;非白名单可访问用户拦截！\&quot;}&quot;)@RequestMapping(path = &quot;/api/queryUserInfo&quot;, method = RequestMethod.GET)public UserInfo queryUserInfo(@RequestParam String userId) {    return new UserInfo(&quot;虫虫:&quot; + userId, 19, &quot;天津市南开区旮旯胡同100号&quot;);}</code></pre><ul><li>这里核心的内容主要是自定义的注解的添加@DoDoor，也就是我们的外观模式中间件化实现。</li><li>key：需要从入参取值的属性字段，如果是对象则从对象中取值，如果是单个值则直接使用。</li><li>returnJson：预设拦截时返回值，是返回对象的Json。</li></ul><h2 id="简单切面注解例子"><a href="#简单切面注解例子" class="headerlink" title="简单切面注解例子"></a>简单切面注解例子</h2><pre><code>import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface Log {}</code></pre><pre><code>@Aspect@Componentpublic class LogJoinPoint {    private Logger logger = LoggerFactory.getLogger(LogJoinPoint.class);    @Pointcut(&quot;@annotation(design.outward.annotation.Log)&quot;)    public void aopPoint() {    }    @Around(&quot;aopPoint()&quot;)    public Object doRouter(ProceedingJoinPoint jp) throws Throwable {        //获取内容        Method method = getMethod(jp);        Log log = method.getAnnotation(Log.class);        String simpleName = getClass(jp).getSimpleName();        JSONObject fieldValue = getFieldValue(method, jp);        logger.info(&quot;当前时间[{}]，调用[{}]类的[{}]方法，参数为：[{}]&quot;, DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;).format(LocalDateTime.now()), simpleName, method.getName(), fieldValue.toJSONString());        return jp.proceed();    }    private JSONObject getFieldValue(Method method, ProceedingJoinPoint jp) {        JSONObject filedValue = new JSONObject();        Parameter[] parameters = method.getParameters();        for (int i = 0; i &lt; parameters.length; i++) {            filedValue.put(parameters[i].getName(), jp.getArgs()[i]);        }        return filedValue;    }    private Method getMethod(JoinPoint jp) throws NoSuchMethodException {        Signature sig = jp.getSignature();        MethodSignature methodSignature = (MethodSignature) sig;        return getClass(jp).getMethod(methodSignature.getName(), methodSignature.getParameterTypes());    }    private Class&lt;? extends Object&gt; getClass(JoinPoint jp) throws NoSuchMethodException {        return jp.getTarget().getClass();    }}</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>以上我们通过中间件的方式实现外观模式，这样的设计可以很好的增强代码的隔离性，以及复用性，不仅使用上非常灵活也降低了每一个系统都开发这样的服务带来的风险。</li><li>外观模式也叫门面模式，主要解决的是降低调用方的使用接口的复杂逻辑组合。这样调用方与实际的接口提供方提供方提供了一个中间层，用于包装逻辑提供API接口。有些时候外观模式也被用在中间件层，对服务中的通用性复杂逻辑进行中间件层包装，让使用方可以只关心业务开发。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>装饰器模式</title>
      <link href="/2021/04/09/2021-04-09-design-zhuang-shi-qi/"/>
      <url>/2021/04/09/2021-04-09-design-zhuang-shi-qi/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="装饰器模式介绍"><a href="#装饰器模式介绍" class="headerlink" title="装饰器模式介绍"></a>装饰器模式介绍</h2><p>装饰器的核心就是在不改原有类的基础上给类新增功能。不改变原有类，可能会想到继承、AOP切面，当然这些方式都可以实现，但是使用装饰器模式会是另外一种思路更为灵活，可以避免继承导致的子类过多，也可以避免AOP带来的复杂性。、</p><p>new BufferedReader(new FileReader(“”));，这段代码你是否熟悉，开发到字节流、字符流、文件流的内容时都见到了这样的代码，一层嵌套一层，一层嵌套一层，字节流转字符流等等，而这样方式的使用就是装饰器模式的一种体现。</p><h2 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h2><p><img src="/2021/04/09/2021-04-09-design-zhuang-shi-qi/itstack-demo-design-9-02.png" alt></p><p>在本案例中我们模拟一个单点登录功能扩充的场景</p><p>一般在业务开发的初期，往往内部的ERP使用只需要判断账户验证即可，验证通过后即可访问ERP的所有资源。但随着业务的不断发展，团队里开始出现专门的运营人员、营销人员、数据人员，每个人员对于ERP的使用需求不同，有些需要创建活动，有些只是查看数据。同时为了保证数据的安全性，不会让每个用户都有最高的权限。</p><p>那么以往使用的SSO是一个组件化通用的服务，不能在里面添加需要的用户访问验证功能。这个时候我们就可以使用装饰器模式，扩充原有的单点登录服务。但同时也保证原有功能不受破坏，可以继续使用。</p><h3 id="场景模拟工程"><a href="#场景模拟工程" class="headerlink" title="场景模拟工程"></a>场景模拟工程</h3><pre><code>itstack-demo-design-9-00└── src    └── main        └── java            └── org.itstack.demo.design                ├── HandlerInterceptor.java                └── SsoInterceptor.java</code></pre><p>这里模拟的是spring中的类：HandlerInterceptor，实现接口功能SsoInterceptor模拟的单点登录拦截服务。</p><h3 id="场景简述"><a href="#场景简述" class="headerlink" title="场景简述"></a>场景简述</h3><h4 id="模拟Spring的HandlerInterceptor"><a href="#模拟Spring的HandlerInterceptor" class="headerlink" title="模拟Spring的HandlerInterceptor"></a>模拟Spring的HandlerInterceptor</h4><pre><code>public interface HandlerInterceptor {    boolean preHandle(String request, String response, Object handler);}</code></pre><p>实际的单点登录开发会基于；org.springframework.web.servlet.HandlerInterceptor 实现。</p><h4 id="模拟单点登录功能"><a href="#模拟单点登录功能" class="headerlink" title="模拟单点登录功能"></a>模拟单点登录功能</h4><pre><code>public class SsoInterceptor implements HandlerInterceptor{    public boolean preHandle(String request, String response, Object handler) {        // 模拟获取cookie        String ticket = request.substring(1, 8);        // 模拟校验        return ticket.equals(&quot;success&quot;);    }}</code></pre><h2 id="用一坨坨代码实现"><a href="#用一坨坨代码实现" class="headerlink" title="用一坨坨代码实现"></a>用一坨坨代码实现</h2><p>继承类的实现方式也是一个比较通用的方式，通过继承后重写方法，并发将自己的逻辑覆盖进去。如果是一些简单的场景且不需要不断维护和扩展的，此类实现并不会有什么，也不会导致子类过多。</p><h3 id="工程结构"><a href="#工程结构" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-9-01└── src    └── main        └── java            └── org.itstack.demo.design                └── LoginSsoDecorator.java</code></pre><p>以上工程结构非常简单，只是通过 LoginSsoDecorator 继承 SsoInterceptor，重写方法功能。</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><pre><code>public class LoginSsoDecorator extends SsoInterceptor {    private static Map&lt;String, String&gt; authMap = new ConcurrentHashMap&lt;String, String&gt;();    static {        authMap.put(&quot;huahua&quot;, &quot;queryUserInfo&quot;);        authMap.put(&quot;doudou&quot;, &quot;queryUserInfo&quot;);    }    @Override    public boolean preHandle(String request, String response, Object handler) {        // 模拟获取cookie        String ticket = request.substring(1, 8);        // 模拟校验        boolean success = ticket.equals(&quot;success&quot;);        if (!success) return false;        String userId = request.substring(9);        String method = authMap.get(userId);        // 模拟方法校验        return &quot;queryUserInfo&quot;.equals(method);    }}</code></pre><p>以上这部分通过继承重写方法，将个人可访问哪些方法的功能添加到方法中。</p><h3 id="测试验证"><a href="#测试验证" class="headerlink" title="测试验证"></a>测试验证</h3><pre><code>@Testpublic void test_LoginSsoDecorator() {    LoginSsoDecorator ssoDecorator = new LoginSsoDecorator();    String request = &quot;1successhuahua&quot;;    boolean success = ssoDecorator.preHandle(request, &quot;ewcdqwt40liuiu&quot;, &quot;t&quot;);    System.out.println(&quot;登录校验：&quot; + request + (success ? &quot; 放行&quot; : &quot; 拦截&quot;));}</code></pre><h2 id="装饰器模式重构代码"><a href="#装饰器模式重构代码" class="headerlink" title="装饰器模式重构代码"></a>装饰器模式重构代码</h2><p>装饰器主要解决的是直接继承下因功能的不断横向扩展导致子类膨胀的问题，而是用装饰器模式后就会比直接继承显得更加灵活同时这样也就不再需要考虑子类的维护。</p><p>在装饰器模式中有四个比较重要点抽象出来的点；</p><ul><li>抽象构件角色(Component) - 定义抽象接口</li><li>具体构件角色(ConcreteComponent) - 实现抽象接口，可以是一组</li><li>装饰角色(Decorator) - 定义抽象类并继承接口中的方法，保证一致性</li><li>具体装饰角色(ConcreteDecorator) - 扩展装饰具体的实现逻辑</li></ul><p>通过以上这四项来实现装饰器模式，主要核心内容会体现在抽象类的定义和实现上。</p><h3 id="工程结构-1"><a href="#工程结构-1" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-9-02└── src    └── main        └── java            └── org.itstack.demo.design                ├── LoginSsoDecorator.java                └── SsoDecorator.java</code></pre><p>装饰器模式模型结构</p><p><img src="/2021/04/09/2021-04-09-design-zhuang-shi-qi/itstack-demo-design-9-03.png" alt></p><p>以上是一个装饰器实现的类图结构，重点的类是SsoDecorator，这个类是一个抽象类主要完成了对接口HandlerInterceptor继承。</p><p>当装饰角色继承接口后会提供构造函数，入参就是继承的接口实现类即可，这样就可以很方便的扩展出不同功能组件。</p><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="抽象类装饰角色"><a href="#抽象类装饰角色" class="headerlink" title="抽象类装饰角色"></a>抽象类装饰角色</h4><pre><code>public abstract class SsoDecorator implements HandlerInterceptor {    private HandlerInterceptor handlerInterceptor;    private SsoDecorator(){}    public SsoDecorator(HandlerInterceptor handlerInterceptor) {        this.handlerInterceptor = handlerInterceptor;    }    public boolean preHandle(String request, String response, Object handler) {        return handlerInterceptor.preHandle(request, response, handler);    }}</code></pre><p>在装饰类中有两个重点的地方是；1)继承了处理接口、2)提供了构造函数、3)覆盖了方法preHandle。</p><p>以上三个点是装饰器模式的核心处理部分，这样可以踢掉对子类继承的方式实现逻辑功能扩展。</p><h4 id="装饰角色逻辑实现"><a href="#装饰角色逻辑实现" class="headerlink" title="装饰角色逻辑实现"></a>装饰角色逻辑实现</h4><pre><code>public class LoginSsoDecorator extends SsoDecorator {    private Logger logger = LoggerFactory.getLogger(LoginSsoDecorator.class);    private static Map&lt;String, String&gt; authMap = new ConcurrentHashMap&lt;String, String&gt;();    static {        authMap.put(&quot;huahua&quot;, &quot;queryUserInfo&quot;);        authMap.put(&quot;doudou&quot;, &quot;queryUserInfo&quot;);    }    public LoginSsoDecorator(HandlerInterceptor handlerInterceptor) {        super(handlerInterceptor);    }    @Override    public boolean preHandle(String request, String response, Object handler) {        boolean success = super.preHandle(request, response, handler);        if (!success) return false;        String userId = request.substring(8);        String method = authMap.get(userId);        logger.info(&quot;模拟单点登录方法访问拦截校验：{} {}&quot;, userId, method);        // 模拟方法校验        return &quot;queryUserInfo&quot;.equals(method);    }}</code></pre><p>在具体的装饰类实现中，继承了装饰类SsoDecorator，那么现在就可以扩展方法；preHandle</p><p>在preHandle的实现中可以看到，这里只关心扩展部分的功能，同时不会影响原有类的核心服务，也不会因为使用继承方式而导致的多余子类，增加了整体的灵活性。</p><h3 id="测试验证-1"><a href="#测试验证-1" class="headerlink" title="测试验证"></a>测试验证</h3><pre><code>@Testpublic void test_LoginSsoDecorator() {    LoginSsoDecorator ssoDecorator = new LoginSsoDecorator(new SsoInterceptor());    String request = &quot;1successhuahua&quot;;    boolean success = ssoDecorator.preHandle(request, &quot;ewcdqwt40liuiu&quot;, &quot;t&quot;);    System.out.println(&quot;登录校验：&quot; + request + (success ? &quot; 放行&quot; : &quot; 拦截&quot;));}</code></pre><p>这里测试了对装饰器模式的使用，通过透传原有单点登录类new SsoInterceptor()，传递给装饰器，让装饰器可以执行扩充的功能。</p><p>同时对于传递者和装饰器都可以是多组的，在一些实际的业务开发中，往往也是由于太多类型的子类实现而导致不易于维护，从而使用装饰器模式替代。</p><h2 id="简单例子-1"><a href="#简单例子-1" class="headerlink" title="简单例子"></a>简单例子</h2><h3 id="结构图"><a href="#结构图" class="headerlink" title="结构图"></a>结构图</h3><p><img src="/2021/04/09/2021-04-09-design-zhuang-shi-qi/Image1.png" alt></p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><pre><code>public interface Verify {    /**     * 验证接口     */    boolean verify(String userId, String code, LocalDate localDate);}</code></pre><pre><code>public class BaseVerifyImpl implements Verify{    /**     * admin验证     */    @Override    public boolean verify(String userId, String code, LocalDate localDate) {        return &quot;admin&quot;.equals(userId) || &quot;add&quot;.equals(code) ? true : false;    }}</code></pre><p>重点看Decorator类和Expand类</p><pre><code>public abstract class Decorator implements Verify {    private Verify verify;    public Decorator(Verify verify) {        this.verify = verify;    }    @Override    public boolean verify(String userId, String code, LocalDate localDate) {        return verify.verify(userId, code, localDate);    }}</code></pre><pre><code>public class Expand extends Decorator{    public Expand(Verify verify) {        super(verify);    }    /**     * admin检验,并且时间校验     */    @Override    public boolean verify(String userId, String code, LocalDate localDate) {        if (super.verify(userId, code, localDate)) {            return !LocalDate.now().isBefore(localDate) ? true : false;        }        return false;    }}</code></pre><pre><code>public class Test {    public static void main(String[] args) {        Expand expand = new Expand(new BaseVerifyImpl());        System.out.println(expand.verify(&quot;admin&quot;, &quot;add&quot;, LocalDate.now()));    }}</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li><p>使用装饰器模式满足单一职责原则，你可以在自己的装饰类中完成功能逻辑的扩展，而不影响主类，同时可以按需在运行时添加和删除这部分逻辑。另外装饰器模式与继承父类重写方法，在某些时候需要按需选择，并不一定某一个就是最好。</p></li><li><p>装饰器实现的重点是对抽象类继承接口方式的使用，同时设定被继承的接口可以通过构造函数传递其实现类，由此增加扩展性并重写方法里可以实现此部分父类实现的功能。</p></li><li><p>就像夏天热你穿短裤，冬天冷你穿棉裤，雨天挨浇你穿雨衣一样，你的根本本身没有被改变，而你的需求却被不同的装饰而实现。</p></li><li><p>装饰器的核心就是在不改原有类的基础上给类新增功能。例如new BufferedReader(new FileReader(“”))。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>组合模式</title>
      <link href="/2021/04/08/2021-04-08-design-zu-he/"/>
      <url>/2021/04/08/2021-04-08-design-zu-he/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="组合模式介绍"><a href="#组合模式介绍" class="headerlink" title="组合模式介绍"></a>组合模式介绍</h2><p><img src="/2021/04/08/2021-04-08-design-zu-he/itstack-demo-design-8-01.png" alt></p><p>从上图可以看到这有点像螺丝🔩和螺母，通过一堆的链接组织出一棵结构树。而这种通过把相似对象(也可以称作是方法)组合成一组可被调用的结构树对象的设计思路叫做组合模式。组合模式也就是把相似对象(也可以称作是方法)组合成一组可被调用的结构树对象。</p><p>这种设计方式可以让你的服务组节点进行自由组合对外提供服务，例如你有三个原子校验功能(A：身份证、B：银行卡、C：手机号)服务并对外提供调用使用。有些调用方需要使用AB组合，有些调用方需要使用到CBA组合，还有一些可能只使用三者中的一个。那么这个时候你就可以使用组合模式进行构建服务，对于不同类型的调用方配置不同的组织关系树，而这个树结构你可以配置到数据库中也可以不断的通过图形界面来控制树结构。</p><p>所以不同的设计模式用在恰当好处的场景可以让代码逻辑非常清晰并易于扩展，同时也可以减少团队新增人员对项目的学习成本。</p><h2 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h2><p><img src="/2021/04/08/2021-04-08-design-zu-he/itstack-demo-design-8-02.png" alt></p><p>以上是一个非常简化版的营销规则决策树，根据性别、年龄来发放不同类型的优惠券，来刺激消费起到精准用户促活的目的。</p><h2 id="用一坨坨代码实现"><a href="#用一坨坨代码实现" class="headerlink" title="用一坨坨代码实现"></a>用一坨坨代码实现</h2><h3 id="工程结构"><a href="#工程结构" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-8-01└── src    └── main        └── java            └── org.itstack.demo.design                └── EngineController.java</code></pre><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><pre><code>public class EngineController {    private Logger logger = LoggerFactory.getLogger(EngineController.class);    public String process(final String userId, final String userSex, final int userAge) {        logger.info(&quot;ifelse实现方式判断用户结果。userId：{} userSex：{} userAge：{}&quot;, userId, userSex, userAge);        if (&quot;man&quot;.equals(userSex)) {            if (userAge &lt; 25) {                return &quot;果实A&quot;;            }            if (userAge &gt;= 25) {                return &quot;果实B&quot;;            }        }        if (&quot;woman&quot;.equals(userSex)) {            if (userAge &lt; 25) {                return &quot;果实C&quot;;            }            if (userAge &gt;= 25) {                return &quot;果实D&quot;;            }        }        return null;    }}</code></pre><h2 id="组合模式重构代码"><a href="#组合模式重构代码" class="headerlink" title="组合模式重构代码"></a>组合模式重构代码</h2><p>接下来的重构部分代码改动量相对来说会比较大一些，为了让我们可以把不同类型的决策节点和最终的果实组装成一棵可被运行的决策树，需要做适配设计和工厂方法调用，具体会体现在定义接口以及抽象类和初始化配置决策节点(性别、年龄)上。建议这部分代码多阅读几次，最好实践下。</p><h3 id="工程结构-1"><a href="#工程结构-1" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-8-02└── src    ├── main    │   └── java    │      └── org.itstack.demo.design.domain    │          ├── model    │          │   ├── aggregates    │          │   │   └── TreeRich.java    │          │   └── vo    │          │       ├── EngineResult.java    │          │       ├── TreeNode.java    │          │       ├── TreeNodeLink.java        │          │       └── TreeRoot.java        │          └── service    │              ├── engine    │              │   ├── impl        │              │   │   └── TreeEngineHandle.java           │              │   ├── EngineBase.java     │              │   ├── EngineConfig.java           │              │   └── IEngine.java        │              └── logic    │                  ├── impl        │                  │   ├── LogicFilter.java         │                  │   └── LogicFilter.java            │                  └── LogicFilter.java        └── test         └── java             └── org.itstack.demo.design.test                 └── ApiTest.java</code></pre><p>组合模式模型结构</p><p><img src="/2021/04/08/2021-04-08-design-zu-he/itstack-demo-design-8-03.png" alt></p><p>LogicFilter开始定义适配的决策过滤器，BaseLogic是对接口的实现，提供最基本的通用方法。UserAgeFilter、UserGenerFilter，是两个具体的实现类用于判断年龄和性别。</p><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="基础对象"><a href="#基础对象" class="headerlink" title="基础对象"></a>基础对象</h4><p>![](Image 1.png)</p><p>以上这部分简单介绍，不包含逻辑只是各项必要属性的get/set</p><h4 id="树节点逻辑过滤器接口"><a href="#树节点逻辑过滤器接口" class="headerlink" title="树节点逻辑过滤器接口"></a>树节点逻辑过滤器接口</h4><pre><code>public interface LogicFilter {    /**     * 逻辑决策器     *     * @param matterValue          决策值     * @param treeNodeLineInfoList 决策节点     * @return 下一个节点Id     */    Long filter(String matterValue, List&lt;TreeNodeLink&gt; treeNodeLineInfoList);    /**     * 获取决策值     *     * @param decisionMatter 决策物料     * @return 决策值     */    String matterValue(Long treeId, String userId, Map&lt;String, String&gt; decisionMatter);}</code></pre><p>这一部分定义了适配的通用接口，逻辑决策器、获取决策值，让每一个提供决策能力的节点都必须实现此接口，保证统一性。</p><h4 id="决策抽象类提供基础服务"><a href="#决策抽象类提供基础服务" class="headerlink" title="决策抽象类提供基础服务"></a>决策抽象类提供基础服务</h4><pre><code>public abstract class BaseLogic implements LogicFilter {    @Override    public Long filter(String matterValue, List&lt;TreeNodeLink&gt; treeNodeLinkList) {        for (TreeNodeLink nodeLine : treeNodeLinkList) {            if (decisionLogic(matterValue, nodeLine)) return nodeLine.getNodeIdTo();        }        return 0L;    }    @Override    public abstract String matterValue(Long treeId, String userId, Map&lt;String, String&gt; decisionMatter);    private boolean decisionLogic(String matterValue, TreeNodeLink nodeLink) {        switch (nodeLink.getRuleLimitType()) {            case 1:                return matterValue.equals(nodeLink.getRuleLimitValue());            case 2:                return Double.parseDouble(matterValue) &gt; Double.parseDouble(nodeLink.getRuleLimitValue());            case 3:                return Double.parseDouble(matterValue) &lt; Double.parseDouble(nodeLink.getRuleLimitValue());            case 4:                return Double.parseDouble(matterValue) &lt;= Double.parseDouble(nodeLink.getRuleLimitValue());            case 5:                return Double.parseDouble(matterValue) &gt;= Double.parseDouble(nodeLink.getRuleLimitValue());            default:                return false;        }    }}</code></pre><p>在抽象方法中实现了接口方法，同时定义了基本的决策方法；1、2、3、4、5，等于、小于、大于、小于等于、大于等于的判断逻辑。</p><p>同时定义了抽象方法，让每一个实现接口的类都必须按照规则提供决策值，这个决策值用于做逻辑比对。</p><h4 id="树节点逻辑实现类"><a href="#树节点逻辑实现类" class="headerlink" title="树节点逻辑实现类"></a>树节点逻辑实现类</h4><p>年龄节点</p><pre><code>public class UserAgeFilter extends BaseLogic {    @Override    public String matterValue(Long treeId, String userId, Map&lt;String, String&gt; decisionMatter) {        return decisionMatter.get(&quot;age&quot;);    }}</code></pre><p>性别节点</p><pre><code>public class UserGenderFilter extends BaseLogic {    @Override    public String matterValue(Long treeId, String userId, Map&lt;String, String&gt; decisionMatter) {        return decisionMatter.get(&quot;gender&quot;);    }}</code></pre><p>以上两个决策逻辑的节点获取值的方式都非常简单，只是获取用户的入参即可。实际的业务开发可以从数据库、RPC接口、缓存运算等各种方式获取。</p><h4 id="决策引擎接口定义"><a href="#决策引擎接口定义" class="headerlink" title="决策引擎接口定义"></a>决策引擎接口定义</h4><pre><code>public interface IEngine {    EngineResult process(final Long treeId, final String userId, TreeRich treeRich, final Map&lt;String, String&gt; decisionMatter);}</code></pre><p>对于使用方来说也同样需要定义统一的接口操作，这样的好处非常方便后续拓展出不同类型的决策引擎，也就是可以建造不同的决策工厂。</p><h4 id="决策节点配置"><a href="#决策节点配置" class="headerlink" title="决策节点配置"></a>决策节点配置</h4><pre><code>public class EngineConfig {    static Map&lt;String, LogicFilter&gt; logicFilterMap;    static {        logicFilterMap = new ConcurrentHashMap&lt;&gt;();        logicFilterMap.put(&quot;userAge&quot;, new UserAgeFilter());        logicFilterMap.put(&quot;userGender&quot;, new UserGenderFilter());    }    public Map&lt;String, LogicFilter&gt; getLogicFilterMap() {        return logicFilterMap;    }    public void setLogicFilterMap(Map&lt;String, LogicFilter&gt; logicFilterMap) {        this.logicFilterMap = logicFilterMap;    }}</code></pre><p>在这里将可提供服务的决策节点配置到map结构中，对于这样的map结构可以抽取到数据库中，那么就可以非常方便的管理。</p><h4 id="基础决策引擎功能"><a href="#基础决策引擎功能" class="headerlink" title="基础决策引擎功能"></a>基础决策引擎功能</h4><pre><code>public abstract class EngineBase extends EngineConfig implements IEngine {    private Logger logger = LoggerFactory.getLogger(EngineBase.class);    @Override    public abstract EngineResult process(Long treeId, String userId, TreeRich treeRich, Map&lt;String, String&gt; decisionMatter);    protected TreeNode engineDecisionMaker(TreeRich treeRich, Long treeId, String userId, Map&lt;String, String&gt; decisionMatter) {        TreeRoot treeRoot = treeRich.getTreeRoot();        Map&lt;Long, TreeNode&gt; treeNodeMap = treeRich.getTreeNodeMap();        // 规则树根ID        Long rootNodeId = treeRoot.getTreeRootNodeId();        TreeNode treeNodeInfo = treeNodeMap.get(rootNodeId);        //节点类型[NodeType]；1子叶、2果实        while (treeNodeInfo.getNodeType().equals(1)) {            String ruleKey = treeNodeInfo.getRuleKey();            LogicFilter logicFilter = logicFilterMap.get(ruleKey);            String matterValue = logicFilter.matterValue(treeId, userId, decisionMatter);            Long nextNode = logicFilter.filter(matterValue, treeNodeInfo.getTreeNodeLinkList());            treeNodeInfo = treeNodeMap.get(nextNode);            logger.info(&quot;决策树引擎=&gt;{} userId：{} treeId：{} treeNode：{} ruleKey：{} matterValue：{}&quot;, treeRoot.getTreeName(), userId, treeId, treeNodeInfo.getTreeNodeId(), ruleKey, matterValue);        }        return treeNodeInfo;    }}</code></pre><p>这里主要提供决策树流程的处理过程，有点像通过链路的关系(性别、年龄)在二叉树中寻找果实节点的过程。</p><p>同时提供一个抽象方法，执行决策流程的方法供外部去做具体的实现。</p><h4 id="决策引擎的实现"><a href="#决策引擎的实现" class="headerlink" title="决策引擎的实现"></a>决策引擎的实现</h4><pre><code>public class TreeEngineHandle extends EngineBase {    @Override    public EngineResult process(Long treeId, String userId, TreeRich treeRich, Map&lt;String, String&gt; decisionMatter) {        // 决策流程        TreeNode treeNode = engineDecisionMaker(treeRich, treeId, userId, decisionMatter);        // 决策结果        return new EngineResult(userId, treeId, treeNode.getTreeNodeId(), treeNode.getNodeValue());    }}</code></pre><pre><code>这里对于决策引擎的实现就非常简单了，通过传递进来的必要信息；决策树信息、决策物料值，来做具体的树形结构决策。</code></pre><h3 id="测试验证"><a href="#测试验证" class="headerlink" title="测试验证"></a>测试验证</h3><h4 id="组装树关系"><a href="#组装树关系" class="headerlink" title="组装树关系"></a>组装树关系</h4><pre><code>@Beforepublic void init() {    // 节点：1    TreeNode treeNode_01 = new TreeNode();    treeNode_01.setTreeId(10001L);    treeNode_01.setTreeNodeId(1L);    treeNode_01.setNodeType(1);    treeNode_01.setNodeValue(null);    treeNode_01.setRuleKey(&quot;userGender&quot;);    treeNode_01.setRuleDesc(&quot;用户性别[男/女]&quot;);    // 链接：1-&gt;11    TreeNodeLink treeNodeLink_11 = new TreeNodeLink();    treeNodeLink_11.setNodeIdFrom(1L);    treeNodeLink_11.setNodeIdTo(11L);    treeNodeLink_11.setRuleLimitType(1);    treeNodeLink_11.setRuleLimitValue(&quot;man&quot;);    // 链接：1-&gt;12    TreeNodeLink treeNodeLink_12 = new TreeNodeLink();    treeNodeLink_12.setNodeIdTo(1L);    treeNodeLink_12.setNodeIdTo(12L);    treeNodeLink_12.setRuleLimitType(1);    treeNodeLink_12.setRuleLimitValue(&quot;woman&quot;);    List&lt;TreeNodeLink&gt; treeNodeLinkList_1 = new ArrayList&lt;&gt;();    treeNodeLinkList_1.add(treeNodeLink_11);    treeNodeLinkList_1.add(treeNodeLink_12);    treeNode_01.setTreeNodeLinkList(treeNodeLinkList_1);    // 节点：11    TreeNode treeNode_11 = new TreeNode();    treeNode_11.setTreeId(10001L);    treeNode_11.setTreeNodeId(11L);    treeNode_11.setNodeType(1);    treeNode_11.setNodeValue(null);    treeNode_11.setRuleKey(&quot;userAge&quot;);    treeNode_11.setRuleDesc(&quot;用户年龄&quot;);    // 链接：11-&gt;111    TreeNodeLink treeNodeLink_111 = new TreeNodeLink();    treeNodeLink_111.setNodeIdFrom(11L);    treeNodeLink_111.setNodeIdTo(111L);    treeNodeLink_111.setRuleLimitType(3);    treeNodeLink_111.setRuleLimitValue(&quot;25&quot;);    // 链接：11-&gt;112    TreeNodeLink treeNodeLink_112 = new TreeNodeLink();    treeNodeLink_112.setNodeIdFrom(11L);    treeNodeLink_112.setNodeIdTo(112L);    treeNodeLink_112.setRuleLimitType(5);    treeNodeLink_112.setRuleLimitValue(&quot;25&quot;);    List&lt;TreeNodeLink&gt; treeNodeLinkList_11 = new ArrayList&lt;&gt;();    treeNodeLinkList_11.add(treeNodeLink_111);    treeNodeLinkList_11.add(treeNodeLink_112);    treeNode_11.setTreeNodeLinkList(treeNodeLinkList_11);    // 节点：12    TreeNode treeNode_12 = new TreeNode();    treeNode_12.setTreeId(10001L);    treeNode_12.setTreeNodeId(12L);    treeNode_12.setNodeType(1);    treeNode_12.setNodeValue(null);    treeNode_12.setRuleKey(&quot;userAge&quot;);    treeNode_12.setRuleDesc(&quot;用户年龄&quot;);    // 链接：12-&gt;121    TreeNodeLink treeNodeLink_121 = new TreeNodeLink();    treeNodeLink_121.setNodeIdFrom(12L);    treeNodeLink_121.setNodeIdTo(121L);    treeNodeLink_121.setRuleLimitType(3);    treeNodeLink_121.setRuleLimitValue(&quot;25&quot;);    // 链接：12-&gt;122    TreeNodeLink treeNodeLink_122 = new TreeNodeLink();    treeNodeLink_122.setNodeIdFrom(12L);    treeNodeLink_122.setNodeIdTo(122L);    treeNodeLink_122.setRuleLimitType(5);    treeNodeLink_122.setRuleLimitValue(&quot;25&quot;);    List&lt;TreeNodeLink&gt; treeNodeLinkList_12 = new ArrayList&lt;&gt;();    treeNodeLinkList_12.add(treeNodeLink_121);    treeNodeLinkList_12.add(treeNodeLink_122);    treeNode_12.setTreeNodeLinkList(treeNodeLinkList_12);    // 节点：111    TreeNode treeNode_111 = new TreeNode();    treeNode_111.setTreeId(10001L);    treeNode_111.setTreeNodeId(111L);    treeNode_111.setNodeType(2);    treeNode_111.setNodeValue(&quot;果实A&quot;);    // 节点：112    TreeNode treeNode_112 = new TreeNode();    treeNode_112.setTreeId(10001L);    treeNode_112.setTreeNodeId(112L);    treeNode_112.setNodeType(2);    treeNode_112.setNodeValue(&quot;果实B&quot;);    // 节点：121    TreeNode treeNode_121 = new TreeNode();    treeNode_121.setTreeId(10001L);    treeNode_121.setTreeNodeId(121L);    treeNode_121.setNodeType(2);    treeNode_121.setNodeValue(&quot;果实C&quot;);    // 节点：122    TreeNode treeNode_122 = new TreeNode();    treeNode_122.setTreeId(10001L);    treeNode_122.setTreeNodeId(122L);    treeNode_122.setNodeType(2);    treeNode_122.setNodeValue(&quot;果实D&quot;);    // 树根    TreeRoot treeRoot = new TreeRoot();    treeRoot.setTreeId(10001L);    treeRoot.setTreeRootNodeId(1L);    treeRoot.setTreeName(&quot;规则决策树&quot;);    Map&lt;Long, TreeNode&gt; treeNodeMap = new HashMap&lt;&gt;();    treeNodeMap.put(1L, treeNode_01);    treeNodeMap.put(11L, treeNode_11);    treeNodeMap.put(12L, treeNode_12);    treeNodeMap.put(111L, treeNode_111);    treeNodeMap.put(112L, treeNode_112);    treeNodeMap.put(121L, treeNode_121);    treeNodeMap.put(122L, treeNode_122);    treeRich = new TreeRich(treeRoot, treeNodeMap);}</code></pre><p><img src="/2021/04/08/2021-04-08-design-zu-he/itstack-demo-design-8-04.png" alt></p><ul><li>重要，这一部分是组合模式非常重要的使用，在我们已经建造好的决策树关系下，可以创建出树的各个节点，以及对节点间使用链路进行串联。</li><li>及时后续你需要做任何业务的扩展都可以在里面添加相应的节点，并做动态化的配置。</li><li>关于这部分手动组合的方式可以提取到数据库中，那么也就可以扩展到图形界面的进行配置操作。</li></ul><h3 id="编写测试类"><a href="#编写测试类" class="headerlink" title="编写测试类"></a>编写测试类</h3><pre><code>@Testpublic void test_tree() {    logger.info(&quot;决策树组合结构信息：\r\n&quot; + JSON.toJSONString(treeRich));    IEngine treeEngineHandle = new TreeEngineHandle();    Map&lt;String, String&gt; decisionMatter = new HashMap&lt;&gt;();    decisionMatter.put(&quot;gender&quot;, &quot;man&quot;);    decisionMatter.put(&quot;age&quot;, &quot;29&quot;);    EngineResult result = treeEngineHandle.process(10001L, &quot;Oli09pLkdjh&quot;, treeRich, decisionMatter);    logger.info(&quot;测试结果：{}&quot;, JSON.toJSONString(result));}</code></pre><p>在这里提供了调用的通过组织模式创建出来的流程决策树，调用的时候传入了决策树的ID，那么如果是业务开发中就可以方便的解耦决策树与业务的绑定关系，按需传入决策树ID即可。</p><p>此外入参我们还提供了需要处理；男(man)、年龄(29岁)，的参数信息。</p><h4 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a>测试结果</h4><pre><code>23:35:05.711 [main] INFO  o.i.d.d.d.service.engine.EngineBase - 决策树引擎=&gt;规则决策树 userId：Oli09pLkdjh treeId：10001 treeNode：11 ruleKey：userGender matterValue：man23:35:05.712 [main] INFO  o.i.d.d.d.service.engine.EngineBase - 决策树引擎=&gt;规则决策树 userId：Oli09pLkdjh treeId：10001 treeNode：112 ruleKey：userAge matterValue：2923:35:05.715 [main] INFO  org.itstack.demo.design.test.ApiTest - 测试结果：{&quot;nodeId&quot;:112,&quot;nodeValue&quot;:&quot;果实B&quot;,&quot;success&quot;:true,&quot;treeId&quot;:10001,&quot;userId&quot;:&quot;Oli09pLkdjh&quot;}Process finished with exit code 0</code></pre><p>从测试结果上看这与我们使用ifelse是一样的，但是目前这与的组合模式设计下，就非常方便后续的拓展和修改。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li><p>从以上的决策树场景来看，组合模式的主要解决的是一系列简单逻辑节点或者扩展的复杂逻辑节点在不同结构的组织下，对于外部的调用是仍然可以非常简单的。</p></li><li><p>这部分设计模式保证了开闭原则，无需更改模型结构你就可以提供新的逻辑节点的使用并配合组织出新的关系树。但如果是一些功能差异化非常大的接口进行包装就会变得比较困难，但也不是不能很好的处理，只不过需要做一些适配和特定化的开发。</p></li><li><p>组合模式也就是把相似对象(也可以称作是方法)组合成一组可被调用的结构树对象。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>桥接模式</title>
      <link href="/2021/04/07/2021-04-07-design-qiao-jie/"/>
      <url>/2021/04/07/2021-04-07-design-qiao-jie/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="桥接模式介绍"><a href="#桥接模式介绍" class="headerlink" title="桥接模式介绍"></a>桥接模式介绍</h2><p>桥接模式的主要作用就是通过将抽象部分与实现部分分离，把多种可匹配的使用进行组合。说白了核心实现也就是在A类中含有B类接口，通过构造函数传递B类的实现，这个B类就是设计的桥。</p><p>那么这样的桥接模式，在我们平常的开发中有哪些场景</p><p>JDBC多种驱动程序的实现、同品牌类型的台式机和笔记本平板、业务实现中的多类接口同组过滤服务等。这些场景都比较适合使用桥接模式进行实现，因为在一些组合中如果有如果每一个类都实现不同的服务可能会出现笛卡尔积，而使用桥接模式就可以非常简单。</p><h2 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h2><p><img src="/2021/04/07/2021-04-07-design-qiao-jie/itstack-demo-design-7-02.png" alt></p><p>随着市场的竞争在支付服务行业出现了微信和支付宝还包括一些其他支付服务，但是对于商家来说并不希望改变用户习惯。就像如果我的地摊只能使用微信或者只能使用支付宝付款，那么就会让我顾客伤心，鸡蛋灌饼也卖不动了。</p><p>在这个时候就出现了第三方平台，把市面上综合占据市场90%以上的支付服务都集中到自己平台中，再把这样的平台提供给店铺、超市、地摊使用，同时支持人脸、扫描、密码多种方式。</p><p>我们这个案例就模拟一个这样的第三方平台来承接各个支付能力，同时使用自家的人脸让用户支付起来更加容易。那么这里就出现了多支付与多模式的融合使用，如果给每一个支付都实现一次不同的模式，即使是继承类也需要开发好多。而且随着后面接入了更多的支付服务或者支付方式，就会呈爆炸似的扩展。</p><h2 id="用一坨坨代码实现"><a href="#用一坨坨代码实现" class="headerlink" title="用一坨坨代码实现"></a>用一坨坨代码实现</h2><h3 id="工程结构"><a href="#工程结构" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-7-01└── src    └── main        └── java            └── org.itstack.demo.design                └── PayController.java</code></pre><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><pre><code>public class PayController {    private Logger logger = LoggerFactory.getLogger(PayController.class);    public boolean doPay(String uId, String tradeId, BigDecimal amount, int channelType, int modeType) {        // 微信支付        if (1 == channelType) {            logger.info(&quot;模拟微信渠道支付划账开始。uId：{} tradeId：{} amount：{}&quot;, uId, tradeId, amount);            if (1 == modeType) {                logger.info(&quot;密码支付，风控校验环境安全&quot;);            } else if (2 == modeType) {                logger.info(&quot;人脸支付，风控校验脸部识别&quot;);            } else if (3 == modeType) {                logger.info(&quot;指纹支付，风控校验指纹信息&quot;);            }        }        // 支付宝支付        else if (2 == channelType) {            logger.info(&quot;模拟支付宝渠道支付划账开始。uId：{} tradeId：{} amount：{}&quot;, uId, tradeId, amount);            if (1 == modeType) {                logger.info(&quot;密码支付，风控校验环境安全&quot;);            } else if (2 == modeType) {                logger.info(&quot;人脸支付，风控校验脸部识别&quot;);            } else if (3 == modeType) {                logger.info(&quot;指纹支付，风控校验指纹信息&quot;);            }        }        return true;    }}</code></pre><h2 id="桥接模式重构代码"><a href="#桥接模式重构代码" class="headerlink" title="桥接模式重构代码"></a>桥接模式重构代码</h2><p>从上面的ifelse方式实现来看，这是两种不同类型的相互组合。那么就可以把支付方式和支付模式进行分离通过抽象类依赖实现类的方式进行桥接，通过这样的拆分后支付与模式其实是可以单独使用的，当需要组合时候只需要把模式传递给支付即可。</p><p>桥接模式的关键是选择的桥接点拆分，是否可以找到这样类似的相互组合，如果没有就不必要非得使用桥接模式。</p><h3 id="工程结构-1"><a href="#工程结构-1" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-7-02└── src    ├── main    │   └── java    │       └── org.itstack.demo.design.pay    │           ├── channel    │           │   ├── Pay.java    │           │   ├── WxPay.java    │           │   └── ZfbPay.java    │           └── mode    │               ├── IPayMode.java    │               ├── PayCypher.java    │               ├── PayFaceMode.java    │               └── PayFingerprintMode.java    └── test         └── java             └── org.itstack.demo.design.test                 └── ApiTest.java</code></pre><p>桥接模式模型结构</p><p><img src="/2021/04/07/2021-04-07-design-qiao-jie/itstack-demo-design-7-03.png" alt></p><ul><li>左侧Pay是一个抽象类，往下是它的两个支付类型实现；微信支付、支付宝支付。</li><li>右侧IPayMode是一个接口，往下是它的两个支付模型；刷脸支付、指纹支付。</li><li>那么，支付类型 × 支付模型 = 就可以得到相应的组合。</li></ul><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="支付类型桥接抽象类"><a href="#支付类型桥接抽象类" class="headerlink" title="支付类型桥接抽象类"></a>支付类型桥接抽象类</h4><pre><code>public abstract class Pay {    protected Logger logger = LoggerFactory.getLogger(Pay.class);    protected IPayMode payMode;    public Pay(IPayMode payMode) {        this.payMode = payMode;    }    public abstract String transfer(String uId, String tradeId, BigDecimal amount);}</code></pre><p>在这个类中定义了支付方式的需要实现的划账接口：transfer，以及桥接接口；IPayMode，并在构造函数中用户方自行选择支付方式。</p><h4 id="微信支付的实现"><a href="#微信支付的实现" class="headerlink" title="微信支付的实现"></a>微信支付的实现</h4><pre><code>public class WxPay extends Pay {    public WxPay(IPayMode payMode) {        super(payMode);    }    public String transfer(String uId, String tradeId, BigDecimal amount) {        logger.info(&quot;模拟微信渠道支付划账开始。uId：{} tradeId：{} amount：{}&quot;, uId, tradeId, amount);        boolean security = payMode.security(uId);        logger.info(&quot;模拟微信渠道支付风控校验。uId：{} tradeId：{} security：{}&quot;, uId, tradeId, security);        if (!security) {            logger.info(&quot;模拟微信渠道支付划账拦截。uId：{} tradeId：{} amount：{}&quot;, uId, tradeId, amount);            return &quot;0001&quot;;        }        logger.info(&quot;模拟微信渠道支付划账成功。uId：{} tradeId：{} amount：{}&quot;, uId, tradeId, amount);        return &quot;0000&quot;;    }}</code></pre><h4 id="支付宝支付"><a href="#支付宝支付" class="headerlink" title="支付宝支付"></a>支付宝支付</h4><pre><code>public class ZfbPay extends Pay {    public ZfbPay(IPayMode payMode) {        super(payMode);    }    public String transfer(String uId, String tradeId, BigDecimal amount) {        logger.info(&quot;模拟支付宝渠道支付划账开始。uId：{} tradeId：{} amount：{}&quot;, uId, tradeId, amount);        boolean security = payMode.security(uId);        logger.info(&quot;模拟支付宝渠道支付风控校验。uId：{} tradeId：{} security：{}&quot;, uId, tradeId, security);        if (!security) {            logger.info(&quot;模拟支付宝渠道支付划账拦截。uId：{} tradeId：{} amount：{}&quot;, uId, tradeId, amount);            return &quot;0001&quot;;        }        logger.info(&quot;模拟支付宝渠道支付划账成功。uId：{} tradeId：{} amount：{}&quot;, uId, tradeId, amount);        return &quot;0000&quot;;    }}</code></pre><p>另外可以看到在支付的时候分别都调用了风控的接口进行验证，也就是不同模式的支付(刷脸、指纹)，都需要过指定的风控，才能保证支付安全。</p><h4 id="定义支付模式接口"><a href="#定义支付模式接口" class="headerlink" title="定义支付模式接口"></a>定义支付模式接口</h4><pre><code>public interface IPayMode {    boolean security(String uId);}</code></pre><p>任何一个支付模式；刷脸、指纹、密码，都会过不同程度的安全风控，这里定义一个安全校验接口。</p><h4 id="刷脸"><a href="#刷脸" class="headerlink" title="刷脸"></a>刷脸</h4><pre><code>public class PayFaceMode implements IPayMode{    protected Logger logger = LoggerFactory.getLogger(PayCypher.class);    public boolean security(String uId) {        logger.info(&quot;人脸支付，风控校验脸部识别&quot;);        return true;    }}</code></pre><h4 id="指纹"><a href="#指纹" class="headerlink" title="指纹"></a>指纹</h4><pre><code>public class PayFingerprintMode implements IPayMode{    protected Logger logger = LoggerFactory.getLogger(PayCypher.class);    public boolean security(String uId) {        logger.info(&quot;指纹支付，风控校验指纹信息&quot;);        return true;    }}</code></pre><h4 id="密码"><a href="#密码" class="headerlink" title="密码"></a>密码</h4><pre><code>public class PayCypher implements IPayMode{    protected Logger logger = LoggerFactory.getLogger(PayCypher.class);    public boolean security(String uId) {        logger.info(&quot;密码支付，风控校验环境安全&quot;);        return true;    }}</code></pre><p>在这里实现了三种支付模式(刷脸、指纹、密码)的风控校验，在用户选择不同支付类型的时候，则会进行相应的风控拦截以此保障支付安全。</p><h4 id="测试验证"><a href="#测试验证" class="headerlink" title="测试验证"></a>测试验证</h4><pre><code>@Testpublic void test_pay() {    System.out.println(&quot;\r\n模拟测试场景；微信支付、人脸方式。&quot;);    Pay wxPay = new WxPay(new PayFaceMode());    wxPay.transfer(&quot;weixin_1092033111&quot;, &quot;100000109893&quot;, new BigDecimal(100));    System.out.println(&quot;\r\n模拟测试场景；支付宝支付、指纹方式。&quot;);    Pay zfbPay = new ZfbPay(new PayFingerprintMode());    zfbPay.transfer(&quot;jlu19dlxo111&quot;,&quot;100000109894&quot;,new BigDecimal(100));}</code></pre><p>目前以上优化主要针对桥接模式的使用进行重构if逻辑部分，关于调用部分可以使用抽象工厂或策略模式配合map结构，将服务配置化。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li><p>通过模拟微信与支付宝两个支付渠道在不同的支付模式下，刷脸、指纹、密码，的组合从而体现了桥接模式的在这类场景中的合理运用。简化了代码的开发，给后续的需求迭代增加了很好的扩展性。</p></li><li><p>从桥接模式的实现形式来看满足了单一职责和开闭原则，让每一部分内容都很清晰易于维护和拓展，但如果我们是实现的高内聚的代码，那么就会很复杂。所以在选择重构代码的时候，需要考虑好整体的设计，否则选不到合理的设计模式，将会让代码变得难以开发。</p></li><li><p>桥接模式的主要作用就是通过将抽象部分与实现部分分离，把多种可匹配的使用进行组合。说白了核心实现也就是在A类中含有B类接口，通过构造函数传递B类的实现，这个B类就是设计的桥。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>适配器模式</title>
      <link href="/2021/04/06/2021-04-06-design-gua-pei-qi/"/>
      <url>/2021/04/06/2021-04-06-design-gua-pei-qi/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="适配器模式介绍"><a href="#适配器模式介绍" class="headerlink" title="适配器模式介绍"></a>适配器模式介绍</h2><p><img src="/2021/04/06/2021-04-06-design-gua-pei-qi/itstack-demo-design-6-01.png" alt></p><p>适配器模式的主要作用就是把原本不兼容的接口，通过适配修改做到统一。</p><p>使得用户方便使用，就像我们提到的万能充、数据线、MAC笔记本的转换头、出国旅游买个插座等等，他们都是为了适配各种不同的口，做的兼容。</p><p>在业务开发中我们会经常的需要做不同接口的兼容。</p><h2 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h2><p><img src="/2021/04/06/2021-04-06-design-gua-pei-qi/itstack-demo-design-6-03.png" alt></p><p>一个系统会接收各种各样的MQ消息或者接口，如果一个个的去开发，就会耗费很大的成本，同时对于后期的拓展也有一定的难度。此时就会希望有一个系统可以配置一下就把外部的MQ接入进行，这些MQ就像上面提到的可能是一些注册开户消息、商品下单消息等等。</p><p>而适配器的思想方式也恰恰可以运用到这里，并且我想强调一下，适配器不只是可以适配接口往往还可以适配一些属性信息。</p><h3 id="场景模拟工程"><a href="#场景模拟工程" class="headerlink" title="场景模拟工程"></a>场景模拟工程</h3><pre><code>itstack-demo-design-6-00└── src    └── main        └── java            └── org.itstack.demo.design                ├── mq                │   ├── create_account.java                │   ├── OrderMq.java                │   └── POPOrderDelivered.java                └── service                    ├── OrderServicejava                    └── POPOrderService.java</code></pre><ul><li>这里模拟了三个不同类型的MQ消息，而在消息体中都有一些必要的字段，比如；用户ID、时间、业务ID，但是每个MQ的字段属性并不一样。就像用户ID在不同的MQ里也有不同的字段：uId、userId等。</li><li>同时还提供了两个不同类型的接口，一个用于查询内部订单订单下单数量，一个用于查询第三方是否首单。</li><li>后面会把这些不同类型的MQ和接口做适配兼容。</li></ul><h3 id="场景简述"><a href="#场景简述" class="headerlink" title="场景简述"></a>场景简述</h3><h4 id="注册开户MQ"><a href="#注册开户MQ" class="headerlink" title="注册开户MQ"></a>注册开户MQ</h4><pre><code>public class create_account {    private String number;      // 开户编号    private String address;     // 开户地    private Date accountDate;   // 开户时间    private String desc;        // 开户描述    // ... get/set     }</code></pre><h4 id="内部订单MQ"><a href="#内部订单MQ" class="headerlink" title="内部订单MQ"></a>内部订单MQ</h4><pre><code>public class OrderMq {    private String uid;           // 用户ID    private String sku;           // 商品    private String orderId;       // 订单ID    private Date createOrderTime; // 下单时间         // ... get/set      }</code></pre><h4 id="第三方订单MQ"><a href="#第三方订单MQ" class="headerlink" title="第三方订单MQ"></a>第三方订单MQ</h4><pre><code>public class POPOrderDelivered {    private String uId;     // 用户ID    private String orderId; // 订单号    private Date orderTime; // 下单时间    private Date sku;       // 商品    private Date skuName;   // 商品名称    private BigDecimal decimal; // 金额    // ... get/set      }</code></pre><h4 id="查询用户内部下单数量接口"><a href="#查询用户内部下单数量接口" class="headerlink" title="查询用户内部下单数量接口"></a>查询用户内部下单数量接口</h4><pre><code>public class OrderService {    private Logger logger = LoggerFactory.getLogger(POPOrderService.class);    public long queryUserOrderCount(String userId){        logger.info(&quot;自营商家，查询用户的订单是否为首单：{}&quot;, userId);        return 10L;    }}</code></pre><h4 id="查询用户第三方下单首单接口"><a href="#查询用户第三方下单首单接口" class="headerlink" title="查询用户第三方下单首单接口"></a>查询用户第三方下单首单接口</h4><pre><code>public class POPOrderService {    private Logger logger = LoggerFactory.getLogger(POPOrderService.class);    public boolean isFirstOrder(String uId) {        logger.info(&quot;POP商家，查询用户的订单是否为首单：{}&quot;, uId);        return true;    }}</code></pre><h2 id="用一坨坨代码实现"><a href="#用一坨坨代码实现" class="headerlink" title="用一坨坨代码实现"></a>用一坨坨代码实现</h2><h3 id="工程结构"><a href="#工程结构" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-6-01└── src    └── main        └── java            └── org.itstack.demo.design                └── create_accountMqService.java                └── OrderMqService.java                └── POPOrderDeliveredService.java</code></pre><p>目前需要接收三个MQ消息，所有就有了三个对应的类，和我们平时的代码几乎一样。如果你的MQ量不多，这样的写法也没什么问题，但是随着数量的增加，就需要考虑用一些设计模式来解决。</p><h3 id="Mq接收消息实现"><a href="#Mq接收消息实现" class="headerlink" title="Mq接收消息实现"></a>Mq接收消息实现</h3><pre><code>public class create_accountMqService {    public void onMessage(String message) {        create_account mq = JSON.parseObject(message, create_account.class);        mq.getNumber();        mq.getAccountDate();        // ... 处理自己的业务    }}</code></pre><p>三组MQ的消息都是一样模拟使用，就不一一展示了。当成千上万个mq处理时怎么办？</p><h2 id="适配器模式重构代码"><a href="#适配器模式重构代码" class="headerlink" title="适配器模式重构代码"></a>适配器模式重构代码</h2><p>适配器模式要解决的主要问题就是多种差异化类型的接口做统一输出，这在我们学习工厂方法模式中也有所提到不同种类的奖品处理，其实那也是适配器的应用。</p><p>在本文中我们还会再另外体现出一个多种MQ接收，使用MQ的场景。来把不同类型的消息做统一的处理，便于减少后续对MQ接收。</p><p>再者，本文所展示的MQ兼容的核心部分，也就是处理适配不同的类型字段。而如果我们接收MQ后，在配置不同的消费类时，如果不希望一个个开发类，那么可以使用代理类的方式进行处理。</p><h3 id="工程结构-1"><a href="#工程结构-1" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-6-02└── src    └── main        └── java            └── org.itstack.demo.design                ├── impl                │   ├── InsideOrderService.java                │   └── POPOrderAdapterServiceImpl.java                ├── MQAdapter,java                ├── OrderAdapterService,java                └── RebateInfo,java</code></pre><p>适配器模型结构</p><p><img src="/2021/04/06/2021-04-06-design-gua-pei-qi/itstack-demo-design-6-04.png" alt></p><ul><li>这里包括了两个类型的适配；接口适配、MQ适配。之所以不只是模拟接口适配，因为很多时候大家都很常见了，所以把适配的思想换一下到MQ消息体上，增加大家多设计模式的认知。</li><li>先是做MQ适配，接收各种各样的MQ消息。</li></ul><h3 id="代码实现-MQ消息适配"><a href="#代码实现-MQ消息适配" class="headerlink" title="代码实现(MQ消息适配)"></a>代码实现(MQ消息适配)</h3><h4 id="统一的MQ消息体"><a href="#统一的MQ消息体" class="headerlink" title="统一的MQ消息体"></a>统一的MQ消息体</h4><pre><code>public class RebateInfo {    private String userId;  // 用户ID    private String bizId;   // 业务ID    private Date bizTime;   // 业务时间    private String desc;    // 业务描述    // ... get/set}</code></pre><h4 id="MQ消息体适配类"><a href="#MQ消息体适配类" class="headerlink" title="MQ消息体适配类"></a>MQ消息体适配类</h4><pre><code>public class MQAdapter {    public static RebateInfo filter(String strJson, Map&lt;String, String&gt; link) throws NoSuchMethodException, InvocationTargetException, IllegalAccessException {        return filter(JSON.parseObject(strJson, Map.class), link);    }    public static RebateInfo filter(Map obj, Map&lt;String, String&gt; link) throws NoSuchMethodException, InvocationTargetException, IllegalAccessException {        RebateInfo rebateInfo = new RebateInfo();        for (String key : link.keySet()) {            Object val = obj.get(link.get(key));            RebateInfo.class.getMethod(&quot;set&quot; + key.substring(0, 1).toUpperCase() + key.substring(1), String.class).invoke(rebateInfo, val.toString());        }        return rebateInfo;    }}</code></pre><ul><li>这个类里的方法非常重要，主要用于把不同类型MQ种的各种属性，映射成我们需要的属性并返回。就像一个属性中有用户ID;uId，映射到我们需要的；userId，做统一处理。</li><li>而在这个处理过程中需要把映射管理传递给Map&lt;String, String&gt; link，也就是准确的描述了，当前MQ中某个属性名称，映射为我们的某个属性名称。</li><li>最终因为我们接收到的mq消息基本都是json格式，可以转换为MAP结构。最后使用反射调用的方式给我们的类型赋值。</li></ul><h4 id="测试适配类"><a href="#测试适配类" class="headerlink" title="测试适配类"></a>测试适配类</h4><pre><code>@Testpublic void test_MQAdapter() throws NoSuchMethodException, IllegalAccessException, InvocationTargetException {    create_account create_account = new create_account();    create_account.setNumber(&quot;100001&quot;);    create_account.setAddress(&quot;河北省.廊坊市.广阳区.大学里职业技术学院&quot;);    create_account.setAccountDate(new Date());    create_account.setDesc(&quot;在校开户&quot;);              HashMap&lt;String, String&gt; link01 = new HashMap&lt;String, String&gt;();    link01.put(&quot;userId&quot;, &quot;number&quot;);    link01.put(&quot;bizId&quot;, &quot;number&quot;);    link01.put(&quot;bizTime&quot;, &quot;accountDate&quot;);    link01.put(&quot;desc&quot;, &quot;desc&quot;);    RebateInfo rebateInfo01 = MQAdapter.filter(create_account.toString(), link01);    System.out.println(&quot;mq.create_account(适配前)&quot; + create_account.toString());    System.out.println(&quot;mq.create_account(适配后)&quot; + JSON.toJSONString(rebateInfo01));    System.out.println(&quot;&quot;);    OrderMq orderMq = new OrderMq();    orderMq.setUid(&quot;100001&quot;);    orderMq.setSku(&quot;10928092093111123&quot;);    orderMq.setOrderId(&quot;100000890193847111&quot;);    orderMq.setCreateOrderTime(new Date());     HashMap&lt;String, String&gt; link02 = new HashMap&lt;String, String&gt;();    link02.put(&quot;userId&quot;, &quot;uid&quot;);    link02.put(&quot;bizId&quot;, &quot;orderId&quot;);    link02.put(&quot;bizTime&quot;, &quot;createOrderTime&quot;);    RebateInfo rebateInfo02 = MQAdapter.filter(orderMq.toString(), link02);    System.out.println(&quot;mq.orderMq(适配前)&quot; + orderMq.toString());    System.out.println(&quot;mq.orderMq(适配后)&quot; + JSON.toJSONString(rebateInfo02));}</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>从上文可以看到不使用适配器模式这些功能同样可以实现，但是使用了适配器模式就可以让代码：干净整洁易于维护、减少大量重复的判断和使用、让代码更加易于维护和拓展。</li><li>尤其是我们对MQ这样的多种消息体中不同属性同类的值，进行适配再加上代理类，就可以使用简单的配置方式接入对方提供的MQ消息，而不需要大量重复的开发。非常利于拓展。</li><li>适配器模式的主要作用就是把原本不兼容的接口，通过适配修改做到统一。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>单例模式</title>
      <link href="/2021/04/05/2021-04-05-design-dan-li/"/>
      <url>/2021/04/05/2021-04-05-design-dan-li/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="单例模式介绍"><a href="#单例模式介绍" class="headerlink" title="单例模式介绍"></a>单例模式介绍</h2><p>单例模式可以说是整个设计中最简单的模式之一，而且这种方式即使在没有看设计模式相关资料也会常用在编码开发中。</p><p>因为在编程开发中经常会遇到这样一种场景，那就是需要保证一个类只有一个实例哪怕多线程同时访问，并需要提供一个全局访问此实例的点。</p><p>综上以及我们平常的开发中，可以总结一条经验，单例模式主要解决的是，一个全局使用的类频繁的创建和消费，从而提升提升整体的代码的性能。</p><h2 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h2><p>出现的场景非常简单也是我们日常开发所能见到的，例如；</p><ul><li>数据库的连接池不会反复创建</li><li>spring中一个单例模式bean的生成和使用</li><li>在我们平常的代码中需要设置全局的的一些属性保存</li></ul><h2 id="7种单例模式实现"><a href="#7种单例模式实现" class="headerlink" title="7种单例模式实现"></a>7种单例模式实现</h2><p>单例模式的实现方式比较多，主要在实现上是否支持懒汉模式、是否线程安全中运用各项技巧。当然也有一些场景不需要考虑懒加载也就是懒汉模式的情况，会直接使用static静态类或属性和方法的方式进行处理，供外部调用。</p><h3 id="静态类使用"><a href="#静态类使用" class="headerlink" title="静态类使用"></a>静态类使用</h3><pre><code>public class Singleton_00 {    public static Map&lt;String,String&gt; cache = new ConcurrentHashMap&lt;String, String&gt;();}</code></pre><ul><li><p>以上这种方式在我们平常的业务开发中非常场常见，这样静态类的方式可以在第一次运行的时候直接初始化Map类，同时这里我们也不需要到延迟加载在使用。</p></li><li><p>在不需要维持任何状态下，仅仅用于全局访问，这个使用使用静态类的方式更加方便。</p></li><li><p>但如果需要被继承以及需要维持一些特定状态的情况下，就适合使用单例模式。</p></li></ul><h3 id="懒汉模式-线程不安全"><a href="#懒汉模式-线程不安全" class="headerlink" title="懒汉模式(线程不安全)"></a>懒汉模式(线程不安全)</h3><pre><code>public class Singleton_01 {    private static Singleton_01 instance;    private Singleton_01() {    }    public static Singleton_01 getInstance(){        if (null != instance) return instance;        instance = new Singleton_01();        return instance;    }}</code></pre><ul><li><p>单例模式有一个特点就是不允许外部直接创建，也就是new Singleton_01()，因此这里在默认的构造函数上添加了私有属性 private。</p></li><li><p>目前此种方式的单例确实满足了懒加载，但是如果有多个访问者同时去获取对象实例你可以想象成一堆人在抢厕所，就会造成多个同样的实例并存，从而没有达到单例的要求。</p></li></ul><h3 id="懒汉模式-线程安全"><a href="#懒汉模式-线程安全" class="headerlink" title="懒汉模式(线程安全)"></a>懒汉模式(线程安全)</h3><pre><code>public class Singleton_02 {    private static Singleton_02 instance;    private Singleton_02() {    }    public static synchronized Singleton_02 getInstance(){        if (null != instance) return instance;        instance = new Singleton_02();        return instance;    }}</code></pre><p>此种模式虽然是安全的，但由于把锁加到方法上后，所有的访问都因需要锁占用导致资源的浪费。如果不是特殊情况下，不建议此种方式实现单例模式。</p><h3 id="饿汉模式-线程安全"><a href="#饿汉模式-线程安全" class="headerlink" title="饿汉模式(线程安全)"></a>饿汉模式(线程安全)</h3><pre><code>public class Singleton_03 {    private static Singleton_03 instance = new Singleton_03();    private Singleton_03() {    }    public static Singleton_03 getInstance() {        return instance;    }}</code></pre><ul><li>此种方式与我们开头的第一个实例化Map基本一致，在程序启动的时候直接运行加载，后续有外部需要使用的时候获取即可。</li><li>但此种方式并不是懒加载，也就是说无论你程序中是否用到这样的类都会在程序启动之初进行创建。</li></ul><h3 id="使用类的内部类-线程安全"><a href="#使用类的内部类-线程安全" class="headerlink" title="使用类的内部类(线程安全)"></a>使用类的内部类(线程安全)</h3><pre><code>public class Singleton_04 {    private static class SingletonHolder {        private static Singleton_04 instance = new Singleton_04();    }    private Singleton_04() {    }    public static Singleton_04 getInstance() {        return SingletonHolder.instance;    }}</code></pre><ul><li>使用类的静态内部类实现的单例模式，既保证了线程安全有保证了懒加载，同时不会因为加锁的方式耗费性能。</li><li>这主要是因为JVM虚拟机可以保证多线程并发访问的正确性，也就是一个类的构造方法在多线程环境下可以被正确的加载。</li><li>此种方式也是非常推荐使用的一种单例模式</li></ul><h3 id="双重锁校验-线程安全"><a href="#双重锁校验-线程安全" class="headerlink" title="双重锁校验(线程安全)"></a>双重锁校验(线程安全)</h3><pre><code>public class Singleton_05 {    private static volatile Singleton_05 instance;    private Singleton_05() {    }    public static Singleton_05 getInstance(){       if(null != instance) return instance;       synchronized (Singleton_05.class){           if (null == instance){               instance = new Singleton_05();           }       }       return instance;    }}</code></pre><ul><li>双重锁的方式是方法级锁的优化，减少了部分获取实例的耗时。</li><li>同时这种方式也满足了懒加载。</li></ul><h3 id="CAS「AtomicReference」-线程安全"><a href="#CAS「AtomicReference」-线程安全" class="headerlink" title="CAS「AtomicReference」(线程安全)"></a>CAS「AtomicReference」(线程安全)</h3><pre><code>public class Singleton_06 {    private static final AtomicReference&lt;Singleton_06&gt; INSTANCE = new AtomicReference&lt;Singleton_06&gt;();    private static Singleton_06 instance;    private Singleton_06() {    }    public static final Singleton_06 getInstance() {        for (; ; ) {            Singleton_06 instance = INSTANCE.get();            if (null != instance) return instance;            INSTANCE.compareAndSet(null, new Singleton_06());            return INSTANCE.get();        }    }    public static void main(String[] args) {        System.out.println(Singleton_06.getInstance()); // org.itstack.demo.design.Singleton_06@2b193f2d        System.out.println(Singleton_06.getInstance()); // org.itstack.demo.design.Singleton_06@2b193f2d    }}</code></pre><ul><li>java并发库提供了很多原子类来支持并发访问的数据安全性；AtomicInteger、AtomicBoolean、AtomicLong、AtomicReference。</li><li>AtomicReference 可以封装引用一个V实例，支持并发访问如上的单例方式就是使用了这样的一个特点。</li><li>使用CAS的好处就是不需要使用传统的加锁方式保证线程安全，而是依赖于CAS的忙等算法，依赖于底层硬件的实现，来保证线程安全。相对于其他锁的实现没有线程的切换和阻塞也就没有了额外的开销，并且可以支持较大的并发性。</li><li>当然CAS也有一个缺点就是忙等，如果一直没有获取到将会处于死循环中。</li></ul><h3 id="Effective-Java作者推荐的枚举单例-线程安全"><a href="#Effective-Java作者推荐的枚举单例-线程安全" class="headerlink" title="Effective Java作者推荐的枚举单例(线程安全)"></a>Effective Java作者推荐的枚举单例(线程安全)</h3><pre><code>public enum Singleton_07 {    INSTANCE;    public void test(){        System.out.println(&quot;hi~&quot;);    }}</code></pre><ul><li>Effective Java 作者推荐使用枚举的方式解决单例模式，此种方式可能是平时最少用到的。</li><li>这种方式解决了最主要的；线程安全、自由串行化、单一实例。</li></ul><p>调用方式</p><pre><code>@Testpublic void test() {    Singleton_07.INSTANCE.test();</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>虽然只是一个很平常的单例模式，但在各种的实现上真的可以看到java的基本功的体现，这里包括了；懒汉、饿汉、线程是否安全、静态类、内部类、加锁、串行化等等。</li><li>单例模式主要解决的是，一个全局使用的类频繁的创建和消费，从而提升提升整体的代码的性能。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>原型模式</title>
      <link href="/2021/04/04/2021-04-04-design-yuan-xing/"/>
      <url>/2021/04/04/2021-04-04-design-yuan-xing/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="原型模式介绍"><a href="#原型模式介绍" class="headerlink" title="原型模式介绍"></a>原型模式介绍</h2><p>原型模式主要解决的问题就是创建重复对象，而这部分对象内容本身比较复杂，生成过程可能从库或者RPC接口中获取数据的耗时较长，因此采用克隆的方式节省时间。</p><p>其实这种场景经常出现在我们的身边，只不过很少用到自己的开发中，就像：</p><p>经常Ctrl+C、Ctrl+V，复制粘贴代码。</p><p>Java多数类中提供的API方法；Object clone()。</p><h2 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h2><p><img src="/2021/04/04/2021-04-04-design-yuan-xing/itstack-demo-design-4-02.jpg" alt></p><p>每个人都经历过考试，从纸制版到上机答题，大大小小也有几百场。而以前坐在教室里答题身边的人都是一套试卷，考试的时候还能偷摸或者别人给发信息抄一抄答案。</p><p>但从一部分可以上机考试的内容开始，在保证大家的公平性一样的题目下，开始出现试题混排更有做的好的答案选项也混排。这样大大的增加了抄的成本，也更好的做到了考试的公平性。</p><p>需要实现一个上机考试抽题的服务，因此在这里建造一个题库题目的场景类信息，用于创建；选择题、问答题。</p><h3 id="场景模拟工程"><a href="#场景模拟工程" class="headerlink" title="场景模拟工程"></a>场景模拟工程</h3><pre><code>itstack-demo-design-4-00└── src    └── main        └── java            └── org.itstack.demo.design                ├── AnswerQuestion.java                └── ChoiceQuestion.java</code></pre><p>在这里模拟了两个试卷题目的类；ChoiceQuestion(选择题)、AnswerQuestion(问答题)。如果是实际的业务场景开发中，会有更多的题目类型，可以回忆一下你的高考试卷。</p><h3 id="场景简述"><a href="#场景简述" class="headerlink" title="场景简述"></a>场景简述</h3><h4 id="选择题"><a href="#选择题" class="headerlink" title="选择题"></a>选择题</h4><pre><code>public class ChoiceQuestion {    private String name;                 // 题目    private Map&lt;String, String&gt; option;  // 选项；A、B、C、D    private String key;                  // 答案；B    public ChoiceQuestion() {    }    public ChoiceQuestion(String name, Map&lt;String, String&gt; option, String key) {        this.name = name;        this.option = option;        this.key = key;    }    // ...get/set}</code></pre><h4 id="问答题"><a href="#问答题" class="headerlink" title="问答题"></a>问答题</h4><pre><code>public class AnswerQuestion {    private String name;  // 问题    private String key;   // 答案    public AnswerQuestion() {    }    public AnswerQuestion(String name, String key) {        this.name = name;        this.key = key;    }    // ...get/set}</code></pre><h2 id="用一坨坨代码实现"><a href="#用一坨坨代码实现" class="headerlink" title="用一坨坨代码实现"></a>用一坨坨代码实现</h2><p>在以下的例子中我们会按照每一个用户创建试卷的题目，并返回给调用方。</p><h3 id="工程结构"><a href="#工程结构" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-4-01└── src    └── main        └── java            └── org.itstack.demo.design                └── QuestionBankController.java</code></pre><h3 id="一把梭实现需求"><a href="#一把梭实现需求" class="headerlink" title="一把梭实现需求"></a>一把梭实现需求</h3><p>一个类几千行的代码你是否见过，嚯？那今天就再让你见识一下有这样潜质的类！</p><pre><code>public class QuestionBankController {    public String createPaper(String candidate, String number) {        List&lt;ChoiceQuestion&gt; choiceQuestionList = new ArrayList&lt;ChoiceQuestion&gt;();        List&lt;AnswerQuestion&gt; answerQuestionList = new ArrayList&lt;AnswerQuestion&gt;();        Map&lt;String, String&gt; map01 = new HashMap&lt;String, String&gt;();        map01.put(&quot;A&quot;, &quot;JAVA2 EE&quot;);        map01.put(&quot;B&quot;, &quot;JAVA2 Card&quot;);        map01.put(&quot;C&quot;, &quot;JAVA2 ME&quot;);        map01.put(&quot;D&quot;, &quot;JAVA2 HE&quot;);        map01.put(&quot;E&quot;, &quot;JAVA2 SE&quot;);        Map&lt;String, String&gt; map02 = new HashMap&lt;String, String&gt;();        map02.put(&quot;A&quot;, &quot;JAVA程序的main方法必须写在类里面&quot;);        map02.put(&quot;B&quot;, &quot;JAVA程序中可以有多个main方法&quot;);        map02.put(&quot;C&quot;, &quot;JAVA程序中类名必须与文件名一样&quot;);        map02.put(&quot;D&quot;, &quot;JAVA程序的main方法中如果只有一条语句，可以不用{}(大括号)括起来&quot;);        Map&lt;String, String&gt; map03 = new HashMap&lt;String, String&gt;();        map03.put(&quot;A&quot;, &quot;变量由字母、下划线、数字、$符号随意组成；&quot;);        map03.put(&quot;B&quot;, &quot;变量不能以数字作为开头；&quot;);        map03.put(&quot;C&quot;, &quot;A和a在java中是同一个变量；&quot;);        map03.put(&quot;D&quot;, &quot;不同类型的变量，可以起相同的名字；&quot;);        Map&lt;String, String&gt; map04 = new HashMap&lt;String, String&gt;();        map04.put(&quot;A&quot;, &quot;STRING&quot;);        map04.put(&quot;B&quot;, &quot;x3x;&quot;);        map04.put(&quot;C&quot;, &quot;void&quot;);        map04.put(&quot;D&quot;, &quot;de$f&quot;);        Map&lt;String, String&gt; map05 = new HashMap&lt;String, String&gt;();        map05.put(&quot;A&quot;, &quot;31&quot;);        map05.put(&quot;B&quot;, &quot;0&quot;);        map05.put(&quot;C&quot;, &quot;1&quot;);        map05.put(&quot;D&quot;, &quot;2&quot;);        choiceQuestionList.add(new ChoiceQuestion(&quot;JAVA所定义的版本中不包括&quot;, map01, &quot;D&quot;));        choiceQuestionList.add(new ChoiceQuestion(&quot;下列说法正确的是&quot;, map02, &quot;A&quot;));        choiceQuestionList.add(new ChoiceQuestion(&quot;变量命名规范说法正确的是&quot;, map03, &quot;B&quot;));        choiceQuestionList.add(new ChoiceQuestion(&quot;以下()不是合法的标识符&quot;, map04, &quot;C&quot;));        choiceQuestionList.add(new ChoiceQuestion(&quot;表达式(11+3*8)/4%3的值是&quot;, map05, &quot;D&quot;));        answerQuestionList.add(new AnswerQuestion(&quot;小红马和小黑马生的小马几条腿&quot;, &quot;4条腿&quot;));        answerQuestionList.add(new AnswerQuestion(&quot;铁棒打头疼还是木棒打头疼&quot;, &quot;头最疼&quot;));        answerQuestionList.add(new AnswerQuestion(&quot;什么床不能睡觉&quot;, &quot;牙床&quot;));        answerQuestionList.add(new AnswerQuestion(&quot;为什么好马不吃回头草&quot;, &quot;后面的草没了&quot;));        // 输出结果        StringBuilder detail = new StringBuilder(&quot;考生：&quot; + candidate + &quot;\r\n&quot; +                &quot;考号：&quot; + number + &quot;\r\n&quot; +                &quot;--------------------------------------------\r\n&quot; +                &quot;一、选择题&quot; + &quot;\r\n\n&quot;);        for (int idx = 0; idx &lt; choiceQuestionList.size(); idx++) {            detail.append(&quot;第&quot;).append(idx + 1).append(&quot;题：&quot;).append(choiceQuestionList.get(idx).getName()).append(&quot;\r\n&quot;);            Map&lt;String, String&gt; option = choiceQuestionList.get(idx).getOption();            for (String key : option.keySet()) {                detail.append(key).append(&quot;：&quot;).append(option.get(key)).append(&quot;\r\n&quot;);                ;            }            detail.append(&quot;答案：&quot;).append(choiceQuestionList.get(idx).getKey()).append(&quot;\r\n\n&quot;);        }        detail.append(&quot;二、问答题&quot; + &quot;\r\n\n&quot;);        for (int idx = 0; idx &lt; answerQuestionList.size(); idx++) {            detail.append(&quot;第&quot;).append(idx + 1).append(&quot;题：&quot;).append(answerQuestionList.get(idx).getName()).append(&quot;\r\n&quot;);            detail.append(&quot;答案：&quot;).append(answerQuestionList.get(idx).getKey()).append(&quot;\r\n\n&quot;);        }        return detail.toString();    }}</code></pre><p>这样的代码往往都非常易于理解，要什么程序就给什么代码，不面向对象，只面向过程。不考虑扩展性，能用就行。</p><p>以上的代码主要就三部分内容；首先创建选择题和问答题到集合中、定义详情字符串包装结果、返回结果内容。</p><p>但以上的代码有一个没有实现的地方就是不能乱序，所有人的试卷顺序都是一样的。如果需要加乱序也是可以的，但复杂度又会增加。</p><h3 id="测试验证"><a href="#测试验证" class="headerlink" title="测试验证"></a>测试验证</h3><pre><code>@Testpublic void test_QuestionBankController() {    QuestionBankController questionBankController = new QuestionBankController();    System.out.println(questionBankController.createPaper(&quot;花花&quot;, &quot;1000001921032&quot;));    System.out.println(questionBankController.createPaper(&quot;豆豆&quot;, &quot;1000001921051&quot;));    System.out.println(questionBankController.createPaper(&quot;大宝&quot;, &quot;1000001921987&quot;));}</code></pre><h2 id="原型模式重构代码"><a href="#原型模式重构代码" class="headerlink" title="原型模式重构代码"></a>原型模式重构代码</h2><p>原型模式主要解决的问题就是创建大量重复的类，而我们模拟的场景就需要给不同的用户都创建相同的试卷，但这些试卷的题目不便于每次都从库中获取，甚至有时候需要从远程的RPC中获取。这样都是非常耗时的，而且随着创建对象的增多将严重影响效率。</p><p>在原型模式中所需要的非常重要的手段就是克隆，在需要用到克隆的类中都需要实现 implements Cloneable 接口。</p><h3 id="工程结构-1"><a href="#工程结构-1" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-4-02└── src    ├── main    │   └── java    │       └── org.itstack.demo.design    │           ├── QuestionBank.java    │           └── QuestionBankController.java     └── test         └── java             └── org.itstack.demo.design.test                 └── ApiTest.java</code></pre><p>原型模式模型结构</p><p><img src="/2021/04/04/2021-04-04-design-yuan-xing/itstack-demo-design-4-03.png" alt></p><p>工程中包括了核心的题库类QuestionBank，题库中主要负责将各个的题目进行组装最终输出试卷。</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="克隆对象处理类"><a href="#克隆对象处理类" class="headerlink" title="克隆对象处理类"></a>克隆对象处理类</h4><pre><code>public class QuestionBank implements Cloneable {    private String candidate; // 考生    private String number;    // 考号    private ArrayList&lt;ChoiceQuestion&gt; choiceQuestionList = new ArrayList&lt;ChoiceQuestion&gt;();    private ArrayList&lt;AnswerQuestion&gt; answerQuestionList = new ArrayList&lt;AnswerQuestion&gt;();    public QuestionBank append(ChoiceQuestion choiceQuestion) {        choiceQuestionList.add(choiceQuestion);        return this;    }    public QuestionBank append(AnswerQuestion answerQuestion) {        answerQuestionList.add(answerQuestion);        return this;    }    @Override    public Object clone() throws CloneNotSupportedException {        QuestionBank questionBank = (QuestionBank) super.clone();        questionBank.choiceQuestionList = (ArrayList&lt;ChoiceQuestion&gt;) choiceQuestionList.clone();        questionBank.answerQuestionList = (ArrayList&lt;AnswerQuestion&gt;) answerQuestionList.clone();        // 题目乱序        Collections.shuffle(questionBank.choiceQuestionList);        Collections.shuffle(questionBank.answerQuestionList);        return questionBank;    }    public void setCandidate(String candidate) {        this.candidate = candidate;    }    public void setNumber(String number) {        this.number = number;    }    @Override    public String toString() {        StringBuilder detail = new StringBuilder(&quot;考生：&quot; + candidate + &quot;\r\n&quot; +                &quot;考号：&quot; + number + &quot;\r\n&quot; +                &quot;--------------------------------------------\r\n&quot; +                &quot;一、选择题&quot; + &quot;\r\n\n&quot;);        for (int idx = 0; idx &lt; choiceQuestionList.size(); idx++) {            detail.append(&quot;第&quot;).append(idx + 1).append(&quot;题：&quot;).append(choiceQuestionList.get(idx).getName()).append(&quot;\r\n&quot;);            Map&lt;String, String&gt; option = choiceQuestionList.get(idx).getOption();            for (String key : option.keySet()) {                detail.append(key).append(&quot;：&quot;).append(option.get(key)).append(&quot;\r\n&quot;);;            }            detail.append(&quot;答案：&quot;).append(choiceQuestionList.get(idx).getKey()).append(&quot;\r\n\n&quot;);        }        detail.append(&quot;二、问答题&quot; + &quot;\r\n\n&quot;);        for (int idx = 0; idx &lt; answerQuestionList.size(); idx++) {            detail.append(&quot;第&quot;).append(idx + 1).append(&quot;题：&quot;).append(answerQuestionList.get(idx).getName()).append(&quot;\r\n&quot;);            detail.append(&quot;答案：&quot;).append(answerQuestionList.get(idx).getKey()).append(&quot;\r\n\n&quot;);        }        return detail.toString();    }}</code></pre><ul><li>两个append()，对各项题目的添加，有点像我们在建造者模式中使用的方式，添加装修物料。</li><li>clone() ，这里的核心操作就是对对象的复制，这里的复制不只是包括了本身，同时对两个集合也做了复制。只有这样的拷贝才能确保在操作克隆对象的时候不影响原对象。</li><li>乱序操作，在list集合中有一个方法，Collections.shuffle，可以将原有集合的顺序打乱，输出一个新的顺序。在这里我们使用此方法对题目进行乱序操作。</li></ul><h3 id="初始化试卷数据"><a href="#初始化试卷数据" class="headerlink" title="初始化试卷数据"></a>初始化试卷数据</h3><pre><code>public class QuestionBankController {    private QuestionBank questionBank = new QuestionBank();    public QuestionBankController() {        Map&lt;String, String&gt; map01 = new HashMap&lt;String, String&gt;();        map01.put(&quot;A&quot;, &quot;JAVA2 EE&quot;);        map01.put(&quot;B&quot;, &quot;JAVA2 Card&quot;);        map01.put(&quot;C&quot;, &quot;JAVA2 ME&quot;);        map01.put(&quot;D&quot;, &quot;JAVA2 HE&quot;);        map01.put(&quot;E&quot;, &quot;JAVA2 SE&quot;);        Map&lt;String, String&gt; map02 = new HashMap&lt;String, String&gt;();        map02.put(&quot;A&quot;, &quot;JAVA程序的main方法必须写在类里面&quot;);        map02.put(&quot;B&quot;, &quot;JAVA程序中可以有多个main方法&quot;);        map02.put(&quot;C&quot;, &quot;JAVA程序中类名必须与文件名一样&quot;);        map02.put(&quot;D&quot;, &quot;JAVA程序的main方法中如果只有一条语句，可以不用{}(大括号)括起来&quot;);        Map&lt;String, String&gt; map03 = new HashMap&lt;String, String&gt;();        map03.put(&quot;A&quot;, &quot;变量由字母、下划线、数字、$符号随意组成；&quot;);        map03.put(&quot;B&quot;, &quot;变量不能以数字作为开头；&quot;);        map03.put(&quot;C&quot;, &quot;A和a在java中是同一个变量；&quot;);        map03.put(&quot;D&quot;, &quot;不同类型的变量，可以起相同的名字；&quot;);        Map&lt;String, String&gt; map04 = new HashMap&lt;String, String&gt;();        map04.put(&quot;A&quot;, &quot;STRING&quot;);        map04.put(&quot;B&quot;, &quot;x3x;&quot;);        map04.put(&quot;C&quot;, &quot;void&quot;);        map04.put(&quot;D&quot;, &quot;de$f&quot;);        Map&lt;String, String&gt; map05 = new HashMap&lt;String, String&gt;();        map05.put(&quot;A&quot;, &quot;31&quot;);        map05.put(&quot;B&quot;, &quot;0&quot;);        map05.put(&quot;C&quot;, &quot;1&quot;);        map05.put(&quot;D&quot;, &quot;2&quot;);        questionBank.append(new ChoiceQuestion(&quot;JAVA所定义的版本中不包括&quot;, map01, &quot;D&quot;))                .append(new ChoiceQuestion(&quot;下列说法正确的是&quot;, map02, &quot;A&quot;))                .append(new ChoiceQuestion(&quot;变量命名规范说法正确的是&quot;, map03, &quot;B&quot;))                .append(new ChoiceQuestion(&quot;以下()不是合法的标识符&quot;,map04, &quot;C&quot;))                .append(new ChoiceQuestion(&quot;表达式(11+3*8)/4%3的值是&quot;, map05, &quot;D&quot;))                .append(new AnswerQuestion(&quot;小红马和小黑马生的小马几条腿&quot;, &quot;4条腿&quot;))                .append(new AnswerQuestion(&quot;铁棒打头疼还是木棒打头疼&quot;, &quot;头最疼&quot;))                .append(new AnswerQuestion(&quot;什么床不能睡觉&quot;, &quot;牙床&quot;))                .append(new AnswerQuestion(&quot;为什么好马不吃回头草&quot;, &quot;后面的草没了&quot;));    }    public String createPaper(String candidate, String number) throws CloneNotSupportedException {        QuestionBank questionBankClone = (QuestionBank) questionBank.clone();        questionBankClone.setCandidate(candidate);        questionBankClone.setNumber(number);        return questionBankClone.toString();    }}</code></pre><ul><li>这个类的内容就比较简单了，主要提供对试卷内容的模式初始化操作(所有考生试卷一样，题目顺序不一致)。</li><li>以及对外部提供创建试卷的方法，在创建的过程中使用的是克隆的方式；(QuestionBank) questionBank.clone();，并最终返回试卷信息。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>以上的实际场景模拟了原型模式在开发中重构的作用，但是原型模式的使用频率确实不是很高。如果有一些特殊场景需要使用到，也可以按照此设计模式进行优化。</li><li>另外原型设计模式的优点包括；便于通过克隆方式创建复杂对象、也可以避免重复做初始化操作、不需要与类中所属的其他类耦合等。但也有一些缺点如果对象中包括了循环引用的克隆，以及类中深度使用对象的克隆，都会使此模式变得异常麻烦。</li><li>原型模式主要解决的问题就是创建重复对象，而这部分对象内容本身比较复杂，生成过程可能从库或者RPC接口中获取数据的耗时较长，因此采用克隆的方式节省时间。</li><li>就是在类中克隆不同的当前类和其他操作，减少了代码冗余。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>建造者模式</title>
      <link href="/2021/04/03/2021-04-03-design-jian-zao-zhe/"/>
      <url>/2021/04/03/2021-04-03-design-jian-zao-zhe/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="建造者模式介绍"><a href="#建造者模式介绍" class="headerlink" title="建造者模式介绍"></a>建造者模式介绍</h2><p>建造者模式所完成的内容就是通过将多个简单对象通过一步步的组装构建出一个复杂对象的过程。</p><p>例如你玩王者荣耀的时的初始化界面；有三条路、有树木、有野怪、有守卫塔等等，甚至依赖于你的网络情况会控制清晰度。而当你换一个场景进行其他不同模式的选择时，同样会建设道路、树木、野怪等等，但是他们的摆放和大小都有不同。这里就可以用到建造者模式来初始化游戏元素。</p><p>而这样的根据相同的物料，不同的组装所产生出的具体的内容，就是建造者模式的最终意图，也就是；将一个复杂的构建与其表示相分离，使得同样的构建过程可以创建不同的表示。</p><h2 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h2><p><img src="/2021/04/03/2021-04-03-design-jian-zao-zhe/itstack-demo-design-3-02.png" alt></p><p>这里我们模拟装修公司对于设计出一些套餐装修服务的场景。</p><p>很多装修公司都会给出自家的套餐服务，一般有；欧式豪华、轻奢田园、现代简约等等，而这些套餐的后面是不同的商品的组合。例如；一级&amp;二级吊顶、多乐士涂料、圣象地板、马可波罗地砖等等，按照不同的套餐的价格选取不同的品牌组合，最终再按照装修面积给出一个整体的报价。</p><p>这里我们就模拟装修公司想推出一些套餐装修服务，按照不同的价格设定品牌选择组合，以达到使用建造者模式的过程。</p><h3 id="场景模拟工程"><a href="#场景模拟工程" class="headerlink" title="场景模拟工程"></a>场景模拟工程</h3><pre><code>itstack-demo-design-3-00└── src    └── main        └── java            └── org.itstack.demo.design                ├── ceilling                │   ├── LevelOneCeiling.java                │   └── LevelTwoCeiling.java                ├── coat                │   ├── DuluxCoat.java                │   └── LiBangCoat.java                │   └── LevelTwoCeiling.java                ├── floor                │   ├── DerFloor.java                │   └── ShengXiangFloor.java                ├── tile                │   ├── DongPengTile.java                │   └── MarcoPoloTile.java                └── Matter.java</code></pre><p>在模拟工程中提供了装修中所需要的物料；ceilling(吊顶)、coat(涂料)、floor(地板)、tile(地砖)，这么四项内容。（实际的装修物料要比这个多的多）</p><h3 id="场景简述"><a href="#场景简述" class="headerlink" title="场景简述"></a>场景简述</h3><h4 id="物料接口"><a href="#物料接口" class="headerlink" title="物料接口"></a>物料接口</h4><pre><code>public interface Matter {    String scene();      // 场景；地板、地砖、涂料、吊顶    String brand();      // 品牌    String model();      // 型号    BigDecimal price();  // 价格    String desc();       // 描述}</code></pre><p>物料接口提供了基本的信息，以保证所有的装修材料都可以按照统一标准进行获取。</p><h4 id="吊顶-ceiling"><a href="#吊顶-ceiling" class="headerlink" title="吊顶(ceiling)"></a>吊顶(ceiling)</h4><pre><code>public class LevelOneCeiling implements Matter {    public String scene() {        return &quot;吊顶&quot;;    }    public String brand() {        return &quot;装修公司自带&quot;;    }    public String model() {        return &quot;一级顶&quot;;    }    public BigDecimal price() {        return new BigDecimal(260);    }    public String desc() {        return &quot;造型只做低一级，只有一个层次的吊顶，一般离顶120-150mm&quot;;    }}</code></pre><pre><code>public class LevelTwoCeiling  implements Matter {    public String scene() {        return &quot;吊顶&quot;;    }    public String brand() {        return &quot;装修公司自带&quot;;    }    public String model() {        return &quot;二级顶&quot;;    }    public BigDecimal price() {        return new BigDecimal(850);    }    public String desc() {        return &quot;两个层次的吊顶，二级吊顶高度一般就往下吊20cm，要是层高很高，也可增加每级的厚度&quot;;    }}</code></pre><h4 id="涂料-coat"><a href="#涂料-coat" class="headerlink" title="涂料(coat)"></a>涂料(coat)</h4><pre><code>public class DuluxCoat  implements Matter {    public String scene() {        return &quot;涂料&quot;;    }    public String brand() {        return &quot;多乐士(Dulux)&quot;;    }    public String model() {        return &quot;第二代&quot;;    }    public BigDecimal price() {        return new BigDecimal(719);    }    public String desc() {        return &quot;多乐士是阿克苏诺贝尔旗下的著名建筑装饰油漆品牌，产品畅销于全球100个国家，每年全球有5000万户家庭使用多乐士油漆。&quot;;    }}</code></pre><pre><code>public class LiBangCoat implements Matter {    public String scene() {        return &quot;涂料&quot;;    }    public String brand() {        return &quot;立邦&quot;;    }    public String model() {        return &quot;默认级别&quot;;    }    public BigDecimal price() {        return new BigDecimal(650);    }    public String desc() {        return &quot;立邦始终以开发绿色产品、注重高科技、高品质为目标，以技术力量不断推进科研和开发，满足消费者需求。&quot;;    }}</code></pre><h4 id="地板-floor"><a href="#地板-floor" class="headerlink" title="地板(floor)"></a>地板(floor)</h4><pre><code>public class DerFloor implements Matter {    public String scene() {        return &quot;地板&quot;;    }    public String brand() {        return &quot;德尔(Der)&quot;;    }    public String model() {        return &quot;A+&quot;;    }    public BigDecimal price() {        return new BigDecimal(119);    }    public String desc() {        return &quot;DER德尔集团是全球领先的专业木地板制造商，北京2008年奥运会家装和公装地板供应商&quot;;    }}</code></pre><pre><code>public class ShengXiangFloor implements Matter {    public String scene() {        return &quot;地板&quot;;    }    public String brand() {        return &quot;圣象&quot;;    }    public String model() {        return &quot;一级&quot;;    }    public BigDecimal price() {        return new BigDecimal(318);    }    public String desc() {        return &quot;圣象地板是中国地板行业著名品牌。圣象地板拥有中国驰名商标、中国名牌、国家免检、中国环境标志认证等多项荣誉。&quot;;    }}</code></pre><h4 id="地砖-tile"><a href="#地砖-tile" class="headerlink" title="地砖(tile)"></a>地砖(tile)</h4><pre><code>public class DongPengTile implements Matter {    public String scene() {        return &quot;地砖&quot;;    }    public String brand() {        return &quot;东鹏瓷砖&quot;;    }    public String model() {        return &quot;10001&quot;;    }    public BigDecimal price() {        return new BigDecimal(102);    }    public String desc() {        return &quot;东鹏瓷砖以品质铸就品牌，科技推动品牌，口碑传播品牌为宗旨，2014年品牌价值132.35亿元，位列建陶行业榜首。&quot;;    }}</code></pre><pre><code>public class MarcoPoloTile implements Matter {    public String scene() {        return &quot;地砖&quot;;    }    public String brand() {        return &quot;马可波罗(MARCO POLO)&quot;;    }    public String model() {        return &quot;缺省&quot;;    }    public BigDecimal price() {        return new BigDecimal(140);    }    public String desc() {        return &quot;“马可波罗”品牌诞生于1996年，作为国内最早品牌化的建陶品牌，以“文化陶瓷”占领市场，享有“仿古砖至尊”的美誉。&quot;;    }}</code></pre><p>以上就是本次装修公司所提供的装修配置单，接下我们会通过案例去使用不同的物料组合出不同的套餐服务。</p><h2 id="用一坨坨代码实现"><a href="#用一坨坨代码实现" class="headerlink" title="用一坨坨代码实现"></a>用一坨坨代码实现</h2><h3 id="工程结构"><a href="#工程结构" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-3-01└── src    └── main        └── java            └── org.itstack.demo.design                └── DecorationPackageController.java</code></pre><p>一个类几千行的代码你是否见过，嚯？那今天就让你见识一下有这样潜质的类！</p><h3 id="ifelse实现需求"><a href="#ifelse实现需求" class="headerlink" title="ifelse实现需求"></a>ifelse实现需求</h3><pre><code>public class DecorationPackageController {    public String getMatterList(BigDecimal area, Integer level) {        List&lt;Matter&gt; list = new ArrayList&lt;Matter&gt;(); // 装修清单        BigDecimal price = BigDecimal.ZERO;          // 装修价格        // 豪华欧式        if (1 == level) {            LevelTwoCeiling levelTwoCeiling = new LevelTwoCeiling(); // 吊顶，二级顶            DuluxCoat duluxCoat = new DuluxCoat();                   // 涂料，多乐士            ShengXiangFloor shengXiangFloor = new ShengXiangFloor(); // 地板，圣象            list.add(levelTwoCeiling);            list.add(duluxCoat);            list.add(shengXiangFloor);            price = price.add(area.multiply(new BigDecimal(&quot;0.2&quot;)).multiply(levelTwoCeiling.price()));            price = price.add(area.multiply(new BigDecimal(&quot;1.4&quot;)).multiply(duluxCoat.price()));            price = price.add(area.multiply(shengXiangFloor.price()));        }        // 轻奢田园        if (2 == level) {            LevelTwoCeiling levelTwoCeiling = new LevelTwoCeiling(); // 吊顶，二级顶            LiBangCoat liBangCoat = new LiBangCoat();                // 涂料，立邦            MarcoPoloTile marcoPoloTile = new MarcoPoloTile();       // 地砖，马可波罗            list.add(levelTwoCeiling);            list.add(liBangCoat);            list.add(marcoPoloTile);            price = price.add(area.multiply(new BigDecimal(&quot;0.2&quot;)).multiply(levelTwoCeiling.price()));            price = price.add(area.multiply(new BigDecimal(&quot;1.4&quot;)).multiply(liBangCoat.price()));            price = price.add(area.multiply(marcoPoloTile.price()));        }        // 现代简约        if (3 == level) {            LevelOneCeiling levelOneCeiling = new LevelOneCeiling();  // 吊顶，二级顶            LiBangCoat liBangCoat = new LiBangCoat();                 // 涂料，立邦            DongPengTile dongPengTile = new DongPengTile();           // 地砖，东鹏            list.add(levelOneCeiling);            list.add(liBangCoat);            list.add(dongPengTile);            price = price.add(area.multiply(new BigDecimal(&quot;0.2&quot;)).multiply(levelOneCeiling.price()));            price = price.add(area.multiply(new BigDecimal(&quot;1.4&quot;)).multiply(liBangCoat.price()));            price = price.add(area.multiply(dongPengTile.price()));        }        StringBuilder detail = new StringBuilder(&quot;\r\n-------------------------------------------------------\r\n&quot; +                &quot;装修清单&quot; + &quot;\r\n&quot; +                &quot;套餐等级：&quot; + level + &quot;\r\n&quot; +                &quot;套餐价格：&quot; + price.setScale(2, BigDecimal.ROUND_HALF_UP) + &quot; 元\r\n&quot; +                &quot;房屋面积：&quot; + area.doubleValue() + &quot; 平米\r\n&quot; +                &quot;材料清单：\r\n&quot;);        for (Matter matter: list) {            detail.append(matter.scene()).append(&quot;：&quot;).append(matter.brand()).append(&quot;、&quot;).append(matter.model()).append(&quot;、平米价格：&quot;).append(matter.price()).append(&quot; 元。\n&quot;);        }        return detail.toString();    }}</code></pre><p>首先这段代码所要解决的问题就是接收入参；装修面积(area)、装修等级(level)，根据不同类型的装修等级选择不同的材料。</p><p>其次在实现过程中可以看到每一段if块里，都包含着不通的材料(吊顶，二级顶、涂料，立邦、地砖，马可波罗)，最终生成装修清单和装修成本。</p><p>最后提供获取装修详细信息的方法，返回给调用方，用于知道装修清单。</p><h2 id="建造者模式重构代码"><a href="#建造者模式重构代码" class="headerlink" title="建造者模式重构代码"></a>建造者模式重构代码</h2><p>建造者模式主要解决的问题是在软件系统中，有时候面临着”一个复杂对象”的创建工作，其通常由各个部分的子对象用一定的过程构成；由于需求的变化，这个复杂对象的各个部分经常面临着重大的变化，但是将它们组合在一起的过程却相对稳定。</p><p>这里我们会把构建的过程交给创建者类，而创建者通过使用我们的构建工具包，去构建出不同的装修套餐。</p><h3 id="工程结构-1"><a href="#工程结构-1" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-3-02└── src    ├── main    │   └── java    │       └── org.itstack.demo.design    │           ├── Builder.java        │           ├── DecorationPackageMenu.java    │           └── IMenu.java     └── test         └── java             └── org.itstack.demo.design.test                 └── ApiTest.java</code></pre><p><img src="/2021/04/03/2021-04-03-design-jian-zao-zhe/itstack-demo-design-3-03.png" alt></p><p>工程中有三个核心类和一个测试类，核心类是建造者模式的具体实现。与ifelse实现方式相比，多出来了两个二外的类。具体功能如下；</p><ul><li>Builder，建造者类具体的各种组装由此类实现。</li><li>DecorationPackageMenu，是IMenu接口的实现类，主要是承载建造过程中的填充器。相当于这是一套承载物料和创建者中间衔接的内容。</li></ul><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="定义装修包接口"><a href="#定义装修包接口" class="headerlink" title="定义装修包接口"></a>定义装修包接口</h4><pre><code>public interface IMenu {    IMenu appendCeiling(Matter matter); // 吊顶    IMenu appendCoat(Matter matter);    // 涂料    IMenu appendFloor(Matter matter);   // 地板    IMenu appendTile(Matter matter);    // 地砖    String getDetail();                 // 明细 }</code></pre><p>接口类中定义了填充各项物料的方法；吊顶、涂料、地板、地砖，以及最终提供获取全部明细的方法。</p><h4 id="装修包实现"><a href="#装修包实现" class="headerlink" title="装修包实现"></a>装修包实现</h4><pre><code>public class DecorationPackageMenu implements IMenu {    private List&lt;Matter&gt; list = new ArrayList&lt;Matter&gt;();  // 装修清单    private BigDecimal price = BigDecimal.ZERO;      // 装修价格    private BigDecimal area;  // 面积    private String grade;     // 装修等级；豪华欧式、轻奢田园、现代简约    private DecorationPackageMenu() {    }    public DecorationPackageMenu(Double area, String grade) {        this.area = new BigDecimal(area);        this.grade = grade;    }    public IMenu appendCeiling(Matter matter) {        list.add(matter);        price = price.add(area.multiply(new BigDecimal(&quot;0.2&quot;)).multiply(matter.price()));        return this;    }    public IMenu appendCoat(Matter matter) {        list.add(matter);        price = price.add(area.multiply(new BigDecimal(&quot;1.4&quot;)).multiply(matter.price()));        return this;    }    public IMenu appendFloor(Matter matter) {        list.add(matter);        price = price.add(area.multiply(matter.price()));        return this;    }    public IMenu appendTile(Matter matter) {        list.add(matter);        price = price.add(area.multiply(matter.price()));        return this;    }    public String getDetail() {        StringBuilder detail = new StringBuilder(&quot;\r\n-------------------------------------------------------\r\n&quot; +                &quot;装修清单&quot; + &quot;\r\n&quot; +                &quot;套餐等级：&quot; + grade + &quot;\r\n&quot; +                &quot;套餐价格：&quot; + price.setScale(2, BigDecimal.ROUND_HALF_UP) + &quot; 元\r\n&quot; +                &quot;房屋面积：&quot; + area.doubleValue() + &quot; 平米\r\n&quot; +                &quot;材料清单：\r\n&quot;);        for (Matter matter: list) {            detail.append(matter.scene()).append(&quot;：&quot;).append(matter.brand()).append(&quot;、&quot;).append(matter.model()).append(&quot;、平米价格：&quot;).append(matter.price()).append(&quot; 元。\n&quot;);        }        return detail.toString();    }}</code></pre><p>装修包的实现中每一个方法都会了 this，也就可以非常方便的用于连续填充各项物料。</p><p>同时在填充时也会根据物料计算平米数下的报价，吊顶和涂料按照平米数适量乘以常熟计算。</p><p>最后同样提供了统一的获取装修清单的明细方法。</p><h3 id="建造者方法"><a href="#建造者方法" class="headerlink" title="建造者方法"></a>建造者方法</h3><pre><code>public class Builder {    public IMenu levelOne(Double area) {        return new DecorationPackageMenu(area, &quot;豪华欧式&quot;)                .appendCeiling(new LevelTwoCeiling())    // 吊顶，二级顶                .appendCoat(new DuluxCoat())             // 涂料，多乐士                .appendFloor(new ShengXiangFloor());     // 地板，圣象    }    public IMenu levelTwo(Double area){        return new DecorationPackageMenu(area, &quot;轻奢田园&quot;)                .appendCeiling(new LevelTwoCeiling())   // 吊顶，二级顶                .appendCoat(new LiBangCoat())           // 涂料，立邦                .appendTile(new MarcoPoloTile());       // 地砖，马可波罗    }    public IMenu levelThree(Double area){        return new DecorationPackageMenu(area, &quot;现代简约&quot;)                .appendCeiling(new LevelOneCeiling())   // 吊顶，二级顶                .appendCoat(new LiBangCoat())           // 涂料，立邦                .appendTile(new DongPengTile());        // 地砖，东鹏    }}</code></pre><p>建造者的使用中就已经非常容易了，统一的建造方式，通过不同物料填充出不同的装修风格；豪华欧式、轻奢田园、现代简约，如果将来业务扩展也可以将这部分内容配置到数据库自动生成。但整体的思想还可以使用创建者模式进行搭建。</p><h3 id="测试验证"><a href="#测试验证" class="headerlink" title="测试验证"></a>测试验证</h3><pre><code>@Testpublic void test_Builder(){    Builder builder = new Builder();    // 豪华欧式    System.out.println(builder.levelOne(132.52D).getDetail());    // 轻奢田园    System.out.println(builder.levelTwo(98.25D).getDetail());    // 现代简约    System.out.println(builder.levelThree(85.43D).getDetail());}</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>通过上面对建造者模式的使用，已经可以摸索出一点心得。那就是什么时候会选择这样的设计模式，当：一些基本物料不会变，而其组合经常变化的时候，就可以选择这样的设计模式来构建代码。</li><li>此设计模式满足了单一职责原则以及可复用的技术、建造者独立、易扩展、便于控制细节风险。但同时当出现特别多的物料以及很多的组合后，类的不断扩展也会造成难以维护的问题。但这种设计结构模型可以把重复的内容抽象到数据库中，按照需要配置。这样就可以减少代码中大量的重复。</li><li>建造者模式所完成的内容就是通过将多个简单对象通过一步步的组装构建出一个复杂对象的过程。</li><li>就是操作不断的返回自身对象，然后返回的自身对象又不断的操作。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>抽象模式</title>
      <link href="/2021/04/02/2021-04-02-design-chou-xiang/"/>
      <url>/2021/04/02/2021-04-02-design-chou-xiang/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="抽象工厂模式介绍"><a href="#抽象工厂模式介绍" class="headerlink" title="抽象工厂模式介绍"></a>抽象工厂模式介绍</h2><p>抽象工厂模式与工厂方法模式虽然主要意图都是为了解决，接口选择问题。但在实现上，抽象工厂是一个中心工厂，创建其他工厂的模式。</p><p>可能在平常的业务开发中很少关注这样的设计模式或者类似的代码结构，但是这种场景确一直在我们身边，例如：</p><ul><li><p>不同系统内的回车换行 </p><p>  Unix系统里，每行结尾只有 <strong>&lt;换行&gt;</strong>，即 <code>\n</code>；  Windows系统里面，每行结尾是 <strong>&lt;换行&gt;&lt;回车&gt;</strong>，即 <code>\n\r</code>；  Mac系统里，每行结尾是 <strong>&lt;回车&gt;</strong></p></li><li><p>IDEA 开发工具的差异展示(Win\Mac)</p><p>  <img src="/2021/04/02/2021-04-02-design-chou-xiang/itstack-demo-design-2-02.png" alt></p></li></ul><p>我们的业务开发中时常也会遇到类似的问题，需要兼容做处理。</p><h2 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h2><p><img src="/2021/04/02/2021-04-02-design-chou-xiang/itstack-demo-design-2-03.png" alt></p><p>预估QPS较低、系统压力较小、并发访问不大、近一年没有大动作等等，在考虑时间投入成本的前提前，并不会投入特别多的人力去构建非常完善的系统。就像对 Redis 的使用，往往可能只要是单机的就可以满足现状。</p><p>但随着业务超过预期的快速发展，系统的负载能力也要随着跟上。原有的单机 Redis 已经满足不了系统需求。这时候就需要更换为更为健壮的Redis集群服务，虽然需要修改但是不能影响目前系统的运行，还要平滑过渡过去。</p><p>随着这次的升级，可以预见的问题会有:</p><ul><li>很多服务用到了Redis需要一起升级到集群。</li><li>需要兼容集群A和集群B，便于后续的灾备。</li><li>两套集群提供的接口和方法各有差异，需要做适配。</li><li>不能影响到目前正常运行的系统。</li></ul><h3 id="场景模拟工程"><a href="#场景模拟工程" class="headerlink" title="场景模拟工程"></a>场景模拟工程</h3><pre><code>itstack-demo-design-2-00└── src    └── main        └── java            └── org.itstack.demo.design                ├── matter                │   ├── EGM.java                │   └── IIR.java                └── RedisUtils.java</code></pre><h3 id="场景简述"><a href="#场景简述" class="headerlink" title="场景简述"></a>场景简述</h3><h4 id="模拟单机服务-RedisUtils"><a href="#模拟单机服务-RedisUtils" class="headerlink" title="模拟单机服务 RedisUtils"></a>模拟单机服务 RedisUtils</h4><p><img src="/2021/04/02/2021-04-02-design-chou-xiang/itstack-demo-design-2-04.png" alt></p><ul><li>模拟Redis功能，也就是假定目前所有的系统都在使用的服务</li><li>类和方法名次都固定写死到各个业务系统中，改动略微麻烦</li></ul><h4 id="模拟集群-EGM"><a href="#模拟集群-EGM" class="headerlink" title="模拟集群 EGM"></a>模拟集群 EGM</h4><p><img src="/2021/04/02/2021-04-02-design-chou-xiang/itstack-demo-design-2-05.png" alt></p><p>模拟一个集群服务，但是方法名与各业务系统中使用的方法名不同。有点像你mac，我用win。做一样的事，但有不同的操作。</p><h4 id="模拟集群-IIR"><a href="#模拟集群-IIR" class="headerlink" title="模拟集群 IIR"></a>模拟集群 IIR</h4><p><img src="/2021/04/02/2021-04-02-design-chou-xiang/itstack-demo-design-2-06.png" alt></p><p>这是另外一套集群服务，有时候在企业开发中就很有可能出现两套服务，这里我们也是为了做模拟案例，所以添加两套实现同样功能的不同服务，来学习抽象工厂模式。</p><p>综上可以看到，我们目前的系统中已经在大量的使用redis服务，但是因为系统不能满足业务的快速发展，因此需要迁移到集群服务中。而这时有两套集群服务需要兼容使用，又要满足所有的业务系统改造的同时不影响线上使用。</p><h3 id="单集群代码使用"><a href="#单集群代码使用" class="headerlink" title="单集群代码使用"></a>单集群代码使用</h3><p>以下是案例模拟中原有的单集群Redis使用方式，后续会通过对这里的代码进行改造。</p><p><img src="/2021/04/02/2021-04-02-design-chou-xiang/itstack-demo-design-2-07.png" alt></p><h4 id="定义使用接口"><a href="#定义使用接口" class="headerlink" title="定义使用接口"></a>定义使用接口</h4><pre><code>public interface CacheService {    String get(final String key);    void set(String key, String value);    void set(String key, String value, long timeout, TimeUnit timeUnit);    void del(String key);}</code></pre><h4 id="实现调用代码"><a href="#实现调用代码" class="headerlink" title="实现调用代码"></a>实现调用代码</h4><pre><code>public class CacheServiceImpl implements CacheService {    private RedisUtils redisUtils = new RedisUtils();    public String get(String key) {        return redisUtils.get(key);    }    public void set(String key, String value) {        redisUtils.set(key, value);    }    public void set(String key, String value, long timeout, TimeUnit timeUnit) {        redisUtils.set(key, value, timeout, timeUnit);    }    public void del(String key) {        redisUtils.del(key);    }}</code></pre><h2 id="用一坨坨代码实现"><a href="#用一坨坨代码实现" class="headerlink" title="用一坨坨代码实现"></a>用一坨坨代码实现</h2><blockquote><p>讲道理没有ifelse解决不了的逻辑，不行就在加一行！哈哈</p></blockquote><p>此时的实现方式并不会修改类结构图，也就是与上面给出的类层级关系一致。通过在接口中添加类型字段区分当前使用的是哪个集群，来作为使用的判断。可以说目前的方式非常难用，其他使用方改动颇多，这里只是做为例子。</p><h3 id="工程结构"><a href="#工程结构" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-2-01└── src    └── main        └── java            └── org.itstack.demo.design                ├── impl                │   └── CacheServiceImpl.java                └── CacheService.java</code></pre><p>此时的只有两个类，类结构非常简单。而我们需要的补充扩展功能也只是在 CacheServiceImpl 中实现。</p><h3 id="ifelse实现需求"><a href="#ifelse实现需求" class="headerlink" title="ifelse实现需求"></a>ifelse实现需求</h3><pre><code>public class CacheServiceImpl implements CacheService {    private RedisUtils redisUtils = new RedisUtils();    private EGM egm = new EGM();    private IIR iir = new IIR();    public String get(String key, int redisType) {        if (1 == redisType) {            return egm.gain(key);        }        if (2 == redisType) {            return iir.get(key);        }        return redisUtils.get(key);    }    public void set(String key, String value, int redisType) {        if (1 == redisType) {            egm.set(key, value);            return;        }        if (2 == redisType) {            iir.set(key, value);            return;        }        redisUtils.set(key, value);    }    //... 同类不做太多展示}</code></pre><p>这里的实现过程非常简单，主要根据类型判断是哪个Redis集群。</p><p>虽然实现是简单了，但是对使用者来说就麻烦了，并且也很难应对后期的拓展和不停的维护。</p><h2 id="抽象工厂模式重构代码"><a href="#抽象工厂模式重构代码" class="headerlink" title="抽象工厂模式重构代码"></a>抽象工厂模式重构代码</h2><p>这里的抽象工厂的创建和获取方式，会采用代理类的方式进行实现。所被代理的类就是目前的Redis操作方法类，让这个类在不需要任何修改下，就可以实现调用集群A和集群B的数据服务。</p><p>并且这里还有一点非常重要，由于集群A和集群B在部分方法提供上是不同的，因此需要做一个接口适配，而这个适配类就相当于工厂中的工厂，用于创建把不同的服务抽象为统一的接口做相同的业务。</p><h3 id="工程结构-1"><a href="#工程结构-1" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-2-02└── src    ├── main    │   └── java    │       └── org.itstack.demo.design    │           ├── factory        │           │   ├── impl    │           │   │   ├── EGMCacheAdapter.java     │           │   │   └── IIRCacheAdapter.java    │           │   ├── ICacheAdapter.java    │           │   ├── JDKInvocationHandler.java    │           │   └── JDKProxy.java    │           ├── impl    │           │   └── CacheServiceImpl.java        │           └── CacheService.java     └── test         └── java             └── org.itstack.demo.design.test                 └── ApiTest.java</code></pre><h3 id="抽象工厂模型结构"><a href="#抽象工厂模型结构" class="headerlink" title="抽象工厂模型结构"></a>抽象工厂模型结构</h3><p><img src="/2021/04/02/2021-04-02-design-chou-xiang/itstack-demo-design-2-08.png" alt></p><ul><li>ICacheAdapter，定义了适配接口，分别包装两个集群中差异化的接口名称。EGMCacheAdapter、IIRCacheAdapter</li><li>JDKProxy、JDKInvocationHandler，是代理类的定义和实现，这部分也就是抽象工厂的另外一种实现方式。通过这样的方式可以很好的把原有操作Redis的方法进行代理操作，通过控制不同的入参对象，控制缓存的使用。</li></ul><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="定义适配接口"><a href="#定义适配接口" class="headerlink" title="定义适配接口"></a>定义适配接口</h4><pre><code>public interface ICacheAdapter {    String get(String key);    void set(String key, String value);    void set(String key, String value, long timeout, TimeUnit timeUnit);    void del(String key);}</code></pre><p>这个类的主要作用是让所有集群的提供方，能在统一的方法名称下进行操作。也方面后续的拓展。</p><h4 id="实现集群使用服务"><a href="#实现集群使用服务" class="headerlink" title="实现集群使用服务"></a>实现集群使用服务</h4><pre><code>public class EGMCacheAdapter implements ICacheAdapter {    private EGM egm = new EGM();    public String get(String key) {        return egm.gain(key);    }    public void set(String key, String value) {        egm.set(key, value);    }    public void set(String key, String value, long timeout, TimeUnit timeUnit) {        egm.setEx(key, value, timeout, timeUnit);    }    public void del(String key) {        egm.delete(key);    }}</code></pre><pre><code>public class IIRCacheAdapter implements ICacheAdapter {    private IIR iir = new IIR();    public String get(String key) {        return iir.get(key);    }    public void set(String key, String value) {        iir.set(key, value);    }    public void set(String key, String value, long timeout, TimeUnit timeUnit) {        iir.setExpire(key, value, timeout, timeUnit);    }    public void del(String key) {        iir.del(key);    }}</code></pre><p>以上两个实现都非常容易，在统一方法名下进行包装。</p><h4 id="定义抽象工程代理类和实现"><a href="#定义抽象工程代理类和实现" class="headerlink" title="定义抽象工程代理类和实现"></a>定义抽象工程代理类和实现</h4><p>JDKProxy:</p><pre><code>public static &lt;T&gt; T getProxy(Class&lt;T&gt; interfaceClass, ICacheAdapter cacheAdapter) throws Exception {    InvocationHandler handler = new JDKInvocationHandler(cacheAdapter);    ClassLoader classLoader = Thread.currentThread().getContextClassLoader();    Class&lt;?&gt;[] classes = interfaceClass.getInterfaces();    return (T) Proxy.newProxyInstance(classLoader, new Class[]{classes[0]}, handler);}</code></pre><p>这里主要的作用就是完成代理类，同时对于使用哪个集群有外部通过入参进行传递。</p><p>JDKInvocationHandler:</p><pre><code>public class JDKInvocationHandler implements InvocationHandler {    private ICacheAdapter cacheAdapter;    public JDKInvocationHandler(ICacheAdapter cacheAdapter) {        this.cacheAdapter = cacheAdapter;    }    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {        return ICacheAdapter.class.getMethod(method.getName(), ClassLoaderUtils.getClazzByArgs(args)).invoke(cacheAdapter, args);    }}</code></pre><p>在代理类的实现中其实也非常简单，通过穿透进来的集群服务进行方法操作。</p><p>另外在invoke中通过使用获取方法名称反射方式，调用对应的方法功能，也就简化了整体的使用。</p><h3 id="测试验证"><a href="#测试验证" class="headerlink" title="测试验证"></a>测试验证</h3><pre><code>@Testpublic void test_CacheService() throws Exception {    CacheService proxy_EGM = JDKProxy.getProxy(CacheServiceImpl.class, new EGMCacheAdapter());    proxy_EGM.set(&quot;user_name_01&quot;,&quot;小哥&quot;);    String val01 = proxy_EGM.get(&quot;user_name_01&quot;);    System.out.println(val01);    CacheService proxy_IIR = JDKProxy.getProxy(CacheServiceImpl.class, new IIRCacheAdapter());    proxy_IIR.set(&quot;user_name_01&quot;,&quot;小哥&quot;);    String val02 = proxy_IIR.get(&quot;user_name_01&quot;);    System.out.println(val02);}</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li><p>抽象工厂模式，所要解决的问题就是在一个产品族，存在多个不同类型的产品(Redis集群、操作系统)情况下，接口选择的问题。而这种场景在业务开发中也是非常多见的，只不过可能有时候没有将它们抽象化出来。</p></li><li><p>当你知道什么场景下何时可以被抽象工程优化代码，那么你的代码层级结构以及满足业务需求上，都可以得到很好的完成功能实现并提升扩展性和优雅度。</p></li><li><p>那么这个设计模式满足了；单一职责、开闭原则、解耦等优点，但如果说随着业务的不断拓展，可能会造成类实现上的复杂度。但也可以说算不上缺点，因为可以随着其他设计方式的引入和代理类以及自动生成加载的方式降低此项缺点。</p></li><li><p>抽象工厂模式与工厂方法模式虽然主要意图都是为了解决，接口选择问题。但在实现上，抽象工厂是一个中心工厂，创建其他工厂的模式。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>工厂模式</title>
      <link href="/2021/04/01/2021-04-01-design-gong-han/"/>
      <url>/2021/04/01/2021-04-01-design-gong-han/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="工厂方法模式介绍"><a href="#工厂方法模式介绍" class="headerlink" title="工厂方法模式介绍"></a>工厂方法模式介绍</h2><p>工厂模式又称工厂方法模式，是一种创建型设计模式，其在父类中提供一个创建对象的方法， 允许子类决定实例化对象的类型。</p><p>这种设计模式也是 Java 开发中最常见的一种模式，它的主要意图是定义一个创建对象的接口，让其子类自己决定实例化哪一个工厂类，工厂模式使其创建过程延迟到子类进行。</p><p>简单说就是为了提供代码结构的扩展性，屏蔽每一个功能类中的具体实现逻辑。让外部可以更加简单的只是知道调用即可，同时，这也是去掉众多ifelse的方式。当然这可能也有一些缺点，比如需要实现的类非常多，如何去维护，怎样减低开发成本。但这些问题都可以在后续的设计模式结合使用中，逐步降低。</p><h2 id="模拟发奖多种商品"><a href="#模拟发奖多种商品" class="headerlink" title="模拟发奖多种商品"></a>模拟发奖多种商品</h2><p><img src="/2021/04/01/2021-04-01-design-gong-han/itstack-demo-design-1-02.png" alt></p><p>由于营销场景的复杂、多变、临时的特性，它所需要的设计需要更加深入，否则会经常面临各种紧急CRUD操作，从而让代码结构混乱不堪，难以维护。</p><p>那么在这里我们模拟积分兑换中的发放多种类型商品，假如现在我们有如下三种类型的商品接口；</p><p>![](Image 1.png)</p><p>从以上接口来看有如下信息：</p><ul><li>三个接口返回类型不同，有对象类型、布尔类型、还有一个空类型。</li><li>入参不同，发放优惠券需要仿重、兑换卡需要卡ID、实物商品需要发货位置(对象中含有)。</li><li>另外可能会随着后续的业务的发展，会新增其他种商品类型。因为你所有的开发需求都是随着业务对市场的拓展而带来的。</li></ul><h2 id="用一坨坨代码实现"><a href="#用一坨坨代码实现" class="headerlink" title="用一坨坨代码实现"></a>用一坨坨代码实现</h2><p>如果不考虑任何扩展性，只为了尽快满足需求，那么对这么几种奖励发放只需使用ifelse语句判断，调用不同的接口即可满足需求。</p><h3 id="工程结构"><a href="#工程结构" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-1-01└── src    ├── main    │   └── java    │       └── org.itstack.demo.design    │           ├── AwardReq.java    │           ├── AwardRes.java    │           └── PrizeController.java     └── test         └── java             └── org.itstack.demo.design.test                 └── ApiTest.java</code></pre><p>工程结构上非常简单，一个入参对象 AwardReq 、一个出参对象 AwardRes，以及一个接口类 PrizeController</p><h3 id="ifelse实现需求"><a href="#ifelse实现需求" class="headerlink" title="ifelse实现需求"></a>ifelse实现需求</h3><pre><code>public class PrizeController {    private Logger logger = LoggerFactory.getLogger(PrizeController.class);    public AwardRes awardToUser(AwardReq req) {        String reqJson = JSON.toJSONString(req);        AwardRes awardRes = null;        try {            logger.info(&quot;奖品发放开始{}。req:{}&quot;, req.getuId(), reqJson);            // 按照不同类型方法商品[1优惠券、2实物商品、3第三方兑换卡(爱奇艺)]            if (req.getAwardType() == 1) {                CouponService couponService = new CouponService();                CouponResult couponResult = couponService.sendCoupon(req.getuId(), req.getAwardNumber(), req.getBizId());                if (&quot;0000&quot;.equals(couponResult.getCode())) {                    awardRes = new AwardRes(&quot;0000&quot;, &quot;发放成功&quot;);                } else {                    awardRes = new AwardRes(&quot;0001&quot;, couponResult.getInfo());                }            } else if (req.getAwardType() == 2) {                GoodsService goodsService = new GoodsService();                DeliverReq deliverReq = new DeliverReq();                deliverReq.setUserName(queryUserName(req.getuId()));                deliverReq.setUserPhone(queryUserPhoneNumber(req.getuId()));                deliverReq.setSku(req.getAwardNumber());                deliverReq.setOrderId(req.getBizId());                deliverReq.setConsigneeUserName(req.getExtMap().get(&quot;consigneeUserName&quot;));                deliverReq.setConsigneeUserPhone(req.getExtMap().get(&quot;consigneeUserPhone&quot;));                deliverReq.setConsigneeUserAddress(req.getExtMap().get(&quot;consigneeUserAddress&quot;));                Boolean isSuccess = goodsService.deliverGoods(deliverReq);                if (isSuccess) {                    awardRes = new AwardRes(&quot;0000&quot;, &quot;发放成功&quot;);                } else {                    awardRes = new AwardRes(&quot;0001&quot;, &quot;发放失败&quot;);                }            } else if (req.getAwardType() == 3) {                String bindMobileNumber = queryUserPhoneNumber(req.getuId());                IQiYiCardService iQiYiCardService = new IQiYiCardService();                iQiYiCardService.grantToken(bindMobileNumber, req.getAwardNumber());                awardRes = new AwardRes(&quot;0000&quot;, &quot;发放成功&quot;);            }            logger.info(&quot;奖品发放完成{}。&quot;, req.getuId());        } catch (Exception e) {            logger.error(&quot;奖品发放失败{}。req:{}&quot;, req.getuId(), reqJson, e);            awardRes = new AwardRes(&quot;0001&quot;, e.getMessage());        }        return awardRes;    }    private String queryUserName(String uId) {        return &quot;花花&quot;;    }    private String queryUserPhoneNumber(String uId) {        return &quot;15200101232&quot;;    }}</code></pre><p>如上就是使用 ifelse 非常直接的实现出来业务需求的一坨代码，如果仅从业务角度看，研发如期甚至提前实现了功能。</p><p>那这样的代码目前来看并不会有什么问题，但如果在经过几次的迭代和拓展，接手这段代码的研发将十分痛苦。重构成本高需要理清之前每一个接口的使用，测试回归验证时间长，需要全部验证一次。这也就是很多人并不愿意接手别人的代码，如果接手了又被压榨开发时间。那么可想而知这样的 ifelse 还会继续增加。</p><h2 id="工厂模式优化代码"><a href="#工厂模式优化代码" class="headerlink" title="工厂模式优化代码"></a>工厂模式优化代码</h2><p>接下来使用工厂方法模式来进行代码优化，也算是一次很小的重构。整理重构会你会发现代码结构清晰了、也具备了下次新增业务需求的扩展性。但在实际使用中还会对此进行完善，目前的只是抽离出最核心的部分体现到你面前，方便学习。</p><h3 id="工程结构-1"><a href="#工程结构-1" class="headerlink" title="工程结构"></a>工程结构</h3><pre><code>itstack-demo-design-1-02└── src    ├── main    │   └── java    │       └── org.itstack.demo.design    │           ├── store        │           │   ├── impl    │           │   │   ├── CardCommodityService.java    │           │   │   ├── CouponCommodityService.java     │           │   │   └── GoodsCommodityService.java      │           │   └── ICommodity.java    │           └── StoreFactory.java     └── test         └── java             └── org.itstack.demo.design.test                 └── ApiTest.java</code></pre><p>首先，从上面的工程结构中你是否一些感觉，比如；它看上去清晰了、这样分层可以更好扩展了、似乎可以想象到每一个类做了什么。</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="定义发奖接口"><a href="#定义发奖接口" class="headerlink" title="定义发奖接口"></a>定义发奖接口</h4><pre><code>public interface ICommodity {    void sendCommodity(JSONObject req);    int getType();}</code></pre><p>所有的奖品无论是实物、虚拟还是第三方，都需要通过我们的程序实现此接口进行处理，以保证最终入参出参的统一性。</p><p>固定的入参和返回结果。</p><h4 id="实现奖品发放接口"><a href="#实现奖品发放接口" class="headerlink" title="实现奖品发放接口"></a>实现奖品发放接口</h4><p>优惠券:</p><pre><code>public class CouponCommodityService implements ICommodity{    @Override    public void sendCommodity(JSONObject req) {        // todo        System.out.println(&quot;------&quot;);    }    @Override    public int getType() {        return 0;    }}</code></pre><p>实物商品:</p><pre><code>public class GoodsCommodityService implements ICommodity{    @Override    public void sendCommodity(JSONObject req){        // todo    }    @Override    public int getType() {        return 1;    }}</code></pre><p>第三方兑换卡:</p><pre><code>public class CardCommodityService implements ICommodity{    @Override    public void sendCommodity(JSONObject req) {        // todo    }    @Override    public int getType() {        return 2;    }}</code></pre><p>从上面可以看到每一种奖品的实现都包括在自己的类中，新增、修改或者删除都不会影响其他奖品功能的测试，降低回归测试的可能。</p><p>后续在新增的奖品只需要按照此结构进行填充即可，非常易于维护和扩展。</p><p>在统一了入参以及出参后，调用方不在需要关心奖品发放的内部逻辑，按照统一的方式即可处理。</p><h3 id="创建商店工厂"><a href="#创建商店工厂" class="headerlink" title="创建商店工厂"></a>创建商店工厂</h3><pre><code>public class CommodityFactory {    private Map&lt;Integer, ICommodity&gt; map;    public CommodityFactory() {        List&lt;ICommodity&gt; strategies = new ArrayList&lt;&gt;();        strategies.add(new CouponCommodityService());        strategies.add(new GoodsCommodityService());        strategies.add(new CardCommodityService());        map = strategies.stream().collect(Collectors.toMap(ICommodity::getType, strategy -&gt; strategy));    }    public static class Holder {        public static CommodityFactory instance = new CommodityFactory();    }    public static CommodityFactory getInstance() {        return Holder.instance;    }    public ICommodity get(Integer type) {        return map.get(type);    }}</code></pre><p>这里我们定义了一个商店的工厂类，在里面按照类型实现各种商品的服务。可以非常干净整洁的处理你的代码，后续新增的商品在这里扩展即可。</p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><pre><code>public class FactoryTest {    public static void main(String[] args) {        ICommodity strategy = CommodityFactory.getInstance().get(0);        strategy.sendCommodity(new JSONObject());    }}</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>从上到下的优化来看，工厂方法模式并不复杂，甚至这样的开发结构在你有所理解后，会发现更加简单了。</li><li>那么这样的开发的好处知道后，也可以总结出来它的优点；<strong>避免创建者与具体的产品逻辑耦合、满足单一职责，每一个业务逻辑实现都在所属自己的类中完成、满足开闭原则，无需更改使用调用方就可以在程序中引入新的产品类型</strong>。但这样也会带来一些问题，比如有非常多的奖品类型，那么实现的子类会极速扩张。因此也需要使用其他的模式进行优化，这些在后续的设计模式中会逐步涉及到。</li><li>工厂模式又称工厂方法模式，是一种创建型设计模式，其在父类中提供一个创建对象的方法， 允许子类决定实例化对象的类型。</li><li>就是根据判断找出多个实现中的一个。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>存储引擎</title>
      <link href="/2021/03/27/2021-03-27-mysql-38/"/>
      <url>/2021/03/27/2021-03-27-mysql-38/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="内存表的数据组织结构"><a href="#内存表的数据组织结构" class="headerlink" title="内存表的数据组织结构"></a>内存表的数据组织结构</h2><p>假设有以下的两张表 t1 和 t2，其中表 t1 使用 Memory 引擎， 表 t2 使用 InnoDB 引擎。</p><pre><code>create table t1(id int primary key, c int) engine=Memory;create table t2(id int primary key, c int) engine=innodb;insert into t1 values(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9),(0,0);insert into t2 values(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9),(0,0);</code></pre><p>分别执行 select * from t1 和 select * from t2。</p><p><img src="/2021/03/27/2021-03-27-mysql-38/3fb1100b6e3390357d4efff0ba4765e6.png" alt></p><p>可以看到，内存表 t1 的返回结果里面 0 在最后一行，而 InnoDB 表 t2 的返回结果里 0 在第一行。</p><p>出现这个区别的原因，要从这两个引擎的主键索引的组织方式说起。</p><p>表 t2 用的是 InnoDB 引擎，它的主键索引 id 的组织方式，你已经很熟悉了：InnoDB 表的数据就放在主键索引树上，主键索引是 B+ 树。所以表 t2 的数据组织方式如下图所示：</p><p><img src="/2021/03/27/2021-03-27-mysql-38/4e29e4f9db55ace6ab09161c68ad8c8d.jpg" alt></p><p>主键索引上的值是有序存储的。在执行 select * 的时候，就会按照叶子节点从左到右扫描，所以得到的结果里，0 就出现在第一行。</p><p>与 InnoDB 引擎不同，Memory 引擎的数据和索引是分开的。我们来看一下表 t1 中的数据内容。</p><p><img src="/2021/03/27/2021-03-27-mysql-38/dde03e92074cecba4154d30cd16a9684.jpg" alt></p><p>可以看到，内存表的数据部分以数组的方式单独存放，而主键 id 索引里，存的是每个数据的位置。主键 id 是 hash 索引，可以看到索引上的 key 并不是有序的。</p><p>在内存表 t1 中，当我执行 select * 的时候，走的是全表扫描，也就是顺序扫描这个数组。因此，0 就是最后一个被读到，并放入结果集的数据。</p><p>可见，InnoDB 和 Memory 引擎的数据组织方式是不同的：</p><ul><li>InnoDB 引擎把数据放在主键索引上，其他索引上保存的是主键 id。这种方式，我们称之为索引组织表（Index Organizied Table）。</li><li>而 Memory 引擎采用的是把数据单独存放，索引上保存数据位置的数据组织形式，我们称之为堆组织表（Heap Organizied Table）。</li></ul><p>从中我们可以看出，这两个引擎的一些典型不同：</p><ul><li>InnoDB 表的数据总是有序存放的，而内存表的数据就是按照写入顺序存放的；</li><li>当数据文件有空洞的时候，InnoDB 表在插入新数据的时候，为了保证数据有序性，只能在固定的位置写入新值，而内存表找到空位就可以插入新值；</li><li>数据位置发生变化的时候，InnoDB 表只需要修改主键索引，而内存表需要修改所有索引；</li><li>InnoDB 表用主键索引查询时需要走一次索引查找，用普通索引查询的时候，需要走两次索引查找。而内存表没有这个区别，所有索引的“地位”都是相同的。</li><li>InnoDB 支持变长数据类型，不同记录的长度可能不同；内存表不支持 Blob 和 Text 字段，并且即使定义了 varchar(N)，实际也当作 char(N)，也就是固定长度字符串来存储，因此内存表的每行数据长度相同。</li></ul><p>由于内存表的这些特性，每个数据行被删除以后，空出的这个位置都可以被接下来要插入的数据复用。比如，如果要在表 t1 中执行：</p><pre><code>delete from t1 where id=5;insert into t1 values(10,10);select * from t1;</code></pre><p>就会看到返回结果里，id=10 这一行出现在 id=4 之后，也就是原来 id=5 这行数据的位置。</p><p>需要指出的是，表 t1 的这个主键索引是哈希索引，因此如果执行范围查询，比如</p><pre><code>select * from t1 where id&lt;5;</code></pre><p>是用不上主键索引的，需要走全表扫描。</p><ul><li><strong>InnoDB 引擎把数据放在主键索引上，其他索引上保存的是主键 id。Memory 引擎采用的是把数据单独存放，索引上保存数据位置。</strong></li><li><strong>Memory 引擎不支持范围查询。</strong></li></ul><h2 id="hash-索引和-B-Tree-索引"><a href="#hash-索引和-B-Tree-索引" class="headerlink" title="hash 索引和 B-Tree 索引"></a>hash 索引和 B-Tree 索引</h2><p>实际上，内存表也是支持 B-Tree 索引的。在 id 列上创建一个 B-Tree 索引，SQL 语句可以这么写：</p><pre><code>alter table t1 add index a_btree_index using btree (id);</code></pre><p>这时，表 t1 的数据组织形式就变成了这样：</p><p><img src="/2021/03/27/2021-03-27-mysql-38/1788deca56cb83c114d8353c92e3bde3.jpg" alt></p><p>新增的这个 B-Tree 索引你看着就眼熟了，这跟 InnoDB 的 b+ 树索引组织形式类似。</p><p>作为对比，你可以看一下这下面这两个语句的输出：</p><p><img src="/2021/03/27/2021-03-27-mysql-38/a85808fcccab24911d257d720550328a.png" alt></p><p>可以看到，执行 select * from t1 where id&lt;5 的时候，优化器会选择 B-Tree 索引，所以返回结果是 0 到 4。 使用 force index 强行使用主键 id 这个索引，id=0 这一行就在结果集的最末尾了。</p><p>其实，一般在我们的印象中，内存表的优势是速度快，其中的一个原因就是 Memory 引擎支持 hash 索引。当然，更重要的原因是，内存表的所有数据都保存在内存，而内存的读写速度总是比磁盘快。</p><p>但是，接下来我要跟你说明，为什么我不建议你在生产环境上使用内存表。这里的原因主要包括两个方面：</p><ul><li>锁粒度问题；</li><li>数据持久化问题。</li></ul><p><strong>Memory 引擎的 hash 索引查询比InnoDB 引擎的B-Tree 索引快。</strong></p><h2 id="内存表的锁"><a href="#内存表的锁" class="headerlink" title="内存表的锁"></a>内存表的锁</h2><p>内存表不支持行锁，只支持表锁。因此，一张表只要有更新，就会堵住其他所有在这个表上的读写操作。</p><p>需要注意的是，这里的表锁跟之前我们介绍过的 MDL 锁不同，但都是表级的锁。接下来，我通过下面这个场景，跟你模拟一下内存表的表级锁。</p><p><img src="/2021/03/27/2021-03-27-mysql-38/f216e2d707559ed2ca98fbe21e509f29.png" alt></p><p>在这个执行序列里，session A 的 update 语句要执行 50 秒，在这个语句执行期间 session B 的查询会进入锁等待状态。session C 的 show processlist 结果输出如下：</p><p><img src="/2021/03/27/2021-03-27-mysql-38/14d88076dad6db573f0b66f2c17df916.png" alt></p><p>跟行锁比起来，表锁对并发访问的支持不够好。所以，内存表的锁粒度问题，决定了它在处理并发事务的时候，性能也不会太好。</p><p><strong>Memory 引擎不支持行锁，只支持表锁。</strong></p><h2 id="数据持久性问题"><a href="#数据持久性问题" class="headerlink" title="数据持久性问题"></a>数据持久性问题</h2><p>数据放在内存中，是内存表的优势，但也是一个劣势。因为，数据库重启的时候，所有的内存表都会被清空。</p><p>你可能会说，如果数据库异常重启，内存表被清空也就清空了，不会有什么问题啊。但是，在高可用架构下，内存表的这个特点简直可以当做 bug 来看待了。为什么这么说呢？</p><h3 id="先看看-M-S-架构下，使用内存表存在的问题"><a href="#先看看-M-S-架构下，使用内存表存在的问题" class="headerlink" title="先看看 M-S 架构下，使用内存表存在的问题"></a>先看看 M-S 架构下，使用内存表存在的问题</h3><p><img src="/2021/03/27/2021-03-27-mysql-38/5b910e4c0f1afa219aeecd1f291c95e9.jpg" alt="M-S 基本架构"></p><p>我们来看一下下面这个时序：</p><ol><li>业务正常访问主库；</li><li>备库硬件升级，备库重启，内存表 t1 内容被清空；</li><li>备库重启后，客户端发送一条 update 语句，修改表 t1 的数据行，这时备库应用线程就会报错“找不到要更新的行”。</li></ol><p>这样就会导致主备同步停止。当然，如果这时候发生主备切换的话，客户端会看到，表 t1 的数据“丢失”了。</p><p>这种有 proxy 的架构里，大家默认主备切换的逻辑是由数据库系统自己维护的。这样对客户端来说，就是“网络断开，重连之后，发现内存表数据丢失了”</p><p>但是，接下来内存表的这个特性就会让使用现象显得更“诡异”了。由于 MySQL 知道重启之后，内存表的数据会丢失。所以，担心主库重启之后，出现主备不一致，MySQL 在实现上做了这样一件事儿：在数据库重启之后，往 binlog 里面写入一行 DELETE FROM t1。</p><h3 id="双-M-结构"><a href="#双-M-结构" class="headerlink" title="双 M 结构"></a>双 M 结构</h3><p><img src="/2021/03/27/2021-03-27-mysql-38/4089c9c1f92ce61d2ed779fd0932ba57.jpg" alt="双 M 结构"></p><p>在备库重启的时候，备库 binlog 里的 delete 语句就会传到主库，然后把主库内存表的内容删除。这样你在使用的时候就会发现，主库的内存表数据突然被清空了。</p><p>内存表并不适合在生产环境上作为普通数据表使用。</p><p>但是内存表执行速度快呀。这个问题，其实你可以这么分析：</p><ul><li>如果你的表更新量大，那么并发度是一个很重要的参考指标，InnoDB 支持行锁，并发度比内存表好；</li><li>能放到内存表的数据量都不大。如果你考虑的是读的性能，一个读 QPS 很高并且数据量不大的表，即使是使用 InnoDB，数据也是都会缓存在 InnoDB Buffer Pool 里的。因此，使用 InnoDB 表的读性能也不会差。</li></ul><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul><li><strong>InnoDB 引擎把数据放在主键索引上，其他索引上保存的是主键 id。Memory 引擎采用的是把数据单独存放，索引上保存数据位置。</strong></li><li><strong>Memory 引擎不支持范围查询。</strong></li><li><strong>Memory 引擎的 hash 索引查询比InnoDB 引擎的B-Tree 索引快。</strong></li><li><strong>Memory 引擎不支持行锁，只支持表锁。</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>join语句优化</title>
      <link href="/2021/03/25/2021-03-25-mysql-35/"/>
      <url>/2021/03/25/2021-03-25-mysql-35/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><p>join 语句的两种算法，分别是 Index Nested-Loop Join(NLJ) 和 Block Nested-Loop Join(BNL)。</p><p>发现在使用 NLJ 算法的时候，其实效果还是不错的，比通过应用层拆分成多个语句然后再拼接查询结果更方便，而且性能也不会差。</p><p>但是，BNL 算法在大表 join 的时候性能就差多了，比较次数等于两个表参与 join 的行数的乘积，很消耗 CPU 资源。</p><p>当然了，这两个算法都还有继续优化的空间</p><p>为了便于分析，还是创建两个表 t1、t2 来展开今天的问题。</p><pre><code>create table t1(id int primary key, a int, b int, index(a));create table t2 like t1;drop procedure idata;delimiter ;;create procedure idata()begin  declare i int;  set i=1;  while(i&lt;=1000)do    insert into t1 values(i, 1001-i, i);    set i=i+1;  end while;  set i=1;  while(i&lt;=1000000)do    insert into t2 values(i, i, i);    set i=i+1;  end while;end;;delimiter ;call idata();</code></pre><p>为了便于后面量化说明，我在表 t1 里，插入了 1000 行数据，每一行的 a=1001-id 的值。也就是说，表 t1 中字段 a 是逆序的。同时，我在表 t2 中插入了 100 万行数据。</p><h2 id="Multi-Range-Read-优化"><a href="#Multi-Range-Read-优化" class="headerlink" title="Multi-Range Read 优化"></a>Multi-Range Read 优化</h2><p>在介绍 join 语句的优化方案之前，我需要先和你介绍一个知识点，即：<strong>Multi-Range Read 优化 (MRR)。这个优化的主要目的是尽量使用顺序读盘</strong>。</p><p>回表是指，InnoDB 在普通索引 a 上查到主键 id 的值后，再根据一个个主键 id 的值到主键索引上去查整行数据的过程。</p><p>回表过程是一行行地查数据，还是批量地查数据？</p><p>我们先来看看这个问题。假设，我执行这个语句：</p><pre><code>select * from t1 where a&gt;=1 and a&lt;=100;</code></pre><p>主键索引是一棵 B+ 树，在这棵树上，每次只能根据一个主键 id 查到一行数据。因此，回表肯定是一行行搜索主键索引的，基本流程如图所示。</p><p><img src="/2021/03/25/2021-03-25-mysql-35/97ae269061192f6d7a632df56fa03605.png" alt="基本回表流程"></p><p>如果随着 a 的值递增顺序查询的话，id 的值就变成随机的，那么就会出现随机访问，性能相对较差。虽然“按行查”这个机制不能改，但是调整查询的顺序，还是能够加速的。</p><p><strong>因为大多数的数据都是按照主键递增顺序插入得到的，所以我们可以认为，如果按照主键的递增顺序查询的话，对磁盘的读比较接近顺序读，能够提升读性能。</strong></p><p>这，就是 MRR 优化的设计思路。此时，语句的执行流程变成了这样：</p><ol><li>根据索引 a，定位到满足条件的记录，将 id 值放入 read_rnd_buffer 中 ;</li><li>将 read_rnd_buffer 中的 id 进行递增排序；</li><li>排序后的 id 数组，依次到主键 id 索引中查记录，并作为结果返回。</li></ol><p>这里，read_rnd_buffer 的大小是由 read_rnd_buffer_size 参数控制的。如果步骤 1 中，read_rnd_buffer 放满了，就会先执行完步骤 2 和 3，然后清空 read_rnd_buffer。之后继续找索引 a 的下个记录，并继续循环。</p><p>另外需要说明的是，如果你想要稳定地使用 MRR 优化的话，需要设置set optimizer_switch=”mrr_cost_based=off”。（官方文档的说法，是现在的优化器策略，判断消耗的时候，会更倾向于不使用 MRR，把 mrr_cost_based 设置为 off，就是固定使用 MRR 了。）</p><p>下面两幅图就是使用了 MRR 优化后的执行流程和 explain 结果。</p><p><img src="/2021/03/25/2021-03-25-mysql-35/d502fbaea7cac6f815c626b078da86c7.jpg" alt="MRR 执行流程"></p><p><img src="/2021/03/25/2021-03-25-mysql-35/a513d07ebaf1ae044d44391c89bc6432.png" alt="MRR 执行流程的 explain 结果"></p><p>从图 3 的 explain 结果中，我们可以看到 Extra 字段多了 Using MRR，表示的是用上了 MRR 优化。而且，由于我们在 read_rnd_buffer 中按照 id 做了排序，所以最后得到的结果集也是按照主键 id 递增顺序的，也就是与图 1 结果集中行的顺序相反。</p><p>到这里，小结一下</p><p><strong>MRR 能够提升性能的核心在于，这条查询语句在索引 a 上做的是一个范围查询（也就是说，这是一个多值查询），可以得到足够多的主键 id。这样通过排序以后，再去主键索引查数据，才能体现出“顺序性”的优势。</strong></p><h2 id="Batched-Key-Access"><a href="#Batched-Key-Access" class="headerlink" title="Batched Key Access"></a>Batched Key Access</h2><p>理解了 MRR 性能提升的原理，我们就能理解 MySQL 在 5.6 版本后开始引入的 Batched Key Access(BKA) 算法了。这个 BKA 算法，其实就是对 NLJ 算法的优化。</p><p>我们再来NLJ 算法的流程图：</p><p><img src="/2021/03/25/2021-03-25-mysql-35/10e14e8b9691ac6337d457172b641a3d.jpg" alt="Index Nested-Loop Join 流程图"></p><p>NLJ 算法执行的逻辑是：从驱动表 t1，一行行地取出 a 的值，再到被驱动表 t2 去做 join。也就是说，对于表 t2 来说，每次都是匹配一个值。这时，MRR 的优势就用不上了。</p><p>那怎么才能一次性地多传些值给表 t2 呢？方法就是，从表 t1 里一次性地多拿些行出来，一起传给表 t2。</p><p>既然如此，我们就把表 t1 的数据取出来一部分，先放到一个临时内存。这个临时内存不是别人，就是 join_buffer。</p><p>join_buffer 在 BNL 算法里的作用，是暂存驱动表的数据。但是在 NLJ 算法里并没有用。那么，我们刚好就可以复用 join_buffer 到 BKA 算法中。</p><p>如图所示，是上面的 NLJ 算法优化后的 BKA 算法的流程。</p><p><img src="/2021/03/25/2021-03-25-mysql-35/682370c5640244fa3474d26cc3bc0388.png" alt="Batched Key Access 流程"></p><p>图中，我在 join_buffer 中放入的数据是 P1至P100，表示的是只会取查询需要的字段。当然，如果 join buffer 放不下 P1至P100 的所有数据，就会把这 100 行数据分成多段执行上图的流程。</p><p>那么，这个 BKA 算法到底要怎么启用呢？</p><p>如果要使用 BKA 优化算法的话，你需要在执行 SQL 语句之前，先设置</p><pre><code>set optimizer_switch=&#39;mrr=on,mrr_cost_based=off,batched_key_access=on&#39;;</code></pre><p>其中，前两个参数的作用是要启用 MRR。这么做的原因是，BKA 算法的优化要依赖于 MRR。</p><h2 id="BNL-算法的性能问题"><a href="#BNL-算法的性能问题" class="headerlink" title="BNL 算法的性能问题"></a>BNL 算法的性能问题</h2><p>说完了 NLJ 算法的优化，我们再来看 BNL 算法的优化。</p><p>使用 Block Nested-Loop Join(BNL) 算法时，可能会对被驱动表做多次扫描。如果这个被驱动表是一个大的冷数据表，除了会导致 IO 压力大以外，还会对系统有什么影响呢？</p><p>由于 InnoDB 对 Bufffer Pool 的 LRU 算法做了优化，即：第一次从磁盘读入内存的数据页，会先放在 old 区域。如果 1 秒之后这个数据页不再被访问了，就不会被移动到 LRU 链表头部，这样对 Buffer Pool 的命中率影响就不大。</p><p>但是，如果一个使用 BNL 算法的 join 语句，多次扫描一个冷表，而且这个语句执行时间超过 1 秒，就会在再次扫描冷表的时候，把冷表的数据页移到 LRU 链表头部。</p><p>这种情况对应的，是冷表的数据量小于整个 Buffer Pool 的 3/8，能够完全放入 old 区域的情况。</p><p>如果这个冷表很大，就会出现另外一种情况：业务正常访问的数据页，没有机会进入 young 区域。</p><p>由于优化机制的存在，一个正常访问的数据页，要进入 young 区域，需要隔 1 秒后再次被访问到。但是，由于我们的 join 语句在循环读磁盘和淘汰内存页，进入 old 区域的数据页，很可能在 1 秒之内就被淘汰了。这样，就会导致这个 MySQL 实例的 Buffer Pool 在这段时间内，young 区域的数据页没有被合理地淘汰。</p><p>也就是说，这两种情况都会影响 Buffer Pool 的正常运作。</p><p>大表 join 操作虽然对 IO 有影响，但是在语句执行结束后，对 IO 的影响也就结束了。但是，对 Buffer Pool 的影响就是持续性的，需要依靠后续的查询请求慢慢恢复内存命中率。</p><p>为了减少这种影响，你可以考虑增大 join_buffer_size 的值，减少对被驱动表的扫描次数。</p><p>也就是说，BNL 算法对系统的影响主要包括三个方面：</p><ol><li>可能会多次扫描被驱动表，占用磁盘 IO 资源；</li><li>判断 join 条件需要执行 M*N 次对比（M、N 分别是两张表的行数），如果是大表就会占用非常多的 CPU 资源；</li><li>可能会导致 Buffer Pool 的热数据被淘汰，影响内存命中率。</li></ol><p>我们执行语句之前，需要通过理论分析和查看 explain 结果的方式，确认是否要使用 BNL 算法。如果确认优化器会使用 BNL 算法，就需要做优化。优化的常见做法是，给被驱动表的 join 字段加上索引，把 BNL 算法转成 BKA 算法。</p><h2 id="BNL-转-BKA"><a href="#BNL-转-BKA" class="headerlink" title="BNL 转 BKA"></a>BNL 转 BKA</h2><p>一些情况下，我们可以直接在被驱动表上建索引，这时就可以直接转成 BKA 算法了。</p><p>但是，有时候你确实会碰到一些不适合在被驱动表上建索引的情况。比如下面这个语句：</p><pre><code>select * from t1 join t2 on (t1.b=t2.b) where t2.b&gt;=1 and t2.b&lt;=2000;</code></pre><p>我们在文章开始的时候，在表 t2 中插入了 100 万行数据，但是经过 where 条件过滤后，需要参与 join 的只有 2000 行数据。如果这条语句同时是一个低频的 SQL 语句，那么再为这个语句在表 t2 的字段 b 上创建一个索引就很浪费了。</p><p>但是，如果使用 BNL 算法来 join 的话，这个语句的执行流程是这样的：</p><ol><li><p>把表 t1 的所有字段取出来，存入 join_buffer 中。这个表只有 1000 行，join_buffer_size 默认值是 256k，可以完全存入。</p></li><li><p>扫描表 t2，取出每一行数据跟 join_buffer 中的数据进行对比，</p><ul><li>如果不满足 t1.b=t2.b，则跳过；</li><li>如果满足 t1.b=t2.b, 再判断其他条件，也就是是否满足 t2.b 处于[1,2000]的条件，如果是，就作为结果集的一部分返回，否则跳过。</li></ul></li></ol><p>对于表 t2 的每一行，判断 join 是否满足的时候，都需要遍历 join_buffer 中的所有行。因此判断等值条件的次数是 1000*100 万 =10 亿次，这个判断的工作量很大。</p><p><img src="/2021/03/25/2021-03-25-mysql-35/92fbdbfc35da3040396401250cb33f60.png" alt></p><p><img src="/2021/03/25/2021-03-25-mysql-35/d862bc3e88305688df2c354a4b26809c.png" alt></p><p>可以看到，explain 结果里 Extra 字段显示使用了 BNL 算法。在我的测试环境里，这条语句需要执行 1 分 11 秒。</p><p>在表 t2 的字段 b 上创建索引会浪费资源，但是不创建索引的话这个语句的等值条件要判断 10 亿次，想想也是浪费。那么，有没有两全其美的办法呢？</p><p>这时候，我们可以考虑使用临时表。使用临时表的大致思路是：</p><ol><li>把表 t2 中满足条件的数据放在临时表 tmp_t 中；</li><li>为了让 join 使用 BKA 算法，给临时表 tmp_t 的字段 b 加上索引；</li><li>让表 t1 和 tmp_t 做 join 操作。</li></ol><p>此时，对应的 SQL 语句的写法如下：</p><pre><code>create temporary table temp_t(id int primary key, a int, b int, index(b))engine=innodb;insert into temp_t select * from t2 where b&gt;=1 and b&lt;=2000;select * from t1 join temp_t on (t1.b=temp_t.b);</code></pre><p>这个语句序列的执行效果:</p><p><img src="/2021/03/25/2021-03-25-mysql-35/a80cdffe8173fa0fd8969ed976ac6ac7.png" alt></p><p>可以看到，整个过程 3 个语句执行时间的总和还不到 1 秒，相比于前面的 1 分 11 秒，性能得到了大幅提升。接下来，我们一起看一下这个过程的消耗：</p><ol><li>执行 insert 语句构造 temp_t 表并插入数据的过程中，对表 t2 做了全表扫描，这里扫描行数是 100 万。</li><li>之后的 join 语句，扫描表 t1，这里的扫描行数是 1000；join 比较过程中，做了 1000 次带索引的查询。相比于优化前的 join 语句需要做 10 亿次条件判断来说，这个优化效果还是很明显的。</li></ol><p>总体来看，不论是在原表上加索引，还是用有索引的临时表，我们的思路都是让 join 语句能够用上被驱动表上的索引，来触发 BKA 算法，提升查询性能。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ol><li>BKA 优化是 MySQL 已经内置支持的，建议默认使用；</li><li>BNL 算法效率低，建议尽量转成 BKA 算法。优化的方向就是给被驱动表的关联字段加上索引；（就是因为被驱动表的关联字段没有索引，所以才用BNL算法，效率很低，加上索引优化就会变成对于NLJ 算法的优化后的BKA算法）</li><li>基于临时表的改进方案，对于能够提前过滤出小数据的 join 语句来说，效果还是很好的。（最终目的还是给被驱动表的关联字段加上索引，用上BKA算法）</li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用join</title>
      <link href="/2021/03/23/2021-03-23-mysql-34/"/>
      <url>/2021/03/23/2021-03-23-mysql-34/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><p>在实际生产中，关于 join 语句使用的问题，一般会集中在以下两类：</p><ul><li>我们 DBA 不让使用 join，使用 join 有什么问题呢？</li><li>如果有两个大小不同的表做 join，应该用哪个表做驱动表呢？</li></ul><p>为了便于量化分析，创建两个表 t1 和 t2 来说明。</p><pre><code>CREATE TABLE `t2` (  `id` int(11) NOT NULL,  `a` int(11) DEFAULT NULL,  `b` int(11) DEFAULT NULL,  PRIMARY KEY (`id`),  KEY `a` (`a`)) ENGINE=InnoDB;drop procedure idata;delimiter ;;create procedure idata()begin  declare i int;  set i=1;  while(i&lt;=1000)do    insert into t2 values(i, i, i);    set i=i+1;  end while;end;;delimiter ;call idata();create table t1 like t2;insert into t1 (select * from t2 where id&lt;=100)</code></pre><p>可以看到，这两个表都有一个主键索引 id 和一个索引 a，字段 b 上无索引。存储过程 idata() 往表 t2 里插入了 1000 行数据，在表 t1 里插入的是 100 行数据。</p><h2 id="Index-Nested-Loop-Join"><a href="#Index-Nested-Loop-Join" class="headerlink" title="Index Nested-Loop Join"></a>Index Nested-Loop Join</h2><pre><code>select * from t1 straight_join t2 on (t1.a=t2.a);</code></pre><p>如果直接使用 join 语句，MySQL 优化器可能会选择表 t1 或 t2 作为驱动表，这样会影响我们分析 SQL 语句的执行过程。所以，为了便于分析执行过程中的性能问题，我改用 straight_join 让 MySQL 使用固定的连接方式执行查询，这样优化器只会按照我们指定的方式去 join。在这个语句里，t1 是驱动表，t2 是被驱动表。</p><p>现在，我们来看一下这条语句的 explain 结果。</p><p><img src="/2021/03/23/2021-03-23-mysql-34/4b9cb0e0b83618e01c9bfde44a0ea990.png" alt="使用索引字段 join 的 explain 结果"></p><p>可以看到，在这条语句里，被驱动表 t2 的字段 a 上有索引，join 过程用上了这个索引，因此这个语句的执行流程是这样的：</p><ol><li>从表 t1 中读入一行数据 R；</li><li>从数据行 R 中，取出 a 字段到表 t2 里去查找；</li><li>取出表 t2 中满足条件的行，跟 R 组成一行，作为结果集的一部分；</li><li>重复执行步骤 1 到 3，直到表 t1 的末尾循环结束。</li></ol><p>这个过程是先遍历表 t1，然后根据从表 t1 中取出的每行数据中的 a 值，去表 t2 中查找满足条件的记录。在形式上，这个过程就跟我们写程序时的嵌套查询类似，并且可以用上被驱动表的索引，所以我们称之为“Index Nested-Loop Join”，简称 NLJ。</p><p>它对应的流程图如下所示：</p><p><img src="/2021/03/23/2021-03-23-mysql-34/d83ad1cbd6118603be795b26d38f8df6.jpg" alt="Index Nested-Loop Join 算法的执行流程"></p><p>在这个流程里：</p><ol><li>对驱动表 t1 做了全表扫描，这个过程需要扫描 100 行；</li><li>而对于每一行 R，根据 a 字段去表 t2 查找，走的是树搜索过程。由于我们构造的数据都是一一对应的，因此每次的搜索过程都只扫描一行，也是总共扫描 100 行；</li><li>所以，整个执行流程，总扫描行数是 200。</li></ol><p>先看第一个问题：能不能使用 join?</p><p>假设不使用 join，那我们就只能用单表查询。我们看看上面这条语句的需求，用单表查询怎么实现。</p><ol><li><p>执行select * from t1，查出表 t1 的所有数据，这里有 100 行；</p></li><li><p>循环遍历这 100 行数据：</p><ul><li>从每一行 R 取出字段 a 的值 $R.a；</li><li>执行select * from t2 where a=$R.a；</li><li>把返回的结果和 R 构成结果集的一行。</li></ul></li></ol><p>可以看到，在这个查询过程，也是扫描了 200 行，但是总共执行了 101 条语句，比直接 join 多了 100 次交互。除此之外，客户端还要自己拼接 SQL 语句和结果。</p><p>显然，这么做还不如直接 join 好。</p><p>我们再来看看第二个问题：怎么选择驱动表？</p><p>在这个 join 语句执行过程中，驱动表是走全表扫描，而被驱动表是走树搜索。</p><p>假设被驱动表的行数是 M。每次在被驱动表查一行数据，要先搜索索引 a，再搜索主键索引。每次搜索一棵树近似复杂度是以 2 为底的 M 的对数，记为 log2M，所以在被驱动表上查一行的时间复杂度是 2*log2M。</p><p>假设驱动表的行数是 N，执行过程就要扫描驱动表 N 行，然后对于每一行，到被驱动表上匹配一次。</p><p>因此整个执行过程，近似复杂度是 N + N<em>2</em>log2M。</p><p>显然，N 对扫描行数的影响更大，因此<strong>应该让小表来做驱动表</strong>。</p><blockquote><p>如果你没觉得这个影响有那么“显然”， 可以这么理解：N 扩大 1000 倍的话，扫描行数就会扩大 1000 倍；而 M 扩大 1000 倍，扫描行数扩大不到 10 倍。</p></blockquote><p>到这里小结一下，通过上面的分析我们得到了两个结论：</p><ol><li><strong>使用 join 语句，性能比强行拆成多个单表执行 SQL 语句的性能要好</strong>；</li><li><strong>如果使用 join 语句的话，需要让小表做驱动表</strong>。</li></ol><p>但是，你需要注意，这个结论的<strong>前提是“可以使用被驱动表的索引”</strong>。</p><p>接下来，我们再看看被驱动表用不上索引的情况。</p><h2 id="Simple-Nested-Loop-Join"><a href="#Simple-Nested-Loop-Join" class="headerlink" title="Simple Nested-Loop Join"></a>Simple Nested-Loop Join</h2><pre><code>select * from t1 straight_join t2 on (t1.a=t2.b);</code></pre><p>由于表 t2 的字段 b 上没有索引，因此再用图 2 的执行流程时，每次到 t2 去匹配的时候，就要做一次全表扫描。</p><p>你可以先设想一下这个问题，继续使用图 2 的算法，是不是可以得到正确的结果呢？如果只看结果的话，这个算法是正确的，而且这个算法也有一个名字，叫做“Simple Nested-Loop Join”。</p><p>但是，这样算来，这个 SQL 请求就要扫描表 t2 多达 100 次，总共扫描 100*1000=10 万行。</p><p>这还只是两个小表，如果 t1 和 t2 都是 10 万行的表（当然了，这也还是属于小表的范围），就要扫描 100 亿行，这个算法看上去太“笨重”了。</p><p>当然，MySQL 也没有使用这个 Simple Nested-Loop Join 算法，而是使用了另一个叫作“Block Nested-Loop Join”的算法，简称 BNL。</p><h2 id="Block-Nested-Loop-Join"><a href="#Block-Nested-Loop-Join" class="headerlink" title="Block Nested-Loop Join"></a>Block Nested-Loop Join</h2><p>这时候，被驱动表上没有可用的索引，算法的流程是这样的：</p><ol><li>这时候，被驱动表上没有可用的索引，算法的流程是这样的：</li><li>扫描表 t2，把表 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回。</li></ol><p>这个过程的流程图如下：</p><p><img src="/2021/03/23/2021-03-23-mysql-34/15ae4f17c46bf71e8349a8f2ef70d573.jpg" alt="Block Nested-Loop Join 算法的执行流程"></p><p>对应地，这条 SQL 语句的 explain 结果如下所示：</p><p><img src="/2021/03/23/2021-03-23-mysql-34/676921fa0883e9463dd34fb2bc5e87e1.png" alt></p><p>可以看到，在这个过程中，对表 t1 和 t2 都做了一次全表扫描，因此总的扫描行数是 1100。由于 join_buffer 是以无序数组的方式组织的，因此对表 t2 中的每一行，都要做 100 次判断，总共需要在内存中做的判断次数是：100*1000=10 万次。</p><p>前面我们说过，如果使用 Simple Nested-Loop Join 算法进行查询，扫描行数也是 10 万行。因此，从时间复杂度上来说，这两个算法是一样的。但是，Block Nested-Loop Join 算法的这 10 万次判断是内存操作，速度上会快很多，性能也更好。</p><p>接下来，我们来看一下，在这种情况下，应该选择哪个表做驱动表。</p><p>假设小表的行数是 N，大表的行数是 M，那么在这个算法里：</p><ol><li>两个表都做一次全表扫描，所以总的扫描行数是 M+N；</li><li>内存中的判断次数是 M*N。</li></ol><p>可以看到，调换这两个算式中的 M 和 N 没差别，因此这时候选择大表还是小表做驱动表，执行耗时是一样的。</p><p>然后，你可能马上就会问了，这个例子里表 t1 才 100 行，要是表 t1 是一个大表，join_buffer 放不下怎么办呢？</p><p>join_buffer 的大小是由参数 join_buffer_size 设定的，默认值是 256k。如果放不下表 t1 的所有数据话，策略很简单，就是分段放。我把 join_buffer_size 改成 1200，再执行：</p><pre><code>select * from t1 straight_join t2 on (t1.a=t2.b);</code></pre><p>执行过程就变成了：</p><ol><li>扫描表 t1，顺序读取数据行放入 join_buffer 中，放完第 88 行 join_buffer 满了，继续第 2 步；</li><li>扫描表 t2，把 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回；</li><li>清空 join_buffer；</li><li>继续扫描表 t1，顺序读取最后的 12 行数据放入 join_buffer 中，继续执行第 2 步。</li></ol><p>执行流程图也就变成这样：</p><p><img src="/2021/03/23/2021-03-23-mysql-34/695adf810fcdb07e393467bcfd2f6ac4.jpg" alt="Block Nested-Loop Join -- 两段"></p><p>图中的步骤 4 和 5，表示清空 join_buffer 再复用。</p><p>这个流程才体现出了这个算法名字中“Block”的由来，表示“分块去 join”。</p><p>可以看到，这时候由于表 t1 被分成了两次放入 join_buffer 中，导致表 t2 会被扫描两次。虽然分成两次放入 join_buffer，但是判断等值条件的次数还是不变的，依然是 (88+12)*1000=10 万次。</p><p>我们再来看下，在这种情况下驱动表的选择问题。</p><p>假设，驱动表的数据行数是 N，需要分 K 段才能完成算法流程，被驱动表的数据行数是 M。</p><p>注意，这里的 K 不是常数，N 越大 K 就会越大，因此把 K 表示为λ*N，显然λ的取值范围是 (0,1)。</p><p>所以，在这个算法的执行过程中：</p><ol><li>扫描行数是 N+λ<em>N</em>M(N+KM)；</li><li>内存判断 N*M 次。</li></ol><p>显然，内存判断次数是不受选择哪个表作为驱动表影响的。而考虑到扫描行数，在 M 和 N 大小确定的情况下，N 小一些，整个算式的结果会更小。</p><p>所以结论是，应该让小表当驱动表。</p><p>当然，你会发现，在 N+λ<em>N</em>M 这个式子里，λ才是影响扫描行数的关键因素，这个值越小越好。</p><p>刚刚我们说了 N 越大，分段数 K 越大。那么，N 固定的时候，什么参数会影响 K 的大小呢？（也就是λ的大小）答案是 join_buffer_size。join_buffer_size 越大，一次可以放入的行越多，分成的段数也就越少，对被驱动表的全表扫描次数就越少。</p><p>这就是为什么，你可能会看到一些建议告诉你，如果你的 join 语句很慢，就把 join_buffer_size 改大。</p><h2 id="能不能使用-join-语句？"><a href="#能不能使用-join-语句？" class="headerlink" title="能不能使用 join 语句？"></a>能不能使用 join 语句？</h2><ol><li>如果可以使用 Index Nested-Loop Join 算法，也就是说可以用上被驱动表上的索引，其实是没问题的；</li><li>如果使用 Block Nested-Loop Join 算法，扫描行数就会过多。尤其是在大表上的 join 操作，这样可能要扫描被驱动表很多次，会占用大量的系统资源。所以这种 join 尽量不要用。</li></ol><p>所以你在判断要不要使用 join 语句时，就是看 explain 结果里面，Extra 字段里面有没有出现“Block Nested Loop”字样。</p><h2 id="如果要使用-join，应该选择大表做驱动表还是选择小表做驱动表？"><a href="#如果要使用-join，应该选择大表做驱动表还是选择小表做驱动表？" class="headerlink" title="如果要使用 join，应该选择大表做驱动表还是选择小表做驱动表？"></a>如果要使用 join，应该选择大表做驱动表还是选择小表做驱动表？</h2><ol><li><p>如果是 Index Nested-Loop Join 算法，应该选择小表做驱动表；</p></li><li><p>如果是 Block Nested-Loop Join 算法：</p><ul><li>在 join_buffer_size 足够大的时候，是一样的；</li><li>在 join_buffer_size 不够大的时候（这种情况更常见），应该选择小表做驱动表。</li></ul></li></ol><p>所以，这个问题的结论就是，总是应该使用小表做驱动表。</p><h2 id="什么叫作“小表”"><a href="#什么叫作“小表”" class="headerlink" title="什么叫作“小表”"></a>什么叫作“小表”</h2><p>我们前面的例子是没有加条件的。如果我在语句的 where 条件加上 t2.id&lt;=50 这个限定条件，再来看下这两条语句：</p><pre><code>select * from t1 straight_join t2 on (t1.b=t2.b) where t2.id&lt;=50;select * from t2 straight_join t1 on (t1.b=t2.b) where t2.id&lt;=50;</code></pre><p>注意，为了让两条语句的被驱动表都用不上索引，所以 join 字段都使用了没有索引的字段 b。</p><p>但如果是用第二个语句的话，join_buffer 只需要放入 t2 的前 50 行，显然是更好的。所以这里，“t2 的前 50 行”是那个相对小的表，也就是“小表”。</p><p>我们再来看另外一组例子：</p><pre><code>select t1.b,t2.* from  t1  straight_join t2 on (t1.b=t2.b) where t2.id&lt;=100;select t1.b,t2.* from  t2  straight_join t1 on (t1.b=t2.b) where t2.id&lt;=100;</code></pre><p>这个例子里，表 t1 和 t2 都是只有 100 行参加 join。但是，这两条语句每次查询放入 join_buffer 中的数据是不一样的：</p><ul><li>表 t1 只查字段 b，因此如果把 t1 放到 join_buffer 中，则 join_buffer 中只需要放入 b 的值；</li><li>表 t2 需要查所有的字段，因此如果把表 t2 放到 join_buffer 中的话，就需要放入三个字段 id、a 和 b。</li></ul><p>这里，我们应该选择表 t1 作为驱动表。也就是说在这个例子里，“只需要一列参与 join 的表 t1”是那个相对小的表。</p><p>所以，更准确地说，在决定哪个表做驱动表的时候，应该是两个表按照各自的条件过滤，过滤完成之后，计算参与 join 的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ol><li><p>NLJ（Index Nested-Loop Join）：驱动表选出一行行数据到被驱动表中查找（被驱动表上有可用的索引）</p></li><li><p>BNL（Block Nested-Loop Join）：把驱动表的数据分段读入join buffer中，然后和被驱动表join（被驱动表上没有可用的索引）</p></li><li><p>能不能使用 join 语句？</p><ul><li>如果可以使用 Index Nested-Loop Join 算法，也就是说可以用上被驱动表上的索引，其实是没问题的；</li><li>如果使用 Block Nested-Loop Join 算法，扫描行数就会过多。尤其是在大表上的 join 操作，这样可能要扫描被驱动表很多次，会占用大量的系统资源。所以这种 join 尽量不要用。</li></ul></li><li><p>如果要使用 join，应该选择大表做驱动表还是选择小表做驱动表？</p><ul><li><p>如果是 Index Nested-Loop Join 算法，应该选择小表做驱动表；</p></li><li><p>如果是 Block Nested-Loop Join 算法：</p><ul><li>在 join_buffer_size 足够大的时候，是一样的；</li><li>在 join_buffer_size 不够大的时候（这种情况更常见），应该选择小表做驱动表。</li></ul></li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>备库延迟</title>
      <link href="/2021/03/21/2021-03-21-mysql-26/"/>
      <url>/2021/03/21/2021-03-21-mysql-26/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><p>不论是偶发性的查询压力，还是备份，对备库延迟的影响一般是分钟级的，而且在备库恢复正常以后都能够追上来。</p><p>但是，如果备库执行日志的速度持续低于主库生成日志的速度，那这个延迟就有可能成了小时级别。而且对于一个压力持续比较高的主库来说，备库很可能永远都追不上主库的节奏。</p><p><img src="/2021/03/21/2021-03-21-mysql-26/1a85a3bac30a32438bfd8862e5a34eef.png" alt="主备流程图"></p><p>谈到主备的并行复制能力，我们要关注的是图中黑色的两个箭头。一个箭头代表了客户端写入主库，另一箭头代表的是备库上 sql_thread 执行中转日志（relay log）。如果用箭头的粗细来代表并行度的话，那么真实情况就如图所示，第一个箭头要明显粗于第二个箭头。</p><p>在主库上，影响并发度的原因就是各种锁了。由于 InnoDB 引擎支持行锁，除了所有并发事务都在更新同一行（热点行）这种极端场景外，它对业务并发度的支持还是很友好的。所以，你在性能测试的时候会发现，并发压测线程 32 就比单线程时，总体吞吐量高。</p><p>而日志在备库上的执行，就是图中备库上 sql_thread 更新数据 (DATA) 的逻辑。如果是用单线程的话，就会导致备库应用日志不够快，造成主备延迟。</p><p>在官方的 5.6 版本之前，MySQL 只支持单线程复制，由此在主库并发高、TPS 高时就会出现严重的主备延迟问题。</p><p>从单线程复制到最新版本的多线程复制，中间的演化经历了好几个版本。接下来，说说 MySQL 多线程复制的演进过程。</p><p>其实说到底，所有的多线程复制机制，都是要把上图中只有一个线程的 sql_thread，拆成多个线程，也就是都符合下面的这个模型：</p><p><img src="/2021/03/21/2021-03-21-mysql-26/bcf75aa3b0f496699fd7885426bc6245.png" alt="多线程模型"></p><p>图中，coordinator 就是原来的 sql_thread, 不过现在它不再直接更新数据了，只负责读取中转日志和分发事务。真正更新日志的，变成了 worker 线程。而 work 线程的个数，就是由参数 slave_parallel_workers 决定的。根据我的经验，把这个值设置为 8~16 之间最好（32 核物理机的情况），毕竟备库还有可能要提供读查询，不能把 CPU 都吃光了。</p><p>接下来，你需要先思考一个问题：事务能不能按照轮询的方式分发给各个 worker，也就是第一个事务分给 worker_1，第二个事务发给 worker_2 呢？</p><p>其实是不行的。因为，事务被分发给 worker 以后，不同的 worker 就独立执行了。但是，由于 CPU 的调度策略，很可能第二个事务最终比第一个事务先执行。而如果这时候刚好这两个事务更新的是同一行，也就意味着，同一行上的两个事务，在主库和备库上的执行顺序相反，会导致主备不一致的问题。</p><p>接下来，请你再设想一下另外一个问题：同一个事务的多个更新语句，能不能分给不同的 worker 来执行呢？</p><p>答案是，也不行。举个例子，一个事务更新了表 t1 和表 t2 中的各一行，如果这两条更新语句被分到不同 worker 的话，虽然最终的结果是主备一致的，但如果表 t1 执行完成的瞬间，备库上有一个查询，就会看到这个事务“更新了一半的结果”，破坏了事务逻辑的隔离性。</p><p>所以，coordinator 在分发的时候，需要满足以下这两个基本要求：</p><ul><li>不能造成更新覆盖。这就要求更新同一行的两个事务，必须被分发到同一个 worker 中(防止先后顺序错乱)。</li><li>同一个事务不能被拆开，必须放到同一个 worker 中(保证完整性，否则破坏了事务逻辑的逻辑性)。</li></ul><p>各个版本的多线程复制，都遵循了这两条基本原则。接下来，我们就看看各个版本的并行复制策略。</p><h2 id="MySQL-5-5-版本的并行复制策略"><a href="#MySQL-5-5-版本的并行复制策略" class="headerlink" title="MySQL 5.5 版本的并行复制策略"></a>MySQL 5.5 版本的并行复制策略</h2><p>官方 MySQL 5.5 版本是不支持并行复制的。但是，在 2012 年的时候，我自己服务的业务出现了严重的主备延迟，原因就是备库只有单线程复制。然后，我就先后写了两个版本的并行策略。</p><p>这里，我给你介绍一下这两个版本的并行策略，即按表分发策略和按行分发策略，以帮助你理解 MySQL 官方版本并行复制策略的迭代。</p><h3 id="按表分发策略"><a href="#按表分发策略" class="headerlink" title="按表分发策略"></a>按表分发策略</h3><p>按表分发事务的基本思路是，如果两个事务更新不同的表，它们就可以并行。因为数据是存储在表里的，所以按表分发，可以保证两个 worker 不会更新同一行。</p><p>当然，如果有跨表的事务，还是要把两张表放在一起考虑的。如下图所示，就是按表分发的规则。</p><p><img src="/2021/03/21/2021-03-21-mysql-26/8b6976fedd6e644022d4026581fb8d76.png" alt="按表并行复制程模型"></p><p>可以看到，每个 worker 线程对应一个 hash 表，用于保存当前正在这个 worker 的“执行队列”里的事务所涉及的表。hash 表的 key 是“库名. 表名”，value 是一个数字，表示队列中有多少个事务修改这个表。</p><p>在有事务分配给 worker 时，事务里面涉及的表会被加到对应的 hash 表中。worker 执行完成后，这个表会被从 hash 表中去掉。</p><p>hash_table_1 表示，现在 worker_1 的“待执行事务队列”里，有 4 个事务涉及到 db1.t1 表，有 1 个事务涉及到 db2.t2 表；hash_table_2 表示，现在 worker_2 中有一个事务会更新到表 t3 的数据。</p><p>假设在图中的情况下，coordinator 从中转日志中读入一个新事务 T，这个事务修改的行涉及到表 t1 和 t3。</p><p>现在我们用事务 T 的分配流程，来看一下分配规则。</p><ol><li>由于事务 T 中涉及修改表 t1，而 worker_1 队列中有事务在修改表 t1，事务 T 和队列中的某个事务要修改同一个表的数据，这种情况我们说事务 T 和 worker_1 是冲突的。</li><li>按照这个逻辑，顺序判断事务 T 和每个 worker 队列的冲突关系，会发现事务 T 跟 worker_2 也冲突。</li><li>事务 T 跟多于一个 worker 冲突，coordinator 线程就进入等待。</li><li>每个 worker 继续执行，同时修改 hash_table。假设 hash_table_2 里面涉及到修改表 t3 的事务先执行完成，就会从 hash_table_2 中把 db1.t3 这一项去掉。</li><li>这样 coordinator 会发现跟事务 T 冲突的 worker 只有 worker_1 了，因此就把它分配给 worker_1。</li><li>coordinator 继续读下一个中转日志，继续分配事务。</li></ol><p>也就是说，每个事务在分发的时候，跟所有 worker 的冲突关系包括以下三种情况：</p><ol><li>如果跟所有 worker 都不冲突，coordinator 线程就会把这个事务分配给最空闲的 woker;</li><li>如果跟多于一个 worker 冲突，coordinator 线程就进入等待状态，直到和这个事务存在冲突关系的 worker 只剩下 1 个；</li><li>如果只跟一个 worker 冲突，coordinator 线程就会把这个事务分配给这个存在冲突关系的 worker。</li></ol><p>这个按表分发的方案，在多个表负载均匀的场景里应用效果很好。但是，如果碰到热点表，比如所有的更新事务都会涉及到某一个表的时候，所有事务都会被分配到同一个 worker 中，就变成单线程复制了。</p><h3 id="按行分发策略"><a href="#按行分发策略" class="headerlink" title="按行分发策略"></a>按行分发策略</h3><p>要解决热点表的并行复制问题，就需要一个按行并行复制的方案。按行复制的核心思路是：如果两个事务没有更新相同的行，它们在备库上可以并行执行。显然，这个模式要求 binlog 格式必须是 row。</p><p>这时候，我们判断一个事务 T 和 worker 是否冲突，用的就规则就不是“修改同一个表”，而是“修改同一行”。</p><p>按行复制和按表复制的数据结构差不多，也是为每个 worker，分配一个 hash 表。只是要实现按行分发，这时候的 key，就必须是“库名 + 表名 + 唯一键的值”。</p><p>但是，这个“唯一键”只有主键 id 还是不够的，我们还需要考虑下面这种场景，表 t1 中除了主键，还有唯一索引 a：</p><pre><code>CREATE TABLE `t1` (  `id` int(11) NOT NULL,  `a` int(11) DEFAULT NULL,  `b` int(11) DEFAULT NULL,  PRIMARY KEY (`id`),  UNIQUE KEY `a` (`a`)) ENGINE=InnoDB;insert into t1 values(1,1,1),(2,2,2),(3,3,3),(4,4,4),(5,5,5);</code></pre><p>假设，接下来我们要在主库执行这两个事务：</p><p><img src="/2021/03/21/2021-03-21-mysql-26/f19916e27b8ff28e87ed3ad9f5473378.png" alt></p><p>可以看到，这两个事务要更新的行的主键值不同，但是如果它们被分到不同的 worker，就有可能 session B 的语句先执行。这时候 id=1 的行的 a 的值还是 1，就会报唯一键冲突。</p><p>因此，基于行的策略，事务 hash 表中还需要考虑唯一键，即 key 应该是“库名 + 表名 + 索引 a 的名字 +a 的值”。</p><p>比如，在上面这个例子中，我要在表 t1 上执行 update t1 set a=1 where id=2 语句，在 binlog 里面记录了整行的数据修改前各个字段的值，和修改后各个字段的值。</p><p>因此，coordinator 在解析这个语句的 binlog 的时候，这个事务的 hash 表就有三个项:</p><ol><li>key=hash_func(db1+t1+“PRIMARY”+2), value=2; 这里 value=2 是因为修改前后的行 id 值不变，出现了两次。</li><li>key=hash_func(db1+t1+“a”+2), value=1，表示会影响到这个表 a=2 的行。</li><li>key=hash_func(db1+t1+“a”+1), value=1，表示会影响到这个表 a=1 的行。</li></ol><p>可见，相比于按表并行分发策略，按行并行策略在决定线程分发的时候，需要消耗更多的计算资源。你可能也发现了，这两个方案其实都有一些约束条件：</p><ul><li>要能够从 binlog 里面解析出表名、主键值和唯一索引的值。也就是说，主库的 binlog 格式必须是 row；</li><li>表必须有主键；</li><li>不能有外键。表上如果有外键，级联更新的行不会记录在 binlog 中，这样冲突检测就不准确。</li></ul><p>但，好在这三条约束规则，本来就是 DBA 之前要求业务开发人员必须遵守的线上使用规范，所以这两个并行复制策略在应用上也没有碰到什么麻烦。</p><p>对比按表分发和按行分发这两个方案的话，按行分发策略的并行度更高。不过，如果是要操作很多行的大事务的话，按行分发的策略有两个问题：</p><ul><li>耗费内存。比如一个语句要删除 100 万行数据，这时候 hash 表就要记录 100 万个项。</li><li>耗费 CPU。解析 binlog，然后计算 hash 值，对于大事务，这个成本还是很高的。</li></ul><p>所以，我在实现这个策略的时候会设置一个阈值，单个事务如果超过设置的行数阈值（比如，如果单个事务更新的行数超过 10 万行），就暂时退化为单线程模式，退化过程的逻辑大概是这样的：</p><ol><li>coordinator 暂时先 hold 住这个事务；</li><li>等待所有 worker 都执行完成，变成空队列；</li><li>coordinator 直接执行这个事务；</li><li>恢复并行模式。</li></ol><h2 id="MySQL-5-6-版本的并行复制策略"><a href="#MySQL-5-6-版本的并行复制策略" class="headerlink" title="MySQL 5.6 版本的并行复制策略"></a>MySQL 5.6 版本的并行复制策略</h2><p>官方 MySQL5.6 版本，支持了并行复制，只是支持的粒度是按库并行。理解了上面介绍的按表分发策略和按行分发策略，你就理解了，用于决定分发策略的 hash 表里，key 就是数据库名。</p><p>这个策略的并行效果，取决于压力模型。如果在主库上有多个 DB，并且各个 DB 的压力均衡，使用这个策略的效果会很好。</p><p>相比于按表和按行分发，这个策略有两个优势：</p><ul><li>构造 hash 值的时候很快，只需要库名；而且一个实例上 DB 数也不会很多，不会出现需要构造 100 万个项这种情况。</li><li>不要求 binlog 的格式。因为 statement 格式的 binlog 也可以很容易拿到库名。</li></ul><p>但是，如果你的主库上的表都放在同一个 DB 里面，这个策略就没有效果了；或者如果不同 DB 的热点不同，比如一个是业务逻辑库，一个是系统配置库，那也起不到并行的效果。</p><p>理论上你可以创建不同的 DB，把相同热度的表均匀分到这些不同的 DB 中，强行使用这个策略。不过据我所知，由于需要特地移动数据，这个策略用得并不多。</p><h2 id="MariaDB-的并行复制策略"><a href="#MariaDB-的并行复制策略" class="headerlink" title="MariaDB 的并行复制策略"></a>MariaDB 的并行复制策略</h2><p>redo log 组提交 (group commit) 优化， 而 MariaDB 的并行复制策略利用的就是这个特性：</p><ul><li>能够在同一组里提交的事务，一定不会修改同一行；</li><li>主库上可以并行执行的事务，备库上也一定是可以并行执行的。</li></ul><p>在实现上，MariaDB 是这么做的：</p><ul><li>在一组里面一起提交的事务，有一个相同的 commit_id，下一组就是 commit_id+1；</li><li>commit_id 直接写到 binlog 里面；</li><li>传到备库应用的时候，相同 commit_id 的事务分发到多个 worker 执行；</li><li>这一组全部执行完成后，coordinator 再去取下一批。</li></ul><p>当时，这个策略出来的时候是相当惊艳的。因为，之前业界的思路都是在“分析 binlog，并拆分到 worker”上。而 MariaDB 的这个策略，目标是“模拟主库的并行模式”。</p><p>但是，这个策略有一个问题，它并没有实现“真正的模拟主库并发度”这个目标。在主库上，一组事务在 commit 的时候，下一组事务是同时处于“执行中”状态的。</p><p>假设了三组事务在主库的执行情况，你可以看到在 trx1、trx2 和 trx3 提交的时候，trx4、trx5 和 trx6 是在执行的。这样，在第一组事务提交完成的时候，下一组事务很快就会进入 commit 状态。</p><p><img src="/2021/03/21/2021-03-21-mysql-26/8fec5fb48d6095aecc80016826efbfc3.png" alt="主库并行事务"></p><p>而按照 MariaDB 的并行复制策略，备库上的执行效果如下图所示</p><p><img src="/2021/03/21/2021-03-21-mysql-26/8ac3799c1ff2f9833619a1624ca3e622.png" alt="MariaDB 并行复制，备库并行效果"></p><p>可以看到，在备库上执行的时候，要等第一组事务完全执行完成后，第二组事务才能开始执行，这样系统的吞吐量就不够。</p><p>另外，这个方案很容易被大事务拖后腿。假设 trx2 是一个超大事务，那么在备库应用的时候，trx1 和 trx3 执行完成后，就只能等 trx2 完全执行完成，下一组才能开始执行。这段时间，只有一个 worker 线程在工作，是对资源的浪费。</p><p>不过即使如此，这个策略仍然是一个很漂亮的创新。因为，它对原系统的改造非常少，实现也很优雅。</p><h2 id="MySQL-5-7-的并行复制策略"><a href="#MySQL-5-7-的并行复制策略" class="headerlink" title="MySQL 5.7 的并行复制策略"></a>MySQL 5.7 的并行复制策略</h2><p>在 MariaDB 并行复制实现之后，官方的 MySQL5.7 版本也提供了类似的功能，由参数 slave-parallel-type 来控制并行复制策略：</p><ul><li>配置为 DATABASE，表示使用 MySQL 5.6 版本的按库并行策略；</li><li>配置为 LOGICAL_CLOCK，表示的就是类似 MariaDB 的策略。不过，MySQL 5.7 这个策略，针对并行度做了优化。这个优化的思路也很有趣儿。</li></ul><p>你可以先考虑这样一个问题：同时处于“执行状态”的所有事务，是不是可以并行？答案是，不能。</p><p>因为，这里面可能有由于锁冲突而处于锁等待状态的事务。如果这些事务在备库上被分配到不同的 worker，就会出现备库跟主库不一致的情况。</p><p>而上面提到的 MariaDB 这个策略的核心，是“所有处于 commit”状态的事务可以并行。事务处于 commit 状态，表示已经通过了锁冲突的检验了。</p><p><img src="/2021/03/21/2021-03-21-mysql-26/5ae7d074c34bc5bd55c82781de670c28.png" alt="两阶段提交细化过程图"></p><p>其实，不用等到 commit 阶段，只要能够到达 redo log prepare 阶段，就表示事务已经通过锁冲突的检验了。</p><p>因此，MySQL 5.7 并行复制策略的思想是：</p><ul><li>同时处于 prepare 状态的事务，在备库执行时是可以并行的；</li><li>处于 prepare 状态的事务，与处于 commit 状态的事务之间，在备库执行时也是可以并行的。</li></ul><p>binlog 的组提交的时候，介绍过两个参数：</p><ul><li>binlog_group_commit_sync_delay 参数，表示延迟多少微秒后才调用 fsync;</li><li>binlog_group_commit_sync_no_delay_count 参数，表示累积多少次以后才调用 fsync。</li></ul><p>这两个参数是用于故意拉长 binlog 从 write 到 fsync 的时间，以此减少 binlog 的写盘次数。在 MySQL 5.7 的并行复制策略里，它们可以用来制造更多的“同时处于 prepare 阶段的事务”。这样就增加了备库复制的并行度。</p><p>也就是说，这两个参数，既可以“故意”让主库提交得慢些，又可以让备库执行得快些。在 MySQL 5.7 处理备库延迟的时候，可以考虑调整这两个参数值，来达到提升备库复制并发度的目的。</p><h2 id="MySQL-5-7-22-的并行复制策略"><a href="#MySQL-5-7-22-的并行复制策略" class="headerlink" title="MySQL 5.7.22 的并行复制策略"></a>MySQL 5.7.22 的并行复制策略</h2><p>在 2018 年 4 月份发布的 MySQL 5.7.22 版本里，MySQL 增加了一个新的并行复制策略，基于 WRITESET 的并行复制。</p><p>相应地，新增了一个参数 binlog-transaction-dependency-tracking，用来控制是否启用这个新策略。这个参数的可选值有以下三种。</p><ul><li>COMMIT_ORDER，表示的就是前面介绍的，根据同时进入 prepare 和 commit 来判断是否可以并行的策略。</li><li>WRITESET，表示的是对于事务涉及更新的每一行，计算出这一行的 hash 值，组成集合 writeset。如果两个事务没有操作相同的行，也就是说它们的 writeset 没有交集，就可以并行。</li><li>WRITESET_SESSION，是在 WRITESET 的基础上多了一个约束，即在主库上同一个线程先后执行的两个事务，在备库执行的时候，要保证相同的先后顺序。</li></ul><p>当然为了唯一标识，这个 hash 值是通过“库名 + 表名 + 索引名 + 值”计算出来的。如果一个表上除了有主键索引外，还有其他唯一索引，那么对于每个唯一索引，insert 语句对应的 writeset 就要多增加一个 hash 值。</p><p>你可能看出来了，这跟我们前面介绍的基于 MySQL 5.5 版本的按行分发的策略是差不多的。不过，MySQL 官方的这个实现还是有很大的优势：</p><ul><li>writeset 是在主库生成后直接写入到 binlog 里面的，这样在备库执行的时候，不需要解析 binlog 内容（event 里的行数据），节省了很多计算量；</li><li>不需要把整个事务的 binlog 都扫一遍才能决定分发到哪个 worker，更省内存；</li><li>由于备库的分发策略不依赖于 binlog 内容，所以 binlog 是 statement 格式也是可以的。</li></ul><p>因此，MySQL 5.7.22 的并行复制策略在通用性上还是有保证的。</p><p>当然，对于“表上没主键”和“外键约束”的场景，WRITESET 策略也是没法并行的，也会暂时退化为单线程模型。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ol><li><p>为什么要有多线程复制呢？这是因为单线程复制的能力全面低于多线程复制，对于更新压力较大的主库，备库是可能一直追不上主库的。从现象上看就是，备库上 seconds_behind_master 的值越来越大。</p></li><li><p>分发并发规则</p><ul><li>不能造成更新覆盖。这就要求更新同一行的两个事务，必须被分发到同一个 worker 中(防止先后顺序错乱)。</li><li>同一个事务不能被拆开，必须放到同一个 worker 中(保证完整性，否则破坏了事务逻辑的逻辑性)。</li></ul></li><li><p>binlog 的组提交的时候，介绍过两个参数：</p><ul><li><p>binlog_group_commit_sync_delay 参数，表示延迟多少微秒后才调用 fsync;</p></li><li><p>binlog_group_commit_sync_no_delay_count 参数，表示累积多少次以后才调用 fsync。</p><p>这两个参数是用于故意拉长 binlog 从 write 到 fsync 的时间，以此减少 binlog 的写盘次数。在 MySQL 5.7 的并行复制策略里，它们可以用来制造更多的“同时处于 prepare 阶段的事务”。这样就增加了备库复制的并行度。</p></li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>高可用</title>
      <link href="/2021/03/19/2021-03-19-mysql-25/"/>
      <url>/2021/03/19/2021-03-19-mysql-25/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><p>在一个主备关系中，每个备库接收主库的 binlog 并执行。</p><p>正常情况下，只要主库执行更新生成的所有 binlog，都可以传到备库并被正确地执行，备库就能达到跟主库一致的状态，这就是最终一致性。</p><p>但是，MySQL 要提供高可用能力，只有最终一致性是不够的。为什么这么说呢？今天我就着重和你分析一下。</p><p><img src="/2021/03/19/2021-03-19-mysql-25/89290bbcf454ff9a3dc5de42a85a69cc.png" alt="主备切换流程 -- 双 M 结构"></p><h2 id="主备延迟"><a href="#主备延迟" class="headerlink" title="主备延迟"></a>主备延迟</h2><p>主备切换可能是一个主动运维动作，比如软件升级、主库所在机器按计划下线等，也可能是被动操作，比如主库所在机器掉电。</p><p>在介绍主动切换流程的详细步骤之前，先说明一个概念，即“同步延迟”。与数据同步有关的时间点主要包括以下三个：</p><ul><li>主库 A 执行完成一个事务，写入 binlog，我们把这个时刻记为 T1;</li><li>之后传给备库 B，我们把备库 B 接收完这个 binlog 的时刻记为 T2;</li><li>备库 B 执行完成这个事务，我们把这个时刻记为 T3。</li></ul><p>所谓主备延迟，就是同一个事务，在备库执行完成的时间和主库执行完成的时间之间的差值，也就是 T3-T1。</p><p>你可以在备库上执行 show slave status 命令，它的返回结果里面会显示 seconds_behind_master，用于表示当前备库延迟了多少秒。</p><p>seconds_behind_master 的计算方法是这样的：</p><ul><li>每个事务的 binlog 里面都有一个时间字段，用于记录主库上写入的时间；</li><li>备库取出当前正在执行的事务的时间字段的值，计算它与当前系统时间的差值，得到 seconds_behind_master。</li></ul><p>可以看到，其实 seconds_behind_master 这个参数计算的就是 T3-T1。所以，我们可以用 seconds_behind_master 来作为主备延迟的值，这个值的时间精度是秒。</p><p>你可能会问，如果主备库机器的系统时间设置不一致，会不会导致主备延迟的值不准？</p><p>其实不会的。因为，备库连接到主库的时候，会通过执行 SELECT UNIX_TIMESTAMP() 函数来获得当前主库的系统时间。如果这时候发现主库的系统时间与自己不一致，备库在执行 seconds_behind_master 计算的时候会自动扣掉这个差值。</p><p>需要说明的是，在网络正常的时候，日志从主库传给备库所需的时间是很短的，即 T2-T1 的值是非常小的。也就是说，网络正常情况下，主备延迟的主要来源是备库接收完 binlog 和执行完这个事务之间的时间差。</p><p>所以说，主备延迟最直接的表现是，备库消费中转日志（relay log）的速度，比主库生产 binlog 的速度要慢。接下来，分析下，这可能是由哪些原因导致的。</p><h2 id="主备延迟的来源"><a href="#主备延迟的来源" class="headerlink" title="主备延迟的来源"></a>主备延迟的来源</h2><h3 id="有些部署条件下，备库所在机器的性能要比主库所在的机器性能差"><a href="#有些部署条件下，备库所在机器的性能要比主库所在的机器性能差" class="headerlink" title="有些部署条件下，备库所在机器的性能要比主库所在的机器性能差"></a>有些部署条件下，备库所在机器的性能要比主库所在的机器性能差</h3><p>一般情况下，有人这么部署时的想法是，反正备库没有请求，所以可以用差一点儿的机器。或者，他们会把 20 个主库放在 4 台机器上，而把备库集中在一台机器上。</p><p>其实我们都知道，更新请求对 IOPS 的压力，在主库和备库上是无差别的。所以，做这种部署时，一般都会将备库设置为“非双 1”的模式。</p><p>但实际上，更新过程中也会触发大量的读操作。所以，当备库主机上的多个备库都在争抢资源的时候，就可能会导致主备延迟了。</p><p>当然，这种部署现在比较少了。因为主备可能发生切换，备库随时可能变成主库，所以主备库选用相同规格的机器，并且做对称部署，是现在比较常见的情况。</p><h3 id="备库的压力大"><a href="#备库的压力大" class="headerlink" title="备库的压力大"></a>备库的压力大</h3><p>一般的想法是，主库既然提供了写能力，那么备库可以提供一些读能力。或者一些运营后台需要的分析语句，不能影响正常业务，所以只能在备库上跑。</p><p>我真就见过不少这样的情况。由于主库直接影响业务，大家使用起来会比较克制，反而忽视了备库的压力控制。结果就是，备库上的查询耗费了大量的 CPU 资源，影响了同步速度，造成主备延迟。</p><p>这种情况，我们一般可以这么处理：</p><ul><li>一主多从。除了备库外，可以多接几个从库，让这些从库来分担读的压力。</li><li>通过 binlog 输出到外部系统，比如 Hadoop 这类系统，让外部系统提供统计类查询的能力。</li></ul><p>其中，一主多从的方式大都会被采用。因为作为数据库系统，还必须保证有定期全量备份的能力。而从库，就很适合用来做备份。</p><blockquote><p>备注：这里需要说明一下，从库和备库在概念上其实差不多。在我们这个专栏里，为了方便描述，我把会在 HA 过程中被选成新主库的，称为备库，其他的称为从库。</p></blockquote><h3 id="大事务"><a href="#大事务" class="headerlink" title="大事务"></a>大事务</h3><p>大事务这种情况很好理解。因为主库上必须等事务执行完成才会写入 binlog，再传给备库。所以，如果一个主库上的语句执行 10 分钟，那这个事务很可能就会导致从库延迟 10 分钟。</p><p>不知道你所在公司的 DBA 有没有跟你这么说过：不要一次性地用 delete 语句删除太多数据。其实，这就是一个典型的大事务场景。</p><p>比如，一些归档类的数据，平时没有注意删除历史数据，等到空间快满了，业务开发人员要一次性地删掉大量历史数据。同时，又因为要避免在高峰期操作会影响业务（至少有这个意识还是很不错的），所以会在晚上执行这些大量数据的删除操作。</p><p>结果，负责的 DBA 同学半夜就会收到延迟报警。然后，DBA 团队就要求你后续再删除数据的时候，要控制每个事务删除的数据量，分成多次删除。</p><h3 id="大表-DDL"><a href="#大表-DDL" class="headerlink" title="大表 DDL"></a>大表 DDL</h3><p>处理方案就是，计划内的 DDL，建议使用 gh-ost 方案</p><p>由于主备延迟的存在，所以在主备切换的时候，就相应的有不同的策略。</p><h2 id="可靠性优先策略"><a href="#可靠性优先策略" class="headerlink" title="可靠性优先策略"></a>可靠性优先策略</h2><p>在双 M 结构下，从状态 1 到状态 2 切换的详细过程是这样的：</p><ol><li>判断备库 B 现在的 seconds_behind_master，如果小于某个值（比如 5 秒）继续下一步，否则持续重试这一步；</li><li>把主库 A 改成只读状态，即把 readonly 设置为 true；</li><li>判断备库 B 的 seconds_behind_master 的值，直到这个值变成 0 为止；</li><li>把备库 B 改成可读写状态，也就是把 readonly 设置为 false；</li><li>把业务请求切到备库 B。</li></ol><p>这个切换流程，一般是由专门的 HA 系统来完成的，我们暂时称之为可靠性优先流程。</p><p><img src="/2021/03/19/2021-03-19-mysql-25/54f4c7c31e6f0f807c2ab77f78c8844a.png" alt="可靠性优先主备切换流程"></p><p>备注：图中的 SBM，是 seconds_behind_master 参数的简写。</p><p>可以看到，这个切换流程中是有不可用时间的。因为在步骤 2 之后，主库 A 和备库 B 都处于 readonly 状态，也就是说这时系统处于不可写状态，直到步骤 5 完成后才能恢复。</p><p>在这个不可用状态中，比较耗费时间的是步骤 3，可能需要耗费好几秒的时间。这也是为什么需要在步骤 1 先做判断，确保 seconds_behind_master 的值足够小。</p><p>试想如果一开始主备延迟就长达 30 分钟，而不先做判断直接切换的话，系统的不可用时间就会长达 30 分钟，这种情况一般业务都是不可接受的。</p><p>当然，系统的不可用时间，是由这个数据可靠性优先的策略决定的。你也可以选择可用性优先的策略，来把这个不可用时间几乎降为 0。</p><h2 id="可用性优先策略"><a href="#可用性优先策略" class="headerlink" title="可用性优先策略"></a>可用性优先策略</h2><p>如果强行把步骤 4、5 调整到最开始执行，也就是说不等主备数据同步，直接把连接切到备库 B，并且让备库 B 可以读写，那么系统几乎就没有不可用时间了。</p><p>我们把这个切换流程，暂时称作可用性优先流程。这个切换流程的代价，就是可能出现数据不一致的情况。</p><p>接下来，分享一个可用性优先流程产生数据不一致的例子。假设有一个表 t：</p><pre><code>mysql&gt; CREATE TABLE `t` (  `id` int(11) unsigned NOT NULL AUTO_INCREMENT,  `c` int(11) unsigned DEFAULT NULL,  PRIMARY KEY (`id`)) ENGINE=InnoDB;insert into t(c) values(1),(2),(3);</code></pre><p>这个表定义了一个自增主键 id，初始化数据后，主库和备库上都是 3 行数据。接下来，业务人员要继续在表 t 上执行两条插入语句的命令，依次是：</p><pre><code>insert into t(c) values(4);insert into t(c) values(5);</code></pre><p>假设，现在主库上其他的数据表有大量的更新，导致主备延迟达到 5 秒。在插入一条 c=4 的语句后，发起了主备切换。</p><p>下图是可用性优先策略，且 binlog_format=mixed 时的切换流程和数据结果。</p><p><img src="/2021/03/19/2021-03-19-mysql-25/3786bd6ad37faa34aca25bf1a1d8af3a.png" alt="可用性优先策略，且 binlog_format=mixed"></p><p>现在，我们一起分析下这个切换流程：</p><ol><li>步骤 2 中，主库 A 执行完 insert 语句，插入了一行数据（4,4），之后开始进行主备切换。</li><li>步骤 3 中，由于主备之间有 5 秒的延迟，所以备库 B 还没来得及应用“插入 c=4”这个中转日志，就开始接收客户端“插入 c=5”的命令。</li><li>步骤 4 中，备库 B 插入了一行数据（4,5），并且把这个 binlog 发给主库 A。</li><li>步骤 5 中，备库 B 执行“插入 c=4”这个中转日志，插入了一行数据（5,4）。而直接在备库 B 执行的“插入 c=5”这个语句，传到主库 A，就插入了一行新数据（5,5）。</li></ol><p>最后的结果就是，主库 A 和备库 B 上出现了两行不一致的数据。可以看到，这个数据不一致，是由可用性优先流程导致的。</p><p>那么，如果我还是用可用性优先策略，但设置 binlog_format=row，情况又会怎样呢？</p><p>因为 row 格式在记录 binlog 的时候，会记录新插入的行的所有字段值，所以最后只会有一行不一致。而且，两边的主备同步的应用线程会报错 duplicate key error 并停止。也就是说，这种情况下，备库 B 的 (5,4) 和主库 A 的 (5,5) 这两行数据，都不会被对方执行。</p><p><img src="/2021/03/19/2021-03-19-mysql-25/b8d2229b2b40dd087fd3b111d1bdda43.png" alt="可用性优先策略，且 binlog_format=row"></p><p>从上面的分析中，可以看到一些结论：</p><ul><li>使用 row 格式的 binlog 时，数据不一致的问题更容易被发现。而使用 mixed 或者 statement 格式的 binlog 时，数据很可能悄悄地就不一致了。如果你过了很久才发现数据不一致的问题，很可能这时的数据不一致已经不可查，或者连带造成了更多的数据逻辑不一致。</li><li>主备切换的可用性优先策略会导致数据不一致。因此，大多数情况下，都建议使用可靠性优先策略。毕竟对数据服务来说的话，数据的可靠性一般还是要优于可用性的。</li></ul><p>但事无绝对，有没有哪种情况数据的可用性优先级更高呢？答案是，有的。</p><p>我曾经碰到过这样的一个场景：</p><ul><li>有一个库的作用是记录操作日志。这时候，如果数据不一致可以通过 binlog 来修补，而这个短暂的不一致也不会引发业务问题。</li><li>同时，业务系统依赖于这个日志写入逻辑，如果这个库不可写，会导致线上的业务操作无法执行。</li></ul><p>这时候，你可能就需要选择先强行切换，事后再补数据的策略。</p><p>当然，事后复盘的时候，我们想到了一个改进措施就是，让业务逻辑不要依赖于这类日志的写入。也就是说，日志写入这个逻辑模块应该可以降级，比如写到本地文件，或者写到另外一个临时库里面。</p><p>这样的话，这种场景就又可以使用可靠性优先策略了。</p><p>接下来我们再看看，按照可靠性优先的思路，异常切换会是什么效果？</p><p>假设，主库 A 和备库 B 间的主备延迟是 30 分钟，这时候主库 A 掉电了，HA 系统要切换 B 作为主库。我们在主动切换的时候，可以等到主备延迟小于 5 秒的时候再启动切换，但这时候已经别无选择了。</p><p><img src="/2021/03/19/2021-03-19-mysql-25/553b7fc2d0dce3ec78bb595e1806eb8b.png" alt="可靠性优先策略，主库不可用"></p><p>采用可靠性优先策略的话，你就必须得等到备库 B 的 seconds_behind_master=0 之后，才能切换。但现在的情况比刚刚更严重，并不是系统只读、不可写的问题了，而是系统处于完全不可用的状态。因为，主库 A 掉电后，我们的连接还没有切到备库 B。</p><p>你可能会问，那能不能直接切换到备库 B，但是保持 B 只读呢？这样也不行。</p><p>因为，这段时间内，中转日志还没有应用完成，如果直接发起主备切换，客户端查询看不到之前执行完成的事务，会认为有“数据丢失”。</p><p>虽然随着中转日志的继续应用，这些数据会恢复回来，但是对于一些业务来说，查询到“暂时丢失数据的状态”也是不能被接受的。</p><p>在满足数据可靠性的前提下，MySQL 高可用系统的可用性，是依赖于主备延迟的。延迟的时间越小，在主库故障的时候，服务恢复需要的时间就越短，可用性就越高。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ol><li><p>主备延迟，就是在同一个事务在备库执行完成的时间和主库执行完成的时间之间的差值，包括主库事务执行完成时间和将binlog发送给备库，备库事务的执行完成时间的差值。每个事务的seconds_behind_master延迟时间，每个事务的 binlog 里面都有一个时间字段，用于记录主库上的写入时间，备库取出当前正在执行的事务的时间字段的值，计算它与当前系统时的差值。在备库上执行 show slave status，采集 seconds_behind_master 的值。</p></li><li><p>主备延迟的来源</p><ul><li>备库所在机器的性能要比主库所在的机器性能差：有些部署条件下，备库所在机器的性能要比主库所在的机器性能差，原因多个备库部署在同一台机器上，大量的查询会导致io资源的竞争，解决办法是配置”双1“，redo log和binlog都只write fs page cache</li><li>备库的压力大：产生的原因大量的查询操作在备库操作，耗费了大量的cpu，导致同步延迟，解决办法，使用一主多从，多个从减少备的查询压力</li><li>大事务：因为如果一个大的事务的dml操作导致执行时间过长，将其事务binlog发送给备库，备库也需执行那么长时间，导致主备延迟，解决办法尽量减少大事务，比如delete操作，使用limit分批删除，可以防止大事务也可以减少锁的范围。</li><li>大表的ddl：会导致主库将其ddl binlog发送给备库，备库解析中转日志，同步，后续的dml binlog发送过来，需等待ddl的mdl写锁释放，导致主备延迟。</li></ul></li><li><p>可靠性优先策略</p><ul><li><p>判断备库 B 现在的 seconds_behind_master如果小于某个值（比如 5 秒）继续下一步，否则持续重试这一步</p></li><li><p>把主库 A 改成只读状态，即把 readonly 设置为 true</p></li><li><p>判断备库 B 的 seconds_behind_master的值，直到这个值变成 0 为止； </p></li><li><p>把备库 B 改成可读写也就是把 readonly 设置为 false；</p></li><li><p>把业务请求切换到备库.</p><p>个人理解如果发送过来的binlog在中转日志中有多个事务，只能readonly，业务不能更新的时间，就是多个事务被运用的总时间，因为要等到seconds_behind_master等于0才能切换主备。如果非正常情况下，主库掉电，会导致出现的问题，如果备库和主库的延迟时间短，在中转日志运用完成，业务才能正常使用（造成一定时间内不可用，不能操作数据库）；如果在中转日志还未运用完成，备库切换为主库会导致之前完成的事务还没同步完成，造成查询数据丢失，随着中转日志的继续应用，这些数据会恢复回来，再次查询又有数据让人感觉很诡异。</p></li></ul></li><li><p>可用性策略，出现的问题：在双m，且binlog_format=mixed，会导致主备数据不一致。使用使用 row 格式的 binlog 时，数据不一致的问题更容易发现，因为binlog row会记录字段的所有值。</p></li><li><p>使用 row 格式的 binlog 时，数据不一致的问题更容易被发现。而使用 mixed 或者 statement 格式的 binlog 时，数据很可能悄悄地就不一致了。如果你过了很久才发现数据不一致的问题，很可能这时的数据不一致已经不可查，或者连带造成了更多的数据逻辑不一致。</p></li><li><p>主备切换的可用性优先策略会导致数据不一致。因此，大多数情况下，我都建议你使用可靠性优先策略。毕竟对数据服务来说的话，数据的可靠性一般还是要优于可用性的。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>主备一致</title>
      <link href="/2021/03/17/2021-03-17-mysql-24/"/>
      <url>/2021/03/17/2021-03-17-mysql-24/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="MySQL-主备的基本原理"><a href="#MySQL-主备的基本原理" class="headerlink" title="MySQL 主备的基本原理"></a>MySQL 主备的基本原理</h2><p><img src="/2021/03/17/2021-03-17-mysql-24/fd75a2b37ae6ca709b7f16fe060c2c10.png" alt="MySQL 主备切换流程"></p><p>在状态 1 中，客户端的读写都直接访问节点 A，而节点 B 是 A 的备库，只是将 A 的更新都同步过来，到本地执行。这样可以保持节点 B 和 A 的数据是相同的。</p><p>当需要切换的时候，就切成状态 2。这时候客户端读写访问的都是节点 B，而节点 A 是 B 的备库。</p><p>在状态 1 中，虽然节点 B 没有被直接访问，但是我依然建议你把节点 B（也就是备库）设置成只读（readonly）模式。这样做，有以下几个考虑：</p><ul><li>有时候一些运营类的查询语句会被放到备库上去查，设置为只读可以防止误操作；</li><li>防止切换逻辑有 bug，比如切换过程中出现双写，造成主备不一致；</li><li>可以用 readonly 状态，来判断节点的角色。</li></ul><p>你可能会问，我把备库设置成只读了，还怎么跟主库保持同步更新呢？</p><p>这个问题，你不用担心。因为 readonly 设置对超级 (super) 权限用户是无效的，而用于同步更新的线程，就拥有超级权限。</p><p>接下来，我们再看看节点 A 到 B 这条线的内部流程是什么样的。图中画出的就是一个 update 语句在节点 A 执行，然后同步到节点 B 的完整流程图。</p><p><img src="/2021/03/17/2021-03-17-mysql-24/a66c154c1bc51e071dd2cc8c1d6ca6a3.png" alt="主备流程图"></p><p>图中，可以看到：主库接收到客户端的更新请求后，执行内部事务的更新逻辑，同时写 binlog。</p><p>备库 B 跟主库 A 之间维持了一个长连接。主库 A 内部有一个线程，专门用于服务备库 B 的这个长连接。一个事务日志同步的完整过程是这样的：</p><ol><li>在备库 B 上通过 change master 命令，设置主库 A 的 IP、端口、用户名、密码，以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量。</li><li>在备库 B 上执行 start slave 命令，这时候备库会启动两个线程，就是图中的 io_thread 和 sql_thread。其中 io_thread 负责与主库建立连接。</li><li>主库 A 校验完用户名、密码后，开始按照备库 B 传过来的位置，从本地读取 binlog，发给 B。</li><li>备库 B 拿到 binlog 后，写到本地文件，称为中转日志（relay log）。</li><li>sql_thread 读取中转日志，解析出日志里的命令，并执行。</li></ol><p>这里需要说明，后来由于多线程复制方案的引入，sql_thread 演化成为了多个线程，跟我们今天要介绍的原理没有直接关系，暂且不展开。</p><p>分析完了这个长连接的逻辑，我们再来看一个问题：binlog 里面到底是什么内容，为什么备库拿过去可以直接执行。</p><h2 id="binlog-的三种格式对比"><a href="#binlog-的三种格式对比" class="headerlink" title="binlog 的三种格式对比"></a>binlog 的三种格式对比</h2><p>binlog 有两种格式，一种是 statement，一种是 row。可能你在其他资料上还会看到有第三种格式，叫作 mixed，其实它就是前两种格式的混合。</p><p>为了便于描述 binlog 的这三种格式间的区别，我创建了一个表，并初始化几行数据。</p><pre><code>mysql&gt; CREATE TABLE `t` (  `id` int(11) NOT NULL,  `a` int(11) DEFAULT NULL,  `t_modified` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,  PRIMARY KEY (`id`),  KEY `a` (`a`),  KEY `t_modified`(`t_modified`)) ENGINE=InnoDB;insert into t values(1,1,&#39;2018-11-13&#39;);insert into t values(2,2,&#39;2018-11-12&#39;);insert into t values(3,3,&#39;2018-11-11&#39;);insert into t values(4,4,&#39;2018-11-10&#39;);insert into t values(5,5,&#39;2018-11-09&#39;);</code></pre><p>如果要在表中删除一行数据的话，我们来看看这个 delete 语句的 binlog 是怎么记录的。</p><p>注意，下面这个语句包含注释，如果你用 MySQL 客户端来做这个实验的话，要记得加 -c 参数，否则客户端会自动去掉注释。</p><pre><code>mysql&gt; delete from t /*comment*/  where a&gt;=4 and t_modified&lt;=&#39;2018-11-10&#39; limit 1;</code></pre><p>当 binlog_format=statement 时，binlog 里面记录的就是 SQL 语句的原文。你可以用命令看 binlog 中的内容。</p><pre><code>mysql&gt; show binlog events in &#39;master.000001&#39;;</code></pre><p><img src="/2021/03/17/2021-03-17-mysql-24/b9818f73cd7d38a96ddcb75350b52931.png" alt></p><ul><li>第一行 SET @@SESSION.GTID_NEXT=’ANONYMOUS’你可以先忽略，后面文章我们会在介绍主备切换的时候再提到；</li><li>第二行是一个 BEGIN，跟第四行的 commit 对应，表示中间是一个事务；</li><li>第三行就是真实执行的语句了。可以看到，在真实执行的 delete 命令之前，还有一个“use ‘test’”命令。这条命令不是我们主动执行的，而是 MySQL 根据当前要操作的表所在的数据库，自行添加的。这样做可以保证日志传到备库去执行的时候，不论当前的工作线程在哪个库里，都能够正确地更新到 test 库的表 t。use ‘test’命令之后的 delete 语句，就是我们输入的 SQL 原文了。可以看到，binlog“忠实”地记录了 SQL 命令，甚至连注释也一并记录了。</li><li>最后一行是一个 COMMIT。你可以看到里面写着 xid=61。</li></ul><p>为了说明 statement 和 row 格式的区别，我们来看一下这条 delete 命令的执行效果图：</p><p><img src="/2021/03/17/2021-03-17-mysql-24/96c2be9c0fcbff66883118526b26652b.png" alt></p><p>可以看到，运行这条 delete 命令产生了一个 warning，原因是当前 binlog 设置的是 statement 格式，并且语句中有 limit，所以这个命令可能是 unsafe 的。</p><p>为什么这么说呢？这是因为 delete 带 limit，很可能会出现主备数据不一致的情况。比如上面这个例子：</p><ul><li>如果 delete 语句使用的是索引 a，那么会根据索引 a 找到第一个满足条件的行，也就是说删除的是 a=4 这一行；</li><li>但如果使用的是索引 t_modified，那么删除的就是 t_modified=’2018-11-09’也就是 a=5 这一行。</li></ul><p>由于 statement 格式下，记录到 binlog 里的是语句原文，因此可能会出现这样一种情况：在主库执行这条 SQL 语句的时候，用的是索引 a；而在备库执行这条 SQL 语句的时候，却使用了索引 t_modified。因此，MySQL 认为这样写是有风险的。</p><p>那么，如果我把 binlog 的格式改为 binlog_format=‘row’， 是不是就没有这个问题了呢？我们先来看看这时候 binog 中的内容吧。</p><p><img src="/2021/03/17/2021-03-17-mysql-24/d67a38db154afff610ae3bb64e266826.png" alt></p><p>可以看到，与 statement 格式的 binlog 相比，前后的 BEGIN 和 COMMIT 是一样的。但是，row 格式的 binlog 里没有了 SQL 语句的原文，而是替换成了两个 event：Table_map 和 Delete_rows。</p><ul><li>Table_map event，用于说明接下来要操作的表是 test 库的表 t;</li><li>Delete_rows event，用于定义删除的行为。</li></ul><p>其实，我们通过上图是看不到详细信息的，还需要借助 mysqlbinlog 工具，用下面这个命令解析和查看 binlog 中的内容。因为上图中的信息显示，这个事务的 binlog 是从 8900 这个位置开始的，所以可以用 start-position 参数来指定从这个位置的日志开始解析。</p><pre><code>mysqlbinlog  -vv data/master.000001 --start-position=8900;</code></pre><p><img src="/2021/03/17/2021-03-17-mysql-24/c342cf480d23b05d30a294b114cebfc2.png" alt></p><p>从这个图中，我们可以看到以下几个信息：</p><ul><li><p>server id 1，表示这个事务是在 server_id=1 的这个库上执行的。</p></li><li><p>每个 event 都有 CRC32 的值，这是因为我把参数 binlog_checksum 设置成了 CRC32。</p></li><li><p>Table_map event 跟在上上图中看到的相同，显示了接下来要打开的表，map 到数字 226。现在我们这条 SQL 语句只操作了一张表，如果要操作多张表呢？每个表都有一个对应的 Table_map event、都会 map 到一个单独的数字，用于区分对不同表的操作。</p></li><li><p>我们在 mysqlbinlog 的命令中，使用了 -vv 参数是为了把内容都解析出来，所以从结果里面可以看到各个字段的值（比如，@1=4、 @2=4 这些值）。</p></li><li><p>binlog_row_image 的默认配置是 FULL，因此 Delete_event 里面，包含了删掉的行的所有字段的值。如果把 binlog_row_image 设置为 MINIMAL，则只会记录必要的信息，在这个例子里，就是只会记录 id=4 这个信息。</p></li><li><p>最后的 Xid event，用于表示事务被正确地提交了。</p></li></ul><p>你可以看到，当 binlog_format 使用 row 格式的时候，binlog 里面记录了真实删除行的主键 id，这样 binlog 传到备库去的时候，就肯定会删除 id=4 的行，不会有主备删除不同行的问题。</p><h2 id="为什么会有-mixed-格式的-binlog？"><a href="#为什么会有-mixed-格式的-binlog？" class="headerlink" title="为什么会有 mixed 格式的 binlog？"></a>为什么会有 mixed 格式的 binlog？</h2><p>基于上面的信息，我们来讨论一个问题：为什么会有 mixed 这种 binlog 格式的存在场景？推论过程是这样的：</p><ul><li>因为有些 statement 格式的 binlog 可能会导致主备不一致，所以要使用 row 格式。</li><li>但 row 格式的缺点是，很占空间。比如你用一个 delete 语句删掉 10 万行数据，用 statement 的话就是一个 SQL 语句被记录到 binlog 中，占用几十个字节的空间。但如果用 row 格式的 binlog，就要把这 10 万条记录都写到 binlog 中。这样做，不仅会占用更大的空间，同时写 binlog 也要耗费 IO 资源，影响执行速度。</li><li>所以，MySQL 就取了个折中方案，也就是有了 mixed 格式的 binlog。mixed 格式的意思是，MySQL 自己会判断这条 SQL 语句是否可能引起主备不一致，如果有可能，就用 row 格式，否则就用 statement 格式。</li></ul><p>也就是说，mixed 格式可以利用 statment 格式的优点，同时又避免了数据不一致的风险。</p><p>因此，如果你的线上 MySQL 设置的 binlog 格式是 statement 的话，那基本上就可以认为这是一个不合理的设置。你至少应该把 binlog 的格式设置为 mixed。</p><p>比如我们这个例子，设置为 mixed 后，就会记录为 row 格式；而如果执行的语句去掉 limit 1，就会记录为 statement 格式。</p><p>当然我要说的是，现在越来越多的场景要求把 MySQL 的 binlog 格式设置成 row。这么做的理由有很多，我来给你举一个可以直接看出来的好处：恢复数据。</p><p>接下来，我们就分别从 delete、insert 和 update 这三种 SQL 语句的角度，来看看数据恢复的问题。</p><p>通过上图你可以看出来，即使我执行的是 delete 语句，row 格式的 binlog 也会把被删掉的行的整行信息保存起来。所以，如果你在执行完一条 delete 语句以后，发现删错数据了，可以直接把 binlog 中记录的 delete 语句转成 insert，把被错删的数据插入回去就可以恢复了。</p><p>如果你是执行错了 insert 语句呢？那就更直接了。row 格式下，insert 语句的 binlog 里会记录所有的字段信息，这些信息可以用来精确定位刚刚被插入的那一行。这时，你直接把 insert 语句转成 delete 语句，删除掉这被误插入的一行数据就可以了。</p><p>如果执行的是 update 语句的话，binlog 里面会记录修改前整行的数据和修改后的整行数据。所以，如果你误执行了 update 语句的话，只需要把这个 event 前后的两行信息对调一下，再去数据库里面执行，就能恢复这个更新操作了。</p><p>其实，由 delete、insert 或者 update 语句导致的数据操作错误，需要恢复到操作之前状态的情况，也时有发生。MariaDB 的Flashback工具就是基于上面介绍的原理来回滚数据的。</p><p>虽然 mixed 格式的 binlog 现在已经用得不多了，但这里我还是要再借用一下 mixed 格式来说明一个问题，来看一下这条 SQL 语句：</p><pre><code>mysql&gt; insert into t values(10,10, now());</code></pre><p>如果我们把 binlog 格式设置为 mixed，你觉得 MySQL 会把它记录为 row 格式还是 statement 格式呢？</p><p>先不要着急说结果，我们一起来看一下这条语句执行的效果。</p><p><img src="/2021/03/17/2021-03-17-mysql-24/0150301698979255a6f27711c35e9eef.png" alt></p><p>可以看到，MySQL 用的居然是 statement 格式。你一定会奇怪，如果这个 binlog 过了 1 分钟才传给备库的话，那主备的数据不就不一致了吗？</p><p>接下来，我们再用 mysqlbinlog 工具来看看：</p><p><img src="/2021/03/17/2021-03-17-mysql-24/1ad3a4c4b9a71955edba5195757dd041.png" alt></p><p>从图中的结果可以看到，原来 binlog 在记录 event 的时候，多记了一条命令：SET TIMESTAMP=1546103491。它用 SET TIMESTAMP 命令约定了接下来的 now() 函数的返回时间。</p><p>因此，不论这个 binlog 是 1 分钟之后被备库执行，还是 3 天后用来恢复这个库的备份，这个 insert 语句插入的行，值都是固定的。也就是说，通过这条 SET TIMESTAMP 命令，MySQL 就确保了主备数据的一致性。</p><p>我之前看过有人在重放 binlog 数据的时候，是这么做的：用 mysqlbinlog 解析出日志，然后把里面的 statement 语句直接拷贝出来执行。</p><p>你现在知道了，这个方法是有风险的。因为有些语句的执行结果是依赖于上下文命令的，直接执行的结果很可能是错误的。</p><p>所以，用 binlog 来恢复数据的标准做法是，用 mysqlbinlog 工具解析出来，然后把解析结果整个发给 MySQL 执行。类似下面的命令：</p><pre><code>mysqlbinlog master.000001  --start-position=2738 --stop-position=2973 | mysql -h127.0.0.1 -P13000 -u$user -p$pwd;</code></pre><p>这个命令的意思是，将 master.000001 文件里面从第 2738 字节到第 2973 字节中间这段内容解析出来，放到 MySQL 去执行。</p><h2 id="循环复制问题"><a href="#循环复制问题" class="headerlink" title="循环复制问题"></a>循环复制问题</h2><p>通过上面对 MySQL 中 binlog 基本内容的理解，你现在可以知道，binlog 的特性确保了在备库执行相同的 binlog，可以得到与主库相同的状态。</p><p>因此，我们可以认为正常情况下主备的数据是一致的。也就是说，第一个图中 A、B 两个节点的内容是一致的。其实，第一个图中画的是 M-S 结构，但实际生产上使用比较多的是双 M 结构，也就是下图所示的主备切换流程。</p><p><img src="/2021/03/17/2021-03-17-mysql-24/20ad4e163115198dc6cf372d5116c956.png" alt="主备切换流程 -- 双 M 结构"></p><p>双 M 结构和 M-S 结构，其实区别只是多了一条线，即：节点 A 和 B 之间总是互为主备关系。这样在切换的时候就不用再修改主备关系。</p><p>但是，双 M 结构还有一个问题需要解决。</p><p>业务逻辑在节点 A 上更新了一条语句，然后再把生成的 binlog 发给节点 B，节点 B 执行完这条更新语句后也会生成 binlog。（我建议你把参数 log_slave_updates 设置为 on，表示备库执行 relay log 后生成 binlog）。</p><p>那么，如果节点 A 同时是节点 B 的备库，相当于又把节点 B 新生成的 binlog 拿过来执行了一次，然后节点 A 和 B 间，会不断地循环执行这个更新语句，也就是循环复制了。这个要怎么解决呢？</p><p>从上面的第六个图中可以看到，MySQL 在 binlog 中记录了这个命令第一次执行时所在实例的 server id。因此，我们可以用下面的逻辑，来解决两个节点间的循环复制的问题：</p><ul><li>规定两个库的 server id 必须不同，如果相同，则它们之间不能设定为主备关系；</li><li>一个备库接到 binlog 并在重放的过程中，生成与原 binlog 的 server id 相同的新的 binlog；</li><li>每个库在收到从自己的主库发过来的日志后，先判断 server id，如果跟自己的相同，表示这个日志是自己生成的，就直接丢弃这个日志。</li></ul><p>按照这个逻辑，如果我们设置了双 M 结构，日志的执行流就会变成这样：</p><ol><li>从节点 A 更新的事务，binlog 里面记的都是 A 的 server id；</li><li>传到节点 B 执行一次以后，节点 B 生成的 binlog 的 server id 也是 A 的 server id；</li><li>再传回给节点 A，A 判断到这个 server id 与自己的相同，就不会再处理这个日志。所以，死循环在这里就断掉了。</li></ol><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ol><li><p>主从复制</p><p> <img src="/2021/03/17/2021-03-17-mysql-24/fd75a2b37ae6ca709b7f16fe060c2c10.png" alt="MySQL 主备切换流程"></p><p> <img src="/2021/03/17/2021-03-17-mysql-24/a66c154c1bc51e071dd2cc8c1d6ca6a3.png" alt="主备流程图"></p><p> 备库 B 跟主库 A 之间维持了一个长连接。主库 A 内部有一个线程，专门用于服务备库 B 的这个长连接。一个事务日志同步的完整过程是这样的：</p><ol><li>在备库 B 上通过 change master 命令，设置主库 A 的 IP、端口、用户名、密码，以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量。</li><li>在备库 B 上执行 start slave 命令，这时候备库会启动两个线程，就是图中的 io_thread 和 sql_thread。其中 io_thread 负责与主库建立连接。</li><li>主库 A 校验完用户名、密码后，开始按照备库 B 传过来的位置，从本地读取 binlog，发给 B。</li><li>备库 B 拿到 binlog 后，写到本地文件，称为中转日志（relay log）。</li><li>sql_thread 读取中转日志，解析出日志里的命令，并执行。</li></ol></li></ol><ol start="2"><li>binlog有三种格式：</li></ol><ul><li>statement，记录数据操作的命令，<br>  优点：对人友好，直接记录原命令，数据量小<br>  缺点：可能会导导致主备数据不一致</li><li>row 记录数据的操作，推荐的格式<br>  优点：会明确操作那一行，不会出现主备数据不一致的情况；会记录原数据的整行信息，数据恢复小能手。<br>  缺点：数据量大，传输带宽和性能有损。例如删除10W行数据，statement数据仅仅记录命令，而row就要把这 10 万条记录都写到 binlog 中。这样做，不仅会占用更大的空间，同时写 binlog 也要耗费 IO 资源，影响执行速度。</li><li>mixed 自行判断使用statement格式还是row格式<br>  优点：MySQL 自己会判断这条 SQL 语句是否可能引起主备不一致，如果有可能，就用 row 格式，否则就用 statement 格式。<br>  缺点：有些情况和常识相冲，其日志不能简单的拿来用于数据恢复</li></ul><ol start="3"><li><p>双 M 结构</p><p> <img src="/2021/03/17/2021-03-17-mysql-24/20ad4e163115198dc6cf372d5116c956.png" alt="主备切换流程 -- 双 M 结构"></p><p> 根据server id来解决循环复制问题</p><ul><li>规定两个库的 server id 必须不同，如果相同，则它们之间不能设定为主备关系；</li><li>一个备库接到 binlog 并在重放的过程中，生成与原 binlog 的 server id 相同的新的 binlog；</li><li>每个库在收到从自己的主库发过来的日志后，先判断 server id，如果跟自己的相同，表示这个日志是自己生成的，就直接丢弃这个日志。</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>保证数据不丢</title>
      <link href="/2021/03/15/2021-03-15-mysql-23/"/>
      <url>/2021/03/15/2021-03-15-mysql-23/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="binlog-的写入机制"><a href="#binlog-的写入机制" class="headerlink" title="binlog 的写入机制"></a>binlog 的写入机制</h2><p>其实，binlog 的写入逻辑比较简单：事务执行过程中，先把日志写到 binlog cache，事务提交的时候，再把 binlog cache 写到 binlog 文件中。</p><p>一个事务的 binlog 是不能被拆开的，因此不论这个事务多大，也要确保一次性写入。这就涉及到了 binlog cache 的保存问题。</p><p>系统给 binlog cache 分配了一片内存，每个线程一个，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。</p><p>事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 中，并清空 binlog cache。状态如图所示。</p><p><img src="/2021/03/15/2021-03-15-mysql-23/9ed86644d5f39efb0efec595abb92e3e.png" alt="binlog 写盘状态"></p><p>可以看到，每个线程有自己 binlog cache，但是共用同一份 binlog 文件。</p><ul><li>图中的 write，指的就是指把日志写入到文件系统的 page cache，并没有把数据持久化到磁盘，所以速度比较快。</li><li>图中的 fsync，才是将数据持久化到磁盘的操作。一般情况下，我们认为 fsync 才占磁盘的 IOPS。</li></ul><p>write 和 fsync 的时机，是由参数 sync_binlog 控制的：</p><ul><li>sync_binlog=0 的时候，表示每次提交事务都只 write，不 fsync；</li><li>sync_binlog=1 的时候，表示每次提交事务都会执行 fsync；</li><li>sync_binlog=N(N&gt;1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。</li></ul><p>因此，在出现 IO 瓶颈的场景里，将 sync_binlog 设置成一个比较大的值，可以提升性能。在实际的业务场景中，考虑到丢失日志量的可控性，一般不建议将这个参数设成 0，比较常见的是将其设置为 100~1000 中的某个数值。</p><p>但是，将 sync_binlog 设置为 N，对应的风险是：如果主机发生异常重启，会丢失最近 N 个事务的 binlog 日志。</p><h2 id="redo-log-的写入机制"><a href="#redo-log-的写入机制" class="headerlink" title="redo log 的写入机制"></a>redo log 的写入机制</h2><p>事务在执行过程中，生成的 redo log 是要先写到 redo log buffer 的。</p><p>redo log buffer 里面的内容，是不是每次生成后都要直接持久化到磁盘呢？答案是，不需要。</p><p>如果事务执行期间 MySQL 发生异常重启，那这部分日志就丢了。由于事务并没有提交，所以这时日志丢了也不会有损失。</p><p>那么，另外一个问题是，事务还没提交的时候，redo log buffer 中的部分日志有没有可能被持久化到磁盘呢？答案是，确实会有。</p><p>这个问题，要从 redo log 可能存在的三种状态说起。这三种状态，对应的就是图中的三个颜色块。</p><p><img src="/2021/03/15/2021-03-15-mysql-23/9d057f61d3962407f413deebc80526d4.png" alt="MySQL redo log 存储状态"></p><p>这三种状态分别是：</p><ul><li>存在 redo log buffer 中，物理上是在 MySQL 进程内存中，就是图中的红色部分；</li><li>写到磁盘 (write)，但是没有持久化（fsync)，物理上是在文件系统的 page cache 里面，也就是图中的黄色部分；</li><li>持久化到磁盘，对应的是 hard disk，也就是图中的绿色部分。</li></ul><p>日志写到 redo log buffer 是很快的，wirte 到 page cache 也差不多，但是持久化到磁盘的速度就慢多了。</p><p>为了控制 redo log 的写入策略，InnoDB 提供了 innodb_flush_log_at_trx_commit 参数，它有三种可能取值：</p><ul><li>设置为 0 的时候，表示每次事务提交时都只是把 redo log 留在 redo log buffer 中 ;</li><li>设置为 1 的时候，表示每次事务提交时都将 redo log 直接持久化到磁盘；</li><li>设置为 2 的时候，表示每次事务提交时都只是把 redo log 写到 page cache。</li></ul><p>InnoDB 有一个后台线程，每隔 1 秒，就会把 redo log buffer 中的日志，调用 write 写到文件系统的 page cache，然后调用 fsync 持久化到磁盘。</p><p>注意，事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些 redo log 也会被后台线程一起持久化到磁盘。也就是说，一个没有提交的事务的 redo log，也是可能已经持久化到磁盘的。</p><p>实际上，除了后台线程每秒一次的轮询操作外，还有两种场景会让一个没有提交的事务的 redo log 写入到磁盘中。</p><ul><li><p>一种是，redo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动写盘。注意，由于这个事务并没有提交，所以这个写盘动作只是 write，而没有调用 fsync，也就是只留在了文件系统的 page cache。</p></li><li><p>另一种是，并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘。假设一个事务 A 执行到一半，已经写了一些 redo log 到 buffer 中，这时候有另外一个线程的事务 B 提交，如果 innodb_flush_log_at_trx_commit 设置的是 1，那么按照这个参数的逻辑，事务 B 要把 redo log buffer 里的日志全部持久化到磁盘。这时候，就会带上事务 A 在 redo log buffer 里的日志一起持久化到磁盘。</p></li></ul><p>这里需要说明的是，我们介绍两阶段提交的时候说过，时序上 redo log 先 prepare， 再写 binlog，最后再把 redo log commit。</p><p>如果把 innodb_flush_log_at_trx_commit 设置成 1，那么 redo log 在 prepare 阶段就要持久化一次，因为有一个崩溃恢复逻辑是要依赖于 prepare 的 redo log，再加上 binlog 来恢复的。</p><p>每秒一次后台轮询刷盘，再加上崩溃恢复这个逻辑，InnoDB 就认为 redo log 在 commit 的时候就不需要 fsync 了，只会 write 到文件系统的 page cache 中就够了。</p><p>通常我们说 MySQL 的“双 1”配置，指的就是 sync_binlog 和 innodb_flush_log_at_trx_commit 都设置成 1。也就是说，一个事务完整提交前，需要等待两次刷盘，一次是 redo log（prepare 阶段），一次是 binlog。</p><p>这时候，你可能有一个疑问，这意味着我从 MySQL 看到的 TPS 是每秒两万的话，每秒就会写四万次磁盘。但是，我用工具测试出来，磁盘能力也就两万左右，怎么能实现两万的 TPS？</p><p>解释这个问题，就要用到组提交（group commit）机制了。</p><p>这里，我需要先和你介绍日志逻辑序列号（log sequence number，LSN）的概念。LSN 是单调递增的，用来对应 redo log 的一个个写入点。每次写入长度为 length 的 redo log， LSN 的值就会加上 length。</p><p>LSN 也会写到 InnoDB 的数据页中，来确保数据页不会被多次执行重复的 redo log。</p><p>如图所示，是三个并发事务 (trx1, trx2, trx3) 在 prepare 阶段，都写完 redo log buffer，持久化到磁盘的过程，对应的 LSN 分别是 50、120 和 160。</p><p><img src="/2021/03/15/2021-03-15-mysql-23/933fdc052c6339de2aa3bf3f65b188cc.png" alt="redo log 组提交"></p><ol><li><p>trx1 是第一个到达的，会被选为这组的 leader；</p></li><li><p>等 trx1 要开始写盘的时候，这个组里面已经有了三个事务，这时候 LSN 也变成了 160；</p></li><li><p>trx1 去写盘的时候，带的就是 LSN=160，因此等 trx1 返回时，所有 LSN 小于等于 160 的 redo log，都已经被持久化到磁盘；</p></li><li><p>这时候 trx2 和 trx3 就可以直接返回了。</p></li></ol><p>所以，一次组提交里面，组员越多，节约磁盘 IOPS 的效果越好。但如果只有单线程压测，那就只能老老实实地一个事务对应一次持久化操作了。</p><p>在并发更新场景下，第一个事务写完 redo log buffer 以后，接下来这个 fsync 越晚调用，组员可能越多，节约 IOPS 的效果就越好。</p><p>为了让一次 fsync 带的组员更多，MySQL 有一个很有趣的优化：拖时间。在介绍两阶段提交的时候，我曾经给你画了一个图，现在我把它截过来。</p><p><img src="/2021/03/15/2021-03-15-mysql-23/98b3b4ff7b36d6d72e38029b86870551.png" alt="两阶段提交"></p><p>图中，我把“写 binlog”当成一个动作。但实际上，写 binlog 是分成两步的：</p><ul><li>先把 binlog 从 binlog cache 中写到磁盘上的 binlog 文件；</li><li>调用 fsync 持久化。</li></ul><p>MySQL 为了让组提交的效果更好，把 redo log 做 fsync 的时间拖到了步骤 1 之后。也就是说，上面的图变成了这样：</p><p><img src="/2021/03/15/2021-03-15-mysql-23/5ae7d074c34bc5bd55c82781de670c28.png" alt="两阶段提交细化"></p><p>这么一来，binlog 也可以组提交了。在执行图中第 4 步把 binlog fsync 到磁盘时，如果有多个事务的 binlog 已经写完了，也是一起持久化的，这样也可以减少 IOPS 的消耗。</p><p>binlog 的 write 和 fsync 间的间隔时间短，导致能集合到一起持久化的 binlog 比较少，因此 binlog 的组提交的效果通常不如 redo log 的效果那么好。</p><p>如果你想提升 binlog 组提交的效果，可以通过设置 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 来实现。</p><ul><li>binlog_group_commit_sync_delay 参数，表示延迟多少微秒后才调用 fsync;</li><li>binlog_group_commit_sync_no_delay_count 参数，表示累积多少次以后才调用 fsync。</li></ul><p>这两个条件是或的关系，也就是说只要有一个满足条件就会调用 fsync。</p><p>所以，当 binlog_group_commit_sync_delay 设置为 0 的时候，binlog_group_commit_sync_no_delay_count 也无效了。</p><p>WAL 机制是减少磁盘写，可是每次提交事务都要写 redo log 和 binlog，这磁盘读写次数也没变少呀？</p><p>现在你就能理解了，WAL 机制主要得益于两个方面：</p><ul><li>redo log 和 binlog 都是顺序写，磁盘的顺序写比随机写速度要快；</li><li>组提交机制，可以大幅度降低磁盘的 IOPS 消耗。</li></ul><p>如果你的 MySQL 现在出现了性能瓶颈，而且瓶颈在 IO 上，可以通过哪些方法来提升性能呢？</p><p>针对这个问题，可以考虑以下三种方法：</p><ul><li>设置 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数，减少 binlog 的写盘次数。这个方法是基于“额外的故意等待”来实现的，因此可能会增加语句的响应时间，但没有丢失数据的风险。</li><li>将 sync_binlog 设置为大于 1 的值（比较常见是 100~1000）。这样做的风险是，主机掉电时会丢 binlog 日志。</li><li>将 innodb_flush_log_at_trx_commit 设置为 2。这样做的风险是，主机掉电的时候会丢数据。</li></ul><p>我不建议你把 innodb_flush_log_at_trx_commit 设置成 0。因为把这个参数设置成 0，表示 redo log 只保存在内存中，这样的话 MySQL 本身异常重启也会丢数据，风险太大。而 redo log 写到文件系统的 page cache 的速度也是很快的，所以将这个参数设置成 2 跟设置成 0 其实性能差不多，但这样做 MySQL 异常重启时就不会丢数据了，相比之下风险会更小。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><img src="/2021/03/15/2021-03-15-mysql-23/9ed86644d5f39efb0efec595abb92e3e.png" alt="binlog 写盘状态"></p><p>事务执行过程中，先把日志写到 binlog cache，事务提交的时候，再把 binlog cache 写到 binlog 文件中;</p><p>图中的 write，指的就是指把日志写入到文件系统的 page cache，并没有把数据持久化到磁盘，所以速度比较快。</p><p>图中的 fsync，才是将数据持久化到磁盘的操作。一般情况下，我们认为 fsync 才占磁盘的 IOPS。</p><p>事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 中，并清空 binlog cache。</p><p><img src="/2021/03/15/2021-03-15-mysql-23/9ed86644d5f39efb0efec595abb92e3e.png" alt="binlog 写盘状态"></p><p>存在 redo log buffer 中，物理上是在 MySQL 进程内存中，就是图中的红色部分；</p><p>写到磁盘 (write)，但是没有持久化（fsync)，物理上是在文件系统的 page cache 里面，也就是图中的黄色部分；</p><p>持久化到磁盘，对应的是 hard disk，也就是图中的绿色部分。</p><p><img src="/2021/03/15/2021-03-15-mysql-23/98b3b4ff7b36d6d72e38029b86870551.png" alt="两阶段提交"></p>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>提高性能</title>
      <link href="/2021/03/13/2021-03-13-mysql-22/"/>
      <url>/2021/03/13/2021-03-13-mysql-22/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><p>业务高峰期，生产环境的 MySQL 压力太大，没法正常响应，需要短期内、临时性地提升一些性能。</p><h2 id="短连接风暴"><a href="#短连接风暴" class="headerlink" title="短连接风暴"></a>短连接风暴</h2><p>正常的短连接模式就是连接到数据库后，执行很少的 SQL 语句就断开，下次需要的时候再重连。如果使用的是短连接，在业务高峰期的时候，就可能出现连接数突然暴涨的情况。</p><p>MySQL 建立连接的过程，成本是很高的。除了正常的网络连接三次握手外，还需要做登录权限判断和获得这个连接的数据读写权限。</p><p>在数据库压力比较小的时候，这些额外的成本并不明显。</p><p>但是，短连接模型存在一个风险，就是一旦数据库处理得慢一些，连接数就会暴涨。max_connections 参数，用来控制一个 MySQL 实例同时存在的连接数的上限，超过这个值，系统就会拒绝接下来的连接请求，并报错提示“Too many connections”。对于被拒绝连接的请求来说，从业务角度看就是数据库不可用。</p><p>在机器负载比较高的时候，处理现有请求的时间变长，每个连接保持的时间也更长。这时，再有新建连接的话，就可能会超过 max_connections 的限制。</p><p>碰到这种情况时，一个比较自然的想法，就是调高 max_connections 的值。但这样做是有风险的。因为设计 max_connections 这个参数的目的是想保护 MySQL，如果我们把它改得太大，让更多的连接都可以进来，那么系统的负载可能会进一步加大，大量的资源耗费在权限验证等逻辑上，结果可能是适得其反，已经连接的线程拿不到 CPU 资源去执行业务的 SQL 请求。</p><p>那么这种情况下，你还有没有别的建议呢？我这里还有两种方法，但要注意，这些方法都是有损的。</p><h3 id="先处理掉那些占着连接但是不工作的线程。"><a href="#先处理掉那些占着连接但是不工作的线程。" class="headerlink" title="先处理掉那些占着连接但是不工作的线程。"></a>先处理掉那些占着连接但是不工作的线程。</h3><p>max_connections 的计算，不是看谁在 running，是只要连着就占用一个计数位置。对于那些不需要保持的连接，我们可以通过 kill connection 主动踢掉。这个行为跟事先设置 wait_timeout 的效果是一样的。设置 wait_timeout 参数表示的是，一个线程空闲 wait_timeout 这么多秒之后，就会被 MySQL 直接断开连接。</p><p>但是需要注意，在 show processlist 的结果里，踢掉显示为 sleep 的线程，可能是有损的。我们来看下面这个例子。</p><p><img src="/2021/03/13/2021-03-13-mysql-22/9091ff280592c8c68665771b1516c62a.png" alt></p><p>在上面这个例子里，如果断开 session A 的连接，因为这时候 session A 还没有提交，所以 MySQL 只能按照回滚事务来处理；而断开 session B 的连接，就没什么大影响。所以，如果按照优先级来说，你应该优先断开像 session B 这样的事务外空闲的连接。</p><p>但是，怎么判断哪些是事务外空闲的呢？session C 在 T 时刻之后的 30 秒执行 show processlist，看到的结果是这样的。</p><p><img src="/2021/03/13/2021-03-13-mysql-22/ae6a9ceecf8517e47f9ebfc565f0f925.png" alt></p><p>图中 id=4 和 id=5 的两个会话都是 Sleep 状态。而要看事务具体状态的话，你可以查 information_schema 库的 innodb_trx 表。</p><p><img src="/2021/03/13/2021-03-13-mysql-22/ca4b455c8eacbf32b98d1fe9ed9876e8.png" alt></p><p>这个结果里，trx_mysql_thread_id=4，表示 id=4 的线程还处在事务中。</p><p>因此，如果是连接数过多，你可以优先断开事务外空闲太久的连接；如果这样还不够，再考虑断开事务内空闲太久的连接。</p><p>从服务端断开连接使用的是 kill connection + id 的命令， 一个客户端处于 sleep 状态时，它的连接被服务端主动断开后，这个客户端并不会马上知道。直到客户端在发起下一个请求的时候，才会收到这样的报错“ERROR 2013 (HY000): Lost connection to MySQL server during query”。</p><p>从数据库端主动断开连接可能是有损的，尤其是有的应用端收到这个错误后，不重新连接，而是直接用这个已经不能用的句柄重试查询。这会导致从应用端看上去，“MySQL 一直没恢复”。</p><h3 id="减少连接过程的消耗"><a href="#减少连接过程的消耗" class="headerlink" title="减少连接过程的消耗"></a>减少连接过程的消耗</h3><p>有的业务代码会在短时间内先大量申请数据库连接做备用，如果现在数据库确认是被连接行为打挂了，那么一种可能的做法，是让数据库跳过权限验证阶段。</p><p>跳过权限验证的方法是：重启数据库，并使用–skip-grant-tables 参数启动。这样，整个 MySQL 会跳过所有的权限验证阶段，包括连接过程和语句执行过程在内。</p><p>但是，这种方法特别符合我们标题里说的“饮鸩止渴”，风险极高，是我特别不建议使用的方案。尤其你的库外网可访问的话，就更不能这么做了。</p><p>在 MySQL 8.0 版本里，如果你启用–skip-grant-tables 参数，MySQL 会默认把 –skip-networking 参数打开，表示这时候数据库只能被本地的客户端连接。可见，MySQL 官方对 skip-grant-tables 这个参数的安全问题也很重视。</p><p>除了短连接数暴增可能会带来性能问题外，实际上，我们在线上碰到更多的是查询或者更新语句导致的性能问题。其中，查询问题比较典型的有两类，一类是由新出现的慢查询导致的，一类是由 QPS（每秒查询数）突增导致的。</p><h2 id="慢查询性能问题"><a href="#慢查询性能问题" class="headerlink" title="慢查询性能问题"></a>慢查询性能问题</h2><p>在 MySQL 中，会引发性能问题的慢查询，大体有以下三种可能：</p><ul><li>索引没有设计好；</li><li>SQL 语句没写好；</li><li>MySQL 选错了索引。</li></ul><h3 id="导致慢查询的第一种可能是，索引没有设计好"><a href="#导致慢查询的第一种可能是，索引没有设计好" class="headerlink" title="导致慢查询的第一种可能是，索引没有设计好"></a>导致慢查询的第一种可能是，索引没有设计好</h3><p>这种场景一般就是通过紧急创建索引来解决。MySQL 5.6 版本以后，创建索引都支持 Online DDL 了，对于那种高峰期数据库已经被这个语句打挂了的情况，最高效的做法就是直接执行 alter table 语句。</p><p>比较理想的是能够在备库先执行。假设你现在的服务是一主一备，主库 A、备库 B，这个方案的大致流程是这样的：</p><ol><li>在备库 B 上执行 set sql_log_bin=off，也就是不写 binlog，然后执行 alter table 语句加上索引；</li><li>执行主备切换；</li><li>这时候主库是 B，备库是 A。在 A 上执行 set sql_log_bin=off，然后执行 alter table 语句加上索引。</li></ol><p>这是一个“古老”的 DDL 方案。平时在做变更的时候，你应该考虑类似 gh-ost 这样的方案，更加稳妥。但是在需要紧急处理时，上面这个方案的效率是最高的。</p><h3 id="导致慢查询的第二种可能是，语句没写好"><a href="#导致慢查询的第二种可能是，语句没写好" class="headerlink" title="导致慢查询的第二种可能是，语句没写好"></a>导致慢查询的第二种可能是，语句没写好</h3><p>这时，我们可以通过改写 SQL 语句来处理。MySQL 5.7 提供了 query_rewrite 功能，可以把输入的一种语句改写成另外一种模式。</p><p>比如，语句被错误地写成了 select * from t where id + 1 = 10000，你可以通过下面的方式，增加一个语句改写规则。</p><pre><code>mysql&gt; insert into query_rewrite.rewrite_rules(pattern, replacement, pattern_database) values (&quot;select * from t where id + 1 = ?&quot;, &quot;select * from t where id = ? - 1&quot;, &quot;db1&quot;);call query_rewrite.flush_rewrite_rules();</code></pre><p>这里，call query_rewrite.flush_rewrite_rules() 这个存储过程，是让插入的新规则生效，也就是我们说的“查询重写”。</p><h3 id="MySQL-选错了索引"><a href="#MySQL-选错了索引" class="headerlink" title="MySQL 选错了索引"></a>MySQL 选错了索引</h3><p>这时候，应急方案就是给这个语句加上 force index。</p><p>同样地，使用查询重写功能，给原来的语句加上 force index，也可以解决这个问题。</p><p>上面我和你讨论的由慢查询导致性能问题的三种可能情况，实际上出现最多的是前两种，即：索引没设计好和语句没写好。而这两种情况，恰恰是完全可以避免的。比如，通过下面这个过程，我们就可以预先发现问题。</p><ol><li>上线前，在测试环境，把慢查询日志（slow log）打开，并且把 long_query_time 设置成 0，确保每个语句都会被记录入慢查询日志；</li><li>在测试表里插入模拟线上的数据，做一遍回归测试；</li><li>观察慢查询日志里每类语句的输出，特别留意 Rows_examined 字段是否与预期一致。</li></ol><h2 id="QPS-突增问题"><a href="#QPS-突增问题" class="headerlink" title="QPS 突增问题"></a>QPS 突增问题</h2><p>有时候由于业务突然出现高峰，或者应用程序 bug，导致某个语句的 QPS 突然暴涨，也可能导致 MySQL 压力过大，影响服务。</p><p>我之前碰到过一类情况，是由一个新功能的 bug 导致的。当然，最理想的情况是让业务把这个功能下掉，服务自然就会恢复。</p><p>而下掉一个功能，如果从数据库端处理的话，对应于不同的背景，有不同的方法可用。我这里再和你展开说明一下。</p><ol><li><p>一种是由全新业务的 bug 导致的。假设你的 DB 运维是比较规范的，也就是说白名单是一个个加的。这种情况下，如果你能够确定业务方会下掉这个功能，只是时间上没那么快，那么就可以从数据库端直接把白名单去掉。</p></li><li><p>如果这个新功能使用的是单独的数据库用户，可以用管理员账号把这个用户删掉，然后断开现有连接。这样，这个新功能的连接不成功，由它引发的 QPS 就会变成 0。</p></li><li><p>如果这个新增的功能跟主体功能是部署在一起的，那么我们只能通过处理语句来限制。这时，我们可以使用上面提到的查询重写功能，把压力最大的 SQL 语句直接重写成”select 1”返回。</p></li></ol><p>当然，这个操作的风险很高，需要你特别细致。它可能存在两个副作用：</p><ol><li>如果别的功能里面也用到了这个 SQL 语句模板，会有误伤；</li><li>很多业务并不是靠这一个语句就能完成逻辑的，所以如果单独把这一个语句以 select 1 的结果返回的话，可能会导致后面的业务逻辑一起失败。</li></ol><p>所以，方案 3 是用于止血的，跟前面提到的去掉权限验证一样，应该是你所有选项里优先级最低的一个方案。</p><p>同时你会发现，其实方案 1 和 2 都要依赖于规范的运维体系：虚拟化、白名单机制、业务账号分离。由此可见，更多的准备，往往意味着更稳定的系统。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ol><li><p>连接数突然暴涨</p><ul><li>先处理掉那些占着连接但是不工作的线程, show processlist, kill connection + id</li><li>减少连接过程的消耗, 跳过权限验证</li></ul></li><li><p>慢查询性能问题</p><ul><li>索引没有设计好</li><li>语句没写好</li><li>选错了索引, 语句加上 force index</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>锁</title>
      <link href="/2021/03/11/2021-03-11-mysql-21/"/>
      <url>/2021/03/11/2021-03-11-mysql-21/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><p>加锁规则前提说明：MySQL 后面的版本可能会改变加锁策略，所以这个规则只限于截止到现在的最新版本，即 5.x 系列 &lt;=5.7.24，8.0 系列 &lt;=8.0.13。</p><p>加锁规则里面，包含了两个“原则”、两个“优化”和一个“bug”。</p><ol><li>原则 1：加锁的基本单位是 next-key lock，next-key lock 是前开后闭区间。</li><li>原则 2：查找过程中访问到的对象才会加锁。</li><li>优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。</li><li>优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。</li><li>一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。</li></ol><pre><code>CREATE TABLE `t` (  `id` int(11) NOT NULL,  `c` int(11) DEFAULT NULL,  `d` int(11) DEFAULT NULL,  PRIMARY KEY (`id`),  KEY `c` (`c`)) ENGINE=InnoDB;insert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);</code></pre><h2 id="等值查询间隙锁"><a href="#等值查询间隙锁" class="headerlink" title="等值查询间隙锁"></a>等值查询间隙锁</h2><p><img src="/2021/03/11/2021-03-11-mysql-21/585dfa8d0dd71171a6fa16bed4ba816c.png" alt></p><p>由于表 t 中没有 id=7 的记录，所以用我们上面提到的加锁规则判断一下的话：</p><ol><li>根据原则 1，加锁单位是 next-key lock，session A 加锁范围就是 (5,10]；</li><li>同时根据优化 2，这是一个等值查询 (id=7)，而 id=10 不满足查询条件，next-key lock 退化成间隙锁，因此最终加锁的范围是 (5,10)。</li></ol><p>所以，session B 要往这个间隙里面插入 id=8 的记录会被锁住，但是 session C 修改 id=10 这行是可以的。</p><h2 id="非唯一索引等值锁"><a href="#非唯一索引等值锁" class="headerlink" title="非唯一索引等值锁"></a>非唯一索引等值锁</h2><p><img src="/2021/03/11/2021-03-11-mysql-21/465990fe8f6b418ca3f9992bd1bb5465.png" alt></p><p>看到这个例子，你是不是有一种“该锁的不锁，不该锁的乱锁”的感觉？我们来分析一下吧。</p><p>这里 session A 要给索引 c 上 c=5 的这一行加上读锁。</p><ol><li>根据原则 1，加锁单位是 next-key lock，因此会给 (0,5]加上 next-key lock。</li><li>要注意 c 是普通索引，因此仅访问 c=5 这一条记录是不能马上停下来的，需要向右遍历，查到 c=10 才放弃。根据原则 2，访问到的都要加锁，因此要给 (5,10]加 next-key lock。</li><li>但是同时这个符合优化 2：等值判断，向右遍历，最后一个值不满足 c=5 这个等值条件，因此退化成间隙锁 (5,10)。</li><li>根据原则 2 ，只有访问到的对象才会加锁，这个查询使用覆盖索引，并不需要访问主键索引，所以主键索引上没有加任何锁，这就是为什么 session B 的 update 语句可以执行完成。</li></ol><p>但 session C 要插入一个 (7,7,7) 的记录，就会被 session A 的间隙锁 (5,10) 锁住。</p><p>需要注意，在这个例子中，lock in share mode 只锁覆盖索引，但是如果是 for update 就不一样了。 执行 for update 时，系统会认为你接下来要更新数据，因此会顺便给主键索引上满足条件的行加上行锁。</p><p>这个例子说明，锁是加在索引上的；同时，它给我们的指导是，如果你要用 lock in share mode 来给行加读锁避免数据被更新的话，就必须得绕过覆盖索引的优化，在查询字段中加入索引中不存在的字段。比如，将 session A 的查询语句改成 select d from t where c=5 lock in share mode。你可以自己验证一下效果。</p><h2 id="主键索引范围锁"><a href="#主键索引范围锁" class="headerlink" title="主键索引范围锁"></a>主键索引范围锁</h2><pre><code>mysql&gt; select * from t where id=10 for update;mysql&gt; select * from t where id&gt;=10 and id&lt;11 for update;</code></pre><p>你可能会想，id 定义为 int 类型，这两个语句就是等价的吧？其实，它们并不完全等价。</p><p>在逻辑上，这两条查语句肯定是等价的，但是它们的加锁规则不太一样。现在，我们就让 session A 执行第二个查询语句，来看看加锁效果。</p><p><img src="/2021/03/11/2021-03-11-mysql-21/30b839bf941f109b04f1a36c302aea80.png" alt></p><p>现在我们就用前面提到的加锁规则，来分析一下 session A 会加什么锁呢？</p><ol><li>开始执行的时候，要找到第一个 id=10 的行，因此本该是 next-key lock(5,10]。 根据优化 1， 主键 id 上的等值条件，退化成行锁，只加了 id=10 这一行的行锁。</li><li>范围查找就往后继续找，找到 id=15 这一行停下来，因此需要加 next-key lock(10,15]。</li></ol><p>所以，session A 这时候锁的范围就是主键索引上，行锁 id=10 和 next-key lock(10,15]。这样，session B 和 session C 的结果你就能理解了。</p><p>这里你需要注意一点，首次 session A 定位查找 id=10 的行的时候，是当做等值查询来判断的，而向右扫描到 id=15 的时候，用的是范围查询判断。</p><h2 id="非唯一索引范围锁"><a href="#非唯一索引范围锁" class="headerlink" title="非唯一索引范围锁"></a>非唯一索引范围锁</h2><p><img src="/2021/03/11/2021-03-11-mysql-21/7381475e9e951628c9fc907f5a57697a.png" alt></p><p>这次 session A 用字段 c 来判断，加锁规则跟案例三唯一的不同是：在第一次用 c=10 定位记录的时候，索引 c 上加了 (5,10]这个 next-key lock 后，由于索引 c 是非唯一索引，没有优化规则，也就是说不会蜕变为行锁，因此最终 sesion A 加的锁是，索引 c 上的 (5,10] 和 (10,15] 这两个 next-key lock。</p><p>所以从结果上来看，sesson B 要插入（8,8,8) 的这个 insert 语句时就被堵住了。</p><p>这里需要扫描到 c=15 才停止扫描，是合理的，因为 InnoDB 要扫到 c=15，才知道不需要继续往后找了。</p><h2 id="唯一索引范围锁-bug"><a href="#唯一索引范围锁-bug" class="headerlink" title="唯一索引范围锁 bug"></a>唯一索引范围锁 bug</h2><p><img src="/2021/03/11/2021-03-11-mysql-21/b105f8c4633e8d3a84e6422b1b1a316d.png" alt></p><p>session A 是一个范围查询，按照原则 1 的话，应该是索引 id 上只加 (10,15]这个 next-key lock，并且因为 id 是唯一键，所以循环判断到 id=15 这一行就应该停止了。</p><p>但是实现上，InnoDB 会往前扫描到第一个不满足条件的行为止，也就是 id=20。而且由于这是个范围扫描，因此索引 id 上的 (15,20]这个 next-key lock 也会被锁上。</p><p>所以你看到了，session B 要更新 id=20 这一行，是会被锁住的。同样地，session C 要插入 id=16 的一行，也会被锁住。</p><p>照理说，这里锁住 id=20 这一行的行为，其实是没有必要的。因为扫描到 id=15，就可以确定不用往后再找了。但实现上还是这么做了，因此我认为这是个 bug。</p><h2 id="非唯一索引上存在”等值”的例子"><a href="#非唯一索引上存在”等值”的例子" class="headerlink" title="非唯一索引上存在”等值”的例子"></a>非唯一索引上存在”等值”的例子</h2><p>为了更好地说明“间隙”这个概念。这里，我给表 t 插入一条新记录。</p><pre><code>mysql&gt; insert into t values(30,10,30);</code></pre><p>新插入的这一行 c=10，也就是说现在表里有两个 c=10 的行。那么，这时候索引 c 上的间隙是什么状态了呢？你要知道，由于非唯一索引上包含主键的值，所以是不可能存在“相同”的两行的。</p><p>可以看到，虽然有两个 c=10，但是它们的主键值 id 是不同的（分别是 10 和 30），因此这两个 c=10 的记录之间，也是有间隙的。</p><p>图中我画出了索引 c 上的主键 id。为了跟间隙锁的开区间形式进行区别，我用 (c=10,id=30) 这样的形式，来表示索引上的一行。</p><p>这次我们用 delete 语句来验证。注意，delete 语句加锁的逻辑，其实跟 select … for update 是类似的，也就是我在文章开始总结的两个“原则”、两个“优化”和一个“bug”。</p><p><img src="/2021/03/11/2021-03-11-mysql-21/b55fb0a1cac3500b60e1cf9779d2da78.png" alt></p><p>这时，session A 在遍历的时候，先访问第一个 c=10 的记录。同样地，根据原则 1，这里加的是 (c=5,id=5) 到 (c=10,id=10) 这个 next-key lock。</p><p>然后，session A 向右查找，直到碰到 (c=15,id=15) 这一行，循环才结束。根据优化 2，这是一个等值查询，向右查找到了不满足条件的行，所以会退化成 (c=10,id=10) 到 (c=15,id=15) 的间隙锁。</p><p>也就是说，这个 delete 语句在索引 c 上的加锁范围，就是下图中蓝色区域覆盖的部分。</p><p><img src="/2021/03/11/2021-03-11-mysql-21/bb0ad92483d71f0dcaeeef278f89cb24.png" alt></p><p>这个蓝色区域左右两边都是虚线，表示开区间，即 (c=5,id=5) 和 (c=15,id=15) 这两行上都没有锁。</p><h2 id="limit-语句加锁"><a href="#limit-语句加锁" class="headerlink" title="limit 语句加锁"></a>limit 语句加锁</h2><p><img src="/2021/03/11/2021-03-11-mysql-21/afc3a08ae7a254b3251e41b2a6dae02e.png" alt></p><p>这个例子里，session A 的 delete 语句加了 limit 2。你知道表 t 里 c=10 的记录其实只有两条，因此加不加 limit 2，删除的效果都是一样的，但是加锁的效果却不同。可以看到，session B 的 insert 语句执行通过了，跟案例六的结果不同。</p><p>这是因为，delete 语句明确加了 limit 2 的限制，因此在遍历到 (c=10, id=30) 这一行之后，满足条件的语句已经有两条，循环就结束了。</p><p>因此，索引 c 上的加锁范围就变成了从（c=5,id=5) 到（c=10,id=30) 这个前开后闭区间，如下图所示：</p><p><img src="/2021/03/11/2021-03-11-mysql-21/e5408ed94b3d44985073255db63bd0d5.png" alt></p><p>可以看到，(c=10,id=30）之后的这个间隙并没有在加锁范围里，因此 insert 语句插入 c=12 是可以执行成功的。</p><p>这个例子对我们实践的指导意义就是，在删除数据的时候尽量加 limit。这样不仅可以控制删除数据的条数，让操作更安全，还可以减小加锁的范围。</p><p>so, <strong>delete后面加limit可以减小next-key lock的范围</strong>。</p><h2 id="一个死锁的例子"><a href="#一个死锁的例子" class="headerlink" title="一个死锁的例子"></a>一个死锁的例子</h2><p><img src="/2021/03/11/2021-03-11-mysql-21/7b911a4c995706e8aa2dd96ff0f36506.png" alt></p><p>现在，我们按时间顺序来分析一下为什么是这样的结果。</p><ol><li>session A 启动事务后执行查询语句加 lock in share mode，在索引 c 上加了 next-key lock(5,10] 和间隙锁 (10,15)；</li><li>session B 的 update 语句也要在索引 c 上加 next-key lock(5,10] ，进入锁等待；</li><li>然后 session A 要再插入 (8,8,8) 这一行，被 session B 的间隙锁锁住。由于出现了死锁，InnoDB 让 session B 回滚。</li></ol><p>你可能会问，session B 的 next-key lock 不是还没申请成功吗？</p><p>其实是这样的，session B 的“加 next-key lock(5,10] ”操作，实际上分成了两步，先是加 (5,10) 的间隙锁，加锁成功；然后加 c=10 的行锁，这时候才被锁住的。</p><p>也就是说，我们在分析加锁规则的时候可以用 next-key lock 来分析。但是要知道，具体执行的时候，是要分成间隙锁和行锁两段来执行的。</p><p>另外，在读提交隔离级别下还有一个优化，即：语句执行过程中加上的行锁，在语句执行完成后，就要把“不满足条件的行”上的行锁直接释放了，不需要等到事务提交。</p><p>也就是说，读提交隔离级别下，锁的范围更小，锁的时间更短，这也是不少业务都默认使用读提交隔离级别的原因。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ol><li>原则 1：加锁的基本单位是 next-key lock，next-key lock 是前开后闭区间。间隙锁是前开后开区间。</li><li>原则 2：查找过程中访问到的对象才会加锁。</li><li>优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。</li><li>优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。</li><li>一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。</li><li>delete后面加limit可以减小next-key lock的范围。</li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>幻读</title>
      <link href="/2021/03/09/2021-03-09-mysql-20/"/>
      <url>/2021/03/09/2021-03-09-mysql-20/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><p>为了便于说明问题，建表和初始化语句如下：</p><pre><code>CREATE TABLE `t` (  `id` int(11) NOT NULL,  `c` int(11) DEFAULT NULL,  `d` int(11) DEFAULT NULL,  PRIMARY KEY (`id`),  KEY `c` (`c`)) ENGINE=InnoDB;insert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);</code></pre><p>这个表除了主键 id 外，还有一个索引 c，初始化语句在表中插入了 6 行数据。</p><pre><code>begin;select * from t where d=5 for update;commit;</code></pre><p>比较好理解的是，这个语句会命中 d=5 的这一行，对应的主键 id=5，因此在 select 语句执行完成后，id=5 这一行会加一个写锁，而且由于两阶段锁协议，这个写锁会在执行 commit 语句的时候释放。</p><p>由于字段 d 上没有索引，因此这条查询语句会做全表扫描。那么，其他被扫描到的，但是不满足条件的 5 行记录上，会不会被加锁呢？</p><p>我们知道，InnoDB 的默认事务隔离级别是可重复读，所以本文接下来没有特殊说明的部分，都是设定在可重复读隔离级别下。</p><h2 id="幻读是什么？"><a href="#幻读是什么？" class="headerlink" title="幻读是什么？"></a>幻读是什么？</h2><p>现在，我们就来分析一下，如果只在 id=5 这一行加锁，而其他行的不加锁的话，会怎么样。</p><p>下面先来看一下这个场景（注意：这是我假设的一个场景）：</p><p><img src="/2021/03/09/2021-03-09-mysql-20/5bc506e5884d21844126d26bbe6fa68b.png" alt></p><p>可以看到，session A 里执行了三次查询，分别是 Q1、Q2 和 Q3。它们的 SQL 语句相同，都是 select * from t where d=5 for update。这个语句的意思你应该很清楚了，查所有 d=5 的行，而且使用的是当前读，并且加上写锁。现在，我们来看一下这三条 SQL 语句，分别会返回什么结果。</p><ol><li>Q1 只返回 id=5 这一行；</li><li>在 T2 时刻，session B 把 id=0 这一行的 d 值改成了 5，因此 T3 时刻 Q2 查出来的是 id=0 和 id=5 这两行；</li><li>在 T4 时刻，session C 又插入一行（1,1,5），因此 T5 时刻 Q3 查出来的是 id=0、id=1 和 id=5 的这三行。</li></ol><p>其中，Q3 读到 id=1 这一行的现象，被称为“幻读”。也就是说，幻读指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。</p><p>这里，我需要对“幻读”做一个说明：</p><ol><li>在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。因此，幻读在“当前读”下才会出现。</li><li>上面 session B 的修改结果，被 session A 之后的 select 语句用“当前读”看到，不能称为幻读。幻读仅专指“新插入的行”。</li></ol><p>因为这三个查询都是加了 for update，都是当前读。而当前读的规则，就是要能读到所有已经提交的记录的最新值。并且，session B 和 sessionC 的两条语句，执行后就会提交，所以 Q2 和 Q3 就是应该看到这两个事务的操作效果，而且也看到了，这跟事务的可见性规则并不矛盾。</p><p>但是，这是不是真的没问题呢？不，这里还真就有问题。</p><h2 id="幻读有什么问题？"><a href="#幻读有什么问题？" class="headerlink" title="幻读有什么问题？"></a>幻读有什么问题？</h2><p>首先是语义上的。session A 在 T1 时刻就声明了，“我要把所有 d=5 的行锁住，不准别的事务进行读写操作”。而实际上，这个语义被破坏了。</p><p>如果现在这样看感觉还不明显的话，我再往 session B 和 session C 里面分别加一条 SQL 语句，你再看看会出现什么现象。</p><p><img src="/2021/03/09/2021-03-09-mysql-20/7a9ffa90ac3cc78db6a51ff9b9075607.png" alt></p><p>session B 的第二条语句 update t set c=5 where id=0，语义是“我把 id=0、d=5 这一行的 c 值，改成了 5”。</p><p>由于在 T1 时刻，session A 还只是给 id=5 这一行加了行锁， 并没有给 id=0 这行加上锁。因此，session B 在 T2 时刻，是可以执行这两条 update 语句的。这样，就破坏了 session A 里 Q1 语句要锁住所有 d=5 的行的加锁声明。</p><p>session C 也是一样的道理，对 id=1 这一行的修改，也是破坏了 Q1 的加锁声明。</p><p>其次，是数据一致性的问题。</p><p>我们知道，锁的设计是为了保证数据的一致性。而这个一致性，不止是数据库内部数据状态在此刻的一致性，还包含了数据和日志在逻辑上的一致性。</p><p>为了说明这个问题，我给 session A 在 T1 时刻再加一个更新语句，即：update t set d=100 where d=5。</p><p><img src="/2021/03/09/2021-03-09-mysql-20/dcea7845ff0bdbee2622bf3c67d31d92.png" alt></p><p>update 的加锁语义和 select …for update 是一致的，所以这时候加上这条 update 语句也很合理。session A 声明说“要给 d=5 的语句加上锁”，就是为了要更新数据，新加的这条 update 语句就是把它认为加上了锁的这一行的 d 值修改成了 100。</p><p>现在，我们来分析一下上图执行完成后，数据库里会是什么结果。</p><ol><li><p>经过 T1 时刻，id=5 这一行变成 (5,5,100)，当然这个结果最终是在 T6 时刻正式提交的 ;</p></li><li><p>经过 T2 时刻，id=0 这一行变成 (0,5,5);</p></li><li><p>经过 T4 时刻，表里面多了一行 (1,5,5);</p></li><li><p>其他行跟这个执行序列无关，保持不变。</p></li></ol><p>这样看，这些数据也没啥问题，但是我们再来看看这时候 binlog 里面的内容。</p><ol><li>T2 时刻，session B 事务提交，写入了两条语句；</li><li>T4 时刻，session C 事务提交，写入了两条语句；</li><li>T6 时刻，session A 事务提交，写入了 update t set d=100 where d=5 这条语句。</li></ol><p>我统一放到一起的话，就是这样的：</p><pre><code>update t set d=5 where id=0; /*(0,0,5)*/update t set c=5 where id=0; /*(0,5,5)*/insert into t values(1,1,5); /*(1,1,5)*/update t set c=5 where id=1; /*(1,5,5)*/update t set d=100 where d=5;/*所有d=5的行，d改成100*/</code></pre><p>好，你应该看出问题了。这个语句序列，不论是拿到备库去执行，还是以后用 binlog 来克隆一个库，这三行的结果，都变成了 (0,5,100)、(1,5,100) 和 (5,5,100)。</p><p>也就是说，id=0 和 id=1 这两行，发生了数据不一致。这个问题很严重，是不行的。</p><p>到这里，我们再回顾一下，这个数据不一致到底是怎么引入的？</p><p>我们分析一下可以知道，这是我们假设“select * from t where d=5 for update 这条语句只给 d=5 这一行，也就是 id=5 的这一行加锁”导致的。</p><p>所以我们认为，上面的设定不合理，要改。</p><p>那怎么改呢？我们把扫描过程中碰到的行，也都加上写锁，再来看看执行效果。</p><p><img src="/2021/03/09/2021-03-09-mysql-20/34ad6478281709da833856084a1e3447.png" alt></p><p>由于 session A 把所有的行都加了写锁，所以 session B 在执行第一个 update 语句的时候就被锁住了。需要等到 T6 时刻 session A 提交以后，session B 才能继续执行。</p><p>这样对于 id=0 这一行，在数据库里的最终结果还是 (0,5,5)。在 binlog 里面，执行序列是这样的：</p><pre><code>insert into t values(1,1,5); /*(1,1,5)*/update t set c=5 where id=1; /*(1,5,5)*/update t set d=100 where d=5;/*所有d=5的行，d改成100*/update t set d=5 where id=0; /*(0,0,5)*/update t set c=5 where id=0; /*(0,5,5)*/</code></pre><p>可以看到，按照日志顺序执行，id=0 这一行的最终结果也是 (0,5,5)。所以，id=0 这一行的问题解决了。</p><p>但同时你也可以看到，id=1 这一行，在数据库里面的结果是 (1,5,5)，而根据 binlog 的执行结果是 (1,5,100)，也就是说幻读的问题还是没有解决。为什么我们已经这么“凶残”地，把所有的记录都上了锁，还是阻止不了 id=1 这一行的插入和更新呢？</p><p>原因很简单。在 T3 时刻，我们给所有行加锁的时候，id=1 这一行还不存在，不存在也就加不上锁。</p><p>也就是说，即使把所有的记录都加上锁，还是阻止不了新插入的记录，这也是为什么“幻读”会被单独拿出来解决的原因。</p><p>到这里，其实我们刚说明完文章的标题 ：幻读的定义和幻读有什么问题。</p><h2 id="如何解决幻读？"><a href="#如何解决幻读？" class="headerlink" title="如何解决幻读？"></a>如何解决幻读？</h2><p>现在你知道了，产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。因此，为了解决幻读问题，InnoDB 只好引入新的锁，也就是间隙锁 (Gap Lock)。</p><p>顾名思义，间隙锁，锁的就是两个值之间的空隙。比如文章开头的表 t，初始化插入了 6 个记录，这就产生了 7 个间隙。</p><p><img src="/2021/03/09/2021-03-09-mysql-20/e7f7ca0d3dab2f48c588d714ee3ac861.png" alt></p><p>这样，当你执行 select * from t where d=5 for update 的时候，就不止是给数据库中已有的 6 个记录加上了行锁，还同时加了 7 个间隙锁。这样就确保了无法再插入新的记录。</p><p>也就是说这时候，在一行行扫描的过程中，不仅将给行加上了行锁，还给行两边的空隙，也加上了间隙锁。</p><p>现在你知道了，数据行是可以加上锁的实体，数据行之间的间隙，也是可以加上锁的实体。但是间隙锁跟我们之前碰到过的锁都不太一样。</p><p>比如行锁，分成读锁和写锁。下图就是这两种类型行锁的冲突关系。</p><p><img src="/2021/03/09/2021-03-09-mysql-20/c435c765556c0f3735a6eda0779ff151.png" alt></p><p>也就是说，跟行锁有冲突关系的是“另外一个行锁”。</p><p>但是间隙锁不一样，跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作。间隙锁之间都不存在冲突关系。</p><p>这句话不太好理解，我给你举个例子：</p><p><img src="/2021/03/09/2021-03-09-mysql-20/7c37732d936650f1cda7dbf27daf7498.png" alt></p><p>这里 session B 并不会被堵住。因为表 t 里并没有 c=7 这个记录，因此 session A 加的是间隙锁 (5,10)。而 session B 也是在这个间隙加的间隙锁。它们有共同的目标，即：保护这个间隙，不允许插入值。但，它们之间是不冲突的。</p><p>间隙锁和行锁合称 next-key lock，每个 next-key lock 是前开后闭区间。也就是说，我们的表 t 初始化以后，如果用 select * from t for update 要把整个表所有记录锁起来，就形成了 7 个 next-key lock，分别是 (-∞,0]、(0,5]、(5,10]、(10,15]、(15,20]、(20, 25]、(25, +supremum]。</p><blockquote><p>备注：这篇文章中，如果没有特别说明，我们把间隙锁记为开区间，把 next-key lock 记为前开后闭区间。</p></blockquote><p>你可能会问说，这个 supremum 从哪儿来的呢？</p><p>这是因为 +∞是开区间。实现上，InnoDB 给每个索引加了一个不存在的最大值 supremum，这样才符合我们前面说的“都是前开后闭区间”。</p><p><strong>间隙锁和 next-key lock 的引入，帮我们解决了幻读的问题</strong>，但同时也带来了一些“困扰”。</p><p>在前面的文章中，就有同学提到了这个问题。我把他的问题转述一下，对应到我们这个例子的表来说，业务逻辑这样的：任意锁住一行，如果这一行不存在的话就插入，如果存在这一行就更新它的数据，代码如下：</p><pre><code>begin;select * from t where id=N for update;/*如果行不存在*/insert into t values(N,N,N);/*如果行存在*/update t set d=N set id=N;commit;</code></pre><p>这个同学碰到的现象是，这个逻辑一旦有并发，就会碰到死锁。你一定也觉得奇怪，这个逻辑每次操作前用 for update 锁起来，已经是最严格的模式了，怎么还会有死锁呢？</p><p>这里，我用两个 session 来模拟并发，并假设 N=9。</p><p><img src="/2021/03/09/2021-03-09-mysql-20/df37bf0bb9f85ea59f0540e24eb6bcbe.png" alt></p><p>你看到了，其实都不需要用到后面的 update 语句，就已经形成死锁了。我们按语句执行顺序来分析一下：</p><ol><li>session A 执行 select … for update 语句，由于 id=9 这一行并不存在，因此会加上间隙锁 (5,10);</li><li>session B 执行 select … for update 语句，同样会加上间隙锁 (5,10)，间隙锁之间不会冲突，因此这个语句可以执行成功；</li><li>session B 试图插入一行 (9,9,9)，被 session A 的间隙锁挡住了，只好进入等待；</li><li>session A 试图插入一行 (9,9,9)，被 session B 的间隙锁挡住了。</li></ol><p>至此，两个 session 进入互相等待状态，形成死锁。当然，InnoDB 的死锁检测马上就发现了这对死锁关系，让 session A 的 insert 语句报错返回了。</p><p>你现在知道了，间隙锁的引入，可能会导致同样的语句锁住更大的范围，这其实是影响了并发度的。其实，这还只是一个简单的例子，在下一篇文章中我们还会碰到更多、更复杂的例子。</p><p>你可能会说，为了解决幻读的问题，我们引入了这么一大串内容，有没有更简单一点的处理方法呢。</p><p>我在文章一开始就说过，如果没有特别说明，今天和你分析的问题都是在可重复读隔离级别下的，<strong>间隙锁是在可重复读隔离级别下才会生效的</strong>。所以，你如果把隔离级别设置为读提交的话，就没有间隙锁了。但同时，你要解决可能出现的数据和日志不一致问题，需要把 binlog 格式设置为 row。这，也是现在不少公司使用的配置组合。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ol><li><p>幻读是指在同一个事务中，存在前后两次查询同一个范围的数据，但是第二次查询却看到了第一次查询没看到的行。</p><p>注意，幻读出现的场景<br>第一：事务的隔离级别为可重复读，且是当前读<br>第二：幻读仅专指新插入的行</p></li><li><p>幻读带来的问题？<br>一是，对行锁语义的破坏<br>二是，破坏了数据一致性</p></li><li><p>怎么避免幻读？<br>存储引擎采用加间隙锁的方式来避免出现幻读</p></li><li><p>为啥会出现幻读？<br>行锁只能锁定存在的行，针对新插入的操作没有限定</p></li><li><p>间隙锁是啥？它怎么避免出现幻读的？它引入了什么新的问题？<br>间隙锁，是专门用于解决幻读这种问题的锁，它锁的了行与行之间的间隙，能够阻塞新插入的操作<br>间隙锁的引入也带来了一些新的问题，比如：降低并发度，可能导致死锁。<br>注意，读读不互斥，读写/写读/写写是互斥的，但是间隙锁之间是不冲突的，间隙锁会阻塞插入操作</p></li><li><p>另外，间隙锁在可重复读级别下才是有效的</p></li><li><p>幻读<br>在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。因此，幻读在“当前读”下才会出现。<br>幻读仅专指“新插入的行”，不包含修改的行。</p></li><li><p>间隙锁</p><p>间隙锁，锁的就是两个值之间的空隙。<br>间隙锁之间都不存在冲突关系。</p><p>间隙锁是在可重复读隔离级别下才会生效的。</p></li><li><p>next-key lock<br>间隙锁和行锁合称 next-key lock，每个 next-key lock 是前开后闭区间。</p><p>间隙锁记为开区间，把 next-key lock 记为前开后闭区间。</p></li><li><p>举个栗子：有三个字段id,c,d，其中c为索引，d不建立索引，现在有三条数据（0，0，0），（5，5，5），（10，10，10），总共有四个区间(-∞,0）、(0,5）、(5,10）、（10，+∞）。一个事务中select * from t where d（c） = 6 for update（update t set c = 6 where id = 6）,d的情况会全表扫描，产生所有区间，c的情况和update情况会产生（5，10）区间间隙锁，其他事务无法insert into t(id, c ,d) value(7,7,7)。</p></li><li><p>RR模式下，在一个事务中，select * from t where d = 4 for update，如果d为索引，则锁住这行数据，如果d不为索引，则锁住全表遍历的行数据。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>查一行数据很慢</title>
      <link href="/2021/03/07/2021-03-07-mysql-19/"/>
      <url>/2021/03/07/2021-03-07-mysql-19/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><p>一般情况下，如果我跟你说查询性能优化，你首先会想到一些复杂的语句，想到查询需要返回大量的数据。但有些情况下，“查一行”，也会执行得特别慢。</p><p>为了便于描述，构造一个表，基于这个表来说明问题。这个表有两个字段 id 和 c，并且我在里面插入了 10 万行记录。</p><pre><code>mysql&gt; CREATE TABLE `t` (  `id` int(11) NOT NULL,  `c` int(11) DEFAULT NULL,  PRIMARY KEY (`id`)) ENGINE=InnoDB;delimiter ;;create procedure idata()begin  declare i int;  set i=1;  while(i&lt;=100000) do    insert into t values(i,i);    set i=i+1;  end while;end;;delimiter ;call idata();</code></pre><h2 id="查询长时间不返回"><a href="#查询长时间不返回" class="headerlink" title="查询长时间不返回"></a>查询长时间不返回</h2><p>在表 t 执行下面的 SQL 语句：</p><pre><code>mysql&gt; select * from t where id=1;</code></pre><p>查询结果长时间不返回。</p><p>一般碰到这种情况的话，大概率是表 t 被锁住了。接下来分析原因的时候，一般都是首先执行一下 show processlist 命令，看看当前语句处于什么状态。</p><p>然后我们再针对每种状态，去分析它们产生的原因、如何复现，以及如何处理。</p><h3 id="等-MDL-锁"><a href="#等-MDL-锁" class="headerlink" title="等 MDL 锁"></a>等 MDL 锁</h3><p>使用 show processlist 命令查看 Waiting for table metadata lock 的示意图。</p><p><img src="/2021/03/07/2021-03-07-mysql-19/5008d7e9e22be88a9c80916df4f4b328.png" alt></p><p>出现这个状态表示的是，现在有一个线程正在表 t 上请求或者持有 MDL 写锁，把 select 语句堵住了。</p><p>需要说明的是，那个复现过程是基于 MySQL 5.6 版本的。而 MySQL 5.7 版本修改了 MDL 的加锁策略，所以就不能复现这个场景了。</p><p>不过，在 MySQL 5.7 版本下复现这个场景，也很容易。如图 3 所示，我给出了简单的复现步骤。</p><p><img src="/2021/03/07/2021-03-07-mysql-19/742249a31b83f4858c51bfe106a5daca.png" alt></p><p>session A 通过 lock table 命令持有表 t 的 MDL 写锁，而 session B 的查询需要获取 MDL 读锁。所以，session B 进入等待状态。</p><p>这类问题的处理方式，就是找到谁持有 MDL 写锁，然后把它 kill 掉。</p><p>但是，由于在 show processlist 的结果里面，session A 的 Command 列是“Sleep”，导致查找起来很不方便。不过有了 performance_schema 和 sys 系统库以后，就方便多了。（MySQL 启动时需要设置 performance_schema=on，相比于设置为 off 会有 10% 左右的性能损失)</p><p>通过查询 sys.schema_table_lock_waits 这张表，我们就可以直接找出造成阻塞的 process id，把这个连接用 kill 命令断开即可。</p><p><img src="/2021/03/07/2021-03-07-mysql-19/74fb24ba3826e3831eeeff1670990c01.png" alt></p><h3 id="等-flush"><a href="#等-flush" class="headerlink" title="等 flush"></a>等 flush</h3><p>接下来，我给你举另外一种查询被堵住的情况。</p><p>我在表 t 上，执行下面的 SQL 语句：</p><pre><code>mysql&gt; select * from information_schema.processlist where id=1;</code></pre><p>我查出来这个线程的状态是 Waiting for table flush，你可以设想一下这是什么原因。</p><p><img src="/2021/03/07/2021-03-07-mysql-19/2d8250398bc7f8f7dce8b6b1923c3724.png" alt></p><p>这个状态表示的是，现在有一个线程正要对表 t 做 flush 操作。MySQL 里面对表做 flush 操作的用法，一般有以下两个：</p><pre><code>flush tables t with read lock;flush tables with read lock;</code></pre><p>这两个 flush 语句，如果指定表 t 的话，代表的是只关闭表 t；如果没有指定具体的表名，则表示关闭 MySQL 里所有打开的表。</p><p>但是正常这两个语句执行起来都很快，除非它们也被别的线程堵住了。</p><p>所以，出现 Waiting for table flush 状态的可能情况是：有一个 flush tables 命令被别的语句堵住了，然后它又堵住了我们的 select 语句。</p><p><img src="/2021/03/07/2021-03-07-mysql-19/2bbc77cfdb118b0d9ef3fdd679d0a69c.png" alt></p><p>在 session A 中，我故意每行都调用一次 sleep(1)，这样这个语句默认要执行 10 万秒，在这期间表 t 一直是被 session A“打开”着。然后，session B 的 flush tables t 命令再要去关闭表 t，就需要等 session A 的查询结束。这样，session C 要再次查询的话，就会被 flush 命令堵住了。</p><p>这个例子的排查也很简单，你看到这个 show processlist 的结果，肯定就知道应该怎么做了。</p><p><img src="/2021/03/07/2021-03-07-mysql-19/398407014180be4146c2d088fc07357e.png" alt></p><h3 id="等行锁"><a href="#等行锁" class="headerlink" title="等行锁"></a>等行锁</h3><p>现在，经过了表级锁的考验，我们的 select 语句终于来到引擎里了。</p><pre><code>mysql&gt; select * from t where id=1 lock in share mode; </code></pre><p>由于访问 id=1 这个记录时要加读锁，如果这时候已经有一个事务在这行记录上持有一个写锁，我们的 select 语句就会被堵住。</p><p>复现步骤和现场如下：</p><p><img src="/2021/03/07/2021-03-07-mysql-19/3e68326b967701c59770612183277475.png" alt></p><p><img src="/2021/03/07/2021-03-07-mysql-19/3c266e23fc307283aa94923ecbbc738f.png" alt></p><p>显然，session A 启动了事务，占有写锁，还不提交，是导致 session B 被堵住的原因。</p><p>这个问题并不难分析，但问题是怎么查出是谁占着这个写锁。如果你用的是 MySQL 5.7 版本，可以通过 sys.innodb_lock_waits 表查到。</p><p>查询方法是：</p><pre><code>mysql&gt; select * from t sys.innodb_lock_waits where locked_table=&#39;`test`.`t`&#39;\G</code></pre><p><img src="/2021/03/07/2021-03-07-mysql-19/d8603aeb4eaad3326699c13c46379118.png" alt></p><p>可以看到，这个信息很全，4 号线程是造成堵塞的罪魁祸首。而干掉这个罪魁祸首的方式，就是 KILL QUERY 4 或 KILL 4。</p><p>不过，这里不应该显示“KILL QUERY 4”。这个命令表示停止 4 号线程当前正在执行的语句，而这个方法其实是没有用的。因为占有行锁的是 update 语句，这个语句已经是之前执行完成了的，现在执行 KILL QUERY，无法让这个事务去掉 id=1 上的行锁。</p><p>实际上，KILL 4 才有效，也就是说直接断开这个连接。这里隐含的一个逻辑就是，连接被断开的时候，会自动回滚这个连接里面正在执行的线程，也就释放了 id=1 上的行锁。</p><h2 id="查询慢"><a href="#查询慢" class="headerlink" title="查询慢"></a>查询慢</h2><p>先来看一条你一定知道原因的 SQL 语句：</p><pre><code>mysql&gt; select * from t where c=50000 limit 1;</code></pre><p>由于字段 c 上没有索引，这个语句只能走 id 主键顺序扫描，因此需要扫描 5 万行。</p><p>作为确认，你可以看一下慢查询日志。注意，这里为了把所有语句记录到 slow log 里，我在连接后先执行了 set long_query_time=0，将慢查询日志的时间阈值设置为 0。</p><p><img src="/2021/03/07/2021-03-07-mysql-19/d8b2b5f97c60ae4fc4a03c616847503c.png" alt></p><p>Rows_examined 显示扫描了 50000 行。你可能会说，不是很慢呀，11.5 毫秒就返回了，我们线上一般都配置超过 1 秒才算慢查询。但你要记住：坏查询不一定是慢查询。我们这个例子里面只有 10 万行记录，数据量大起来的话，执行时间就线性涨上去了。</p><p>扫描行数多，所以执行慢，这个很好理解。</p><p>但是接下来，我们再看一个只扫描一行，但是执行很慢的语句。</p><pre><code>mysql&gt; select * from t where id=1；</code></pre><p>虽然扫描行数是 1，但执行时间却长达 800 毫秒。</p><p><img src="/2021/03/07/2021-03-07-mysql-19/66f26bb885401e8e460451ff6b0c0746.png" alt></p><p>是不是有点奇怪呢，这些时间都花在哪里了？</p><p>如果我把这个 slow log 的截图再往下拉一点，你可以看到下一个语句，select * from t where id=1 lock in share mode，执行时扫描行数也是 1 行，执行时间是 0.2 毫秒。</p><p><img src="/2021/03/07/2021-03-07-mysql-19/bde83e269d9fa185b27900c8aa8137d2.png" alt></p><p>看上去是不是更奇怪了？按理说 lock in share mode 还要加锁，时间应该更长才对啊。</p><p>这两个语句的执行输出结果:</p><p><img src="/2021/03/07/2021-03-07-mysql-19/1fbb84bb392b6bfa93786fe032690b1c.png" alt></p><p>第一个语句的查询结果里 c=1，带 lock in share mode 的语句返回的是 c=1000001。看到这里应该有更多的同学知道原因了。如果你还是没有头绪的话，也别着急。我先跟你说明一下复现步骤，再分析原因。</p><p><img src="/2021/03/07/2021-03-07-mysql-19/84667a3449dc846e393142600ee7a2ff.png" alt></p><p>你看到了，session A 先用 start transaction with consistent snapshot 命令启动了一个事务，之后 session B 才开始执行 update 语句。</p><p>session B 执行完 100 万次 update 语句后，id=1 这一行处于什么状态呢？你可以从图 16 中找到答案。</p><p><img src="/2021/03/07/2021-03-07-mysql-19/46bb9f5e27854678bfcaeaf0c3b8a98c.png" alt></p><p>session B 更新完 100 万次，生成了 100 万个回滚日志 (undo log)。</p><p>带 lock in share mode 的 SQL 语句，是当前读，因此会直接读到 1000001 这个结果，所以速度很快；而 select * from t where id=1 这个语句，是一致性读，因此需要从 1000001 开始，依次执行 undo log，执行了 100 万次以后，才将 1 这个结果返回。</p><p>注意，undo log 里记录的其实是“把 2 改成 1”，“把 3 改成 2”这样的操作逻辑，画成减 1 的目的是方便你看图。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>sql查询慢的原因：</p><ul><li><p>全部语句特别慢：CPU高，IO高。</p></li><li><p>个别语句特别慢：等表锁，等行锁，等MDL锁。</p></li><li><p>个别语句有时候慢：没上索引，表数据太多了；一致性读前的事务过多，导致依次执行undo log，速度慢，而当前读速度很快。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>性能差异</title>
      <link href="/2021/03/05/2021-03-05-mysql-18/"/>
      <url>/2021/03/05/2021-03-05-mysql-18/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="条件字段函数操作"><a href="#条件字段函数操作" class="headerlink" title="条件字段函数操作"></a>条件字段函数操作</h2><p>假设你现在维护了一个交易系统，其中交易记录表 tradelog 包含交易流水号（tradeid）、交易员 id（operator）、交易时间（t_modified）等字段。为了便于描述，我们先忽略其他字段。这个表的建表语句如下：</p><pre><code>mysql&gt; CREATE TABLE `tradelog` (  `id` int(11) NOT NULL,  `tradeid` varchar(32) DEFAULT NULL,  `operator` int(11) DEFAULT NULL,  `t_modified` datetime DEFAULT NULL,  PRIMARY KEY (`id`),  KEY `tradeid` (`tradeid`),  KEY `t_modified` (`t_modified`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;</code></pre><p>假设，现在已经记录了从 2016 年初到 2018 年底的所有数据，运营部门有一个需求是，要统计发生在所有年份中 7 月份的交易记录总数。这个逻辑看上去并不复杂，你的 SQL 语句可能会这么写：</p><pre><code>mysql&gt; select count(*) from tradelog where month(t_modified)=7;</code></pre><p>由于 t_modified 字段上有索引，于是你就很放心地在生产库中执行了这条语句，但却发现执行了特别久，才返回了结果。</p><p>如果你问 DBA 同事为什么会出现这样的情况，他大概会告诉你：如果对字段做了函数计算，就用不上索引了，这是 MySQL 的规定。</p><p>现在你已经学过了 InnoDB 的索引结构了，可以再追问一句为什么？为什么条件是 where t_modified=’2018-7-1’的时候可以用上索引，而改成 where month(t_modified)=7 的时候就不行了？</p><p>下面是这个 t_modified 索引的示意图。方框上面的数字就是 month() 函数对应的值。</p><p><img src="/2021/03/05/2021-03-05-mysql-18/3e30d9a5e67f711f5af2e2599e800286.png" alt="t_modified 索引示意图"></p><p>如果你的 SQL 语句条件用的是 where t_modified=’2018-7-1’的话，引擎就会按照上面绿色箭头的路线，快速定位到 t_modified=’2018-7-1’需要的结果。</p><p>实际上，B+ 树提供的这个快速定位能力，来源于同一层兄弟节点的有序性。</p><p>但是，如果计算 month() 函数的话，你会看到传入 7 的时候，在树的第一层就不知道该怎么办了。</p><p>也就是说，对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。</p><p>需要注意的是，优化器并不是要放弃使用这个索引。</p><p>在这个例子里，放弃了树搜索功能，优化器可以选择遍历主键索引，也可以选择遍历索引 t_modified，优化器对比索引大小后发现，索引 t_modified 更小，遍历这个索引比遍历主键索引来得更快。因此最终还是会选择索引 t_modified。</p><p>接下来，我们使用 explain 命令，查看一下这条 SQL 语句的执行结果。</p><p><img src="/2021/03/05/2021-03-05-mysql-18/27c2f5ff3549b18ba37a28f4919f3655.png" alt></p><p>key=”t_modified”表示的是，使用了 t_modified 这个索引；我在测试表数据中插入了 10 万行数据，rows=100335，说明这条语句扫描了整个索引的所有值；Extra 字段的 Using index，表示的是使用了覆盖索引。</p><p>也就是说，由于在 t_modified 字段加了 month() 函数操作，导致了全索引扫描。为了能够用上索引的快速定位能力，我们就要把 SQL 语句改成基于字段本身的范围查询。按照下面这个写法，优化器就能按照我们预期的，用上 t_modified 索引的快速定位能力了。</p><pre><code>mysql&gt; select count(*) from tradelog where    -&gt; (t_modified &gt;= &#39;2016-7-1&#39; and t_modified&lt;&#39;2016-8-1&#39;) or    -&gt; (t_modified &gt;= &#39;2017-7-1&#39; and t_modified&lt;&#39;2017-8-1&#39;) or     -&gt; (t_modified &gt;= &#39;2018-7-1&#39; and t_modified&lt;&#39;2018-8-1&#39;);</code></pre><p>当然，如果你的系统上线时间更早，或者后面又插入了之后年份的数据的话，你就需要再把其他年份补齐。</p><p>到这里我给你说明了，由于加了 month() 函数操作，MySQL 无法再使用索引快速定位功能，而只能使用全索引扫描。</p><p>不过优化器在个问题上确实有“偷懒”行为，即使是对于不改变有序性的函数，也不会考虑使用索引。比如，对于 select * from tradelog where id + 1 = 10000 这个 SQL 语句，这个加 1 操作并不会改变有序性，但是 MySQL 优化器还是不能用 id 索引快速定位到 9999 这一行。所以，需要你在写 SQL 语句的时候，手动改写成 where id = 10000 -1 才可以。</p><h2 id="隐式类型转换"><a href="#隐式类型转换" class="headerlink" title="隐式类型转换"></a>隐式类型转换</h2><p>接下来我再跟你说一说，另一个经常让程序员掉坑里的例子。</p><p>我们一起看一下这条 SQL 语句：</p><pre><code>mysql&gt; select * from tradelog where tradeid=110717;</code></pre><p>交易编号 tradeid 这个字段上，本来就有索引，但是 explain 的结果却显示，这条语句需要走全表扫描。你可能也发现了，tradeid 的字段类型是 varchar(32)，而输入的参数却是整型，所以需要做类型转换。</p><p>那么，现在这里就有两个问题：</p><ol><li>数据类型转换的规则是什么？</li><li>为什么有数据类型转换，就需要走全索引扫描？</li></ol><p>先来看第一个问题，你可能会说，数据库里面类型这么多，这种数据类型转换规则更多，我记不住，应该怎么办呢？</p><p>这里有一个简单的方法，看 select “10” &gt; 9 的结果：</p><ol><li>如果规则是“将字符串转成数字”，那么就是做数字比较，结果应该是 1；</li><li>如果规则是“将数字转成字符串”，那么就是做字符串比较，结果应该是 0。</li></ol><p><img src="/2021/03/05/2021-03-05-mysql-18/2b67fc38f1651e2622fe21d49950b214.png" alt></p><p>从图中可知，select “10” &gt; 9 返回的是 1，所以你就能确认 MySQL 里的转换规则了：在 MySQL 中，字符串和数字做比较的话，是将字符串转换成数字。</p><p>这时，你再看这个全表扫描的语句：</p><pre><code>mysql&gt; select * from tradelog where tradeid=110717;</code></pre><p>就知道对于优化器来说，这个语句相当于：</p><pre><code>mysql&gt; select * from tradelog where  CAST(tradid AS signed int) = 110717;</code></pre><p>也就是说，这条语句触发了我们上面说到的规则：对索引字段做函数操作，优化器会放弃走树搜索功能。</p><p>现在，我留给你一个小问题，id 的类型是 int，如果执行下面这个语句，是否会导致全表扫描呢？</p><pre><code>select * from tradelog where id=&quot;83126&quot;;</code></pre><p>答案是不会。因为将字符串转成数字，“83126”转为83126.</p><h2 id="隐式字符编码转换"><a href="#隐式字符编码转换" class="headerlink" title="隐式字符编码转换"></a>隐式字符编码转换</h2><p>假设系统里还有另外一个表 trade_detail，用于记录交易的操作细节。为了便于量化分析和复现，我往交易日志表 tradelog 和交易详情表 trade_detail 这两个表里插入一些数据。</p><pre><code>mysql&gt; CREATE TABLE `trade_detail` (  `id` int(11) NOT NULL,  `tradeid` varchar(32) DEFAULT NULL,  `trade_step` int(11) DEFAULT NULL, /*操作步骤*/  `step_info` varchar(32) DEFAULT NULL, /*步骤信息*/  PRIMARY KEY (`id`),  KEY `tradeid` (`tradeid`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;insert into tradelog values(1, &#39;aaaaaaaa&#39;, 1000, now());insert into tradelog values(2, &#39;aaaaaaab&#39;, 1000, now());insert into tradelog values(3, &#39;aaaaaaac&#39;, 1000, now());insert into trade_detail values(1, &#39;aaaaaaaa&#39;, 1, &#39;add&#39;);insert into trade_detail values(2, &#39;aaaaaaaa&#39;, 2, &#39;update&#39;);insert into trade_detail values(3, &#39;aaaaaaaa&#39;, 3, &#39;commit&#39;);insert into trade_detail values(4, &#39;aaaaaaab&#39;, 1, &#39;add&#39;);insert into trade_detail values(5, &#39;aaaaaaab&#39;, 2, &#39;update&#39;);insert into trade_detail values(6, &#39;aaaaaaab&#39;, 3, &#39;update again&#39;);insert into trade_detail values(7, &#39;aaaaaaab&#39;, 4, &#39;commit&#39;);insert into trade_detail values(8, &#39;aaaaaaac&#39;, 1, &#39;add&#39;);insert into trade_detail values(9, &#39;aaaaaaac&#39;, 2, &#39;update&#39;);insert into trade_detail values(10, &#39;aaaaaaac&#39;, 3, &#39;update again&#39;);insert into trade_detail values(11, &#39;aaaaaaac&#39;, 4, &#39;commit&#39;);</code></pre><p>这时候，如果要查询 id=2 的交易的所有操作步骤信息，SQL 语句可以这么写：</p><pre><code>mysql&gt; select d.* from tradelog l, trade_detail d where d.tradeid=l.tradeid and l.id=2; /*语句Q1*/</code></pre><p>我们一起来看下这个结果：</p><ol><li>第一行显示优化器会先在交易记录表 tradelog 上查到 id=2 的行，这个步骤用上了主键索引，rows=1 表示只扫描一行；</li><li>第二行 key=NULL，表示没有用上交易详情表 trade_detail 上的 tradeid 索引，进行了全表扫描。</li></ol><p>在这个执行计划里，是从 tradelog 表中取 tradeid 字段，再去 trade_detail 表里查询匹配字段。因此，我们把 tradelog 称为驱动表，把 trade_detail 称为被驱动表，把 tradeid 称为关联字段。</p><p>接下来，我们看下这个 explain 结果表示的执行流程：</p><p><img src="/2021/03/05/2021-03-05-mysql-18/8289c184c8529acea0269a7460dc62a9.png" alt></p><ul><li>第 1 步，是根据 id 在 tradelog 表里找到 L2 这一行；</li><li>第 2 步，是从 L2 中取出 tradeid 字段的值；</li><li>第 3 步，是根据 tradeid 值到 trade_detail 表中查找条件匹配的行。explain 的结果里面第二行的 key=NULL 表示的就是，这个过程是通过遍历主键索引的方式，一个一个地判断 tradeid 的值是否匹配。</li></ul><p>进行到这里，你会发现第 3 步不符合我们的预期。因为表 trade_detail 里 tradeid 字段上是有索引的，我们本来是希望通过使用 tradeid 索引能够快速定位到等值的行。但，这里并没有。</p><p>如果你去问 DBA 同学，他们可能会告诉你，因为这两个表的字符集不同，一个是 utf8，一个是 utf8mb4，所以做表连接查询的时候用不上关联字段的索引。这个回答，也是通常你搜索这个问题时会得到的答案。</p><p>但是你应该再追问一下，为什么字符集不同就用不上索引呢？</p><p>我们说问题是出在执行步骤的第 3 步，如果单独把这一步改成 SQL 语句的话，那就是：</p><pre><code>mysql&gt; select * from trade_detail where tradeid=$L2.tradeid.value; </code></pre><p>其中，$L2.tradeid.value 的字符集是 utf8mb4。</p><p>参照前面的两个例子，你肯定就想到了，字符集 utf8mb4 是 utf8 的超集，所以当这两个类型的字符串在做比较的时候，MySQL 内部的操作是，先把 utf8 字符串转成 utf8mb4 字符集，再做比较。</p><blockquote><p>这个设定很好理解，utf8mb4 是 utf8 的超集。类似地，在程序设计语言里面，做自动类型转换的时候，为了避免数据在转换过程中由于截断导致数据错误，也都是“按数据长度增加的方向”进行转换的。</p></blockquote><p>因此， 在执行上面这个语句的时候，需要将被驱动数据表里的字段一个个地转换成 utf8mb4，再跟 L2 做比较。</p><p>也就是说，实际上这个语句等同于下面这个写法：</p><pre><code>select * from trade_detail  where CONVERT(traideid USING utf8mb4)=$L2.tradeid.value; </code></pre><p>CONVERT() 函数，在这里的意思是把输入的字符串转成 utf8mb4 字符集。</p><p>这就再次触发了我们上面说到的原则：对索引字段做函数操作，优化器会放弃走树搜索功能。</p><p>到这里，你终于明确了，字符集不同只是条件之一，连接过程中要求在被驱动表的索引字段上加函数操作，是直接导致对被驱动表做全表扫描的原因。</p><p>作为对比验证，我给你提另外一个需求，“查找 trade_detail 表里 id=4 的操作，对应的操作者是谁”，再来看下这个语句和它的执行计划。</p><pre><code>mysql&gt;select l.operator from tradelog l , trade_detail d where d.tradeid=l.tradeid and d.id=4;</code></pre><p><img src="/2021/03/05/2021-03-05-mysql-18/92cb498ceb3557e41700fae53ce9bd11.png" alt></p><p>这个语句里 trade_detail 表成了驱动表，但是 explain 结果的第二行显示，这次的查询操作用上了被驱动表 tradelog 里的索引 (tradeid)，扫描行数是 1。</p><p>这也是两个 tradeid 字段的 join 操作，为什么这次能用上被驱动表的 tradeid 索引呢？我们来分析一下。</p><p>假设驱动表 trade_detail 里 id=4 的行记为 R4，那么在连接的时候，被驱动表 tradelog 上执行的就是类似这样的 SQL 语句：</p><pre><code>select operator from tradelog  where traideid =$R4.tradeid.value; </code></pre><p>这时候 $R4.tradeid.value 的字符集是 utf8, 按照字符集转换规则，要转成 utf8mb4，所以这个过程就被改写成：</p><pre><code>select operator from tradelog  where traideid =CONVERT($R4.tradeid.value USING utf8mb4); </code></pre><p>你看，这里的 CONVERT 函数是加在输入参数上的，这样就可以用上被驱动表的 traideid 索引。</p><p>理解了原理以后，就可以用来指导操作了。如果要优化语句</p><pre><code>select d.* from tradelog l, trade_detail d where d.tradeid=l.tradeid and l.id=2;</code></pre><p>的执行过程，有两种做法：</p><ul><li><p>比较常见的优化方法是，把 trade_detail 表上的 tradeid 字段的字符集也改成 utf8mb4，这样就没有字符集转换的问题了。</p><pre><code>  alter table trade_detail modify tradeid varchar(32) CHARACTER SET utf8mb4 default null;</code></pre></li><li><p>如果能够修改字段的字符集的话，是最好不过了。但如果数据量比较大， 或者业务上暂时不能做这个 DDL 的话，那就只能采用修改 SQL 语句的方法了。</p><pre><code>  mysql&gt; select d.* from tradelog l , trade_detail d where d.tradeid=CONVERT(l.tradeid USING utf8) and l.id=2; </code></pre><p>  这里，我主动把 l.tradeid 转成 utf8，就避免了被驱动表上的字符编码转换，从 explain 结果可以看到，这次索引走对了。</p></li></ul><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>字段发生了转换,导致本该使用索引而没有用到索引</p><ul><li><p>条件字段函数操作</p><p>  对索引字段做函数操作，MySQL 无法再使用索引快速定位功能，而只能使用全索引扫描（不使用索引）。</p></li><li><p>隐式类型转换</p><p>  隐在 MySQL 中，字符串和数字做比较的话，是将字符串转换成数字。<br>  select * from tradelog where tradeid=110717;<br>  将tradeid从String转换为int，对索引做了函数操作，不走索引；<br>  select * from tradelog where id=”83126”;<br>  将『83126』String转int，没涉及到索引函数操作，还是走索引</p></li><li><p>隐式字符编码转换</p><p>  如果驱动表的字符集比被驱动表得字符集小，关联列就能用到索引,如果更大,需要发生隐式编码转换,则不能用到索引,latin&lt;gbk&lt;utf8&lt;utf8mb4</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>显示随机消息</title>
      <link href="/2021/03/03/2021-03-03-mysql-17/"/>
      <url>/2021/03/03/2021-03-03-mysql-17/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><p>直接就是从一个单词表中随机选出三个单词。这个表的建表语句和初始数据的命令如下：</p><pre><code>mysql&gt; CREATE TABLE `words` (  `id` int(11) NOT NULL AUTO_INCREMENT,  `word` varchar(64) DEFAULT NULL,  PRIMARY KEY (`id`)) ENGINE=InnoDB;delimiter ;;create procedure idata()begin  declare i int;  set i=0;  while i&lt;10000 do    insert into words(word) values(concat(char(97+(i div 1000)), char(97+(i % 1000 div 100)), char(97+(i % 100 div 10)), char(97+(i % 10))));    set i=i+1;  end while;end;;delimiter ;call idata();</code></pre><h2 id="内存临时表"><a href="#内存临时表" class="headerlink" title="内存临时表"></a>内存临时表</h2><p>首先，你会想到用 order by rand() 来实现这个逻辑。</p><pre><code>mysql&gt; select word from words order by rand() limit 3;</code></pre><p>这个语句的意思很直白，随机排序取前 3 个。虽然这个 SQL 语句写法很简单，但执行流程却有点复杂的。</p><p>我们先用 explain 命令来看看这个语句的执行情况。</p><p><img src="/2021/03/03/2021-03-03-mysql-17/59a4fb0165b7ce1184e41f2d061ce350.png" alt></p><p>Extra 字段显示 Using temporary，表示的是需要使用临时表；Using filesort，表示的是需要执行排序操作。</p><p>因此这个 Extra 的意思就是，需要临时表，并且需要在临时表上排序。</p><p>对于 InnoDB 表来说，相对于rowid排序来说，执行全字段排序会减少磁盘访问，因此会被优先选择。</p><p>对于内存表，回表过程只是简单地根据数据行的位置，直接访问内存得到数据，根本不会导致多访问磁盘。优化器没有了这一层顾虑，那么它会优先考虑的，就是用于排序的行越小越好了，所以，MySQL 这时就会选择 rowid 排序。</p><p>这条语句的执行流程是这样的：</p><ol><li><p>创建一个临时表。这个临时表使用的是 memory 引擎，表里有两个字段，第一个字段是 double 类型，为了后面描述方便，记为字段 R，第二个字段是 varchar(64) 类型，记为字段 W。并且，这个表没有建索引。</p></li><li><p>从 words 表中，按主键顺序取出所有的 word 值。对于每一个 word 值，调用 rand() 函数生成一个大于 0 小于 1 的随机小数，并把这个随机小数和 word 分别存入临时表的 R 和 W 字段中，到此，扫描行数是 10000。</p></li><li><p>现在临时表有 10000 行数据了，接下来你要在这个没有索引的内存临时表上，按照字段 R 排序。</p></li><li><p>初始化 sort_buffer。sort_buffer 中有两个字段，一个是 double 类型，另一个是整型。</p></li><li><p>从内存临时表中一行一行地取出 R 值和位置信息（我后面会和你解释这里为什么是“位置信息”），分别存入 sort_buffer 中的两个字段里。这个过程要对内存临时表做全表扫描，此时扫描行数增加 10000，变成了 20000。</p></li><li><p>在 sort_buffer 中根据 R 的值进行排序。注意，这个过程没有涉及到表操作，所以不会增加扫描行数。</p></li><li><p>排序完成后，取出前三个结果的位置信息，依次到内存临时表中取出 word 值，返回给客户端。这个过程中，访问了表的三行数据，总扫描行数变成了 20003。</p></li></ol><p>接下来，我们通过慢查询日志（slow log）来验证一下我们分析得到的扫描行数是否正确。</p><pre><code># Query_time: 0.900376  Lock_time: 0.000347 Rows_sent: 3 Rows_examined: 20003SET timestamp=1541402277;select word from words order by rand() limit 3;</code></pre><p>其中，Rows_examined：20003 就表示这个语句执行过程中扫描了 20003 行，也就验证了我们分析得出的结论。</p><p>现在，我来把完整的排序执行流程图画出来。</p><p><img src="/2021/03/03/2021-03-03-mysql-17/2abe849faa7dcad0189b61238b849ffc.png" alt></p><p>图中的 pos 就是位置信息，你可能会觉得奇怪，这里的“位置信息”是个什么概念？在上一篇文章中，我们对 InnoDB 表排序的时候，明明用的还是 ID 字段。</p><p>这时候，我们就要回到一个基本概念：MySQL 的表是用什么方法来定位“一行数据”的。</p><p>其实不是的。如果你创建的表没有主键，或者把一个表的主键删掉了，那么 InnoDB 会自己生成一个长度为 6 字节的 rowid 来作为主键。</p><p>这也就是排序模式里面，rowid 名字的来历。实际上它表示的是：每个引擎用来唯一标识数据行的信息。</p><ul><li>对于有主键的 InnoDB 表来说，这个 rowid 就是主键 ID；</li><li>对于没有主键的 InnoDB 表来说，这个 rowid 就是由系统生成的；</li><li>MEMORY 引擎不是索引组织表。在这个例子里面，你可以认为它就是一个数组。因此，这个 rowid 其实就是数组的下标。</li></ul><p>order by rand() 使用了内存临时表，内存临时表排序的时候使用了 rowid 排序方法。</p><h2 id="磁盘临时表"><a href="#磁盘临时表" class="headerlink" title="磁盘临时表"></a>磁盘临时表</h2><p>那么，是不是所有的临时表都是内存表呢？</p><p>其实不是的。tmp_table_size 这个配置限制了内存临时表的大小，默认值是 16M。如果临时表大小超过了 tmp_table_size，那么内存临时表就会转成磁盘临时表。</p><p>磁盘临时表使用的引擎默认是 InnoDB，是由参数 internal_tmp_disk_storage_engine 控制的。</p><p>当使用磁盘临时表的时候，对应的就是一个没有显式索引的 InnoDB 表的排序过程。</p><p>为了复现这个过程，我把 tmp_table_size 设置成 1024，把 sort_buffer_size 设置成 32768, 把 max_length_for_sort_data 设置成 16。</p><pre><code>set tmp_table_size=1024;set sort_buffer_size=32768;set max_length_for_sort_data=16;/* 打开 optimizer_trace，只对本线程有效 */SET optimizer_trace=&#39;enabled=on&#39;; /* 执行语句 */select word from words order by rand() limit 3;/* 查看 OPTIMIZER_TRACE 输出 */SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\G</code></pre><p><img src="/2021/03/03/2021-03-03-mysql-17/78d2db9a4fdba81feadccf6e878b4aab.png" alt></p><p>因为将 max_length_for_sort_data 设置成 16，小于 word 字段的长度定义，所以我们看到 sort_mode 里面显示的是 rowid 排序，这个是符合预期的，参与排序的是随机值 R 字段和 rowid 字段组成的行。</p><p>这时候你可能心算了一下，发现不对。R 字段存放的随机值就 8 个字节，rowid 是 6 个字节（至于为什么是 6 字节，就留给你课后思考吧），数据总行数是 10000，这样算出来就有 140000 字节，超过了 sort_buffer_size 定义的 32768 字节了。但是，number_of_tmp_files 的值居然是 0，难道不需要用临时文件吗？</p><p>这个 SQL 语句的排序确实没有用到临时文件，采用是 MySQL 5.6 版本引入的一个新的排序算法，即：优先队列排序算法。接下来，我们就看看为什么没有使用临时文件的算法，也就是归并排序算法，而是采用了优先队列排序算法。</p><p>其实，我们现在的 SQL 语句，只需要取 R 值最小的 3 个 rowid。但是，如果使用归并排序算法的话，虽然最终也能得到前 3 个值，但是这个算法结束后，已经将 10000 行数据都排好序了。</p><p>也就是说，后面的 9997 行也是有序的了。但，我们的查询并不需要这些数据是有序的。所以，想一下就明白了，这浪费了非常多的计算量。</p><p>而优先队列算法，就可以精确地只得到三个最小值，执行流程如下：</p><ol><li>对于这 10000 个准备排序的 (R,rowid)，先取前三行，构造成一个堆；</li><li>取下一个行 (R’,rowid’)，跟当前堆里面最大的 R 比较，如果 R’小于 R，把这个 (R,rowid) 从堆中去掉，换成 (R’,rowid’)；</li><li>重复第 2 步，直到第 10000 个 (R’,rowid’) 完成比较。</li></ol><p><img src="/2021/03/03/2021-03-03-mysql-17/e9c29cb20bf9668deba8981e444f6897.png" alt></p><p>通过优先队列排序找到最小的三个 R 值的行的过程。整个排序过程中，为了最快地拿到当前堆的最大值，总是保持最大值在堆顶，因此这是一个最大堆。</p><p>OPTIMIZER_TRACE 结果中，filesort_priority_queue_optimization 这个部分的 chosen=true，就表示使用了优先队列排序算法，这个过程不需要临时文件，因此对应的 number_of_tmp_files 是 0。</p><p>这个流程结束后，我们构造的堆里面，就是这个 10000 行里面 R 值最小的三行。然后，依次把它们的 rowid 取出来，去临时表里面拿到 word 字段，这个过程就跟上一篇文章的 rowid 排序的过程一样了。</p><pre><code>select city,name,age from t where city=&#39;杭州&#39; order by name limit 1000  ;</code></pre><p>你可能会问，这里也用到了 limit，为什么没用优先队列排序算法呢？原因是，这条 SQL 语句是 limit 1000，如果使用优先队列算法的话，需要维护的堆的大小就是 1000 行的 (name,rowid)，超过了我设置的 sort_buffer_size 大小，所以只能使用归并排序算法。</p><p>总之，不论是使用哪种类型的临时表，order by rand() 这种写法都会让计算过程非常复杂，需要大量的扫描行数，因此排序过程的资源消耗也会很大。</p><h2 id="随机排序方法"><a href="#随机排序方法" class="headerlink" title="随机排序方法"></a>随机排序方法</h2><p>我们先把问题简化一下，如果只随机选择 1 个 word 值，可以怎么做呢？思路上是这样的：</p><ol><li>取得这个表的主键 id 的最大值 M 和最小值 N;</li><li>用随机函数生成一个最大值到最小值之间的数 X = (M-N)*rand() + N;</li><li>取不小于 X 的第一个 ID 的行。</li></ol><pre><code>mysql&gt; select max(id),min(id) into @M,@N from t ;set @X= floor((@M-@N+1)*rand() + @N);select * from t where id &gt;= @X limit 1;</code></pre><p>这个方法效率很高，因为取 max(id) 和 min(id) 都是不需要扫描索引的，而第三步的 select 也可以用索引快速定位，可以认为就只扫描了 3 行。但实际上，这个算法本身并不严格满足题目的随机要求，因为 ID 中间可能有空洞，因此选择不同行的概率不一样，不是真正的随机。</p><p>比如你有 4 个 id，分别是 1、2、4、5，如果按照上面的方法，那么取到 id=4 的这一行的概率是取得其他行概率的两倍。</p><p>如果这四行的 id 分别是 1、2、40000、40001 呢？这个算法基本就能当 bug 来看待了。</p><p>所以，为了得到严格随机的结果，你可以用下面这个流程:</p><ol><li>取得整个表的行数，并记为 C。</li><li>取得 Y = floor(C * rand())。 floor 函数在这里的作用，就是取整数部分。</li><li>再用 limit Y,1 取得一行。</li></ol><p>我们把这个算法，称为随机算法 2。下面这段代码，就是上面流程的执行语句的序列。</p><pre><code>mysql&gt; select count(*) into @C from t;set @Y = floor(@C * rand());set @sql = concat(&quot;select * from t limit &quot;, @Y, &quot;,1&quot;);prepare stmt from @sql;execute stmt;DEALLOCATE prepare stmt;</code></pre><p>由于 limit 后面的参数不能直接跟变量，所以我在上面的代码中使用了 prepare+execute 的方法。你也可以把拼接 SQL 语句的方法写在应用程序中，会更简单些。</p><p>这个随机算法 2，解决了算法 1 里面明显的概率不均匀问题。</p><p>MySQL 处理 limit Y,1 的做法就是按顺序一个一个地读出来，丢掉前 Y 个，然后把下一个记录作为返回结果，因此这一步需要扫描 Y+1 行。再加上，第一步扫描的 C 行，总共需要扫描 C+Y+1 行，执行代价比随机算法 1 的代价要高。</p><p>当然，随机算法 2 跟直接 order by rand() 比起来，执行代价还是小很多的。</p><p>现在，我们再看看，如果我们按照随机算法 2 的思路，要随机取 3 个 word 值呢？你可以这么做：</p><ol><li>取得整个表的行数，记为 C；</li><li>根据相同的随机方法得到 Y1、Y2、Y3；</li><li>再执行三个 limit Y, 1 语句得到三行数据。</li></ol><p>我们把这个算法，称作随机算法 3。下面这段代码，就是上面流程的执行语句的序列。</p><pre><code>mysql&gt; select count(*) into @C from t;set @Y1 = floor(@C * rand());set @Y2 = floor(@C * rand());set @Y3 = floor(@C * rand());select * from t limit @Y1，1； //在应用代码里面取Y1、Y2、Y3值，拼出SQL后执行select * from t limit @Y2，1；select * from t limit @Y3，1；</code></pre><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ol><li>创建的表没有主键，或者把一个表的主键删掉了，那么 InnoDB 会自己生成一个长度为 6 字节的 rowid 来作为主键。这也就是排序模式里面，rowid 名字的来历。实际上它表示的是：每个引擎用来唯一标识数据行的信息。</li><li>对于有主键的 InnoDB 表来说，这个 rowid 就是主键 ID；</li><li>对于没有主键的 InnoDB 表来说，这个 rowid 就是由系统生成的；</li><li>order by rand() 使用了内存临时表，内存临时表排序的时候使用了 rowid 排序方法。</li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>“order by”排序</title>
      <link href="/2021/03/01/2021-03-01-mysql-16/"/>
      <url>/2021/03/01/2021-03-01-mysql-16/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="全字段排序"><a href="#全字段排序" class="headerlink" title="全字段排序"></a>全字段排序</h2><p>假设你要查询城市是“杭州”的所有人名字，并且按照姓名排序返回前 1000 个人的姓名、年龄。</p><pre><code>CREATE TABLE `t` (  `id` int(11) NOT NULL,  `city` varchar(16) NOT NULL,  `name` varchar(16) NOT NULL,  `age` int(11) NOT NULL,  `addr` varchar(128) DEFAULT NULL,  PRIMARY KEY (`id`),  KEY `city` (`city`)) ENGINE=InnoDB;</code></pre><pre><code>select city,name,age from t where city=&#39;杭州&#39; order by name limit 1000  ;</code></pre><p>为避免全表扫描，我们需要在 city 字段加上索引。</p><p>在 city 字段上创建索引之后，我们用 explain 命令来看看这个语句的执行情况。</p><p><img src="/2021/03/01/2021-03-01-mysql-16/826579b63225def812330ef6c344a303.png" alt></p><p>Extra 这个字段中的“Using filesort”表示的就是需要排序，MySQL 会给每个线程分配一块内存用于排序，称为 sort_buffer。</p><p><img src="/2021/03/01/2021-03-01-mysql-16/5334cca9118be14bde95ec94b02f0a3e.png" alt></p><p>从图中可以看到，满足 city=’杭州’条件的行，是从 ID_X 到 ID_(X+N) 的这些记录。</p><p>通常情况下，这个语句执行流程如下所示 ：</p><ol><li>初始化 sort_buffer，确定放入 name、city、age 这三个字段；</li><li>从索引 city 找到第一个满足 city=’杭州’条件的主键 id，也就是图中的 ID_X；</li><li>到主键 id 索引取出整行，取 name、city、age 三个字段的值，存入 sort_buffer 中；</li><li>从索引 city 取下一个记录的主键 id；</li><li>重复步骤 3、4 直到 city 的值不满足查询条件为止，对应的主键 id 也就是图中的 ID_Y；</li><li>对 sort_buffer 中的数据按照字段 name 做快速排序；</li><li>按照排序结果取前 1000 行返回给客户端。</li></ol><p>我们暂且把这个排序过程，称为全字段排序，执行流程的示意图如下所示</p><p><img src="/2021/03/01/2021-03-01-mysql-16/6c821828cddf46670f9d56e126e3e772.jpg" alt="全字段排序"></p><p>图中“按 name 排序”这个动作，可能在内存中完成，也可能需要使用外部排序，这取决于排序所需的内存和参数 sort_buffer_size。</p><p>sort_buffer_size，就是 MySQL 为排序开辟的内存（sort_buffer）的大小。如果要排序的数据量小于 sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。</p><p>可以用下面介绍的方法，来确定一个排序语句是否使用了临时文件。</p><pre><code>/* 打开optimizer_trace，只对本线程有效 */SET optimizer_trace=&#39;enabled=on&#39;; /* @a保存Innodb_rows_read的初始值 */select VARIABLE_VALUE into @a from  performance_schema.session_status where variable_name = &#39;Innodb_rows_read&#39;;/* 执行语句 */select city, name,age from t where city=&#39;杭州&#39; order by name limit 1000; /* 查看 OPTIMIZER_TRACE 输出 */SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\G/* @b保存Innodb_rows_read的当前值 */select VARIABLE_VALUE into @b from performance_schema.session_status where variable_name = &#39;Innodb_rows_read&#39;;/* 计算Innodb_rows_read差值 */select @b-@a;</code></pre><p>这个方法是通过查看 OPTIMIZER_TRACE 的结果来确认的，你可以从 number_of_tmp_files 中看到是否使用了临时文件。</p><p><img src="/2021/03/01/2021-03-01-mysql-16/89baf99cdeefe90a22370e1d6f5e6495.png" alt></p><p>number_of_tmp_files 表示的是，排序过程中使用的临时文件数。你一定奇怪，为什么需要 12 个文件？内存放不下时，就需要使用外部排序，外部排序一般使用归并排序算法。可以这么简单理解，MySQL 将需要排序的数据分成 12 份，每一份单独排序后存在这些临时文件中。然后把这 12 个有序文件再合并成一个有序的大文件。</p><p>如果 sort_buffer_size 超过了需要排序的数据量的大小，number_of_tmp_files 就是 0，表示排序可以直接在内存中完成。</p><p>否则就需要放在临时文件中排序。sort_buffer_size 越小，需要分成的份数越多，number_of_tmp_files 的值就越大。</p><h2 id="rowid-排序"><a href="#rowid-排序" class="headerlink" title="rowid 排序"></a>rowid 排序</h2><p>在上面这个算法过程里面，只对原表的数据读了一遍，剩下的操作都是在 sort_buffer 和临时文件中执行的。但这个算法有一个问题，就是如果查询要返回的字段很多的话，那么 sort_buffer 里面要放的字段数太多，这样内存里能够同时放下的行数很少，要分成很多个临时文件，排序的性能会很差。</p><p>所以如果单行很大，这个方法效率不够好。</p><p>那么，如果 MySQL 认为排序的单行长度太大会怎么做呢？</p><p>接下来，我来修改一个参数，让 MySQL 采用另外一种算法。</p><pre><code>SET max_length_for_sort_data = 16;</code></pre><p>max_length_for_sort_data，是 MySQL 中专门控制用于排序的行数据的长度的一个参数。它的意思是，如果单行的长度超过这个值，MySQL 就认为单行太大，要换一个算法。</p><p>city、name、age 这三个字段的定义总长度是 36，我把 max_length_for_sort_data 设置为 16，我们再来看看计算过程有什么改变。</p><p>新的算法放入 sort_buffer 的字段，只有要排序的列（即 name 字段）和主键 id。</p><p>但这时，排序的结果就因为少了 city 和 age 字段的值，不能直接返回了，整个执行流程就变成如下所示的样子：</p><ol><li>初始化 sort_buffer，确定放入两个字段，即 name 和 id；</li><li>从索引 city 找到第一个满足 city=’杭州’条件的主键 id，也就是图中的 ID_X；</li><li>到主键 id 索引取出整行，取 name、id 这两个字段，存入 sort_buffer 中；</li><li>从索引 city 取下一个记录的主键 id；</li><li>重复步骤 3、4 直到不满足 city=’杭州’条件为止，也就是图中的 ID_Y；</li><li>对 sort_buffer 中的数据按照字段 name 进行排序；</li><li>遍历排序结果，取前 1000 行，并按照 id 的值回到原表中取出 city、name 和 age 三个字段返回给客户端。</li></ol><p>这个执行流程的示意图如下，我把它称为 rowid 排序。</p><p><img src="/2021/03/01/2021-03-01-mysql-16/dc92b67721171206a302eb679c83e86d.jpg" alt="rowid 排序"></p><p>对比全字段排序流程图你会发现，rowid 排序多访问了一次表 t 的主键索引，就是步骤 7。</p><p>需要说明的是，最后的“结果集”是一个逻辑概念，实际上 MySQL 服务端从排序后的 sort_buffer 中依次取出 id，然后到原表查到 city、name 和 age 这三个字段的结果，不需要在服务端再耗费内存存储结果，是直接返回给客户端的。</p><h2 id="全字段排序-VS-rowid-排序"><a href="#全字段排序-VS-rowid-排序" class="headerlink" title="全字段排序 VS rowid 排序"></a>全字段排序 VS rowid 排序</h2><p>如果 MySQL 实在是担心排序内存太小，会影响排序效率，才会采用 rowid 排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据。</p><p>如果 MySQL 认为内存足够大，会优先选择全字段排序，把需要的字段都放到 sort_buffer 中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据。</p><p>这也就体现了 MySQL 的一个设计思想：如果内存够，就要多利用内存，尽量减少磁盘访问。</p><p>对于 InnoDB 表来说，rowid 排序会要求回表多造成磁盘读，因此不会被优先选择。</p><p>其实，并不是所有的 order by 语句，都需要排序操作的。从上面分析的执行过程，我们可以看到，MySQL 之所以需要生成临时表，并且在临时表上做排序操作，其原因是原来的数据都是无序的。</p><p>你可以设想下，如果能够保证从 city 这个索引上取出来的行，天然就是按照 name 递增排序的话，是不是就可以不用再排序了呢？</p><p>所以，我们可以在这个市民表上创建一个 city 和 name 的联合索引，对应的 SQL 语句是</p><pre><code>alter table t add index city_user(city, name);</code></pre><p>作为与 city 索引的对比，我们来看看这个索引的示意图。</p><p><img src="/2021/03/01/2021-03-01-mysql-16/f980201372b676893647fb17fac4e2bf.png" alt></p><p>在这个索引里面，我们依然可以用树搜索的方式定位到第一个满足 city=’杭州’的记录，并且额外确保了，接下来按顺序取“下一条记录”的遍历过程中，只要 city 的值是杭州，name 的值就一定是有序的。</p><p>这样整个查询过程的流程就变成了：</p><ol><li>从索引 (city,name) 找到第一个满足 city=’杭州’条件的主键 id；</li><li>到主键 id 索引取出整行，取 name、city、age 三个字段的值，作为结果集的一部分直接返回；</li><li>从索引 (city,name) 取下一个记录主键 id；</li><li>重复步骤 2、3，直到查到第 1000 条记录，或者是不满足 city=’杭州’条件时循环结束。</li></ol><p><img src="/2021/03/01/2021-03-01-mysql-16/3f590c3a14f9236f2d8e1e2cb9686692.jpg" alt="引入 (city,name) 联合索引后，查询语句的执行计划"></p><p>可以看到，这个查询过程不需要临时表，也不需要排序。接下来，我们用 explain 的结果来印证一下。</p><p><img src="/2021/03/01/2021-03-01-mysql-16/fc53de303811ba3c46d344595743358a.png" alt></p><p>从图中可以看到，Extra 字段中没有 Using filesort 了，也就是不需要排序了。而且由于 (city,name) 这个联合索引本身有序，所以这个查询也不用把 4000 行全都读一遍，只要找到满足条件的前 1000 条记录就可以退出了。也就是说，在我们这个例子里，只需要扫描 1000 次。</p><p>覆盖索引是指，索引上的信息足够满足查询请求，不需要再回到主键索引上去取数据。</p><p>按照覆盖索引的概念，我们可以再优化一下这个查询语句的执行流程。</p><p>针对这个查询，我们可以创建一个 city、name 和 age 的联合索引，对应的 SQL 语句就是：</p><pre><code>alter table t add index city_user_age(city, name, age);</code></pre><p>这时，对于 city 字段的值相同的行来说，还是按照 name 字段的值递增排序的，此时的查询语句也就不再需要排序了。这样整个查询语句的执行流程就变成了：</p><ol><li>从索引 (city,name,age) 找到第一个满足 city=’杭州’条件的记录，取出其中的 city、name 和 age 这三个字段的值，作为结果集的一部分直接返回；</li><li>从索引 (city,name,age) 取下一个记录，同样取出这三个字段的值，作为结果集的一部分直接返回；</li><li>重复执行步骤 2，直到查到第 1000 条记录，或者是不满足 city=’杭州’条件时循环结束。</li></ol><p><img src="/2021/03/01/2021-03-01-mysql-16/df4b8e445a59c53df1f2e0f115f02cd6.jpg" alt="引入 (city,name,age) 覆盖索引后，查询语句的执行流程"></p><p>然后，我们再来看看 explain 的结果。</p><p><img src="/2021/03/01/2021-03-01-mysql-16/9e40b7b8f0e3f81126a9171cc22e3423.png" alt></p><p>可以看到，Extra 字段里面多了“Using index”，表示的就是使用了覆盖索引，性能上会快很多。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ol><li><p>MySQL会为每个线程分配一个内存（sort_buffer）用于排序该内存大小为sort_buffer_size</p><ul><li>如果排序的数据量小于sort_buffer_size，排序将会在内存中完成</li><li>如果排序数据量很大，内存中无法存下这么多数据，则会使用磁盘临时文件来辅助排序，也称外部排序</li><li>在使用外部排序时，MySQL会分成好几份单独的临时文件用来存放排序后的数据，然后在将这些文件合并成一个大文件</li><li>按照情况建立联合索引来避免排序所带来的性能损耗，允许的情况下也可以建立覆盖索引来避免回表</li></ul></li><li><p>全字段排序</p><ul><li>通过索引将所需的字段全部读取到sort_buffer中</li><li>按照排序字段进行排序</li><li>将结果集返回给客户端<br>缺点：</li><li>造成sort_buffer中存放不下很多数据，因为除了排序字段还存放其他字段，对sort_buffer的利用效率不高</li><li>当所需排序数据量很大时，会有很多的临时文件，排序性能也会很差<br>优点：MySQL认为内存足够大时会优先选择全字段排序，因为这种方式比rowid 排序避免了一次回表操作</li></ul></li><li><p>rowid排序</p><ul><li>通过控制排序的行数据的长度来让sort_buffer中尽可能多的存放数据，max_length_for_sort_data</li><li>只将需要排序的字段和主键读取到sort_buffer中，并按照排序字段进行排序</li><li>按照排序后的顺序，取id进行回表取出想要获取的数据</li><li>将结果集返回给客户端<br>优点：更好的利用内存的sort_buffer进行排序操作，尽量减少对磁盘的访问<br>缺点：回表的操作是随机IO，会造成大量的随机读，不一定就比全字段排序减少对磁盘的访问</li></ul></li><li><p>全字段排序 VS rowid 排序</p><ul><li>如果 MySQL 实在是担心排序内存太小，会影响排序效率，才会采用 rowid 排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据。</li><li>如果 MySQL 认为内存足够大，会优先选择全字段排序，把需要的字段都放到 sort_buffer 中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据。</li></ul></li><li><p>如果遇到order by，尽量使用索引，因为索引本来就已经排好序了的。甚至可以利用覆盖索引，减少回表查询，提高查询效率。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日志相关问题</title>
      <link href="/2021/02/28/2021-02-28-mysql-15/"/>
      <url>/2021/02/28/2021-02-28-mysql-15/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="日志相关问题"><a href="#日志相关问题" class="headerlink" title="日志相关问题"></a>日志相关问题</h2><p><img src="/2021/02/28/2021-02-28-mysql-15/ee9af616e05e4b853eba27048351f62a.jpg" alt="两阶段提交示意图"></p><p>在两阶段提交的不同时刻，MySQL 异常重启会出现什么现象。</p><p>如果在图中时刻 A 的地方，也就是写入 redo log 处于 prepare 阶段之后、写 binlog 之前，发生了崩溃（crash），由于此时 binlog 还没写，redo log 也还没提交，所以崩溃恢复的时候，这个事务会回滚。这时候，binlog 还没写，所以也不会传到备库。</p><p>大家出现问题的地方，主要集中在时刻 B，也就是 binlog 写完，redo log 还没 commit 前发生 crash，那崩溃恢复的时候 MySQL 会怎么处理？</p><ul><li>如果 redo log 里面的事务是完整的，也就是已经有了 commit 标识，则直接提交；</li><li>如果 redo log 里面的事务只有完整的 prepare，则判断对应的事务 binlog 是否存在并完整：如果是，则提交事务； 否则，回滚事务。</li></ul><p>这里，时刻 B 发生 crash 对应的就是 2(a) 的情况，崩溃恢复过程中事务会被提交。</p><p>为什么还要两阶段提交呢？干脆先 redo log 写完，再写 binlog。崩溃恢复的时候，必须得两个日志都完整才可以。是不是一样的逻辑？</p><p>对于 InnoDB 引擎来说，如果 redo log 提交完成了，事务就不能回滚（如果这还允许回滚，就可能覆盖掉别的事务的更新）。而如果 redo log 直接提交，然后 binlog 写入的时候失败，InnoDB 又回滚不了，数据和 binlog 日志又不一致了。</p>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>count(*)最快</title>
      <link href="/2021/02/27/2021-02-27-mysql-14/"/>
      <url>/2021/02/27/2021-02-27-mysql-14/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="count-的实现方式"><a href="#count-的实现方式" class="headerlink" title="count(*) 的实现方式"></a>count(*) 的实现方式</h2><p>在不同的 MySQL 引擎中，count(*) 有不同的实现方式。</p><ul><li>MyISAM 引擎把一个表的总行数存在了磁盘上，因此执行 count(*) 的时候会直接返回这个数，效率很高；</li><li>而 InnoDB 引擎就麻烦了，它执行 count(*) 的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。</li></ul><p>这里需要注意的是，我们在这篇文章里讨论的是没有过滤条件的 count(*)，如果加了 where 条件的话，MyISAM 表也是不能返回得这么快的。</p><p>为什么 InnoDB 不跟 MyISAM 一样，也把数字存起来呢？</p><p>这是因为即使是在同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB 表“应该返回多少行”也是不确定的。这里，我用一个算 count(*) 的例子来为你解释一下。</p><p>假设表 t 中现在有 10000 条记录，我们设计了三个用户并行的会话。</p><ul><li>会话 A 先启动事务并查询一次表的总行数；</li><li>会话 B 启动事务，插入一行后记录后，查询表的总行数；</li><li>会话 C 先启动一个单独的语句，插入一行记录后，查询表的总行数。</li></ul><p><img src="/2021/02/27/2021-02-27-mysql-14/20210221230417.jpg" alt="会话 A、B、C 的执行流程"></p><p>你会看到，在最后一个时刻，三个会话 A、B、C 会同时查询表 t 的总行数，但拿到的结果却不同。</p><p>这和 InnoDB 的事务设计有关系，可重复读是它默认的隔离级别，在代码上就是通过多版本并发控制，也就是 MVCC 来实现的。每一行记录都要判断自己是否对这个会话可见，因此对于 count(*) 请求来说，InnoDB 只好把数据一行一行地读出依次判断，可见的行才能够用于计算“基于这个查询”的表的总行数。</p><p><strong>InnoDB 是索引组织表，主键索引树的叶子节点是数据，而普通索引树的叶子节点是主键值。所以，普通索引树比主键索引树小很多。对于 count(*) 这样的操作，遍历哪个索引树得到的结果逻辑上都是一样的。因此，MySQL 优化器会找到最小的那棵树来遍历。在保证逻辑正确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一。</strong></p><p>如果你用过 show table status 命令的话，就会发现这个命令的输出结果里面也有一个 TABLE_ROWS 用于显示这个表当前有多少行，这个命令执行挺快的，那这个 TABLE_ROWS 能代替 count(*) 吗？</p><p>索引统计的值是通过采样来估算的。实际上，TABLE_ROWS 就是从这个采样估算得来的，因此它也很不准。有多不准呢，官方文档说误差可能达到 40% 到 50%。所以，show table status 命令显示的行数也不能直接使用。</p><p>小结一下：</p><ul><li>MyISAM 表虽然 count(*) 很快，但是不支持事务；</li><li>show table status 命令虽然返回很快，但是不准确；</li><li>InnoDB 表直接 count(*) 会遍历全表，虽然结果准确，但会导致性能问题。</li></ul><h2 id="不同的-count-用法"><a href="#不同的-count-用法" class="headerlink" title="不同的 count 用法"></a>不同的 count 用法</h2><p>在 select count(?) from t 这样的查询语句里面，count(*)、count(主键 id)、count(字段) 和 count(1) 等不同用法的性能，有哪些差别?</p><p>count() 是一个聚合函数，对于返回的结果集，一行行地判断，如果 count 函数的参数不是 NULL，累计值就加 1，否则不加。最后返回累计值。</p><p><strong>所以，count(*)、count(主键 id) 和 count(1) 都表示返回满足条件的结果集的总行数；而 count(字段），则表示返回满足条件的数据行里面，参数“字段”不为 NULL 的总个数。</strong></p><p>至于分析性能差别的时候，你可以记住这么几个原则：</p><ul><li>server 层要什么就给什么；</li><li>InnoDB 只给必要的值；</li><li>现在的优化器只优化了 count(*) 的语义为“取行数”，其他“显而易见”的优化并没有做。</li></ul><p>对于 count(主键 id) 来说，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加。</p><p>对于 count(1) 来说，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。</p><p>单看这两个用法的差别的话，你能对比出来，count(1) 执行得要比 count(主键 id) 快。因为从引擎返回 id 会涉及到解析数据行，以及拷贝字段值的操作。</p><p>对于 count(字段) 来说：</p><ul><li>如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加；</li><li>如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。</li></ul><p>也就是前面的第一条原则，server 层要什么字段，InnoDB 就返回什么字段。</p><p><strong>但是 count(<em>) 是例外，并不会把全部字段取出来，而是专门做了优化，不取值。count(</em>) 肯定不是 null，按行累加。</strong></p><p>所以结论是：按照效率排序的话，count(字段)&lt;count(主键 id)&lt;count(1)≈count(<em>)，所以我建议你，尽量使用 count(</em>)。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ol><li><p>按照效率排序的话，count(字段)&lt;count(主键 id)&lt;count(1)≈count(<em>)，建议尽量使用 count(</em>)</p></li><li><p>MyISAM 引擎把一个表的总行数存在了磁盘上，因此执行 count(*) 的时候会直接返回这个数，效率很高；</p></li><li><p>而 InnoDB 引擎就麻烦了，它执行 count(*) 的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。</p></li><li><p>count() 是一个聚合函数，对于返回的结果集，一行行地判断，如果 count 函数的参数不是 NULL，累计值就加 1，否则不加，最后返回累计值。</p></li><li><p>如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加；</p></li><li><p>如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。</p></li><li><p>从引擎返回的字段会涉及到解析数据行，以及拷贝字段值的操作。</p></li><li><p>对于 count(主键 id) 来说，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加。从引擎返回的 主键id 会涉及到解析数据行，以及拷贝字段值的操作。</p></li><li><p>对于 count(1) 来说，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。</p></li><li><p>对于count(<em>)来说，并不会把全部字段取出来，而是专门做了优化，不取值。count(</em>) 肯定不是 null，按行累加。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>删数据但表文件大小不变</title>
      <link href="/2021/02/25/2021-02-25-mysql-13/"/>
      <url>/2021/02/25/2021-02-25-mysql-13/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><p>这里，我们还是针对 MySQL 中应用最广泛的 InnoDB 引擎展开讨论。一个 InnoDB 表包含两部分，即：表结构定义和数据。在 MySQL 8.0 版本以前，表结构是存在以.frm 为后缀的文件里。而 MySQL 8.0 版本，则已经允许把表结构定义放在系统数据表中了。因为表结构定义占用的空间很小，所以我们今天主要讨论的是表数据。</p><h2 id="参数-innodb-file-per-table"><a href="#参数-innodb-file-per-table" class="headerlink" title="参数 innodb_file_per_table"></a>参数 innodb_file_per_table</h2><p>表数据既可以存在共享表空间里，也可以是单独的文件。这个行为是由参数 innodb_file_per_table 控制的：</p><ul><li>这个参数设置为 OFF 表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起；</li><li>这个参数设置为 ON 表示的是，每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中。</li></ul><p>从 MySQL 5.6.6 版本开始，它的默认值就是 ON 了。</p><p>我建议你不论使用 MySQL 的哪个版本，都将这个值设置为 ON。因为，一个表单独存储为一个文件更容易管理，而且在你不需要这个表的时候，通过 drop table 命令，系统就会直接删除这个文件。而如果是放在共享表空间中，即使表删掉了，空间也是不会回收的。</p><p>所以，将 innodb_file_per_table 设置为 ON，是推荐做法，我们接下来的讨论都是基于这个设置展开的。</p><p>我们在删除整个表的时候，可以使用 drop table 命令回收表空间。但是，我们遇到的更多的删除数据的场景是删除某些行，这时就遇到了我们文章开头的问题：表中的数据被删除了，但是表空间却没有被回收。</p><h2 id="数据删除流程"><a href="#数据删除流程" class="headerlink" title="数据删除流程"></a>数据删除流程</h2><p><img src="/2021/02/25/2021-02-25-mysql-13/f0b1e4ac610bcb5c5922d0b18563f3c8.png" alt="B+ 树索引示意图"></p><p>假设，我们要删掉 R4 这个记录，InnoDB 引擎只会把 R4 这个记录标记为删除。如果之后要再插入一个 ID 在 300 和 600 之间的记录时，可能会复用这个位置。但是，磁盘文件的大小并不会缩小。</p><p>现在，你已经知道了 InnoDB 的数据是按页存储的，那么如果我们删掉了一个数据页上的所有记录，会怎么样？</p><p>答案是，整个数据页就可以被复用了。</p><p>但是，数据页的复用跟记录的复用是不同的。</p><p>记录的复用，只限于符合范围条件的数据。比如上面的这个例子，R4 这条记录被删除后，如果插入一个 ID 是 400 的行，可以直接复用这个空间。但如果插入的是一个 ID 是 800 的行，就不能复用这个位置了。</p><p>而当整个页从 B+ 树里面摘掉以后，可以复用到任何位置。以图 1 为例，如果将数据页 page A 上的所有记录删除以后，page A 会被标记为可复用。这时候如果要插入一条 ID=50 的记录需要使用新页的时候，page A 是可以被复用的。</p><p>如果相邻的两个数据页利用率都很小，系统就会把这两个页上的数据合到其中一个页上，另外一个数据页就被标记为可复用。</p><p>进一步地，如果我们用 delete 命令把整个表的数据删除呢？结果就是，所有的数据页都会被标记为可复用。但是磁盘上，文件不会变小。</p><p>你现在知道了，delete 命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的。也就是说，通过 delete 命令是不能回收表空间的。这些可以复用，而没有被使用的空间，看起来就像是“空洞”。</p><p>实际上，不止是删除数据会造成空洞，插入数据也会。</p><p>如果数据是按照索引递增顺序插入的，那么索引是紧凑的。但如果数据是随机插入的，就可能造成索引的数据页分裂。</p><p>假设page A 已经满了，这时我要再插入一行数据，会怎样呢？</p><p><img src="/2021/02/25/2021-02-25-mysql-13/8083f05a4a4c0372833a6e01d5a8e6ea.png" alt="插入数据导致页分裂"></p><p>可以看到，由于 page A 满了，再插入一个 ID 是 550 的数据时，就不得不再申请一个新的页面 page B 来保存数据了。页分裂完成后，page A 的末尾就留下了空洞（注意：实际上，可能不止 1 个记录的位置是空洞）。</p><p>另外，更新索引上的值，可以理解为删除一个旧的值，再插入一个新值。不难理解，这也是会造成空洞的。</p><p>也就是说，经过大量增删改的表，都是可能是存在空洞的。所以，如果能够把这些空洞去掉，就能达到收缩表空间的目的。</p><p>而重建表，就可以达到这样的目的。</p><h2 id="重建表"><a href="#重建表" class="headerlink" title="重建表"></a>重建表</h2><p>试想一下，如果你现在有一个表 A，需要做空间收缩，为了把表中存在的空洞去掉，你可以怎么做呢？</p><p>你可以新建一个与表 A 结构相同的表 B，然后按照主键 ID 递增的顺序，把数据一行一行地从表 A 里读出来再插入到表 B 中。</p><p>由于表 B 是新建的表，所以表 A 主键索引上的空洞，在表 B 中就都不存在了。显然地，表 B 的主键索引更紧凑，数据页的利用率也更高。如果我们把表 B 作为临时表，数据从表 A 导入表 B 的操作完成后，用表 B 替换 A，从效果上看，就起到了收缩表 A 空间的作用。</p><p>这里，你可以使用 alter table A engine=InnoDB 命令来重建表。在 MySQL 5.5 版本之前，这个命令的执行流程跟我们前面描述的差不多，区别只是这个临时表 B 不需要你自己创建，MySQL 会自动完成转存数据、交换表名、删除旧表的操作。</p><p><img src="/2021/02/25/2021-02-25-mysql-13/02e083adaec6e1191f54992f7bc13dcd.png" alt="改锁表 DDL"></p><p>显然，花时间最多的步骤是往临时表插入数据的过程，如果在这个过程中，有新的数据要写入到表 A 的话，就会造成数据丢失。因此，在整个 DDL 过程中，表 A 中不能有更新。也就是说，这个 DDL 不是 Online 的。</p><p>而在 MySQL 5.6 版本开始引入的 Online DDL，对这个操作流程做了优化。</p><p>引入了 Online DDL 之后，重建表的流程：</p><ol><li>建立一个临时文件，扫描表 A 主键的所有数据页；</li><li>用数据页中表 A 的记录生成 B+ 树，存储到临时文件中；</li><li>生成临时文件的过程中，将所有对 A 的操作记录在一个日志文件（row log）中，对应的是图中 state2 的状态；</li><li>临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表 A 相同的数据文件，对应的就是图中 state3 的状态；</li><li>用临时文件替换表 A 的数据文件。</li></ol><p><img src="/2021/02/25/2021-02-25-mysql-13/2d1cfbbeb013b851a56390d38b5321f0.png" alt="Online DDL"></p><p>可以看到，与图 3 过程的不同之处在于，由于日志文件记录和重放操作这个功能的存在，这个方案在重建表的过程中，允许对表 A 做增删改操作。这也就是 Online DDL 名字的来源。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>delete 命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的。也就是说，通过 delete 命令是不能回收表空间的。</p><p>如果要收缩一个表，只是 delete 掉表里面不用的数据的话，表文件的大小是不会变的，你还要通过 alter table A engine=InnoDB 命令重建表，才能达到表文件变小的目的。</p>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>查询性能不稳定</title>
      <link href="/2021/02/23/2021-02-23-mysql-12/"/>
      <url>/2021/02/23/2021-02-23-mysql-12/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="你的-SQL-语句为什么变“慢”了"><a href="#你的-SQL-语句为什么变“慢”了" class="headerlink" title="你的 SQL 语句为什么变“慢”了"></a>你的 SQL 语句为什么变“慢”了</h2><ol><li><p>第一种场景是，粉板满了，记不下了。这时候如果再有人来赊账，掌柜就只得放下手里的活儿，将粉板上的记录擦掉一些，留出空位以便继续记账。当然在擦掉之前，他必须先将正确的账目记录到账本中才行。</p><p> 也就是InnoDB 的 redo log 写满了。这时候系统会停止所有更新操作，把 checkpoint 往前推进，redo log 留出空间可以继续写。</p><p> <img src="/2021/02/23/2021-02-23-mysql-12/a25bdbbfc2cfc5d5e20690547fe7f2e5.jpg" alt="redo log 状态图"></p><p> checkpoint 可不是随便往前修改一下位置就可以的。比如上图中，把 checkpoint 位置从 CP 推进到 CP’，就需要将两个点之间的日志（浅绿色部分），对应的所有脏页都 flush 到磁盘上。之后，图中从 write pos 到 CP’之间就是可以再写入的 redo log 的区域。</p></li><li><p>第二种场景是，这一天生意太好，要记住的事情太多，掌柜发现自己快记不住了，赶紧找出账本把孔乙己这笔账先加进去。</p><p> 这种场景，对应的就是系统内存不足。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。</p></li></ol><ol start="3"><li><p>第三种场景是，生意不忙的时候，或者打烊之后。这时候柜台没事，掌柜闲着也是闲着，不如更新账本。</p><p> 这种场景，对应的就是 MySQL 认为系统“空闲”的时候。当然，MySQL“这家酒店”的生意好起来可是会很快就能把粉板记满的，所以“掌柜”要合理地安排时间，即使是“生意好”的时候，也要见缝插针地找时间，只要有机会就刷一点“脏页”。</p></li><li><p>第四种场景是，年底了咸亨酒店要关门几天，需要把账结清一下。这时候掌柜要把所有账都记到账本上，这样过完年重新开张的时候，就能就着账本明确账目情况了。</p><p> 这种场景，对应的就是 MySQL 正常关闭的情况。这时候，MySQL 会把内存的脏页都 flush 到磁盘上，这样下次 MySQL 启动的时候，就可以直接从磁盘上读数据，启动速度会很快。</p></li></ol><p>分析一下上面四种场景对性能的影响。</p><p>其中，第三种情况是属于 MySQL 空闲时的操作，这时系统没什么压力，而第四种场景是数据库本来就要关闭了。这两种情况下，你不会太关注“性能”问题。所以这里，我们主要来分析一下前两种场景下的性能问题。</p><p>第一种是“redo log 写满了，要 flush 脏页”，这种情况是 InnoDB 要尽量避免的。因为出现这种情况的时候，整个系统就不能再接受更新了，所有的更新都必须堵住。如果你从监控上看，这时候更新数会跌为 0。</p><p>第二种是“内存不够用了，要先将脏页写到磁盘”，这种情况其实是常态。InnoDB 用缓冲池（buffer pool）管理内存，缓冲池中的内存页有三种状态：</p><ul><li>第一种是，还没有使用的；</li><li>第二种是，使用了并且是干净页；</li><li>第三种是，使用了并且是脏页。</li></ul><p>InnoDB 的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。</p><p>而当要读入的数据页没有在内存的时候，就必须到缓冲池中申请一个数据页。这时候只能把最久不使用的数据页从内存中淘汰掉：如果要淘汰的是一个干净页，就直接释放出来复用；但如果是脏页呢，就必须将脏页先刷到磁盘，变成干净页后才能复用。</p><p>所以，刷脏页虽然是常态，但是出现以下这两种情况，都是会明显影响性能的：</p><ul><li>一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长；</li><li>日志写满，更新全部堵住，写性能跌为 0，这种情况对敏感业务来说，是不能接受的。</li></ul><p>所以，InnoDB 需要有控制脏页比例的机制，来尽量避免上面的这两种情况。</p><h2 id="InnoDB-刷脏页的控制策略"><a href="#InnoDB-刷脏页的控制策略" class="headerlink" title="InnoDB 刷脏页的控制策略"></a>InnoDB 刷脏页的控制策略</h2><p>首先，你要正确地告诉 InnoDB 所在主机的 IO 能力，这样 InnoDB 才能知道需要全力刷脏页的时候，可以刷多快。</p><p>这就要用到 innodb_io_capacity 这个参数了，它会告诉 InnoDB 你的磁盘能力。这个值我建议你设置成磁盘的 IOPS。磁盘的 IOPS 可以通过 fio 这个工具来测试，下面的语句是我用来测试磁盘随机读写的命令：</p><pre><code> fio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest </code></pre><p>虽然我们现在已经定义了“全力刷脏页”的行为，但平时总不能一直是全力刷吧？毕竟磁盘能力不能只用来刷脏页，还需要服务用户请求。所以接下来，我们就一起看看 InnoDB 怎么控制引擎按照“全力”的百分比来刷脏页。</p><p>这个问题可以这么想，如果刷太慢，会出现什么情况？首先是内存脏页太多，其次是 redo log 写满。</p><p>所以，InnoDB 的刷盘速度就是要参考这两个因素：一个是脏页比例，一个是 redo log 写盘速度。</p><p>InnoDB 会根据这两个因素先单独算出两个数字。</p><p>参数 innodb_max_dirty_pages_pct 是脏页比例上限，默认值是 75%。InnoDB 会根据当前的脏页比例（假设为 M），算出一个范围在 0 到 100 之间的数字.</p><p>现在你知道了，InnoDB 会在后台刷脏页，而刷脏页的过程是要将内存页写入磁盘。所以，无论是你的查询语句在需要内存的时候可能要求淘汰一个脏页，还是由于刷脏页的逻辑会占用 IO 资源并可能影响到了你的更新语句，都可能是造成你从业务端感知到 MySQL“抖”了一下的原因。</p><p>要尽量避免这种情况，你就要合理地设置 innodb_io_capacity 的值，并且平时要多关注脏页比例，不要让它经常接近 75%。</p><p>一旦一个查询请求需要在执行过程中先 flush 掉一个脏页时，这个查询就可能要比平时慢了。而 MySQL 中的一个机制，可能让你的查询会更慢：在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉；而且这个把“邻居”拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷。</p><p>在 InnoDB 中，innodb_flush_neighbors 参数就是用来控制这个行为的，值为 1 的时候会有上述的“连坐”机制，值为 0 时表示不找邻居，自己刷自己的。</p><p>找“邻居”这个优化在机械硬盘时代是很有意义的，可以减少很多随机 IO。机械硬盘的随机 IOPS 一般只有几百，相同的逻辑操作减少随机 IO 就意味着系统性能的大幅度提升。</p><p>而如果使用的是 SSD 这类 IOPS 比较高的设备的话，我就建议你把 innodb_flush_neighbors 的值设置成 0。因为这时候 IOPS 往往不是瓶颈，而“只刷自己”，就能更快地执行完必要的刷脏页操作，减少 SQL 语句响应时间。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ol><li><p>MySQL抖一下是什么意思？</p><p> 抖我认为就是不稳定的意思，一个SQL语句平时速度都挺快的，偶尔会慢一下且没啥规律，就是抖啦！</p></li><li><p>MySQL为啥会抖一下？</p><p> 因为运行的不正常了，或者不稳定了，需要花费更多的资源处理别的事情，会使SQL语句的执行效率明显变慢。针对innoDB导致MySQL抖的原因，主要是InnoDB 会在后台刷脏页，而刷脏页的过程是要将内存页写入磁盘。所以，无论是你的查询语句在需要内存的时候可能要求淘汰一个脏页，还是由于刷脏页的逻辑会占用 IO 资源并可能影响到了你的更新语句，都可能是造成你从业务端感知MySQL“抖”了一下的原因。</p></li><li><p>MySQL抖一下有啥问题？</p><p> 很明显系统不稳定，性能突然下降对业务端是很不友好的。</p></li><li><p>怎么让MySQL不抖？</p><p> 设置合理参数配配置，尤其是设置 好innodb_io_capacity 的值，并且平时要多关注脏页比例，不要让它经常接近 75%</p></li><li><p>啥是脏页？</p><p> 当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。</p><p> 按照这个定义感觉脏页是不可避免的，写的时候总会先写内存再写磁盘和有没有用WAL没啥关系？</p></li><li><p>啥是干净页？</p><p> 内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。</p></li><li><p>脏页是咋产生的？</p><p> 因为使用了WAL技术，这个技术会把数据库的随机写转化为顺序写，但副作用就是会产生脏页。</p></li><li><p>啥是随机写？为啥那么耗性能？</p><p> 随机写我的理解是，这次写磁盘的那个扇区和上一次没啥关系，需要重新定位位置，机械运动是很慢的即使不是机械运动重新定位写磁盘的位置也是很耗时的。</p></li><li><p>啥是顺序写？</p><p> 顺序写我的理解是，这次写磁盘那个扇区就在上一次的下一个位置，不需要重新定位写磁盘的位置速度当然会快一些。</p></li><li><p>WAL怎么把随机写转化为顺序写的？</p><p>写redolog是顺序写的，先写redolog等合适的时候再写磁盘，间接的将随机写变成了顺序写，性能确实会提高不少。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>合适建立索引</title>
      <link href="/2021/02/21/2021-02-21-mysql-11/"/>
      <url>/2021/02/21/2021-02-21-mysql-11/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="前缀索引长度"><a href="#前缀索引长度" class="headerlink" title="前缀索引长度"></a>前缀索引长度</h2><p>现在，几乎所有的系统都支持邮箱登录，如何在邮箱这样的字段上建立合理的索引，是个问题。</p><p>假设，你现在维护一个支持邮箱登录的系统，用户表是这么定义的：</p><pre><code>mysql&gt; create table SUser(ID bigint unsigned primary key,email varchar(64), ... )engine=innodb; </code></pre><p>由于要使用邮箱登录，所以业务代码中一定会出现类似于这样的语句：</p><pre><code>mysql&gt; select f1, f2 from SUser where email=&#39;xxx&#39;;</code></pre><p>如果 email 这个字段上没有索引，那么这个语句就只能做全表扫描。</p><p>同时，MySQL 是支持前缀索引的，也就是说，你可以定义字符串的一部分作为索引。默认地，如果你创建索引的语句不指定前缀长度，那么索引就会包含整个字符串。</p><p>比如，这两个在 email 字段上创建索引的语句：</p><pre><code>mysql&gt; alter table SUser add index index1(email);或mysql&gt; alter table SUser add index index2(email(6));</code></pre><p>第一个语句创建的 index1 索引里面，包含了每个记录的整个字符串；而第二个语句创建的 index2 索引里面，对于每个记录都是只取前 6 个字节。</p><p>那么，这两种不同的定义在数据结构和存储上有什么区别呢？</p><p><img src="/2021/02/21/2021-02-21-mysql-11/d31da662bee595991862c439a5567eb7.jpg" alt="email 索引结构"></p><p><img src="/2021/02/21/2021-02-21-mysql-11/134583875561de914991fc2e192cf842.jpg" alt="email(6) 索引结构"></p><p>从图中你可以看到，由于 email(6) 这个索引结构中每个邮箱字段都只取前 6 个字节（即：zhangs），所以占用的空间会更小，这就是使用前缀索引的优势。</p><p>但，这同时带来的损失是，可能会增加额外的记录扫描次数。</p><p>接下来，我们再看看下面这个语句，在这两个索引定义下分别是怎么执行的。</p><pre><code>select id,name,email from SUser where email=&#39;zhangssxyz@xxx.com&#39;;</code></pre><p>如果使用的是 index1（即 email 整个字符串的索引结构），执行顺序是这样的：</p><ol><li>从 index1 索引树找到满足索引值是’<a href="mailto:zhangssxyz@xxx.com" target="_blank" rel="noopener">zhangssxyz@xxx.com</a>’的这条记录，取得 ID2 的值；</li><li>到主键上查到主键值是 ID2 的行，判断 email 的值是正确的，将这行记录加入结果集；</li><li>取 index1 索引树上刚刚查到的位置的下一条记录，发现已经不满足 email=’zhangssxyz@xxx.com’的条件了，循环结束。</li></ol><p>这个过程中，只需要回主键索引取一次数据，所以系统认为只扫描了一行。</p><p>如果使用的是 index2（即 email(6) 索引结构），执行顺序是这样的：</p><ol><li>从 index2 索引树找到满足索引值是’zhangs’的记录，找到的第一个是 ID1；</li><li>到主键上查到主键值是 ID1 的行，判断出 email 的值不是’<a href="mailto:zhangssxyz@xxx.com" target="_blank" rel="noopener">zhangssxyz@xxx.com</a>’，这行记录丢弃；</li><li>取 index2 上刚刚查到的位置的下一条记录，发现仍然是’zhangs’，取出 ID2，再到 ID 索引上取整行然后判断，这次值对了，将这行记录加入结果集；</li><li>重复上一步，直到在 idxe2 上取到的值不是’zhangs’时，循环结束。</li></ol><p>在这个过程中，要回主键索引取 4 次数据，也就是扫描了 4 行。</p><p>通过这个对比，你很容易就可以发现，使用前缀索引后，可能会导致查询语句读数据的次数变多。</p><p>但是，对于这个查询语句来说，如果你定义的 index2 不是 email(6) 而是 email(7），也就是说取 email 字段的前 7 个字节来构建索引的话，即满足前缀’zhangss’的记录只有一个，也能够直接查到 ID2，只扫描一行就结束了。</p><p>也就是说使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。</p><p>于是，你就有个问题：当要给字符串创建前缀索引时，有什么方法能够确定我应该使用多长的前缀呢？</p><p>实际上，我们在建立索引时关注的是区分度，区分度越高越好。因为区分度越高，意味着重复的键值越少。因此，我们可以通过统计索引上有多少个不同的值来判断要使用多长的前缀。</p><p>首先，你可以使用下面这个语句，算出这个列上有多少个不同的值：</p><pre><code>mysql&gt; select count(distinct email) as L from SUser;</code></pre><p>然后，依次选取不同长度的前缀来看这个值，比如我们要看一下 4~7 个字节的前缀索引，可以用这个语句：</p><pre><code>mysql&gt; select   count(distinct left(email,4)）as L4,  count(distinct left(email,5)）as L5,  count(distinct left(email,6)）as L6,  count(distinct left(email,7)）as L7,from SUser;</code></pre><p>当然，使用前缀索引很可能会损失区分度，所以你需要预先设定一个可以接受的损失比例，比如 5%。然后，在返回的 L4~L7 中，找出不小于 L * 95% 的值，假设这里 L6、L7 都满足，你就可以选择前缀长度为 6。</p><h2 id="前缀索引对覆盖索引的影响"><a href="#前缀索引对覆盖索引的影响" class="headerlink" title="前缀索引对覆盖索引的影响"></a>前缀索引对覆盖索引的影响</h2><p>前面我们说了使用前缀索引可能会增加扫描行数，这会影响到性能。其实，前缀索引的影响不止如此，我们再看一下另外一个场景。</p><pre><code>select id,email from SUser where email=&#39;zhangssxyz@xxx.com&#39;;</code></pre><p>与前面例子中的 SQL 语句</p><pre><code>select id,name,email from SUser where email=&#39;zhangssxyz@xxx.com&#39;;</code></pre><p>相比，这个语句只要求返回 id 和 email 字段。</p><p>所以，如果使用 index1（即 email 整个字符串的索引结构）的话，可以利用覆盖索引，从 index1 查到结果后直接就返回了，不需要回到 ID 索引再去查一次。而如果使用 index2（即 email(6) 索引结构）的话，就不得不回到 ID 索引再去判断 email 字段的值。</p><p>也就是说，使用前缀索引就用不上覆盖索引对查询性能的优化了，这也是你在选择是否使用前缀索引时需要考虑的一个因素。</p><h2 id="其他方式"><a href="#其他方式" class="headerlink" title="其他方式"></a>其他方式</h2><p>对于类似于邮箱这样的字段来说，使用前缀索引的效果可能还不错。但是，遇到前缀的区分度不够好的情况时，我们要怎么办呢？</p><p>比如，我们国家的身份证号，一共 18 位，其中前 6 位是地址码，所以同一个县的人的身份证号前 6 位一般会是相同的。</p><p>假设你维护的数据库是一个市的公民信息系统，这时候如果对身份证号做长度为 6 的前缀索引的话，这个索引的区分度就非常低了。</p><p>按照我们前面说的方法，可能你需要创建长度为 12 以上的前缀索引，才能够满足区分度要求。</p><p>但是，索引选取的越长，占用的磁盘空间就越大，相同的数据页能放下的索引值就越少，搜索的效率也就会越低。</p><p>那么，如果我们能够确定业务需求里面只有按照身份证进行等值查询的需求，还有没有别的处理方法呢？这种方法，既可以占用更小的空间，也能达到相同的查询效率。</p><p>第一种方式是使用倒序存储。如果你存储身份证号的时候把它倒过来存，每次查询的时候，你可以这么写：</p><pre><code>mysql&gt; select field_list from t where id_card = reverse(&#39;input_id_card_string&#39;);</code></pre><p>由于身份证号的最后 6 位没有地址码这样的重复逻辑，所以最后这 6 位很可能就提供了足够的区分度。当然了，实践中你不要忘记使用 count(distinct) 方法去做个验证。</p><p>第二种方式是使用 hash 字段。你可以在表上再创建一个整数字段，来保存身份证的校验码，同时在这个字段上创建索引。</p><pre><code>mysql&gt; alter table t add id_card_crc int unsigned, add index(id_card_crc);</code></pre><p>然后每次插入新记录的时候，都同时用 crc32() 这个函数得到校验码填到这个新字段。由于校验码可能存在冲突，也就是说两个不同的身份证号通过 crc32() 函数得到的结果可能是相同的，所以你的查询语句 where 部分要判断 id_card 的值是否精确相同。</p><pre><code>mysql&gt; select field_list from t where id_card_crc=crc32(&#39;input_id_card_string&#39;) and id_card=&#39;input_id_card_string&#39;</code></pre><p>这样，索引的长度变成了 4 个字节，比原来小了很多。</p><p>接下来，我们再一起看看使用倒序存储和使用 hash 字段这两种方法的异同点。</p><p>首先，它们的相同点是，都不支持范围查询。倒序存储的字段上创建的索引是按照倒序字符串的方式排序的，已经没有办法利用索引方式查出身份证号码在[ID_X, ID_Y]的所有市民了。同样地，hash 字段的方式也只能支持等值查询。</p><p>它们的区别，主要体现在以下三个方面：</p><ol><li>从占用的额外空间来看，倒序存储方式在主键索引上，不会消耗额外的存储空间，而 hash 字段方法需要增加一个字段。当然，倒序存储方式使用 4 个字节的前缀长度应该是不够的，如果再长一点，这个消耗跟额外这个 hash 字段也差不多抵消了。</li><li>在 CPU 消耗方面，倒序方式每次写和读的时候，都需要额外调用一次 reverse 函数，而 hash 字段的方式需要额外调用一次 crc32() 函数。如果只从这两个函数的计算复杂度来看的话，reverse 函数额外消耗的 CPU 资源会更小些。</li><li>从查询效率上看，使用 hash 字段方式的查询性能相对更稳定一些。因为 crc32 算出来的值虽然有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近 1。而倒序存储方式毕竟还是用的前缀索引的方式，也就是说还是会增加扫描行数。</li></ol><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ol><li><p>直接创建完整索引，这样可能比较占用空间；<br> 这种方式最简单，如果性能没问题，我会这么创建，简单直接且存储空间的费用越来越低</p></li><li><p>创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引；<br> 这种方式需要判断出前缀的长度多少合适，需要根据自己的业务来定，主要是看区分度多少合适</p></li><li><p>倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题；<br> 这种方式用于前缀区分度不高后缀区分度高的场景，目的还是要提高索引的区分度，使用这种方式不适合范围检索</p></li><li><p>创建 hash 字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>选错索引</title>
      <link href="/2021/02/20/2021-02-20-mysql-10/"/>
      <url>/2021/02/20/2021-02-20-mysql-10/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="优化器的逻辑"><a href="#优化器的逻辑" class="headerlink" title="优化器的逻辑"></a>优化器的逻辑</h2><p>选择索引是优化器的工作。</p><p>而优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句。在数据库里面，扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的 CPU 资源越少。</p><p>当然，扫描行数并不是唯一的判断标准，优化器还会结合是否使用临时表、是否排序等因素进行综合判断。</p><p>那么，问题就是：扫描行数是怎么判断的？</p><p>MySQL 在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数。</p><p>这个统计信息就是索引的“区分度”。显然，一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同的值的个数，我们称之为“基数”（cardinality）。也就是说，这个基数越大，索引的区分度越好。</p><p>我们可以使用 show index from t方法，看到每一个索引的基数。</p><p>那么，MySQL 是怎样得到索引的基数的呢？ MySQL 采样统计的方法。</p><p>为什么要采样统计呢？因为把整张表取出来一行行统计，虽然可以得到精确的结果，但是代价太高了，所以只能选择“采样统计”。</p><p>采样统计的时候，InnoDB 默认会选择 N 个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。</p><p>而数据表是会持续更新的，索引统计信息也不会固定不变。所以，当变更的数据行数超过 1/M 的时候，会自动触发重新做一次索引统计。</p><p>在 MySQL 中，有两种存储索引统计的方式，可以通过设置参数 innodb_stats_persistent 的值来选择：</p><ul><li>设置为 on 的时候，表示统计信息会持久化存储。这时，默认的 N 是 20，M 是 10。</li><li>设置为 off 的时候，表示统计信息只存储在内存中。这时，默认的 N 是 8，M 是 16。</li></ul><p>由于是采样统计，所以不管 N 是 20 还是 8，这个基数都是很容易不准的。</p><p>虽然不够精确，但大体上还是差不多的，选错索引一定还有别的原因。</p><p>其实索引统计只是一个输入，对于一个具体的语句来说，优化器还要判断，执行这个语句本身要扫描多少行。</p><p>优化器会估算使用普通索引需要把回表的代价算进去。</p><p>所以在实践中，如果你发现 explain 的结果预估的 rows 值跟实际情况差距比较大，可以采用analyze table t 命令，可以用来重新统计索引信息，来处理。</p><h2 id="索引选择异常和处理"><a href="#索引选择异常和处理" class="headerlink" title="索引选择异常和处理"></a>索引选择异常和处理</h2><p>一种方法是，采用 force index （select * from t force index(a) where ….）强行选择一个索引。MySQL 会根据词法解析的结果分析出可能可以使用的索引作为候选项，然后在候选列表中依次判断每个索引需要扫描多少行。如果 force index 指定的索引在候选索引列表中，就直接选择这个索引，不再评估其他索引的执行代价。</p><p>不过很多程序员不喜欢使用 force index，一来这么写不优美，二来如果索引改了名字，这个语句也得改，显得很麻烦。而且如果以后迁移到别的数据库的话，这个语法还可能会不兼容。</p><p>但其实使用 force index 最主要的问题还是变更的及时性。因为选错索引的情况还是比较少出现的，所以开发的时候通常不会先写上 force index。而是等到线上出现问题的时候，你才会再去修改 SQL 语句、加上 force index。但是修改之后还要测试和发布，对于生产系统来说，这个过程不够敏捷。</p><p>第二种方法就是，我们可以考虑修改语句，引导 MySQL 使用我们期望的索引。</p><p>比如，显然把“order by b limit 1” 改成 “order by b,a limit 1” ，语义的逻辑是相同的。</p><p>之前优化器选择使用索引 b，是因为它认为使用索引 b 可以避免排序（b 本身是索引，已经是有序的了，如果选择索引 b 的话，不需要再做排序，只需要遍历），所以即使扫描行数多，也判定为代价更小。</p><p>现在 order by b,a 这种写法，要求按照 b,a 排序，就意味着使用这两个索引都需要排序。因此，扫描行数成了影响决策的主要条件，于是此时优化器选了只需要扫描更少行的索引 a。</p><p>第三种方法是，在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ol><li><p>优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句。在数据库里面，扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的 CPU 资源越少。</p></li><li><p>MySQL 在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数。MySQL 采样统计的方法。</p></li><li><p>采样统计的时候，InnoDB 默认会选择 N 个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。</p></li><li><p>对于由于索引统计信息不准确导致的问题，你可以用 analyze table t来解决。</p></li><li><p>而对于其他优化器误判的情况，你可以在应用端用 force index  （select * from t force index(a) where ….）来强行指定索引，也可以通过修改语句来引导优化器，还可以通过增加或者删除索引来绕过这个问题。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hystrix</title>
      <link href="/2021/01/23/2021-01-23-hystrix/"/>
      <url>/2021/01/23/2021-01-23-hystrix/</url>
      
        <content type="html"><![CDATA[<h2 id="Hystrix介绍"><a href="#Hystrix介绍" class="headerlink" title="Hystrix介绍"></a>Hystrix介绍</h2><h3 id="Hystrix-是什么"><a href="#Hystrix-是什么" class="headerlink" title="Hystrix 是什么"></a>Hystrix 是什么</h3><p>在分布式系统中，每个服务都可能会调用很多其他服务，被调用的那些服务就是依赖服务，有的时候某些依赖服务出现故障也是很正常的。</p><p>Hystrix 可以让我们在分布式系统中对服务间的调用进行控制，加入一些调用延迟或者依赖故障的容错机制。</p><p>Hystrix 通过将依赖服务进行资源隔离，进而阻止某个依赖服务出现故障时在整个系统所有的依赖服务调用中进行蔓延；同时 Hystrix 还提供故障时的 fallback 降级机制。</p><p>总而言之，Hystrix 通过这些方法帮助我们提升分布式系统的可用性和稳定性。</p><h3 id="Hystrix-的设计原则"><a href="#Hystrix-的设计原则" class="headerlink" title="Hystrix 的设计原则"></a>Hystrix 的设计原则</h3><ul><li>对依赖服务调用时出现的调用延迟和调用失败进行控制和容错保护。</li><li>在复杂的分布式系统中，阻止某一个依赖服务的故障在整个系统中蔓延。比如某一个服务故障了，导致其它服务也跟着故障。</li><li>提供 fail-fast（快速失败）和快速恢复的支持。</li><li>提供 fallback 优雅降级的支持。</li><li>支持近实时的监控、报警以及运维操作。</li></ul><p>举个栗子。</p><p>有这样一个分布式系统，服务 A 依赖于服务 B，服务 B 依赖于服务 C/D/E。在这样一个成熟的系统内，比如说最多可能只有 100 个线程资源。正常情况下，40 个线程并发调用服务 C，各 30 个线程并发调用 D/E。</p><p>调用服务 C，只需要 20ms，现在因为服务 C 故障了，比如延迟，或者挂了，此时线程会 hang 住 2s 左右。40 个线程全部被卡住，由于请求不断涌入，其它的线程也用来调用服务 C，同样也会被卡住。这样导致服务 B 的线程资源被耗尽，无法接收新的请求，甚至可能因为大量线程不断的运转，导致自己宕机。这种影响势必会蔓延至服务 A，导致服务 A 也跟着挂掉。</p><p><img src="/2021/01/23/2021-01-23-hystrix/service-invoke-road.png" alt></p><p>Hystrix 可以对其进行资源隔离，比如限制服务 B 只有 40 个线程调用服务 C。当此 40 个线程被 hang 住时，其它 60 个线程依然能正常调用工作。从而确保整个系统不会被拖垮。</p><h3 id="Hystrix-更加细节的设计原则"><a href="#Hystrix-更加细节的设计原则" class="headerlink" title="Hystrix 更加细节的设计原则"></a>Hystrix 更加细节的设计原则</h3><ul><li>阻止任何一个依赖服务耗尽所有的资源，比如 tomcat 中的所有线程资源。</li><li>避免请求排队和积压，采用限流和 fail fast 来控制故障。</li><li>提供 fallback 降级机制来应对故障。</li><li>使用资源隔离技术，比如 bulkhead（舱壁隔离技术）、swimlane（泳道技术）、circuit breaker（断路技术）来限制任何一个依赖服务的故障的影响。</li><li>通过近实时的统计/监控/报警功能，来提高故障发现的速度。</li><li>通过近实时的属性和配置热修改功能，来提高故障处理和恢复的速度。</li><li>保护依赖服务调用的所有故障情况，而不仅仅只是网络故障情况。</li></ul><h2 id="基于-Hystrix-线程池技术实现资源隔离"><a href="#基于-Hystrix-线程池技术实现资源隔离" class="headerlink" title="基于 Hystrix 线程池技术实现资源隔离"></a>基于 Hystrix 线程池技术实现资源隔离</h2><p>资源隔离，就是说，你如果要把对某一个依赖服务的所有调用请求，全部隔离在同一份资源池内，不会去用其它资源了，这就叫资源隔离。哪怕对这个依赖服务，比如说商品服务，现在同时发起的调用量已经到了 1000，但是分配给商品服务线程池内就 10 个线程，最多就只会用这 10 个线程去执行。不会因为对商品服务调用的延迟，将 Tomcat 内部所有的线程资源全部耗尽。</p><p>Hystrix 进行资源隔离，其实是提供了一个抽象，叫做 Command。这也是 Hystrix 最最基本的资源隔离技术。</p><h2 id="基于-Hystrix-信号量机制实现资源隔离"><a href="#基于-Hystrix-信号量机制实现资源隔离" class="headerlink" title="基于 Hystrix 信号量机制实现资源隔离"></a>基于 Hystrix 信号量机制实现资源隔离</h2><p>Hystrix 里面核心的一项功能，其实就是所谓的资源隔离，要解决的最最核心的问题，就是将多个依赖服务的调用分别隔离到各自的资源池内。避免说对某一个依赖服务的调用，因为依赖服务的接口调用的延迟或者失败，导致服务所有的线程资源全部耗费在这个服务的接口调用上。一旦说某个服务的线程资源全部耗尽的话，就可能导致服务崩溃，甚至说这种故障会不断蔓延。</p><p>Hystrix 实现资源隔离，主要有两种技术：</p><ul><li>线程池</li><li>信号量</li></ul><p>默认情况下，Hystrix 使用线程池模式。</p><h3 id="信号量机制"><a href="#信号量机制" class="headerlink" title="信号量机制"></a>信号量机制</h3><p>信号量的资源隔离只是起到一个开关的作用，比如，服务 A 的信号量大小为 10，那么就是说它同时只允许有 10 个 tomcat 线程来访问服务 A，其它的请求都会被拒绝，从而达到资源隔离和限流保护的作用。</p><p><img src="/2021/01/23/2021-01-23-hystrix/hystrix-semphore.png" alt></p><h3 id="线程池与信号量区别"><a href="#线程池与信号量区别" class="headerlink" title="线程池与信号量区别"></a>线程池与信号量区别</h3><p>线程池隔离技术，并不是说去控制类似 tomcat 这种 web 容器的线程。更加严格的意义上来说，Hystrix 的线程池隔离技术，控制的是 tomcat 线程的执行。Hystrix 线程池满后，会确保说，tomcat 的线程不会因为依赖服务的接口调用延迟或故障而被 hang 住，tomcat 其它的线程不会卡死，可以快速返回，然后支撑其它的事情。</p><p>线程池隔离技术，是用 Hystrix 自己的线程去执行调用；而信号量隔离技术，是直接让 tomcat 线程去调用依赖服务。信号量隔离，只是一道关卡，信号量有多少，就允许多少个 tomcat 线程通过它，然后去执行。</p><p><img src="/2021/01/23/2021-01-23-hystrix/hystrix-semphore-thread-pool.png" alt></p><p>适用场景：</p><ul><li>线程池技术，适合绝大多数场景，比如说我们对依赖服务的网络请求的调用和访问、需要对调用的 timeout 进行控制（捕捉 timeout 超时异常）。</li><li>信号量技术，适合说你的访问不是对外部依赖的访问，而是对内部的一些比较复杂的业务逻辑的访问，并且系统内部的代码，其实不涉及任何的网络请求，那么只要做信号量的普通限流就可以了，因为不需要去捕获 timeout 类似的问题。</li></ul><h2 id="Hystrix-隔离策略细粒度控制"><a href="#Hystrix-隔离策略细粒度控制" class="headerlink" title="Hystrix 隔离策略细粒度控制"></a>Hystrix 隔离策略细粒度控制</h2><p>线程池机制，每个 command 运行在一个线程中，限流是通过线程池的大小来控制的；信号量机制，command 是运行在调用线程中（也就是 Tomcat 的线程池），通过信号量的容量来进行限流。</p><p>如何在线程池和信号量之间做选择？</p><p><strong>默认的策略就是线程池。</strong></p><p>线程池其实最大的好处就是对于网络访问请求，如果有超时的话，可以避免调用线程阻塞住。</p><p>而使用信号量的场景，通常是针对超大并发量的场景下，每个服务实例每秒都几百的 QPS，那么此时你用线程池的话，线程一般不会太多，可能撑不住那么高的并发，如果要撑住，可能要耗费大量的线程资源，那么就是用信号量，来进行限流保护。一般用信号量常见于那种基于纯内存的一些业务逻辑服务，而不涉及到任何网络访问请求。</p><h3 id="command-key-amp-command-group"><a href="#command-key-amp-command-group" class="headerlink" title="command key &amp; command group"></a>command key &amp; command group</h3><p>我们使用线程池隔离，要怎么对依赖服务、依赖服务接口、线程池三者做划分呢？</p><p>每一个 command，都可以设置一个自己的名称 command key，同时可以设置一个自己的组 command group。</p><pre><code>private static final Setter cachedSetter = Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(&quot;ExampleGroup&quot;))                                                 .andCommandKey(HystrixCommandKey.Factory.asKey(&quot;HelloWorld&quot;));public CommandHelloWorld(String name) {    super(cachedSetter);    this.name = name;}</code></pre><p>command group 是一个非常重要的概念，默认情况下，就是通过 command group 来定义一个线程池的，而且还会通过 command group 来聚合一些监控和报警信息。同一个 command group 中的请求，都会进入同一个线程池中。</p><h3 id="command-thread-pool"><a href="#command-thread-pool" class="headerlink" title="command thread pool"></a>command thread pool</h3>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hystrix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dubbo隐式参数</title>
      <link href="/2021/01/17/2021-01-17-dubbo-yin-shi-can-shu/"/>
      <url>/2021/01/17/2021-01-17-dubbo-yin-shi-can-shu/</url>
      
        <content type="html"><![CDATA[<pre><code>import com.alibaba.dubbo.common.Constants;import com.alibaba.dubbo.common.extension.Activate;import com.alibaba.dubbo.rpc.*;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.context.i18n.LocaleContextHolder;import java.util.Locale;/** * @className: DubboContextLanguageFilter * @description: * @author: feiliang * @create: 2020-04-27 11:46 **/@Activate(group = Constants.PROVIDER)public class DubboContextLanguageFilter implements Filter {    private final Logger log = LoggerFactory.getLogger(this.getClass());    @Override    public Result invoke(Invoker&lt;?&gt; invoker, Invocation invocation) throws RpcException {        try {            // 处理你所想要的逻辑            String language = RpcContext.getContext().getAttachment(&quot;session:org.springframework.web.servlet.i18n.SessionLocaleResolver.LOCALE&quot;);            if (&quot;en&quot;.equals(language)) {                LocaleContextHolder.setDefaultLocale(new Locale(&quot;en&quot;, &quot;US&quot;));            } else {                LocaleContextHolder.setDefaultLocale(new Locale(&quot;zh&quot;, &quot;CN&quot;));            }        } catch (Throwable t) {            log.error(&quot;[DubboContextEnterFilter - invoke - language fail] 获取并设置language失败！&quot;, t);        }        return invoker.invoke(invocation);    }}</code></pre><p>在META-INF下新建一个dubbo文件夹，在dubbo文件夹内创建一个com.alibaba.dubbo.rpc.Filter文件<br>s</p><pre><code>DubboContextLanguageFilter=com.xxx.infrastructure.filter.DubboContextLanguageFilter</code></pre>]]></content>
      
      
      <categories>
          
          <category> Work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Dubbo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>普通索引和唯一索引</title>
      <link href="/2021/01/01/2021-01-01-mysql-09/"/>
      <url>/2021/01/01/2021-01-01-mysql-09/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><p>在不同的业务场景下，应该选择普通索引，还是唯一索引？</p><p>假设你在维护一个市民系统，每个人都有一个唯一的身份证号，而且业务代码已经保证了不会写入两个重复的身份证号。如果市民系统需要按照身份证号查姓名，就会执行类似这样的 SQL 语句：</p><pre><code>select name from CUser where id_card = &#39;xxxxxxxyyyyyyzzzzz&#39;;</code></pre><p>所以，你一定会考虑在 id_card 字段上建索引。</p><p>由于身份证号字段比较大，我不建议你把身份证号当做主键，那么现在你有两个选择，要么给 id_card 字段创建唯一索引，要么创建一个普通索引。如果业务代码已经保证了不会写入重复的身份证号，那么这两个选择逻辑上都是正确的。</p><p><img src="/2021/01/01/2021-01-01-mysql-09/1ed9536031d6698570ea175a7b7f9a46.png" alt="InnoDB 的索引组织结构"></p><p>接下来，我们就从这两种索引对查询语句和更新语句的性能影响来进行分析。</p><h2 id="查询过程"><a href="#查询过程" class="headerlink" title="查询过程"></a>查询过程</h2><p>假设，执行查询的语句是 select id from T where k=5。这个查询语句在索引树上查找的过程，先是通过 B+ 树从树根开始，按层搜索到叶子节点，也就是图中右下角的这个数据页，然后可以认为数据页内部通过二分法来定位记录。</p><ul><li>对于普通索引来说，查找到满足条件的第一个记录 (5,500) 后，需要查找下一个记录，直到碰到第一个不满足 k=5 条件的记录。</li><li>对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。</li></ul><p>那么，这个不同带来的性能差距会有多少呢？答案是，微乎其微。</p><p><strong>InnoDB 的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在 InnoDB 中，每个数据页的大小默认是 16KB。</strong></p><p>因为引擎是按页读写的，所以说，当找到 k=5 的记录的时候，它所在的数据页就都在内存里了。那么，对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针寻找和一次计算。</p><p>当然，如果 k=5 这个记录刚好是这个数据页的最后一个记录，那么要取下一个记录，必须读取下一个数据页，这个操作会稍微复杂一些。</p><p>但是，我们之前计算过，对于整型字段，一个数据页可以放近千个 key，因此出现这种情况的概率会很低。所以，我们计算平均性能差异时，仍可以认为这个操作成本对于现在的 CPU 来说可以忽略不计。</p><h2 id="更新过程"><a href="#更新过程" class="headerlink" title="更新过程"></a>更新过程</h2><p>为了说明普通索引和唯一索引对更新语句性能的影响这个问题，我需要先跟你介绍一下 <strong>change buffer</strong>。</p><p>当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。</p><p>需要说明的是，虽然名字叫作 change buffer，实际上它是可以持久化的数据。也就是说，change buffer 在内存中有拷贝，也会被写入到磁盘上。</p><p>将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。</p><p>显然，如果能够将更新操作先记录在 change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，提高内存利用率。</p><p>那么，什么条件下可以使用 change buffer 呢？</p><p>对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入 (4,400) 这个记录，就要先判断现在表中是否已经存在 k=4 的记录，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用 change buffer 了。</p><p>因此，<strong>唯一索引的更新就不能使用 change buffer，实际上也只有普通索引可以使用</strong>。</p><p>change buffer 用的是 buffer pool 里的内存，因此不能无限增大。change buffer 的大小，可以通过参数 innodb_change_buffer_max_size 来动态设置。这个参数设置为 50 的时候，表示 change buffer 的大小最多只能占用 buffer pool 的 50%。</p><p>现在，你已经理解了 change buffer 的机制，那么我们再一起来看看如果要在这张表中插入一个新记录 (4,400) 的话，InnoDB 的处理流程是怎样的。</p><p>第一种情况是，这个记录要更新的目标页在内存中。这时，InnoDB 的处理流程如下：</p><ul><li>对于唯一索引来说，找到 3 和 5 之间的位置，判断到没有冲突，插入这个值，语句执行结束；</li><li>对于普通索引来说，找到 3 和 5 之间的位置，插入这个值，语句执行结束。</li></ul><p>这样看来，普通索引和唯一索引对更新语句性能影响的差别，只是一个判断，只会耗费微小的 CPU 时间。</p><p>但，这不是我们关注的重点。</p><p>第二种情况是，这个记录要更新的目标页不在内存中。这时，InnoDB 的处理流程如下：</p><ul><li><p>对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束；</p></li><li><p>对于普通索引来说，则是将更新记录在 change buffer，语句执行就结束了。</p></li></ul><p>将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。</p><h2 id="change-buffer-的使用场景"><a href="#change-buffer-的使用场景" class="headerlink" title="change buffer 的使用场景"></a>change buffer 的使用场景</h2><p>通过上面的分析，你已经清楚了使用 change buffer 对更新过程的加速作用，也清楚了 change buffer 只限于用在普通索引的场景下，而不适用于唯一索引。那么，现在有一个问题就是：普通索引的所有场景，使用 change buffer 都可以起到加速作用吗？</p><p>因为 merge 的时候是真正进行数据更新的时刻，而 change buffer 的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做 merge 之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。</p><p>因此，对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。</p><p>反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。所以，对于这种业务模式来说，change buffer 反而起到了副作用。</p><h2 id="索引选择和实践"><a href="#索引选择和实践" class="headerlink" title="索引选择和实践"></a>索引选择和实践</h2><p>回到我们文章开头的问题，普通索引和唯一索引应该怎么选择。其实，这两类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响。所以，我建议你尽量选择普通索引。</p><p>如果所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭 change buffer。而在其他情况下，change buffer 都能提升更新性能。</p><p>在实际使用中，你会发现，普通索引和 change buffer 的配合使用，对于数据量大的表的更新优化还是很明显的。</p><p>特别地，在使用机械硬盘时，change buffer 这个机制的收效是非常显著的。所以，当你有一个类似“历史数据”的库，并且出于成本考虑用的是机械硬盘时，那你应该特别关注这些表里的索引，尽量使用普通索引，然后把 change buffer 尽量开大，以确保这个“历史数据”表的数据写入速度。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ol><li><p>选择普通索引还是唯一索引？</p><ul><li>对于查询过程来说：<br>a、普通索引，查到满足条件的第一个记录后，继续查找下一个记录，直到第一个不满足条件的记录<br>b、唯一索引，由于索引唯一性，查到第一个满足条件的记录后，停止检索<br>但是，两者的性能差距微乎其微。因为InnoDB根据数据页来读写的。</li><li>对于更新过程来说：<br>概念：change buffer<br>当需要更新一个数据页，如果数据页在内存中就直接更新，如果不在内存中，在不影响数据一致性的前提下，InnoDB会将这些更新操作缓存在change buffer中。下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中的与这个页有关的操作。<br>change buffer是可以持久化的数据。在内存中有拷贝，也会被写入到磁盘上。<br>merge:将change buffer中的操作应用到原数据页上，得到最新结果的过程，访问这个数据页会触发merge，系统有后台线程定期merge，在数据库正常关闭的过程中，也会执行merge。<br>唯一索引的更新不能使用change buffer。对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，更新直接修改内存。<br>change buffer用的是buffer pool里的内存，change buffer的大小，可以通过参数innodb_change_buffer_max_size来动态设置。这个参数设置为50的时候，表示change buffer的大小最多只能占用buffer pool的50%。<br>将数据从磁盘读入内存涉及随机IO的访问，是数据库里面成本最高的操作之一。<br>change buffer 因为减少了随机磁盘访问，所以对更新性能的提升很明显。</li></ul></li><li><p>change buffer使用场景<br> 在一个数据页做merge之前，change buffer记录的变更越多，收益就越大。<br> 对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。<br> 反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在change buffer,但之后由于马上要访问这个数据页，会立即触发merge过程。这样随机访问IO的次数不会减少，反而增加了change buffer的维护代价。所以，对于这种业务模式来说，change buffer反而起到了副作用。</p></li><li><p>索引的选择和实践<br> 尽可能使用普通索引。<br> redo log主要节省的是随机写磁盘的IO消耗(转成顺序写)，而change buffer主要节省的则是随机读磁盘的IO消耗。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>两级缓存</title>
      <link href="/2020/11/11/2020-11-11-liang-ji-huan-cun/"/>
      <url>/2020/11/11/2020-11-11-liang-ji-huan-cun/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本篇主要记录和总结工作中用到的两级缓存实现</p></blockquote><h2 id="二级缓存"><a href="#二级缓存" class="headerlink" title="二级缓存"></a>二级缓存</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>将数据读取出来放到内存里，这样当需要获取数据时，就能够直接从内存中拿到数据返回，能够很大程度的提高速度。但是一般Redis是单独部署成集群，所以会有网络IO上的消耗，虽然与Redis集群的链接已经有连接池这种工具，但是数据传输上也还是会有一定消耗。所以就有了应用内缓存，如：Caffeine。当应用内缓存有符合条件的数据时，就可以直接使用，而不用通过网络到Redis中去获取，这样就形成了两级缓存。应用内缓存（如Caffeine）叫做一级缓存，远程缓存（如Redis）叫做二级缓存。</p><h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><p>本文档主要结合于Spring Boot+Spring Cache实现两级缓存(Redis+Caffeine)。</p><h3 id="逻辑"><a href="#逻辑" class="headerlink" title="逻辑"></a>逻辑</h3><p>业务获取数据，从一级缓存获取数据判断是否获取成功，失败则进入二级缓存获取数据，获取数据失败，进入查询数据，返回数据后，更新到二级缓存，在更新到一级缓存，最后响应业务。</p><p><img src="/2020/11/11/2020-11-11-liang-ji-huan-cun/%E4%BA%8C%E7%BA%A7%E7%BC%93%E5%AD%98.png" alt></p><h3 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h3><h4 id="引入依赖"><a href="#引入依赖" class="headerlink" title="引入依赖"></a>引入依赖</h4><p>pom.xml中加入以下starter依赖，cache-redis-caffeine-spring-boot-starter为开发好的二级缓存组件</p><pre class=" language-java"><code class="language-java"><span class="token operator">&lt;</span>dependency<span class="token operator">></span>  <span class="token operator">&lt;</span>groupId<span class="token operator">></span>com<span class="token punctuation">.</span>demo<span class="token operator">&lt;</span><span class="token operator">/</span>groupId<span class="token operator">></span>  <span class="token operator">&lt;</span>artifactId<span class="token operator">></span>cache<span class="token operator">-</span>redis<span class="token operator">-</span>caffeine<span class="token operator">-</span>spring<span class="token operator">-</span>boot<span class="token operator">-</span>starter<span class="token operator">&lt;</span><span class="token operator">/</span>artifactId<span class="token operator">></span>  <span class="token operator">&lt;</span>version<span class="token operator">></span><span class="token number">1.0</span><span class="token punctuation">.</span><span class="token number">0</span><span class="token operator">-</span>SNAPSHOT<span class="token operator">&lt;</span><span class="token operator">/</span>version<span class="token operator">></span><span class="token operator">&lt;</span><span class="token operator">/</span>dependency<span class="token operator">></span></code></pre><h4 id="开启缓存支持"><a href="#开启缓存支持" class="headerlink" title="开启缓存支持"></a>开启缓存支持</h4><p>@EnableCaching：启用spring cache缓存，作为总的开关，在spring boot的启动类或配置类上需要加上此注解才会生效</p><pre><code>@EnableCaching@SpringBootApplicationpublic class FrameworkBoot {    public static void main(String[] args) {        new SpringApplicationBuilder(FrameworkBoot.class)                .web(WebApplicationType.SERVLET)                .run(args);    }}</code></pre><h4 id="设置缓存配置"><a href="#设置缓存配置" class="headerlink" title="设置缓存配置"></a>设置缓存配置</h4><pre><code>spring:  #redis配置,默认单机配置  redis:    host: 127.0.0.1    #    password:    port: 6379    #    cluster:    ##    配置集群    ##      nodes: 127.0.0.1:6379,127.0.0.1:6380,127.0.0.1:6381    #      nodes:    #        - 127.0.0.1:6379    #        - 127.0.0.1:6380    #        - 127.0.0.1:6381    #连接池配置    jedis:      pool:        #连接池最大连接数        max-active: 1000        #连接池最大阻塞等待时间(使用负值没有限制)        max-wait: -1        #连接池中的最大空闲连接        max-idle: 10        #连接池中的最小空闲连接        min-idle: 5  cache:    #开启缓存    enabled: true    #redis模式，true默认单机    stand-alone: true    #开启二级缓存    multistage-enabled: true    #缓存key前缀配置    cache-prefix: cachePrefix    #是否存储空值，默认true，防止缓存穿透    cache-null-values: true    caffeine:      #统一配置L1缓存初始化容量      initial-capacity: 10      #统一配置L2缓存最大容量      maximum-size: 1000      #全局配置写入后的过期时间      expire-after-write: 3000000    redis:      #优先级最高过期时间      expires:        #单个key过期时间        user: 6000        test: 60000      #全局配置过期时间      default-expiration: 3000000</code></pre><h4 id="常用注解"><a href="#常用注解" class="headerlink" title="常用注解"></a>常用注解</h4><ul><li>@Cacheable：主要应用到查询数据的方法上；</li></ul><pre><code>@Cacheable(value = &quot;user&quot;, key = &quot;#id&quot;,condition = &quot;#result != null&quot;)public Object findById(String id){    return null;}</code></pre><ul><li>@CacheEvict：清除缓存，主要应用到删除数据的方法上；</li></ul><pre><code>@CacheEvict(value = &quot;user&quot;, key = &quot;#id&quot;)public void delete(String id){    taskLogMapper.deleteById(id);}</code></pre><ul><li>@CachePut：放入缓存，主要用到对数据有更新的方法上；</li></ul><pre><code> @CachePut(value = &quot;user&quot;, key = &quot;#tasklog.id&quot;) public Tasklog update(Tasklog tasklog){     taskLogMapper.update(tasklog);     return tasklog; }</code></pre><ul><li>@Caching：用于在一个方法上配置多种注解。</li></ul><pre><code>@Caching(put = {@CachePut(value = &quot;user&quot;, key = &quot;#user.id&quot;),@CachePut(value = &quot;user&quot;, key = &quot;#user.username&quot;),@CachePut(value = &quot;user&quot;, key = &quot;#user.email&quot;)})public User save(User user) {}</code></pre><h4 id="自定义缓存注解"><a href="#自定义缓存注解" class="headerlink" title="自定义缓存注解"></a>自定义缓存注解</h4><p>比如之前的那个@Caching组合，会让方法上的注解显得整个代码比较乱，此时可以使用自定义注解把这些注解组合到一个注解中</p><pre><code>@Caching(put = {    @CachePut(value = &quot;user&quot;, key = &quot;#user.id&quot;),    @CachePut(value = &quot;user&quot;, key = &quot;#user.username&quot;),    @CachePut(value = &quot;user&quot;, key = &quot;#user.email&quot;)})@Target({ElementType.METHOD, ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Inheritedpublic @interface UserSaveCache {}</code></pre><h4 id="注解属性"><a href="#注解属性" class="headerlink" title="注解属性"></a>注解属性</h4><blockquote><ul><li><code>cacheNames/value</code> ：用来指定缓存组件的名字</li><li><code>key</code> ：缓存数据时使用的 key，可以用它来指定。默认是使用方法参数的值。（这个 key 你可以使用 spEL 表达式来编写）</li><li><code>keyGenerator</code> ：key 的生成器。 key 和 keyGenerator 二选一使用</li><li><code>cacheManager</code> ：可以用来指定缓存管理器。从哪个缓存管理器里面获取缓存。</li><li><code>condition</code> ：可以用来指定符合条件的情况下才缓存</li><li><code>unless</code> ：否定缓存。当 unless 指定的条件为 true ，方法的返回值就不会被缓存。当然你也可以获取到结果进行判断。（通过 <code>#result</code> 获取方法结果）</li><li><code>sync</code> ：是否使用异步模式。</li></ul></blockquote><h4 id="属性运用"><a href="#属性运用" class="headerlink" title="属性运用"></a>属性运用</h4><h5 id="cacheNames"><a href="#cacheNames" class="headerlink" title="cacheNames"></a>cacheNames</h5><p>用来指定缓存组件的名字，将方法的返回结果放在哪个缓存中，可以是数组的方式，支持指定多个缓存。</p><p><img src="/2020/11/11/2020-11-11-liang-ji-huan-cun/785f265003eb730802290ea1411d79cc.png" alt></p><h5 id="key"><a href="#key" class="headerlink" title="key"></a>key</h5><p>缓存数据时使用的 key。默认使用的是方法参数的值。可以使用 spEL 表达式去编写。</p><p><img src="/2020/11/11/2020-11-11-liang-ji-huan-cun/42ec5ae45e41ccc0563f0619e348b022.png" alt></p><h5 id="keyGenerator"><a href="#keyGenerator" class="headerlink" title="keyGenerator"></a>keyGenerator</h5><p>key 的生成器，可以自己指定 key 的生成器，通过这个生成器来生成 key。</p><p><img src="/2020/11/11/2020-11-11-liang-ji-huan-cun/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20220123184344.png" alt></p><p><img src="/2020/11/11/2020-11-11-liang-ji-huan-cun/b0e751d61ac847c47cd5bc27970cbda3.png" alt></p><h5 id="condition"><a href="#condition" class="headerlink" title="condition"></a>condition</h5><p>符合条件的情况下才缓存。方法返回的数据要不要缓存，可以做一个动态判断。</p><p><img src="/2020/11/11/2020-11-11-liang-ji-huan-cun/06942c3052714c303f0af1a6e26d8aeb.png" alt></p><h5 id="unless"><a href="#unless" class="headerlink" title="unless"></a>unless</h5><p>否定缓存。当 unless 指定的条件为 true ，方法的返回值就不会被缓存。</p><p><img src="/2020/11/11/2020-11-11-liang-ji-huan-cun/1f962ae5f31510cbd79341b8dee77cea.png" alt></p><h5 id="sync"><a href="#sync" class="headerlink" title="sync"></a>sync</h5><p>是否使用异步模式。默认是方法执行完，以同步的方式将方法返回的结果存在缓存中。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>spring  cache是spring-context包中提供的基于注解方式使用的缓存组件，定义了一些标准接口，通过实现这些接口，就可以通过在方法上增加注解来实现缓存。这样就能够避免缓存代码与业务处理耦合在一起的问题。spring cache的实现是使用spring aop中对方法切面（MethodInterceptor）封装的扩展。</p><p>spring cache核心的接口就两个：<strong>Cache</strong>和<strong>CacheManager</strong>。</p><h4 id="Cache接口"><a href="#Cache接口" class="headerlink" title="Cache接口"></a>Cache接口</h4><p>提供缓存的具体操作，比如缓存的放入、读取、清理，spring框架中默认提供的实现有：</p><p><img src="/2020/11/11/2020-11-11-liang-ji-huan-cun/2018227134610317.png" alt></p><pre><code>#Cache.javapackage org.springframework.cache;import java.util.concurrent.Callable;public interface Cache { // cacheName，缓存的名字，默认实现中一般是CacheManager创建Cache的bean时传入cacheName String getName(); // 获取实际使用的缓存，如：RedisTemplate、com.github.benmanes.caffeine.cache.Cache&lt;Object, Object&gt;。暂时没发现实际用处，可能只是提供获取原生缓存的bean，以便需要扩展一些缓存操作或统计之类的东西 Object getNativeCache(); // 通过key获取缓存值，注意返回的是ValueWrapper，为了兼容存储空值的情况，将返回值包装了一层，通过get方法获取实际值 ValueWrapper get(Object key); // 通过key获取缓存值，返回的是实际值，即方法的返回值类型 &lt;T&gt; T get(Object key, Class&lt;T&gt; type); // 通过key获取缓存值，可以使用valueLoader.call()来调使用@Cacheable注解的方法。当@Cacheable注解的sync属性配置为true时使用此方法。因此方法内需要保证回源到数据库的同步性。避免在缓存失效时大量请求回源到数据库 &lt;T&gt; T get(Object key, Callable&lt;T&gt; valueLoader); // 将@Cacheable注解方法返回的数据放入缓存中 void put(Object key, Object value); // 当缓存中不存在key时才放入缓存。返回值是当key存在时原有的数据 ValueWrapper putIfAbsent(Object key, Object value); // 删除缓存 void evict(Object key); // 删除缓存中的所有数据。需要注意的是，具体实现中只删除使用@Cacheable注解缓存的所有数据，不要影响应用内的其他缓存 void clear(); // 缓存返回值的包装 interface ValueWrapper { // 返回实际缓存的对象 Object get(); } // 当{@link #get(Object, Callable)}抛出异常时，会包装成此异常抛出 @SuppressWarnings(&quot;serial&quot;) class ValueRetrievalException extends RuntimeException { private final Object key; public ValueRetrievalException(Object key, Callable&lt;?&gt; loader, Throwable ex) {  super(String.format(&quot;Value for key &#39;%s&#39; could not be loaded using &#39;%s&#39;&quot;, key, loader), ex);  this.key = key; } public Object getKey() {  return this.key; } }}</code></pre><h4 id="CacheManager接口"><a href="#CacheManager接口" class="headerlink" title="CacheManager接口"></a>CacheManager接口</h4><p>主要提供Cache实现bean的创建，每个应用里可以通过cacheName来对Cache进行隔离，每个cacheName对应一个Cache实现。spring框架中默认提供的实现与Cache的实现都是成对出现</p><pre><code>#CacheManager.javapackage org.springframework.cache;import java.util.Collection;public interface CacheManager { // 通过cacheName创建Cache的实现bean，具体实现中需要存储已创建的Cache实现bean，避免重复创建，也避免内存缓存对象（如Caffeine）重新创建后原来缓存内容丢失的情况 Cache getCache(String name); // 返回所有的cacheName Collection&lt;String&gt; getCacheNames();}</code></pre><h3 id="自定义实现"><a href="#自定义实现" class="headerlink" title="自定义实现"></a>自定义实现</h3><h4 id="引入依赖-1"><a href="#引入依赖-1" class="headerlink" title="引入依赖"></a>引入依赖</h4><pre><code>&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt;    &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-autoconfigure&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.geronimo.bundles&lt;/groupId&gt;    &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt;    &lt;version&gt;1.6.8_2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.aspectj&lt;/groupId&gt;    &lt;artifactId&gt;aspectjrt&lt;/artifactId&gt;    &lt;version&gt;1.9.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;com.alibaba&lt;/groupId&gt;    &lt;artifactId&gt;fastjson&lt;/artifactId&gt;    &lt;version&gt;1.2.78&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;    &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;com.github.ben-manes.caffeine&lt;/groupId&gt;    &lt;artifactId&gt;caffeine&lt;/artifactId&gt;&lt;/dependency&gt;</code></pre><h4 id="定义properties配置属性类"><a href="#定义properties配置属性类" class="headerlink" title="定义properties配置属性类"></a>定义properties配置属性类</h4><pre><code>package com.cache.config;import org.springframework.boot.context.properties.ConfigurationProperties;import java.util.HashMap;import java.util.HashSet;import java.util.Map;import java.util.Set;/** * Description: * &lt;br/&gt; * CacheRedisCaffeineProperties * * @author feiliang */@ConfigurationProperties(prefix = CacheRedisCaffeineProperties.CACHE_PREFIX)public class CacheRedisCaffeineProperties {    /**     * 配置前缀     */    public static final String CACHE_PREFIX = &quot;spring.cache&quot;;    private Set&lt;String&gt; cacheNames = new HashSet&lt;&gt;();    /**     * 是否存储空值，默认true，防止缓存穿透     */    private boolean cacheNullValues = true;    /**     * 是否动态根据cacheName创建Cache的实现，默认true     */    private boolean dynamic = true;    /**     * 缓存key的前缀     */    private String cachePrefix;    private Redis redis = new Redis();    private Caffeine caffeine = new Caffeine();    public class Redis {        /**         * 全局过期时间，单位毫秒，默认不过期         */        private long defaultExpiration = 0;        /**         * 每个cacheName的过期时间，单位毫秒，优先级比defaultExpiration高         */        private Map&lt;String, Long&gt; expires = new HashMap&lt;&gt;();        /**         * 缓存更新时通知其他节点的topic名称         */        private String topic = &quot;cache:redis:caffeine:topic&quot;;        public long getDefaultExpiration() {            return defaultExpiration;        }        public void setDefaultExpiration(long defaultExpiration) {            this.defaultExpiration = defaultExpiration;        }        public Map&lt;String, Long&gt; getExpires() {            return expires;        }        public void setExpires(Map&lt;String, Long&gt; expires) {            this.expires = expires;        }        public String getTopic() {            return topic;        }        public void setTopic(String topic) {            this.topic = topic;        }    }    public class Caffeine {        /**         * 访问后过期时间，单位毫秒         */        private long expireAfterAccess;        /**         * 写入后过期时间，单位毫秒         */        private long expireAfterWrite;        /**         * 写入后刷新时间，单位毫秒         */        private long refreshAfterWrite;        /**         * 初始化大小         */        private int initialCapacity;        /**         * 最大缓存对象个数，超过此数量时之前放入的缓存将失效         */        private long maximumSize;        /**         * 由于权重需要缓存对象来提供，对于使用spring cache这种场景不是很适合，所以暂不支持配置         */        // private long maximumWeight;        public long getExpireAfterAccess() {            return expireAfterAccess;        }        public void setExpireAfterAccess(long expireAfterAccess) {            this.expireAfterAccess = expireAfterAccess;        }        public long getExpireAfterWrite() {            return expireAfterWrite;        }        public void setExpireAfterWrite(long expireAfterWrite) {            this.expireAfterWrite = expireAfterWrite;        }        public long getRefreshAfterWrite() {            return refreshAfterWrite;        }        public void setRefreshAfterWrite(long refreshAfterWrite) {            this.refreshAfterWrite = refreshAfterWrite;        }        public int getInitialCapacity() {            return initialCapacity;        }        public void setInitialCapacity(int initialCapacity) {            this.initialCapacity = initialCapacity;        }        public long getMaximumSize() {            return maximumSize;        }        public void setMaximumSize(long maximumSize) {            this.maximumSize = maximumSize;        }    }    public Set&lt;String&gt; getCacheNames() {        return cacheNames;    }    public void setCacheNames(Set&lt;String&gt; cacheNames) {        this.cacheNames = cacheNames;    }    public boolean isCacheNullValues() {        return cacheNullValues;    }    public void setCacheNullValues(boolean cacheNullValues) {        this.cacheNullValues = cacheNullValues;    }    public boolean isDynamic() {        return dynamic;    }    public void setDynamic(boolean dynamic) {        this.dynamic = dynamic;    }    public String getCachePrefix() {        return cachePrefix;    }    public void setCachePrefix(String cachePrefix) {        this.cachePrefix = cachePrefix;    }    public Redis getRedis() {        return redis;    }    public void setRedis(Redis redis) {        this.redis = redis;    }    public Caffeine getCaffeine() {        return caffeine;    }    public void setCaffeine(Caffeine caffeine) {        this.caffeine = caffeine;    }}</code></pre><h4 id="实现Cache接口"><a href="#实现Cache接口" class="headerlink" title="实现Cache接口"></a>实现Cache接口</h4><p>spring cache中有实现Cache接口的一个抽象类AbstractValueAdaptingCache，包含了空值的包装和缓存值的包装，所以就不用实现Cache接口了，直接实现AbstractValueAdaptingCache抽象类</p><pre><code>package com.cache.support;import java.lang.reflect.Constructor;import java.util.Map;import java.util.Set;import java.util.concurrent.Callable;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.ReentrantLock;import com.cache.config.CacheRedisCaffeineProperties;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.cache.support.AbstractValueAdaptingCache;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.util.StringUtils;import com.github.benmanes.caffeine.cache.Cache;/** * Description: * &lt;br/&gt; * RedisCaffeineCache * * @author feiliang */public class RedisCaffeineCache extends AbstractValueAdaptingCache {    private final Logger logger = LoggerFactory.getLogger(RedisCaffeineCache.class);    private String name;    private RedisTemplate&lt;Object, Object&gt; redisTemplate;    private Cache&lt;Object, Object&gt; caffeineCache;    private String cachePrefix;    private long defaultExpiration = 0;    private Map&lt;String, Long&gt; expires;    private String topic = &quot;cache:redis:caffeine:topic&quot;;    protected RedisCaffeineCache(boolean allowNullValues) {        super(allowNullValues);    }    public RedisCaffeineCache(String name, RedisTemplate&lt;Object, Object&gt; redisTemplate, Cache&lt;Object, Object&gt; caffeineCache, CacheRedisCaffeineProperties cacheRedisCaffeineProperties) {        super(cacheRedisCaffeineProperties.isCacheNullValues());        this.name = name;        this.redisTemplate = redisTemplate;        this.caffeineCache = caffeineCache;        this.cachePrefix = cacheRedisCaffeineProperties.getCachePrefix();        this.defaultExpiration = cacheRedisCaffeineProperties.getRedis().getDefaultExpiration();        this.expires = cacheRedisCaffeineProperties.getRedis().getExpires();        this.topic = cacheRedisCaffeineProperties.getRedis().getTopic();    }    @Override    public String getName() {        return this.name;    }    @Override    public Object getNativeCache() {        return this;    }    @SuppressWarnings(&quot;unchecked&quot;)    @Override    public &lt;T&gt; T get(Object key, Callable&lt;T&gt; valueLoader) {        Object value = lookup(key);        if (value != null) {            return (T) value;        }        ReentrantLock lock = new ReentrantLock();        try {            lock.lock();            value = lookup(key);            if (value != null) {                return (T) value;            }            value = valueLoader.call();            Object storeValue = toStoreValue(valueLoader.call());            put(key, storeValue);            return (T) value;        } catch (Exception e) {            try {                Class&lt;?&gt; c = Class.forName(&quot;org.springframework.cache.Cache$ValueRetrievalException&quot;);                Constructor&lt;?&gt; constructor = c.getConstructor(Object.class, Callable.class, Throwable.class);                RuntimeException exception = (RuntimeException) constructor.newInstance(key, valueLoader, e.getCause());                throw exception;            } catch (Exception e1) {                throw new IllegalStateException(e1);            }        } finally {            lock.unlock();        }    }    @Override    public void put(Object key, Object value) {        if (!super.isAllowNullValues() &amp;&amp; value == null) {            this.evict(key);            return;        }        long expire = getExpire();        if (expire &gt; 0) {            redisTemplate.opsForValue().set(getKey(key), toStoreValue(value), expire, TimeUnit.MILLISECONDS);        } else {            redisTemplate.opsForValue().set(getKey(key), toStoreValue(value));        }        push(new CacheMessage(this.name, key));        caffeineCache.put(key, value);    }    @Override    public ValueWrapper putIfAbsent(Object key, Object value) {        Object cacheKey = getKey(key);        Object prevValue = null;        // 考虑使用分布式锁，或者将redis的setIfAbsent改为原子性操作        synchronized (key) {            prevValue = redisTemplate.opsForValue().get(cacheKey);            if (prevValue == null) {                long expire = getExpire();                if (expire &gt; 0) {                    redisTemplate.opsForValue().set(getKey(key), toStoreValue(value), expire, TimeUnit.MILLISECONDS);                } else {                    redisTemplate.opsForValue().set(getKey(key), toStoreValue(value));                }                push(new CacheMessage(this.name, key));                caffeineCache.put(key, toStoreValue(value));            }        }        return toValueWrapper(prevValue);    }    @Override    public void evict(Object key) {        // 先清除redis中缓存数据，然后清除caffeine中的缓存，避免短时间内如果先清除caffeine缓存后其他请求会再从redis里加载到caffeine中        redisTemplate.delete(getKey(key));        push(new CacheMessage(this.name, key));        caffeineCache.invalidate(key);    }    @Override    public void clear() {        // 先清除redis中缓存数据，然后清除caffeine中的缓存，避免短时间内如果先清除caffeine缓存后其他请求会再从redis里加载到caffeine中        Set&lt;Object&gt; keys = redisTemplate.keys(this.name.concat(&quot;:&quot;));        for (Object key : keys) {            redisTemplate.delete(key);        }        push(new CacheMessage(this.name, null));        caffeineCache.invalidateAll();    }    @Override    protected Object lookup(Object key) {        Object cacheKey = getKey(key);        Object value = caffeineCache.getIfPresent(key);        if (value != null) {            logger.debug(&quot;get cache from caffeine, the key is : {}&quot;, cacheKey);            return value;        }        value = redisTemplate.opsForValue().get(cacheKey);        if (value != null) {            logger.debug(&quot;get cache from redis and put in caffeine, the key is : {}&quot;, cacheKey);            caffeineCache.put(key, value);        }        return value;    }    private Object getKey(Object key) {        return StringUtils.isEmpty(cachePrefix) ? this.name.concat(&quot;:&quot;).concat(key.toString()) : cachePrefix.concat(&quot;:&quot;).concat(this.name).concat(&quot;:&quot;).concat(key.toString());    }    private long getExpire() {        long expire = defaultExpiration;        Long cacheNameExpire = expires.get(this.name);        return cacheNameExpire == null ? expire : cacheNameExpire.longValue();    }    /**     * @param message     * @description 缓存变更时通知其他节点清理本地缓存     */    private void push(CacheMessage message) {        redisTemplate.convertAndSend(topic, message);    }    /**     * @param key     * @description 清理本地缓存     */    public void clearLocal(Object key) {        logger.debug(&quot;clear local cache, the key is : {}&quot;, key);        if (key == null) {            caffeineCache.invalidateAll();        } else {            caffeineCache.invalidate(key);        }    }}</code></pre><h4 id="实现CacheManager接口"><a href="#实现CacheManager接口" class="headerlink" title="实现CacheManager接口"></a>实现CacheManager接口</h4><pre><code>package com.cache.support;import java.util.Collection;import java.util.Set;import java.util.concurrent.ConcurrentHashMap;import java.util.concurrent.ConcurrentMap;import java.util.concurrent.TimeUnit;import com.cache.config.CacheRedisCaffeineProperties;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.cache.Cache;import org.springframework.cache.CacheManager;import org.springframework.data.redis.core.RedisTemplate;import com.github.benmanes.caffeine.cache.Caffeine;/** * Description: * &lt;br/&gt; * RedisCaffeineCacheManager * * @author feiliang */public class RedisCaffeineCacheManager implements CacheManager {    private final Logger logger = LoggerFactory.getLogger(RedisCaffeineCacheManager.class);    private ConcurrentMap&lt;String, Cache&gt; cacheMap = new ConcurrentHashMap();    private CacheRedisCaffeineProperties cacheRedisCaffeineProperties;    private RedisTemplate&lt;Object, Object&gt; redisTemplate;    private boolean dynamic = true;    private Set&lt;String&gt; cacheNames;    public RedisCaffeineCacheManager(CacheRedisCaffeineProperties cacheRedisCaffeineProperties,                                     RedisTemplate&lt;Object, Object&gt; redisTemplate) {        super();        this.cacheRedisCaffeineProperties = cacheRedisCaffeineProperties;        this.redisTemplate = redisTemplate;        this.dynamic = cacheRedisCaffeineProperties.isDynamic();        this.cacheNames = cacheRedisCaffeineProperties.getCacheNames();    }    @Override    public Cache getCache(String name) {        Cache cache = cacheMap.get(name);        if(cache != null) {            return cache;        }        if(!dynamic &amp;&amp; !cacheNames.contains(name)) {            return cache;        }        cache = new RedisCaffeineCache(name, redisTemplate, caffeineCache(), cacheRedisCaffeineProperties);        Cache oldCache = cacheMap.putIfAbsent(name, cache);        logger.debug(&quot;create cache instance, the cache name is : {}&quot;, name);        return oldCache == null ? cache : oldCache;    }    public com.github.benmanes.caffeine.cache.Cache&lt;Object, Object&gt; caffeineCache(){        Caffeine&lt;Object, Object&gt; cacheBuilder = Caffeine.newBuilder();        if(cacheRedisCaffeineProperties.getCaffeine().getExpireAfterAccess() &gt; 0) {            cacheBuilder.expireAfterAccess(cacheRedisCaffeineProperties.getCaffeine().getExpireAfterAccess(), TimeUnit.MILLISECONDS);        }        if(cacheRedisCaffeineProperties.getCaffeine().getExpireAfterWrite() &gt; 0) {            cacheBuilder.expireAfterWrite(cacheRedisCaffeineProperties.getCaffeine().getExpireAfterWrite(), TimeUnit.MILLISECONDS);        }        if(cacheRedisCaffeineProperties.getCaffeine().getInitialCapacity() &gt; 0) {            cacheBuilder.initialCapacity(cacheRedisCaffeineProperties.getCaffeine().getInitialCapacity());        }        if(cacheRedisCaffeineProperties.getCaffeine().getMaximumSize() &gt; 0) {            cacheBuilder.maximumSize(cacheRedisCaffeineProperties.getCaffeine().getMaximumSize());        }        if(cacheRedisCaffeineProperties.getCaffeine().getRefreshAfterWrite() &gt; 0) {            cacheBuilder.refreshAfterWrite(cacheRedisCaffeineProperties.getCaffeine().getRefreshAfterWrite(), TimeUnit.MILLISECONDS);        }        return cacheBuilder.build();    }    @Override    public Collection&lt;String&gt; getCacheNames() {        return this.cacheNames;    }    public void clearLocal(String cacheName, Object key) {        Cache cache = cacheMap.get(cacheName);        if(cache == null) {            return ;        }        RedisCaffeineCache redisCaffeineCache = (RedisCaffeineCache) cache;        redisCaffeineCache.clearLocal(key);    }}</code></pre><h4 id="redis消息发布-订阅，传输的消息类"><a href="#redis消息发布-订阅，传输的消息类" class="headerlink" title="redis消息发布/订阅，传输的消息类"></a>redis消息发布/订阅，传输的消息类</h4><pre><code>package com.cache.support;import java.io.Serializable;/** * Description: * &lt;br/&gt; * CacheMessage * * @author feiliang */public class CacheMessage implements Serializable {    private static final long serialVersionUID = 5987219310442078193L;    private String cacheName;    private Object key;    public CacheMessage(String cacheName, Object key) {        super();        this.cacheName = cacheName;        this.key = key;    }    public String getCacheName() {        return cacheName;    }    public void setCacheName(String cacheName) {        this.cacheName = cacheName;    }    public Object getKey() {        return key;    }    public void setKey(Object key) {        this.key = key;    }}</code></pre><h4 id="监听redis消息需要实现MessageListener接口"><a href="#监听redis消息需要实现MessageListener接口" class="headerlink" title="监听redis消息需要实现MessageListener接口"></a>监听redis消息需要实现MessageListener接口</h4><pre><code>package com.cache.support;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.data.redis.connection.Message;import org.springframework.data.redis.connection.MessageListener;import org.springframework.data.redis.core.RedisTemplate;/** * Description: * &lt;br/&gt; * CacheMessageListener * * @author feiliang */public class CacheMessageListener implements MessageListener {    private final Logger logger = LoggerFactory.getLogger(CacheMessageListener.class);    private RedisTemplate&lt;Object, Object&gt; redisTemplate;    private RedisCaffeineCacheManager redisCaffeineCacheManager;    public CacheMessageListener(RedisTemplate&lt;Object, Object&gt; redisTemplate,                                RedisCaffeineCacheManager redisCaffeineCacheManager) {        super();        this.redisTemplate = redisTemplate;        this.redisCaffeineCacheManager = redisCaffeineCacheManager;    }    @Override    public void onMessage(Message message, byte[] pattern) {        CacheMessage cacheMessage = (CacheMessage) redisTemplate.getValueSerializer().deserialize(message.getBody());        logger.debug(&quot;recevice a redis topic message, clear local cache, the cacheName is {}, the key is {}&quot;, cacheMessage.getCacheName(), cacheMessage.getKey());        redisCaffeineCacheManager.clearLocal(cacheMessage.getCacheName(), cacheMessage.getKey());    }}</code></pre><h4 id="增加spring-boot配置类"><a href="#增加spring-boot配置类" class="headerlink" title="增加spring boot配置类"></a>增加spring boot配置类</h4><pre><code>package com.cache.config;import com.fasterxml.jackson.annotation.JsonAutoDetect;import com.fasterxml.jackson.annotation.PropertyAccessor;import com.fasterxml.jackson.databind.ObjectMapper;import com.fasterxml.jackson.databind.jsontype.impl.LaissezFaireSubTypeValidator;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.redis.connection.RedisConnectionFactory;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.data.redis.serializer.Jackson2JsonRedisSerializer;import org.springframework.data.redis.serializer.StringRedisSerializer;/** * Description: * &lt;br/&gt; * RedisAutoConfig * * @author feiliang */@Configurationpublic class RedisAutoConfig {    @Bean    public RedisTemplate&lt;Object, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) {        RedisTemplate&lt;Object, Object&gt; redisTemplate = new RedisTemplate&lt;&gt;();        redisTemplate.setConnectionFactory(redisConnectionFactory);        // 用Jackson2JsonRedisSerializer来序列化和反序列化redis的value值        StringRedisSerializer stringRedisSerializer = new StringRedisSerializer();        // key        redisTemplate.setKeySerializer(stringRedisSerializer);        Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class);        ObjectMapper objectMapper = new ObjectMapper();        // 指定要序列化的域(field,get,set)，访问修饰符(public,private,protected)        objectMapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY);        objectMapper.activateDefaultTyping(LaissezFaireSubTypeValidator.instance, ObjectMapper.DefaultTyping.NON_FINAL);        jackson2JsonRedisSerializer.setObjectMapper(objectMapper);        //value        redisTemplate.setValueSerializer(jackson2JsonRedisSerializer);        redisTemplate.setHashKeySerializer(stringRedisSerializer);        redisTemplate.setHashValueSerializer(jackson2JsonRedisSerializer);        redisTemplate.afterPropertiesSet();        return redisTemplate;    }}</code></pre><pre><code>package com.cache.config;import com.cache.support.CacheMessageListener;import com.cache.support.RedisCaffeineCacheManager;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.autoconfigure.AutoConfigureAfter;import org.springframework.boot.autoconfigure.AutoConfigureBefore;import org.springframework.boot.autoconfigure.condition.ConditionalOnBean;import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;import org.springframework.boot.autoconfigure.data.redis.RedisAutoConfiguration;import org.springframework.boot.context.properties.EnableConfigurationProperties;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.data.redis.listener.ChannelTopic;import org.springframework.data.redis.listener.RedisMessageListenerContainer;/** * Description: * &lt;br/&gt; * CacheRedisCaffeineAutoConfiguration * * @author feiliang */@Configuration@AutoConfigureBefore(RedisAutoConfiguration.class)@AutoConfigureAfter(RedisAutoConfig.class)@EnableConfigurationProperties(CacheRedisCaffeineProperties.class)@ConditionalOnProperty(prefix = CacheRedisCaffeineProperties.CACHE_PREFIX, name = &quot;multistage-enabled&quot;, havingValue = &quot;true&quot;)public class CacheRedisCaffeineAutoConfiguration {    @Autowired    private CacheRedisCaffeineProperties cacheRedisCaffeineProperties;    @Bean    @ConditionalOnBean(RedisTemplate.class)    public RedisCaffeineCacheManager cacheManager(RedisTemplate&lt;Object, Object&gt; redisTemplate) {        return new RedisCaffeineCacheManager(cacheRedisCaffeineProperties, redisTemplate);    }    @Bean    public RedisMessageListenerContainer redisMessageListenerContainer(RedisTemplate&lt;Object, Object&gt; redisTemplate,                                                                       RedisCaffeineCacheManager redisCaffeineCacheManager) {        RedisMessageListenerContainer redisMessageListenerContainer = new RedisMessageListenerContainer();        redisMessageListenerContainer.setConnectionFactory(redisTemplate.getConnectionFactory());        CacheMessageListener cacheMessageListener = new CacheMessageListener(redisTemplate, redisCaffeineCacheManager);        redisMessageListenerContainer.addMessageListener(cacheMessageListener, new ChannelTopic(cacheRedisCaffeineProperties.getRedis().getTopic()));        return redisMessageListenerContainer;    }}</code></pre><h4 id="在resources-META-INF-spring-factories文件中增加spring-boot配置扫描"><a href="#在resources-META-INF-spring-factories文件中增加spring-boot配置扫描" class="headerlink" title="在resources/META-INF/spring.factories文件中增加spring boot配置扫描"></a>在resources/META-INF/spring.factories文件中增加spring boot配置扫描</h4><pre><code>org.springframework.boot.autoconfigure.EnableAutoConfiguration=\  com.cache.config.RedisAutoConfig,\  com.cache.config.CacheRedisCaffeineAutoConfiguration</code></pre><h4 id="接下来就可以使用maven引入使用了"><a href="#接下来就可以使用maven引入使用了" class="headerlink" title="接下来就可以使用maven引入使用了"></a>接下来就可以使用maven引入使用了</h4><pre><code>&lt;dependency&gt;  &lt;groupId&gt;com.demo&lt;/groupId&gt;  &lt;artifactId&gt;cache-redis-caffeine-spring-boot-starter&lt;/artifactId&gt;  &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt;</code></pre><h4 id="开启缓存支持-1"><a href="#开启缓存支持-1" class="headerlink" title="开启缓存支持"></a>开启缓存支持</h4><p>@EnableCaching：启用spring cache缓存，作为总的开关，在spring boot的启动类或配置类上需要加上此注解才会生效</p><pre><code>@EnableCaching@SpringBootApplicationpublic class FrameworkBoot {    public static void main(String[] args) {        new SpringApplicationBuilder(FrameworkBoot.class)                .web(WebApplicationType.SERVLET)                .run(args);    }}</code></pre><h4 id="设置缓存配置-1"><a href="#设置缓存配置-1" class="headerlink" title="设置缓存配置"></a>设置缓存配置</h4><pre><code>spring:  #redis配置,默认单机配置  redis:    host: 127.0.0.1    #    password:    port: 6379    #    cluster:    ##    配置集群    ##      nodes: 127.0.0.1:6379,127.0.0.1:6380,127.0.0.1:6381    #      nodes:    #        - 127.0.0.1:6379    #        - 127.0.0.1:6380    #        - 127.0.0.1:6381    #连接池配置    jedis:      pool:        #连接池最大连接数        max-active: 1000        #连接池最大阻塞等待时间(使用负值没有限制)        max-wait: -1        #连接池中的最大空闲连接        max-idle: 10        #连接池中的最小空闲连接        min-idle: 5  cache:    #开启缓存    enabled: true    #redis模式，true默认单机    stand-alone: true    #开启二级缓存    multistage-enabled: true    #缓存key前缀配置    cache-prefix: cachePrefix    #是否存储空值，默认true，防止缓存穿透    cache-null-values: true    caffeine:      #统一配置L1缓存初始化容量      initial-capacity: 10      #统一配置L2缓存最大容量      maximum-size: 1000      #全局配置写入后的过期时间      expire-after-write: 3000000    redis:      #优先级最高过期时间      expires:        #单个key过期时间        user: 6000        test: 60000      #全局配置过期时间      default-expiration: 3000000</code></pre>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> starter组件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ZooKeeper</title>
      <link href="/2020/09/30/2020-09-30-zookeeper/"/>
      <url>/2020/09/30/2020-09-30-zookeeper/</url>
      
        <content type="html"><![CDATA[<blockquote><p>第三次总结…<br>感觉之前总结的不够全面，乏了，乏了…<br>书写百遍，其义自见，哈哈哈</p></blockquote><h2 id="ZooKeeper-简介"><a href="#ZooKeeper-简介" class="headerlink" title="ZooKeeper 简介"></a>ZooKeeper 简介</h2><h3 id="ZooKeeper-是什么"><a href="#ZooKeeper-是什么" class="headerlink" title="ZooKeeper 是什么"></a>ZooKeeper 是什么</h3><p>简单的说，zookeeper=文件系统+通知机制。</p><p>ZooKeeper 是 Apache 的顶级项目。ZooKeeper 为分布式应用提供了高效且可靠的分布式协调服务，提供了诸如统一命名服务、配置管理和分布式锁等分布式的基础服务。在解决分布式数据一致性方面，ZooKeeper 并没有直接采用 Paxos 算法，而是采用了名为 ZAB 的一致性协议。</p><p>ZooKeeper 主要用来解决分布式集群中应用系统的一致性问题，它能提供基于类似于文件系统的目录节点树方式的数据存储。但是 ZooKeeper 并不是用来专门存储数据的，它的作用主要是用来维护和监控存储数据的状态变化。通过监控这些数据状态的变化，从而可以达到基于数据的集群管理。</p><p>很多大名鼎鼎的框架都基于 ZooKeeper 来实现分布式高可用，如：Dubbo、Kafka 等。</p><p><img src="/2020/09/30/2020-09-30-zookeeper/hjevsx78.bmp" alt></p><h3 id="ZooKeeper-的特性"><a href="#ZooKeeper-的特性" class="headerlink" title="ZooKeeper 的特性"></a>ZooKeeper 的特性</h3><ul><li><p>顺序一致性</p><p>  所有客户端看到的服务端数据模型都是一致的；从一个客户端发起的事务请求，最终都会严格按照其发起顺序被应用到 ZooKeeper 中。具体的实现可见：原子广播</p></li><li><p>原子性</p><p>  所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，即整个集群要么都成功应用了某个事务，要么都没有应用。 实现方式可见：事务</p></li><li><p>单一视图</p><p>  无论客户端连接的是哪个 Zookeeper 服务器，其看到的服务端数据模型都是一致的。</p></li><li><p>高性能</p><p>  ZooKeeper 将数据全量存储在内存中，所以其性能很高。需要注意的是：由于 ZooKeeper 的所有更新和删除都是基于事务的，因此 ZooKeeper 在读多写少的应用场景中有性能表现较好，如果写操作频繁，性能会大大下滑。</p></li><li><p>高可用</p><p>  ZooKeeper 的高可用是基于副本机制实现的，此外 ZooKeeper 支持故障恢复，可见：选举 Leader</p></li></ul><h2 id="ZooKeeper-核心概念"><a href="#ZooKeeper-核心概念" class="headerlink" title="ZooKeeper 核心概念"></a>ZooKeeper 核心概念</h2><h3 id="ACL"><a href="#ACL" class="headerlink" title="ACL"></a>ACL</h3><p><strong>ZooKeeper 采用 ACL（Access Control Lists）策略来进行权限控制。</strong></p><p>每个 znode 创建时都会带有一个 ACL 列表，用于决定谁可以对它执行何种操作。</p><p>ACL 依赖于 ZooKeeper 的客户端认证机制。ZooKeeper 提供了以下几种认证方式：</p><ul><li>digest：用户名和密码 来识别客户端</li><li>sasl：通过 kerberos 来识别客户端</li><li>ip：通过 IP 来识别客户端</li></ul><p>ZooKeeper 定义了如下五种权限：</p><ul><li>CREATE：允许创建子节点</li><li>READ：允许从节点获取数据并列出其子节点</li><li>WRITE：允许为节点设置数据</li><li>DELETE：允许删除子节点</li><li>ADMIN：允许为节点设置权限</li></ul><h3 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h3><p><strong>ZooKeeper 的数据模型是一个树形结构的文件系统。</strong></p><p>树中的节点被称为 znode，其中根节点为 /，每个节点上都会保存自己的数据和节点信息。</p><p>znode 可以用于存储数据，并且有一个与之相关联的 ACL 列表。ZooKeeper 的设计目标是实现协调服务，而不是真的作为一个文件存储，因此 znode 存储数据的大小被限制在 1MB 以内。</p><p><img src="/2020/09/30/2020-09-30-zookeeper/ff387091cfa24afa8849ac175aa483b9~tplv-k3u1fbpfcp-zoom-1.image.png" alt></p><p><strong>ZooKeeper 的数据具有原子性</strong>。其读写操作都是要么全部成功，要么全部失败。</p><p>znode 通过路径被引用。<strong>znode 节点路径必须是绝对路径</strong>。</p><p>znode 有两种类型：</p><ul><li>临时的（ EPHEMERAL ）：户端会话结束时，ZooKeeper 就会删除临时的 znode。</li><li>持久的（PERSISTENT ）：除非客户端主动执行删除操作，否则 ZooKeeper 不会删除持久的 znode。</li></ul><h3 id="节点信息"><a href="#节点信息" class="headerlink" title="节点信息"></a>节点信息</h3><p>znode 上有一个顺序标志（ SEQUENTIAL ）。如果在创建 znode 时，设置了顺序标志（ SEQUENTIAL ），那么 ZooKeeper 会使用计数器为 znode 添加一个单调递增的数值，即 zxid。ZooKeeper 正是利用 zxid 实现了严格的顺序访问控制能力。</p><p>每个 znode 节点在存储数据的同时，都会维护一个叫做 Stat 的数据结构，里面存储了关于该节点的全部状态信息。如下：</p><table><thead><tr><th align="left">状态属性</th><th align="right">说明</th></tr></thead><tbody><tr><td align="left">czxid</td><td align="right">数据节点创建时的事务 ID</td></tr><tr><td align="left">ctime</td><td align="right">数据节点创建时的时间</td></tr><tr><td align="left">mzxid</td><td align="right">数据节点最后一次更新时的事务 ID</td></tr><tr><td align="left">mtime</td><td align="right">数据节点最后一次更新时的时间</td></tr><tr><td align="left">pzxid</td><td align="right">数据节点的子节点最后一次被修改时的事务 ID</td></tr><tr><td align="left">cversion</td><td align="right">子节点的更改次数</td></tr><tr><td align="left">version</td><td align="right">节点数据的更改次数</td></tr><tr><td align="left">aversion</td><td align="right">节点的 ACL 的更改次数</td></tr><tr><td align="left">ephemeralOwner</td><td align="right">如果节点是临时节点，则表示创建该节点的会话的 SessionID；如果节点是持久节点，则该属性值为 0</td></tr><tr><td align="left">dataLength</td><td align="right">数据内容的长度</td></tr><tr><td align="left">numChildren</td><td align="right">数据节点当前的子节点个数</td></tr></tbody></table><h3 id="集群角色"><a href="#集群角色" class="headerlink" title="集群角色"></a>集群角色</h3><p>Zookeeper 集群是一个基于主从复制的高可用集群，每个服务器承担如下三种角色中的一种</p><ul><li><p>Leader</p><p>  它负责发起并维护与各 Follwer 及 Observer 间的心跳。所有的写操作必须要通过 Leader 完成再由 Leader 将写操作广播给其它服务器。一个 Zookeeper 集群同一时间只会有一个实际工作的 Leader。</p></li><li><p>Follower</p><p>  它会响应 Leader 的心跳。Follower 可直接处理并返回客户端的读请求，同时会将写请求转发给 Leader 处理，并且负责在 Leader 处理写请求时对请求进行投票。一个 Zookeeper 集群可能同时存在多个 Follower。</p></li><li><p>Observer</p><p>  角色与 Follower 类似，但是无投票权。</p></li></ul><h2 id="ZooKeeper-工作原理"><a href="#ZooKeeper-工作原理" class="headerlink" title="ZooKeeper 工作原理"></a>ZooKeeper 工作原理</h2><h3 id="读操作"><a href="#读操作" class="headerlink" title="读操作"></a>读操作</h3><p><strong>Leader/Follower/Observer 都可直接处理读请求，从本地内存中读取数据并返回给客户端即可。</strong></p><p>由于处理读请求不需要服务器之间的交互，Follower/Observer 越多，整体系统的读请求吞吐量越大，也即读性能越好。</p><p><img src="/2020/09/30/2020-09-30-zookeeper/142bc3f8b397423997787617f10eb6ca~tplv-k3u1fbpfcp-zoom-1.image.png" alt></p><h3 id="写操作"><a href="#写操作" class="headerlink" title="写操作"></a>写操作</h3><p><strong>所有的写请求实际上都要交给 Leader 处理。Leader 将写请求以事务形式发给所有 Follower 并等待 ACK，一旦收到半数以上 Follower 的 ACK，即认为写操作成功。</strong></p><h4 id="写-Leader"><a href="#写-Leader" class="headerlink" title="写 Leader"></a>写 Leader</h4><p><img src="/2020/09/30/2020-09-30-zookeeper/7878598a3a7045edb490d97dde4974f7~tplv-k3u1fbpfcp-zoom-1.image.png" alt></p><p>由上图可见，通过 Leader 进行写操作，主要分为五步：</p><ol><li>客户端向 Leader 发起写请求</li><li>Leader 将写请求以事务 Proposal 的形式发给所有 Follower 并等待 ACK</li><li>Follower 收到 Leader 的事务 Proposal 后返回 ACK</li><li>Leader 得到过半数的 ACK（Leader 对自己默认有一个 ACK）后向所有的 Follower 和 Observer 发送 Commmit</li><li>Leader 将处理结果返回给客户端</li></ol><blockquote><p>Leader 不需要得到 Observer 的 ACK，即 Observer 无投票权。<br>Leader 不需要得到所有 Follower 的 ACK，只要收到过半的 ACK 即可，同时 Leader 本身对自己有一个 ACK。上图中有 4 个 Follower，只需其中两个返回 ACK 即可，因为 (2+1)/(4+1)&gt;1/2<br>Observer 虽然无投票权，但仍须同步 Leader 的数据从而在处理读请求时可以返回尽可能新的数据。</p></blockquote><h4 id="写-Follower-Observer"><a href="#写-Follower-Observer" class="headerlink" title="写 Follower/Observer"></a>写 Follower/Observer</h4><p><img src="/2020/09/30/2020-09-30-zookeeper/a2cc9400d1ce410787fb86e3b6fa4148~tplv-k3u1fbpfcp-zoom-1.image.png" alt></p><p>Follower/Observer 均可接受写请求，但不能直接处理，而需要将写请求转发给 Leader 处理。<br>除了多了一步请求转发，其它流程与直接写 Leader 无任何区别。</p><h3 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h3><p>对于来自客户端的每个更新请求，ZooKeeper 具备严格的顺序访问控制能力。</p><p><strong>为了保证事务的顺序一致性，ZooKeeper 采用了递增的事务 id 号（zxid）来标识事务。</strong></p><p>Leader 服务会为每一个 Follower 服务器分配一个单独的队列，然后将事务 Proposal 依次放入队列中，并根据 FIFO(先进先出) 的策略进行消息发送。Follower 服务在接收到 Proposal 后，会将其以事务日志的形式写入本地磁盘中，并在写入成功后反馈给 Leader 一个 Ack 响应。当 Leader 接收到超过半数 Follower 的 Ack 响应后，就会广播一个 Commit 消息给所有的 Follower 以通知其进行事务提交，之后 Leader 自身也会完成对事务的提交。而每一个 Follower 则在接收到 Commit 消息后，完成事务的提交。</p><p>所有的提议（proposal）都在被提出的时候加上了 zxid。zxid 是一个 64 位的数字，它的高 32 位是 epoch 用来标识 Leader 关系是否改变，每次一个 Leader 被选出来，它都会有一个新的 epoch，标识当前属于那个 leader 的统治时期。低 32 位用于递增计数。</p><p>详细过程如下：</p><ol><li>Leader 等待 Server 连接；</li><li>Follower 连接 Leader，将最大的 zxid 发送给 Leader；</li><li>Leader 根据 Follower 的 zxid 确定同步点；</li><li>完成同步后通知 follower 已经成为 uptodate 状态；</li><li>Follower 收到 uptodate 消息后，又可以重新接受 client 的请求进行服务了。</li></ol><h3 id="观察"><a href="#观察" class="headerlink" title="观察"></a>观察</h3><p><strong>客户端注册监听它关心的 znode，当 znode 状态发生变化（数据变化、子节点增减变化）时，ZooKeeper 服务会通知客户端。</strong></p><p>客户端和服务端保持连接一般有两种形式：</p><ul><li>客户端向服务端不断轮询</li><li>服务端向客户端推送状态</li></ul><p>Zookeeper 的选择是服务端主动推送状态，也就是观察机制（ Watch ）。</p><p>ZooKeeper 的观察机制允许用户在指定节点上针对感兴趣的事件注册监听，当事件发生时，监听器会被触发，并将事件信息推送到客户端。</p><p>客户端使用 getData 等接口获取 znode 状态时传入了一个用于处理节点变更的回调，那么服务端就会主动向客户端推送节点的变更：</p><pre><code>public byte[] getData(final String path, Watcher watcher, Stat stat)</code></pre><p>从这个方法中传入的 Watcher 对象实现了相应的 process 方法，每次对应节点出现了状态的改变，WatchManager 都会通过以下的方式调用传入 Watcher 的方法：</p><pre><code>Set&lt;Watcher&gt; triggerWatch(String path, EventType type, Set&lt;Watcher&gt; supress) {    WatchedEvent e = new WatchedEvent(type, KeeperState.SyncConnected, path);    Set&lt;Watcher&gt; watchers;    synchronized (this) {        watchers = watchTable.remove(path);    }    for (Watcher w : watchers) {        w.process(e);    }    return watchers;}</code></pre><p>Zookeeper 中的所有数据其实都是由一个名为 DataTree 的数据结构管理的，所有的读写数据的请求最终都会改变这颗树的内容，在发出读请求时可能会传入 Watcher 注册一个回调函数，而写请求就可能会触发相应的回调，由 WatchManager 通知客户端数据的变化。</p><p>通知机制的实现其实还是比较简单的，通过读请求设置 Watcher 监听事件，写请求在触发事件时就能将通知发送给指定的客户端。</p><h3 id="会话"><a href="#会话" class="headerlink" title="会话"></a>会话</h3><p>ZooKeeper 客户端通过 TCP 长连接连接到 ZooKeeper 服务集群。会话 (Session) 从第一次连接开始就已经建立，之后通过心跳检测机制来保持有效的会话状态。通过这个连接，客户端可以发送请求并接收响应，同时也可以接收到 Watch 事件的通知。</p><p>每个 ZooKeeper 客户端配置中都配置了 ZooKeeper 服务器集群列表。启动时，客户端会遍历列表去尝试建立连接。如果失败，它会尝试连接下一个服务器，依次类推。</p><p>一旦一台客户端与一台服务器建立连接，这台服务器会为这个客户端创建一个新的会话。每个会话都会有一个超时时间，若服务器在超时时间内没有收到任何请求，则相应会话被视为过期。一旦会话过期，就无法再重新打开，且任何与该会话相关的临时 znode 都会被删除。</p><p>通常来说，会话应该长期存在，而这需要由客户端来保证。客户端可以通过心跳方式（ping）来保持会话不过期。</p><p><img src="/2020/09/30/2020-09-30-zookeeper/334d76538df54aae9c66b27ee283204e~tplv-k3u1fbpfcp-zoom-1.image.png" alt></p><p>ZooKeeper 的会话具有四个属性：</p><ul><li><p>sessionID</p><p>  会话 ID，唯一标识一个会话，每次客户端创建新的会话时，Zookeeper 都会为其分配一个全局唯一的 sessionID。</p></li><li><p>TimeOut</p><p>  会话超时时间，客户端在构造 Zookeeper 实例时，会配置 sessionTimeout 参数用于指定会话的超时时间，Zookeeper 客户端向服务端发送这个超时时间后，服务端会根据自己的超时时间限制最终确定会话的超时时间。</p></li><li><p>TickTime</p><p>  下次会话超时时间点，为了便于 Zookeeper 对会话实行”分桶策略”管理，同时为了高效低耗地实现会话的超时检查与清理，Zookeeper 会为每个会话标记一个下次会话超时时间点，其值大致等于当前时间加上 TimeOut。</p></li><li><p>isClosing</p><p>  标记一个会话是否已经被关闭，当服务端检测到会话已经超时失效时，会将该会话的 isClosing 标记为”已关闭”，这样就能确保不再处理来自该会话的新请求了。</p></li></ul><p>Zookeeper 的会话管理主要是通过 SessionTracker 来负责，其采用了分桶策略（将类似的会话放在同一区块中进行管理）进行管理，以便 Zookeeper 对会话进行不同区块的隔离处理以及同一区块的统一处理。</p><h2 id="ZAB-协议"><a href="#ZAB-协议" class="headerlink" title="ZAB 协议"></a>ZAB 协议</h2><p><strong>ZooKeeper 并没有直接采用 Paxos 算法，而是采用了名为 ZAB 的一致性协议</strong>。ZAB 协议不是 Paxos 算法，只是比较类似，二者在操作上并不相同。</p><p>ZAB 协议是 Zookeeper 专门设计的一种支持崩溃恢复的原子广播协议。</p><p><strong>ZAB 协议是 ZooKeeper 的数据一致性和高可用解决方案。</strong></p><p>ZAB 协议定义了两个可以无限循环的流程：</p><ul><li>选举 Leader：用于故障恢复，从而保证高可用。</li><li>原子广播：用于主从同步，从而保证数据一致性。</li></ul><h3 id="选举-Leader"><a href="#选举-Leader" class="headerlink" title="选举 Leader"></a>选举 Leader</h3><p>ZooKeeper 集群采用一主（称为 Leader）多从（称为 Follower）模式，主从节点通过副本机制保证数据一致。</p><ul><li>如果 Follower 节点挂了 - ZooKeeper 集群中的每个节点都会单独在内存中维护自身的状态，并且各节点之间都保持着通讯，只要集群中有半数机器能够正常工作，那么整个集群就可以正常提供服务。</li><li>如果 Leader 节点挂了 - 如果 Leader 节点挂了，系统就不能正常工作了。此时，需要通过 ZAB 协议的选举 Leader 机制来进行故障恢复。</li></ul><p>ZAB 协议的选举 Leader 机制简单来说，就是：基于过半选举机制产生新的 Leader，之后其他机器将从新的 Leader 上同步状态，当有过半机器完成状态同步后，就退出选举 Leader 模式，进入原子广播模式。</p><h4 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h4><ul><li><p>myid</p><p>  每个 Zookeeper 服务器，都需要在数据文件夹下创建一个名为 myid 的文件，该文件包含整个 Zookeeper 集群唯一的 ID（整数）。</p></li><li><p>zxid</p><p>  类似于 RDBMS 中的事务 ID，用于标识一次更新操作的 Proposal ID。为了保证顺序性，该 zkid 必须单调递增。因此 Zookeeper 使用一个 64 位的数来表示，高 32 位是 Leader 的 epoch，从 1 开始，每次选出新的 Leader，epoch 加一。低 32 位为该 epoch 内的序号，每次 epoch 变化，都将低 32 位的序号重置。这样保证了 zkid 的全局递增性。</p></li></ul><h4 id="服务器状态"><a href="#服务器状态" class="headerlink" title="服务器状态"></a>服务器状态</h4><ul><li><p>LOOKING</p><p>  不确定 Leader 状态。该状态下的服务器认为当前集群中没有 Leader，会发起 Leader 选举</p></li><li><p>FOLLOWING</p><p>  跟随者状态。表明当前服务器角色是 Follower，并且它知道 Leader 是谁</p></li><li><p>LEADING</p><p>  领导者状态。表明当前服务器角色是 Leader，它会维护与 Follower 间的心跳</p></li><li><p>OBSERVING</p><p>  观察者状态。表明当前服务器角色是 Observer，与 Folower 唯一的不同在于不参与选举，也不参与集群写操作时的投票</p></li></ul><h4 id="选票数据结构"><a href="#选票数据结构" class="headerlink" title="选票数据结构"></a>选票数据结构</h4><p>每个服务器在进行领导选举时，会发送如下关键信息</p><ul><li><p>logicClock</p><p>  每个服务器会维护一个自增的整数，名为 logicClock，它表示这是该服务器发起的第多少轮投票</p></li><li><p>state</p><p>  当前服务器的状态</p></li><li><p>self_id</p><p>  当前服务器的 myid</p></li><li><p>self_zxid</p><p>  当前服务器上所保存的数据的最大 zxid</p></li><li><p>vote_id</p><p>  被推举的服务器的 myid</p></li><li><p>vote_zxid</p><p>  被推举的服务器上所保存的数据的最大 zxid</p></li></ul><h4 id="投票流程"><a href="#投票流程" class="headerlink" title="投票流程"></a>投票流程</h4><ol><li><p>自增选举轮次</p><p> Zookeeper 规定所有有效的投票都必须在同一轮次中。每个服务器在开始新一轮投票时，会先对自己维护的 logicClock 进行自增操作。</p></li><li><p>初始化选票</p><p> 每个服务器在广播自己的选票前，会将自己的投票箱清空。该投票箱记录了所收到的选票。例：服务器 2 投票给服务器 3，服务器 3 投票给服务器 1，则服务器 1 的投票箱为(2, 3), (3, 1), (1, 1)。票箱中只会记录每一投票者的最后一票，如投票者更新自己的选票，则其它服务器收到该新选票后会在自己票箱中更新该服务器的选票。</p></li><li><p>发送初始化选票</p><p> 每个服务器最开始都是通过广播把票投给自己</p></li><li><p>接收外部投票</p><p> 服务器会尝试从其它服务器获取投票，并记入自己的投票箱内。如果无法获取任何外部投票，则会确认自己是否与集群中其它服务器保持着有效连接。如果是，则再次发送自己的投票；如果否，则马上与之建立连接。</p></li><li><p>判断选举轮次</p><p> 收到外部投票后，首先会根据投票信息中所包含的 logicClock 来进行不同处理</p><p> 外部投票的 logicClock 大于自己的 logicClock。说明该服务器的选举轮次落后于其它服务器的选举轮次，立即清空自己的投票箱并将自己的 logicClock 更新为收到的 logicClock，然后再对比自己之前的投票与收到的投票以确定是否需要变更自己的投票，最终再次将自己的投票广播出去。</p><p> 外部投票的 logicClock 小于自己的 logicClock。当前服务器直接忽略该投票，继续处理下一个投票。</p><p> 外部投票的 logickClock 与自己的相等。当时进行选票 PK。</p></li><li><p>选票 PK</p><p> 选票 PK 是基于(self_id, self_zxid)与(vote_id, vote_zxid)的对比</p><p> 外部投票的 logicClock 大于自己的 logicClock，则将自己的 logicClock 及自己的选票的 logicClock 变更为收到的 logicClock</p><p> 若 logicClock 一致，则对比二者的 vote_zxid，若外部投票的 vote_zxid 比较大，则将自己的票中的 vote_zxid 与 vote_myid 更新为收到的票中的 vote_zxid 与 vote_myid 并广播出去，另外将收到的票及自己更新后的票放入自己的票箱。如果票箱内已存在(self_myid, self_zxid)相同的选票，则直接覆盖</p><p> 若二者 vote_zxid 一致，则比较二者的 vote_myid，若外部投票的 vote_myid 比较大，则将自己的票中的 vote_myid 更新为收到的票中的 vote_myid 并广播出去，另外将收到的票及自己更新后的票放入自己的票箱</p></li><li><p>统计选票</p><p> 如果已经确定有过半服务器认可了自己的投票（可能是更新后的投票），则终止投票。否则继续接收其它服务器的投票。</p></li><li><p>更新服务器状态</p><p> 投票终止后，服务器开始更新自身状态。若过半的票投给了自己，则将自己的服务器状态更新为 LEADING，否则将自己的状态更新为 FOLLOWING</p></li></ol><p>通过以上流程分析，我们不难看出：要使 Leader 获得多数 Server 的支持，则 ZooKeeper 集群节点数必须是奇数。且存活的节点数目不得少于 N + 1 。</p><p>每个 Server 启动后都会重复以上流程。在恢复模式下，如果是刚从崩溃状态恢复的或者刚启动的 server 还会从磁盘快照中恢复数据和会话信息，zk 会记录事务日志并定期进行快照，方便在恢复时进行状态恢复。</p><h3 id="原子广播（Atomic-Broadcast）"><a href="#原子广播（Atomic-Broadcast）" class="headerlink" title="原子广播（Atomic Broadcast）"></a>原子广播（Atomic Broadcast）</h3><p>ZooKeeper 通过副本机制来实现高可用。</p><p>那么，ZooKeeper 是如何实现副本机制的呢？答案是：ZAB 协议的原子广播。</p><p><img src="/2020/09/30/2020-09-30-zookeeper/f685ebb785e44cc28a496f827d74bb1f~tplv-k3u1fbpfcp-zoom-1.image.png" alt></p><p>ZAB 协议的原子广播要求：</p><p>所有的写请求都会被转发给 Leader，Leader 会以原子广播的方式通知 Follow。当半数以上的 Follow 已经更新状态持久化后，Leader 才会提交这个更新，然后客户端才会收到一个更新成功的响应。这有些类似数据库中的两阶段提交协议。</p><p>在整个消息的广播过程中，Leader 服务器会每个事物请求生成对应的 Proposal，并为其分配一个全局唯一的递增的事务 ID(ZXID)，之后再对其进行广播。</p><h2 id="ZooKeeper-应用"><a href="#ZooKeeper-应用" class="headerlink" title="ZooKeeper 应用"></a>ZooKeeper 应用</h2><h3 id="命名服务"><a href="#命名服务" class="headerlink" title="命名服务"></a>命名服务</h3><p>在分布式系统中，通常需要一个全局唯一的名字，如生成全局唯一的订单号等，ZooKeeper 可以通过顺序节点的特性来生成全局唯一 ID，从而可以对分布式系统提供命名服务。</p><p><img src="/2020/09/30/2020-09-30-zookeeper/0398226fd4e0440687572ad0aa3638aa~tplv-k3u1fbpfcp-zoom-1.image.png" alt></p><h3 id="配置管理"><a href="#配置管理" class="headerlink" title="配置管理"></a>配置管理</h3><p>利用 ZooKeeper 的观察机制，可以将其作为一个高可用的配置存储器，允许分布式应用的参与者检索和更新配置文件。</p><h3 id="分布式锁"><a href="#分布式锁" class="headerlink" title="分布式锁"></a>分布式锁</h3><p>可以通过 ZooKeeper 的临时节点和 Watcher 机制来实现分布式锁。</p><p>举例来说，有一个分布式系统，有三个节点 A、B、C，试图通过 ZooKeeper 获取分布式锁。</p><ol><li><p>访问 /lock （这个目录路径由程序自己决定），创建 带序列号的临时节点（EPHEMERAL） 。</p><p> <img src="/2020/09/30/2020-09-30-zookeeper/303d5ab927c347ab870300e4274b185d~tplv-k3u1fbpfcp-zoom-1.image.png" alt></p></li><li><p>每个节点尝试获取锁时，拿到 /locks节点下的所有子节点（id_0000,id_0001,id_0002），判断自己创建的节点是不是最小的</p><ul><li><p>如果是，则拿到锁。释放锁：执行完操作后，把创建的节点给删掉。</p></li><li><p>如果不是，则监听比自己要小1的节点变化。</p><p><img src="/2020/09/30/2020-09-30-zookeeper/91ae64a585e94c5dbaf8d3cbc74918a5~tplv-k3u1fbpfcp-zoom-1.image.png" alt></p></li></ul></li><li><p>释放锁，即删除自己创建的节点。</p><p> <img src="/2020/09/30/2020-09-30-zookeeper/1c90f3d3bab440fa87f6ef4fde432b23~tplv-k3u1fbpfcp-zoom-1.image.png" alt></p><p> 图中，NodeA 删除自己创建的节点 id_0000，NodeB 监听到变化，发现自己的节点已经是最小节点，即可获取到锁。</p></li></ol><h3 id="集群管理"><a href="#集群管理" class="headerlink" title="集群管理"></a>集群管理</h3><p>ZooKeeper 还能解决大多数分布式系统中的问题：</p><ul><li>如可以通过创建临时节点来建立心跳检测机制。如果分布式系统的某个服务节点宕机了，则其持有的会话会超时，此时该临时节点会被删除，相应的监听事件就会被触发。</li><li>分布式系统的每个服务节点还可以将自己的节点状态写入临时节点，从而完成状态报告或节点工作进度汇报。</li><li>通过数据的订阅和发布功能，ZooKeeper 还能对分布式系统进行模块的解耦和任务的调度。</li><li>通过监听机制，还能对分布式系统的服务节点进行动态上下线，从而实现服务的动态扩容。</li></ul><h3 id="选举-Leader-节点"><a href="#选举-Leader-节点" class="headerlink" title="选举 Leader 节点"></a>选举 Leader 节点</h3><p>分布式系统一个重要的模式就是主从模式 (Master/Salves)，ZooKeeper 可以用于该模式下的 Matser 选举。可以让所有服务节点去竞争性地创建同一个 ZNode，由于 ZooKeeper 不能有路径相同的 ZNode，必然只有一个服务节点能够创建成功，这样该服务节点就可以成为 Master 节点。</p><h3 id="队列管理"><a href="#队列管理" class="headerlink" title="队列管理"></a>队列管理</h3><p>ZooKeeper 可以处理两种类型的队列：</p><ul><li>当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达，这种是同步队列。</li><li>队列按照 FIFO 方式进行入队和出队操作，例如实现生产者和消费者模型。</li></ul><p>同步队列用 ZooKeeper 实现的实现思路如下：</p><p>创建一个父目录 /synchronizing，每个成员都监控标志（Set Watch）位目录 /synchronizing/start 是否存在，然后每个成员都加入这个队列，加入队列的方式就是创建 /synchronizing/member_i 的临时目录节点，然后每个成员获取 / synchronizing 目录的所有目录节点，也就是 member_i。判断 i 的值是否已经是成员的个数，如果小于成员个数等待 /synchronizing/start 的出现，如果已经相等就创建 /synchronizing/start。</p>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ZooKeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux超常用命令</title>
      <link href="/2020/09/29/2020-09-29-linux-chao-chang-yong-ming-ling/"/>
      <url>/2020/09/29/2020-09-29-linux-chao-chang-yong-ming-ling/</url>
      
        <content type="html"><![CDATA[<blockquote><p>入秋的第一篇总结博客biubiubiu~本篇主要记录和总结工作中常用的Linux命令<br>一个月没总结博客，心态有点崩o(╥﹏╥)o</p></blockquote><h2 id="系统信息"><a href="#系统信息" class="headerlink" title="系统信息"></a>系统信息</h2><ul><li>查看IP</li></ul><pre><code>ifconfig</code></pre><ul><li>显示机器的处理器架构</li></ul><pre><code>arch</code></pre><ul><li>显示正在使用的内核版本</li></ul><pre><code>uname -r</code></pre><ul><li>显示系统日期</li></ul><pre><code>date</code></pre><ul><li>查看日历</li></ul><pre><code>cal</code></pre><p>如果想查看任一年的日历</p><pre><code>cal 2020</code></pre><ul><li>清屏</li></ul><pre><code>clear</code></pre><h2 id="文件和目录"><a href="#文件和目录" class="headerlink" title="文件和目录"></a>文件和目录</h2><ul><li>创建一个文件</li></ul><pre><code>touch Test.java</code></pre><ul><li>编辑文件</li></ul><pre><code>vi Test.java</code></pre><p>按i进行编辑，如果编辑完毕，则按键盘左上角的ESC键退出编辑模式，如果想保存并退出，输入:wq</p><ul><li>查看文件</li></ul><pre><code>cat Test.java</code></pre><p>如果查看的文件行数过多呢？</p><pre><code>less Test.java</code></pre><p>如果less Test.java 还想搜索关键词呢？</p><p>输入 /guanjianci 即可</p><p>按q退出</p><ul><li>查看日志</li></ul><pre><code>tail -fn800 xxx.log</code></pre><p>指定查看800行log</p><p>如果想搜索日志中的关键词呢？</p><pre><code>grep &#39;guanjianci&#39; xxx.log</code></pre><ul><li>打开目录</li></ul><pre><code>cd mulu/</code></pre><ul><li>返回上一级目录</li></ul><pre><code>cd ..</code></pre><p>如果返回上两级目录呢？</p><pre><code>cd ../..</code></pre><ul><li>返回主目录</li></ul><pre><code>cd ~</code></pre><ul><li>返回上次所在的目录 </li></ul><pre><code>cd -</code></pre><ul><li>显示目录路径</li></ul><pre><code>pwd</code></pre><ul><li>查看目录中的文件 </li></ul><pre><code>ls</code></pre><p>如果想显示文件和目录的详情呢？</p><pre><code>ls -l</code></pre><ul><li>创建一个新目录</li></ul><pre><code>mkdir test</code></pre><p>如果想创建多层目录呢？</p><pre><code>mkdir -p test1/test2</code></pre><ul><li>删除文件</li></ul><pre><code>rm -f testFile</code></pre><ul><li>删除目录</li></ul><pre><code>rmdir testDir</code></pre><p>如果想删除的目录里面包含有文件呢？</p><pre><code>rm -rf testDir</code></pre><blockquote><p>千万别执行 <strong>rm -rf /</strong> 会删了整个Linux根目录下的所有文件<br>如果误删了，别慌，提前跑路吧</p></blockquote><p><img src="/2020/09/29/2020-09-29-linux-chao-chang-yong-ming-ling/c81m1m9e.bmp" alt></p><ul><li>copy一个文件</li></ul><pre><code>cp file1 toFile2</code></pre><p>如果想copy整个目录呢？</p><pre><code>cp -a testDir1/ testDir2</code></pre><p>testDir2为当前路径新的目标目录</p><h2 id="压缩与解压"><a href="#压缩与解压" class="headerlink" title="压缩与解压"></a>压缩与解压</h2><ul><li>压缩一个tar.gz文件</li></ul><pre><code>tar -zcvf name.tar.gz testDir1/</code></pre><p>嗷嗷，如果想把两个文件夹的内容压缩到一个压缩包呢？</p><pre><code>tar -zcvf name.tar.gz testDir1/ testDir2/</code></pre><ul><li>解压一个tar.gz文件</li></ul><pre><code>tar -zxvf name.tar.gz</code></pre><p>如果想把解压的文件放到一个目录里面呢？</p><pre><code>tar -zxvf name.tar.gz ./testDir</code></pre>]]></content>
      
      
      <categories>
          
          <category> Work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL的EXPLAIN参数</title>
      <link href="/2020/08/14/2020-08-14-explain-can-shu-jie-xi/"/>
      <url>/2020/08/14/2020-08-14-explain-can-shu-jie-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="EXPLAIN如何用"><a href="#EXPLAIN如何用" class="headerlink" title="EXPLAIN如何用"></a>EXPLAIN如何用</h2><p>explain + SQL语句即可,如下:</p><pre><code>explain select * from tbl_dept;</code></pre><h2 id="EXPLAIN结果参数含义"><a href="#EXPLAIN结果参数含义" class="headerlink" title="EXPLAIN结果参数含义"></a>EXPLAIN结果参数含义</h2><h3 id="id"><a href="#id" class="headerlink" title="id"></a>id</h3><p>id代表执行select子句或操作表的顺序,id分别有三种不同的执行结果,分别如下: </p><ul><li>id相同,执行顺序由上至下</li></ul><p><img src="/2020/08/14/2020-08-14-explain-can-shu-jie-xi/20180521160555926.png" alt></p><ul><li>id不同,如果是子查询,id的序号会递增,id值越大,优先级越高,越先被执行</li></ul><p><img src="/2020/08/14/2020-08-14-explain-can-shu-jie-xi/20180521161124499.png" alt></p><ul><li>id相同和不同,同时存在,遵从优先级高的优先执行,优先级相同的按照由上至下的顺序执行</li></ul><p><img src="/2020/08/14/2020-08-14-explain-can-shu-jie-xi/20180521161430126.png" alt></p><h3 id="select-type"><a href="#select-type" class="headerlink" title="select_type"></a>select_type</h3><p>查询的类型,主要用于区别普通查询,联合查询,子查询等复杂查询</p><ul><li>simple:简单的select查询,查询中不包含子查询或union查询</li><li>primary:查询中若包含任何复杂的子部分,最外层查询则被标记为primary</li><li>subquery 在select 或where 列表中包含了子查询</li><li>derived 在from列表中包含的子查询被标记为derived,mysql会递归这些子查询,把结果放在临时表里</li><li>union 做第二个select出现在union之后,则被标记为union,若union包含在from子句的子查询中,外层select将被标记为derived</li><li>union result 从union表获取结果的select</li></ul><h3 id="table"><a href="#table" class="headerlink" title="table"></a>table</h3><p>显示一行的数据时关于哪张表的</p><h3 id="type"><a href="#type" class="headerlink" title="type"></a>type</h3><p><strong>查询类型从最好到最差依次是:system&gt;const&gt;eq_ref&gt;ref&gt;range&gt;index&gt;All,一般情况下,得至少保证达到range级别,最好能达到ref</strong></p><ul><li><p>system:表只有一行记录,这是const类型的特例,平时不会出现</p></li><li><p>const:表示通过索引一次就找到了,const即常量,它用于比较primary key或unique索引,因为只匹配一行数据,所以效率很快,如将主键置于where条件中,mysql就能将该查询转换为一个常量</p><p>  <img src="/2020/08/14/2020-08-14-explain-can-shu-jie-xi/2018052116390992.png" alt></p></li><li><p>eq_ref:唯一性索引扫描,对于每个索引键,表中只有一条记录与之匹配,常见于主键或唯一索引扫描</p></li><li><p>ref:非唯一性索引扫描,返回匹配某个单独值的行,它可能会找到多个符合条件的行,所以他应该属于查找和扫描的混合体</p></li><li><p>range:只检索给定范围的行,使用一个索引来选择行,如where语句中出现了between,&lt;,&gt;,in等查询,这种范围扫描索引比全表扫描要好，因为它只需要开始于索引的某一点，而结束于另一点，不用扫描全部索引。</p></li><li><p>index:index类型只遍历索引树,这通常比All快,因为索引文件通常比数据文件小,index是从索引中读取,all从硬盘中读取</p></li><li><p>all:全表扫描,是最差的一种查询类型</p></li></ul><h3 id="possible-keys"><a href="#possible-keys" class="headerlink" title="possible_keys"></a>possible_keys</h3><p>显示可能应用在这张表中的索引,一个或多个,查询到的索引不一定是真正被用到的</p><h3 id="key"><a href="#key" class="headerlink" title="key"></a>key</h3><p>实际使用的索引,如果为null,则没有使用索引,因此会出现possible_keys列有可能被用到的索引,但是key列为null,表示实际没用索引。</p><h3 id="key-len"><a href="#key-len" class="headerlink" title="key_len"></a>key_len</h3><p>表示索引中使用的字节数,而通过该列计算查询中使用的 索引长度,在不损失精确性的情况下,长度越短越好,key_len显示的值为索引字段的最大可能长度,并非实际使用长度,即,key_len是根据表定义计算而得么不是通过表内检索出的</p><h3 id="ref"><a href="#ref" class="headerlink" title="ref"></a>ref</h3><p>显示索引的哪一列被使用了,如果可能的话是一个常数,哪些列或常量被用于查找索引列上的值</p><h3 id="rows"><a href="#rows" class="headerlink" title="rows"></a>rows</h3><p>根据表统计信息及索引选用情况,大只估算出找到所需的记录所需要读取的行数</p><h3 id="Extra"><a href="#Extra" class="headerlink" title="Extra"></a>Extra</h3><ul><li>Using filesort:说明mysql会对数据使用一个外部的索引排序,而不是按照表内的索引顺序进行读取,mysql中无法利用索引完成的排序操作称为”文件排序”</li><li>Using temporary :使用了临时表保存中间结果,mysql在对查询结果排序时使用临时表,常见于order by和分组查询group by</li><li>Using index:表示相应的select操作中使用了覆盖索引（Covering Index），避免访问了表的数据行，效率不错。如果同时出现using where，表明索引被用来执行索引键值的查找；如果没有同时出现using where，表明索引用来读取数据而非执行查找动作。 其中的覆盖索引含义是所查询的列是和建立的索引字段和个数是一一对应的</li><li>Using where:表明使用了where过滤</li><li>Using join buffer:表明使用了连接缓存,如在查询的时候会有多次join,则可能会产生临时表</li><li>impossible where:表示where子句的值总是false,不能用来获取任何元祖</li><li>select tables optimized away:在没有GROUPBY子句的情况下，基于索引优化MIN/MAX操作或者对于MyISAM存储引擎优化COUNT(*)操作，不必等到执行阶段再进行计算，查询执行计划生成的阶段即完成优化。</li><li>distinct:优化distinct操作，在找到第一匹配的元组后即停止找同样值的动作</li></ul>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis计数</title>
      <link href="/2020/08/13/2020-08-13-redis-ji-shu/"/>
      <url>/2020/08/13/2020-08-13-redis-ji-shu/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本篇主要记录和总结工作中用到Redis实现计数统计的一种实现方式</p></blockquote><p>Redis实现计数统计的方式有很多，这里主要简单总结ZSet的一种实现方式。</p><h2 id="计数"><a href="#计数" class="headerlink" title="计数"></a>计数</h2><pre><code>redisTemplate.opsForZSet().add(BASE_KEY + id, System.currentTimeMillis() + &quot;:&quot; + Math.random(), System.currentTimeMillis());</code></pre><p>key: BASE_KEY + id<br>value: System.currentTimeMillis() + “:” + Math.random()   加上Math.random()主要防止并发重复<br>scope: System.currentTimeMillis())</p><h2 id="统计"><a href="#统计" class="headerlink" title="统计"></a>统计</h2><p>统计今天scope从0到System.currentTimeMillis()范围的数量</p><pre><code>Set&lt;Object&gt; todayClickVolumeSet = redisTemplate.opsForZSet().rangeByScore(BASE_KEY + id(), 0, System.currentTimeMillis());int todayClickVolume = ObjectU.len(todayClickVolumeSet);</code></pre>]]></content>
      
      
      <categories>
          
          <category> Work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>工厂 + 策略模式</title>
      <link href="/2020/08/12/2020-08-12-ce-lue-mo-shi/"/>
      <url>/2020/08/12/2020-08-12-ce-lue-mo-shi/</url>
      
        <content type="html"><![CDATA[<blockquote><p>工作中常用的工厂 + 策略模式，避免写过多冗余的if</p></blockquote><pre><code>@Autowiredprivate BatchFactory batchFactory;// 工厂 + 策略模式SiteBatch siteBatch = batchFactory.get(site);List&lt;BO&gt; BOS = siteBatch.analyzeFile(is);</code></pre><pre><code>@Componentpublic class BatchFactory {    @Lazy    @Autowired    private Map&lt;String, SiteBatch&gt; siteMap;    public SiteBatch get(String type) {        return siteMap.get(type);    }}</code></pre><pre><code>@Servicepublic interface SiteBatch {    /**     * 工厂模式时用     * @return     */    String getType();    List&lt;ShopBatchBO&gt; analyzeFile(InputStream is);    void checkParam(BatchBO batchBO);}</code></pre><pre><code>@Service(SiteBatchType.CDISCOUNT)public class CdiscountBatch implements SiteBatch {    @Override    public String getType() {        return SiteBatchType.CDISCOUNT;    }    @Override    public List&lt;BatchBO&gt; analyzeFile(InputStream is) {        List&lt;BatchBO&gt; list = Lists.newArrayList();        return list;    }    @Override    public void checkParam(BatchBO batchBO) {    }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java基础</title>
      <link href="/2020/08/01/2020-08-01-zhong-ji-yi-zhan/"/>
      <url>/2020/08/01/2020-08-01-zhong-ji-yi-zhan/</url>
      
        <content type="html"><![CDATA[<h2 id="AQS"><a href="#AQS" class="headerlink" title="AQS"></a>AQS</h2><h3 id="AQS实现原理："><a href="#AQS实现原理：" class="headerlink" title="AQS实现原理："></a>AQS实现原理：</h3><ul><li>CountDownLatch、CyclicBarriar、Semaphore信号量、ReentrantLock、ReentrantReadWriteLock读写锁底层都有Sync类继承AQS类，都属于AQS框架。</li><li>AQS中 维护了一个volatile int state（代表共享资源）和一个FIFO双向等待CLH队列（多线程争用资源被阻塞时会进入此队列）。</li><li>另外state的操作都是通过CAS来保证其并发修改的安全性。</li><li>这里volatile能够保证多线程下的可见性，当state=1则代表当前对象锁已经被占有，其他线程来加锁时则会失败，加锁失败的线程会被放入一个FIFO的等待队列中，比列会被UNSAFE.park()操作挂起，等待其他获取锁的线程释放锁才能够被唤醒。</li><li>当线程执行完逻辑后，会释放同步状态，释放后会唤醒其后继节点，使其再次尝试获取同步状态。</li></ul><h3 id="加锁和挂起线程流程："><a href="#加锁和挂起线程流程：" class="headerlink" title="加锁和挂起线程流程："></a>加锁和挂起线程流程：</h3><p><img src="/2020/08/01/2020-08-01-zhong-ji-yi-zhan/642.png" alt></p><ol><li><p>首先调用nonfairTryAcquire()获取<code>state</code>的值。如果为0，利用<code>CAS</code>尝试抢占锁compareAndSetState(0, 1)，且设置对象独占锁线程为当前线程setExclusiveOwnerThread(Thread.currentThread())；如果不为0则说明当前对象的锁已经被其他线程所占有，接着判断占有锁的线程是否为当前线程，如果是则累加<code>state</code>值。获取锁成功。</p></li><li><p>如果获取锁失败：首先执行<code>addWaiter(Node.EXCLUSIVE)</code>，加入到一个<code>FIFO</code>   CLH等待队列中。<code>addWaiter()</code>方法执行完后，会返回当前线程创建的节点信息。继续往后执行<code>acquireQueued(addWaiter(Node.EXCLUSIVE), arg)</code>逻辑。<code>acquireQueued()</code>这个方法会先判断当前传入的<code>Node</code>对应的前置节点是否为<code>head</code>，如果是则尝试加锁。加锁成功过则将当前节点设置为<code>head</code>节点，然后空置之前的<code>head</code>节点，方便后续被垃圾回收掉。如果加锁失败或者<code>Node</code>的前置节点不是<code>head</code>节点，就会通过<code>shouldParkAfterFailedAcquire</code>方法 将前一个节点的<code>waitStatus</code>变为了<code>SIGNAL=-1</code>，最后执行<code>parkAndChecknIterrupt</code>方法，调用<code>LockSupport.park()</code>挂起当前线程，等着其他线程释放锁来唤醒它。</p></li></ol><h3 id="释放锁和唤醒线程流程："><a href="#释放锁和唤醒线程流程：" class="headerlink" title="释放锁和唤醒线程流程："></a>释放锁和唤醒线程流程：</h3><ol><li>首先是拥有锁的线程释放锁，释放锁后会唤醒<code>head</code>节点的后置节点。</li><li>首先会执行<code>tryRelease()</code>方法，执行完<code>ReentrantLock.tryRelease()</code>后，<code>state</code>被设置成0，Lock对象的独占锁被设置为null。如果<code>tryRelease</code>执行成功，则继续判断<code>head</code>节点的<code>waitStatus</code>是否为0，就会执行<code>unparkSuccessor()</code>方法来唤醒<code>head</code>的后置节点。<code>unparkSuccessor()</code>主要是将<code>head</code>节点的<code>waitStatus</code>设置为0，然后解除<code>head</code>节点<code>next</code>的指向，使<code>head</code>节点空置，等待着被垃圾回收。然后唤醒的节点就可以执行加锁流程了。</li></ol><hr><h2 id="CAS"><a href="#CAS" class="headerlink" title="CAS"></a>CAS</h2><ul><li>CAS，Compare And Swap，即比较和交换。</li><li>在CAS中有三个参数：内存值V、旧的预期值A、要更新的值B，当且仅当内存值V的值等于旧的预期值A时才会将内存值V的值修改为B，否则什么都不干。</li><li>CAS虽然高效地解决了原子操作，但是还是存在一些缺陷的，主要表现在三个方法：循环时间太长、只能保证一个共享变量原子操作、ABA问题。<ul><li>循环时间太长：如果自旋CAS长时间地不成功，则会给CPU带来非常大的开销。在JUC中有些地方就限制了CAS自旋的次数，例如BlockingQueue的SynchronousQueue。Java 8 推出了一个新的类，LongAdder，他就是尝试使用分段 CAS 以及自动分段迁移的方式来大幅度提升多线程高并发执行 CAS 操作的性能.</li><li>只能保证一个共享变量原子操作：看了CAS的实现就知道这只能针对一个共享变量，如果是多个共享变量就只能使用锁了，当然如果你有办法把多个变量整成一个变量，利用CAS也不错。</li><li>ABA问题：对于ABA问题其解决方案是加上版本号，即在每个变量都加上一个版本号，每次改变时加1，即A —&gt; B —&gt; A，变成1A —&gt; 2B —&gt; 3A。</li></ul></li></ul><hr><h2 id="JMM-内存模型"><a href="#JMM-内存模型" class="headerlink" title="JMM(内存模型)"></a>JMM(内存模型)</h2><ul><li>Java采用内存共享的模式来实现线程之间的通信。</li><li>并发编程模式中，势必会遇到三个概念：<ul><li>原子性：一个操作或者多个操作要么全部执行要么全部不执行；</li><li>可见性：当多个线程同时访问一个共享变量时，如果其中某个线程更改了该共享变量，其他线程应该可以立刻看到这个改变；</li><li>有序性：程序的执行要按照代码的先后顺序执行；</li></ul></li></ul><hr><h2 id="DCL（中卫双重检查锁定）"><a href="#DCL（中卫双重检查锁定）" class="headerlink" title="DCL（中卫双重检查锁定）"></a>DCL（中卫双重检查锁定）</h2><pre><code>public class Singleton {    // 通过volatile关键字来确保创建对象的三个步骤的指令不重排    private volatile static Singleton singleton;    private Singleton(){}    public static Singleton getInstance(){        if(singleton == null){            synchronized (Singleton.class){                if(singleton == null){                    singleton = new Singleton();                }            }        }        return singleton;    }}</code></pre><hr><h2 id="volatile"><a href="#volatile" class="headerlink" title="volatile"></a>volatile</h2><ul><li><p>在JMM（内存模型）中，线程之间的通信采用共享内存来实现的。</p></li><li><p>volatile的写内存语义是直接刷新到主内存中，读的内存语义是直接从主内存中读取。</p></li><li><p>实现volatile的内存语义JMM会限制重排序，其重排序规则如下：</p><ul><li>如果当前为volatile读时，后面的操作不能重排序。（这个操作确保volatile读之后的操作不会被编译器重排序到volatile读之前）；</li><li>如果当前为volatile写时，前面的操作不能重排序。（这个操作确保volatile写之前的操作不会被编译器重排序到volatile写之后）；</li><li>如果当前为volatile写时，紧接操作为volatile读时，不能重排序。</li></ul></li><li><p>volatile的底层是通过插入内存屏障实现的。</p><ul><li>在每一个volatile写操作前面插入一个StoreStore屏障</li><li>在每一个volatile写操作后面插入一个StoreLoad屏障</li><li>在每一个volatile读操作后面插入一个LoadLoad屏障</li><li>在每一个volatile读操作后面插入一个LoadStore屏障</li></ul><ol><li>StoreStore屏障可以保证在volatile写之前，其前面的所有普通写操作都已经刷新到主内存中。</li><li>StoreLoad屏障的作用是避免volatile写与后面可能有的volatile读/写操作重排序。</li><li>LoadLoad屏障用来禁止处理器把上面的volatile读与下面的普通读重排序。</li><li>LoadStore屏障用来禁止处理器把上面的volatile读与下面的普通写重排序。</li></ol></li><li><p>volatile可以保证可见性，对一个volatile的读，总可以看到对这个变量最终的写；</p></li><li><p>volatile不可以保证原子性，volatile对单个读/写具有原子性（32位Long、Double），但是复合操作除外，例如i++;</p></li><li><p>volatile可以保证有序性，JVM底层采用“内存屏障”来实现。</p></li><li><p>volatile 主要解决的是一个线程修改变量值之后，其他线程立马可以读到最新的值，是解决这个问题的，也就是可见性！但是如果是多个线程同时修改一个变量的值，那还是可能出现多线程并发的安全问题，导致数据值修改错乱，volatile 是不负责解决这个问题的，也就是不负责解决原子性问题！原子性问题，得依赖 synchronized、ReentrantLock 等加锁机制来解决。</p></li><li><p>对一个变量加了 volatile 关键字修饰之后，只要一个线程修改了这个变量的值，立马强制刷回主内存。接着强制过期其他线程的本地工作内存中的缓存，最后其他线程读取变量值的时候，强制重新从主内存来加载最新的值！</p></li></ul><hr><h2 id="synchronized"><a href="#synchronized" class="headerlink" title="synchronized"></a>synchronized</h2><ul><li><p>普通同步方法，锁是当前实例对象；静态同步方法，锁是当前类的class对象；同步代码块，锁是括号里面的对象。</p></li><li><p>同步代码块：同步代码块是使用monitorenter和monitorexit指令实现的。monitorenter指令插入到同步代码块的开始位置，monitorexit指令插入到同步代码块的结束位置，JVM需要保证每一个monitorenter都有一个monitorexit与之相对应。任何对象都有一个monitor与之相关联，当且一个monitor被持有之后，他将处于锁定状态。线程执行到monitorenter指令时，将会尝试获取对象所对应的monitor所有权，即尝试获取对象的锁。</p></li><li><p>同步方法：在Class文件的方法表中将该方法的accessflags字段中的synchronized标志位置1，表示该方法是同步方法，并使用调用该方法的对象或该方法所属的Class在JVM的内部对象做为锁对象。</p></li><li><p>Java对象头和monitor是实现synchronized的基础，synchronized用的锁是存在Java对象头里的。对象头包含哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程 ID等等。</p></li><li><p>jdk1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。</p></li><li><p>锁消除：JVM检测到不可能存在共享数据竞争，这是JVM会对这些同步锁进行锁消除。锁消除的依据是逃逸分析的数据支持。例如方法中定义的局部变量没有逃逸出方法之外，可以加锁操作消除。</p></li><li><p>锁粗化：如果一系列的连续加锁解锁操作，可能会导致不必要的性能损耗，所以引入锁粗化的概念。锁粗化概念比较好理解，就是将多个连续的加锁、解锁操作连接在一起，扩展成一个范围更大的锁。例如一个for不断枷锁解锁，可以把锁放到for循环外。</p></li><li><p>锁主要存在四种状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈而逐渐升级。锁只可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率。</p></li><li><p>自旋锁：就是让该线程等待一段时间，不断循环去尝试获取锁。</p></li><li><p>自适应自旋锁：自旋的次数不再是固定的，它是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。线程如果自旋成功了，那么下次自旋的次数会更加多，反之，如果对于某个锁，很少有自旋能够成功的，那么在以后要或者这个锁的时候自旋的次数会减少。</p></li><li><p>对象头的Mark Word结构图<br>  <img src="/2020/08/01/2020-08-01-zhong-ji-yi-zhan/a6e378ec.bmp" alt></p></li><li><p>偏向锁<br>  <img src="/2020/08/01/2020-08-01-zhong-ji-yi-zhan/yaiopcog.bmp" alt="偏向锁的获取和释放流程"></p><ul><li><p>获取锁</p><ol><li>检测Mark Word是否为可偏向状态，即是否为偏向锁1，锁标识位为01；</li><li>若为可偏向状态，则测试线程ID是否为当前线程ID，如果是，执行同步代码块；</li><li>如果线程ID不为当前线程ID，则通过CAS操作竞争锁，竞争成功，则将Mark Word的线程ID替换为当前线程ID，执行同步代码块；</li><li>通过CAS竞争锁失败，证明当前存在多线程竞争情况，获得偏向锁的线程被挂起，偏向锁升级为轻量级锁，然后被阻塞的线程继续往下执行同步代码块；</li></ol></li><li><p>释放锁：偏向锁的释放采用了一种只有竞争才会释放锁的机制，线程是不会主动去释放偏向锁，需要等待其他线程来竞争。</p><ol><li>暂停拥有偏向锁的线程，判断锁对象是否还处于被锁定状态；</li><li>撤销偏向锁，恢复到无锁状态（01）或者轻量级锁的状态。</li></ol></li></ul></li><li><p>轻量级锁</p><p>  <img src="/2020/08/01/2020-08-01-zhong-ji-yi-zhan/uta8xosj.bmp" alt="轻量级锁的获取和释放过程"></p><ul><li><p>获取锁</p><ol><li>判断当前对象是否处于无锁状态（hashcode、0、01），若是，则JVM首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝；</li><li>JVM利用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指针，如果成功表示竞争到锁，则将锁标志位变成00（表示此对象处于轻量级锁状态），执行同步代码块；</li><li>否则只能说明该锁对象已经被其他线程抢占了，这时轻量级锁需要膨胀为重量级锁，锁标志位变成10，后面等待的线程将会进入阻塞状态。</li></ol></li><li><p>释放锁</p><ol><li>取出在获取轻量级锁保存在栈中的数据；</li><li>用CAS操作将取出的数据替换当前对象的Mark Word中，如果成功，则说明释放锁成功；</li><li>如果CAS操作替换失败，说明有其他线程尝试获取该锁，则需要在释放锁的同时需要唤醒被挂起的线程。</li></ol></li></ul></li><li><p>重量级锁：重量级锁通过对象内部的监视器（monitor）实现，操作系统实现线程之间的切换需要从用户态到内核态的切换，切换成本非常高。</p></li></ul><hr><h2 id="as-if-serial语义"><a href="#as-if-serial语义" class="headerlink" title="as-if-serial语义"></a>as-if-serial语义</h2><p>as-if-serial语义的意思是，所有的操作均可以为了优化而被重排序，但是你必须要保证重排序后执行的结果不能被改变，编译器、runtime、处理器都必须遵守as-if-serial语义。注意as-if-serial只保证单线程环境，多线程环境下无效。</p><h2 id="happens-before原则"><a href="#happens-before原则" class="headerlink" title="happens-before原则"></a>happens-before原则</h2><ul><li>如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。</li><li>两个操作之间存在happens-before关系，并不意味着一定要按照happens-before原则制定的顺序来执行。如果重排序之后的执行结果与按照happens-before关系来执行的结果一致，那么这种重排序并不非法。</li></ul><hr><h2 id="ThreadLoacal"><a href="#ThreadLoacal" class="headerlink" title="ThreadLoacal"></a>ThreadLoacal</h2><p><img src="/2020/08/01/2020-08-01-zhong-ji-yi-zhan/y8gischv.bmp" alt></p><ul><li>ThreadLocal与线程同步机制不同，线程同步机制是多个线程共享同一个变量，而ThreadLocal是为每一个线程创建一个单独的变量副本，故而每个线程都可以独立地改变自己所拥有的变量副本，而不会影响其他线程所对应的副本。（ThreadLocal在多线程下，每个线程对共享变量都会创建一份变量副本去操作，不会影响其他线程的变量副本）</li><li>ThreadLocal内部还有一个静态内部类ThreadLocalMap，该内部类才是实现线程隔离机制的关键。ThreadLocalMap提供了一种用键值对方式存储每一个线程的变量副本的方法，key为当前ThreadLocal对象，value则是对应线程的变量副本。</li><li>ThreadLocal为什么会内存泄漏？每个Thread都有一个ThreadLocal.ThreadLocalMap的map，该map的key为ThreadLocal实例，它为一个弱引用，我们知道弱引用有利于GC回收。当ThreadLocal的key == null时，GC就会回收这部分空间，但是value却不一定能够被回收，因为他还与Current Thread存在一个强引用关系。由于存在这个强引用关系，会导致value无法回收。如果这个线程对象不会销毁那么这个强引用关系则会一直存在，就会出现内存泄漏情况。那么要怎么避免这个问题呢？在ThreadLocalMap中的setEntry()、getEntry()，如果遇到key == null的情况，会对value设置为null。当然我们也可以显示调用ThreadLocal的remove()方法进行处理。</li></ul><p>个人术语：</p><ul><li>ThreadLocal在多线程下，每个线程对共享变量都会创建一份变量副本去操作，不会影响其他线程的变量副本。</li><li>ThreadLocal内部有一个静态内部类ThreadLocalMap，该内部类才是实现线程隔离机制的关键。ThreadLocalMap是以key/value的形式存在的，key为当前ThreadLocal对象，value则是对应线程的变量副本。</li><li>key为ThreadLocal实例，它为一个弱引用，弱引用有利于GC回收。当ThreadLocal的key == null时，GC就会回收这部分空间，但是value却不一定能够被回收，因为变量副本还与当前线程存在一个强引用关系，如果这个线程对象不会销毁那么这个强引用关系则会一直存在，可能出现内存泄漏情况。在ThreadLocalMap中的set/get方法中，当key == null的情况，会对value设置为null，有利于GC回收。当然我们也可以显示调用ThreadLocal的remove()方法进行处理。</li></ul><hr><h2 id="什么是线程和进程"><a href="#什么是线程和进程" class="headerlink" title="什么是线程和进程"></a>什么是线程和进程</h2><ul><li>进程：进程是程序的一次执行过程，是系统运行程序的基本单位。当我们启动 main 函数时其实就是启动了一个 JVM 的进程，而 main 函数所在的线程就是这个进程中的一个线程，也称主线程。</li><li>线程：。一个进程在其执行的过程中可以产生多个线程。多个线程共享进程的堆和方法区资源，但每个线程有自己的程序计数器、虚拟机栈和本地方法栈。</li></ul><hr><h2 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h2><p>并发编程的目的就是为了能提高程序的执行效率提高程序运行速度，但并发编程可能会遇到很多问题，比如：内存泄漏、上下文切换、死锁还有受限于硬件和软件的资源闲置问题。</p><hr><h2 id="线程的生命周期和状态"><a href="#线程的生命周期和状态" class="headerlink" title="线程的生命周期和状态"></a>线程的生命周期和状态</h2><p>当线程被创建并启动以后，它既不是一启动就进入了执行状态，也不是一直处于执行状态。在线程的生命周期中，它要经过新建(New)、就绪（Runnable）、运行（Running）、阻塞(Blocked)和死亡(Dead)5 种状态。尤其是当线程启动以后，它不可能一直”霸占”着 CPU 独自运行，所以 CPU 需要在多条线程之间切换，于是线程状态也会多次在运行、阻塞之间切换。</p><h3 id="新建状态（NEW）"><a href="#新建状态（NEW）" class="headerlink" title="新建状态（NEW）"></a>新建状态（NEW）</h3><p>当程序使用 new 关键字创建了一个线程之后，该线程就处于新建状态，此时仅由 JVM 为其分配内存，并初始化其成员变量的值</p><h3 id="就绪状态（RUNNABLE）"><a href="#就绪状态（RUNNABLE）" class="headerlink" title="就绪状态（RUNNABLE）"></a>就绪状态（RUNNABLE）</h3><p>当线程对象调用了 start()方法之后，该线程处于就绪状态。Java 虚拟机会为其创建方法调用栈和程序计数器，等待调度运行。</p><h3 id="运行状态（RUNNING）"><a href="#运行状态（RUNNING）" class="headerlink" title="运行状态（RUNNING）"></a>运行状态（RUNNING）</h3><p>如果处于就绪状态的线程获得了 CPU，开始执行 run()方法的线程执行体，则该线程处于运行状态。</p><h3 id="阻塞状态（BLOCKED）"><a href="#阻塞状态（BLOCKED）" class="headerlink" title="阻塞状态（BLOCKED）"></a>阻塞状态（BLOCKED）</h3><p>阻塞状态是指线程因为某种原因放弃了 cpu 使用权，也即让出了 cpu timeslice，暂时停止运行。直到线程进入可运行(runnable)状态，才有机会再次获得 cpu timeslice 转到运行(running)状态。阻塞的情况分三种：等待阻塞（o.wait-&gt;等待对列），同步阻塞(lock-&gt;锁池)，其他阻塞(sleep/join)。</p><h3 id="线程死亡（DEAD）"><a href="#线程死亡（DEAD）" class="headerlink" title="线程死亡（DEAD）"></a>线程死亡（DEAD）</h3><p>线程结束后就是死亡状态。</p><hr><h2 id="start-与-run-区别"><a href="#start-与-run-区别" class="headerlink" title="start 与 run 区别"></a>start 与 run 区别</h2><p>start（）方法来启动线程，真正实现了多线程运行。这时无需等待 run 方法体代码执行完毕，可以直接继续执行下面的代码。</p><p>通过调用 Thread 类的 start()方法来启动一个线程， 这时此线程是处于就绪状态， 并没有运行。</p><p>方法 run()称为线程体，它包含了要执行的这个线程的内容，线程就进入了运行状态，开始运行 run 函数当中的代码。 Run 方法运行结束， 此线程终止。然后 CPU 再调度其它线程（就是一个方法）。</p><hr><h2 id="sleep-方法和-wait-方法区别和共同点"><a href="#sleep-方法和-wait-方法区别和共同点" class="headerlink" title="sleep() 方法和 wait() 方法区别和共同点"></a>sleep() 方法和 wait() 方法区别和共同点</h2><ul><li><strong>两者最主要的区别在于：sleep 方法没有释放锁，而 wait 方法释放了锁 。</strong></li><li>两者都可以暂停线程的执行。</li><li>Wait 通常被用于线程间交互/通信，sleep 通常被用于暂停执行。</li><li><strong>wait() 方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的 notify() 或者 notifyAll() 方法。sleep() 方法执行完成后，线程会自动苏醒。或者可以使用 wait(long timeout)超时后线程会自动苏醒。</strong></li></ul><hr><h2 id="线程池的主要参数"><a href="#线程池的主要参数" class="headerlink" title="线程池的主要参数"></a>线程池的主要参数</h2><ul><li>corePoolSize（线程池核心线程大小）：线程池中会维护一个最小的线程数量，即使这些线程处理空闲状态，他们也不会 被销毁，除非设置了allowCoreThreadTimeOut。这里的最小线程数量即是corePoolSize。</li><li>maximumPoolSize（线程池最大线程数量）：一个任务被提交到线程池后，首先会缓存到工作队列（后面会介绍）中，如果工作队列满了，则会创建一个新线程，然后从工作队列中的取出一个任务交由新线程来处理，而将刚提交的任务放入工作队列。线程池不会无限制的去创建新线程，它会有一个最大线程数量的限制，这个数量即由maximunPoolSize来指定。</li><li>keepAliveTime（空闲线程存活时间）：一个线程如果处于空闲状态，并且当前的线程数量大于corePoolSize，那么在指定时间后，这个空闲线程会被销毁，这里的指定时间由keepAliveTime来设定</li><li>unit（空间线程存活时间单位）：keepAliveTime的计量单位</li><li>workQueue（工作队列）：新任务被提交后，会先进入到此工作队列中，任务调度时再从队列中取出任务。jdk中提供了四种工作队列：<ul><li>ArrayBlockingQueue：基于数组的有界阻塞队列，按FIFO排序。新任务进来后，会放到该队列的队尾，有界的数组可以防止资源耗尽问题。当线程池中线程数量达到corePoolSize后，再有新任务进来，则会将任务放入该队列的队尾，等待被调度。如果队列已经是满的，则创建一个新线程，如果线程数量已经达到maxPoolSize，则会执行拒绝策略。</li><li>LinkedBlockingQuene：基于链表的无界阻塞队列（其实最大容量为Interger.MAX），按照FIFO排序。由于该队列的近似无界性，当线程池中线程数量达到corePoolSize后，再有新任务进来，会一直存入该队列，而不会去创建新线程直到maxPoolSize，因此使用该工作队列时，参数maxPoolSize其实是不起作用的。</li><li>SynchronousQuene：一个不缓存任务的阻塞队列，生产者放入一个任务必须等到消费者取出这个任务。也就是说新任务进来时，不会缓存，而是直接被调度执行该任务，如果没有可用线程，则创建新线程，如果线程数量达到maxPoolSize，则执行拒绝策略。</li><li>PriorityBlockingQueue：具有优先级的无界阻塞队列，优先级通过参数Comparator实现。</li></ul></li><li>threadFactory（线程工厂）：创建一个新线程时使用的工厂，可以用来设定线程名、是否为daemon线程等等</li><li>handler（拒绝策略）：当工作队列中的任务已到达最大限制，并且线程池中的线程数量也达到最大限制，这时如果有新任务提交进来，该如何处理呢。这里的拒绝策略，就是解决这个问题的，jdk中提供了4中拒绝策略：<ul><li>CallerRunsPolicy：该策略下，在调用者线程中直接执行被拒绝任务的run方法，除非线程池已经shutdown，则直接抛弃任务。</li><li>AbortPolicy：该策略下，直接丢弃任务，并抛出RejectedExecutionException异常。</li><li>DiscardPolicy：该策略下，直接丢弃任务，什么都不做。</li><li>DiscardOldestPolicy：该策略下，抛弃进入队列最早的那个任务，然后尝试把这次拒绝的任务放入队列</li></ul></li></ul><p>个人术语：</p><ul><li>corePoolSize（线程池核心线程大小）：线程池中会维护一个最小的线程数量，即使这些线程处理空闲状态，他们也不会 被销毁。</li><li>maximumPoolSize（线程池最大线程数量）：最大线程数量。一个任务被提交到线程池后，首先会缓存到工作队列中，如果工作队列满了，则会创建一个新线程来处理最早的一个任务。线程池不会无限制的去创建新线程，它会有一个最大线程数量的限制。</li><li>keepAliveTime（空闲线程存活时间）：一个线程如果处于空闲状态，并且当前的线程数量大于corePoolSize，那么在指定时间后，这个空闲线程会被销毁</li><li>unit（空间线程存活时间单位）：空闲线程存活时间的计量单位</li><li>threadFactory（线程工厂）：创建一个新线程时使用的工厂，可以用来设定线程名等</li><li>workQueue（工作队列）：新任务被提交后，会先进入到此工作队列中，任务调度时再从队列中取出任务。jdk中提供了四种工作队列：<ul><li>ArrayBlockingQueue：基于数组的有界阻塞队列，按FIFO排序。新任务进来后，会放到该队列的队尾，有界的数组可以防止资源耗尽问题。当线程池中线程数量达到corePoolSize后，再有新任务进来，则会将任务放入该队列的队尾，等待被调度。如果队列已经是满的，则创建一个新线程，如果线程数量已经达到maxPoolSize，则会执行拒绝策略。</li><li>LinkedBlockingQuene：基于链表的无界阻塞队列，按照FIFO排序。由于该队列的近似无界性，当线程池中线程数量达到corePoolSize后，再有新任务进来，会一直存入该队列，而不会去创建新线程，因此使用该工作队列时，参数maxPoolSize其实是不起作用的。</li><li>SynchronousQuene：一个不缓存任务的阻塞队列，生产者放入一个任务必须等到消费者取出这个任务。也就是说新任务进来时，不会缓存，而是直接被调度执行该任务，如果没有可用线程，则创建新线程，如果线程数量达到maxPoolSize，则执行拒绝策略。</li><li>PriorityBlockingQueue：具有优先级的无界阻塞队列，优先级通过参数Comparator实现。</li></ul></li><li>handler（拒绝策略）：当工作队列中的任务已到达最大限制，并且线程池中的线程数量也达到最大限制，这时如果有新任务提交进来，该如何处理呢。这里的拒绝策略，就是解决这个问题的，jdk中提供了4中拒绝策略：<ul><li>CallerRunsPolicy：该策略下，在调用者线程中直接执行被拒绝任务的run方法，除非线程池已经shutdown，则直接抛弃任务。<ul><li>AbortPolicy：该策略下，直接丢弃任务，并抛出RejectedExecutionException异常。</li><li>DiscardPolicy：该策略下，直接丢弃任务，什么都不做。</li><li>DiscardOldestPolicy：该策略下，抛弃进入队列最早的那个任务，然后尝试把这次拒绝的任务放入队列</li></ul></li></ul></li></ul><hr><h2 id="线程池的优势"><a href="#线程池的优势" class="headerlink" title="线程池的优势"></a>线程池的优势</h2><ul><li>降低系统资源消耗，通过重用已存在的线程，降低线程创建和销毁造成的消耗；</li><li>提高系统响应速度，当有任务到达时，通过复用已存在的线程，无需等待新线程的创建便能立即执行；</li><li>方便线程并发数的管控。因为线程若是无限制的创建，可能会导致内存占用过多而产生OOM，并且会造成cpu过度切换。</li></ul><hr><h2 id="线程池为什么要使用阻塞队列而不使用非阻塞队列"><a href="#线程池为什么要使用阻塞队列而不使用非阻塞队列" class="headerlink" title="线程池为什么要使用阻塞队列而不使用非阻塞队列"></a>线程池为什么要使用阻塞队列而不使用非阻塞队列</h2><ul><li>因为线程若是无限制的创建，可能会导致内存占用过多而产生OOM，并且会造成cpu过度切换。</li><li>阻塞队列可以保证任务队列中没有任务时阻塞获取任务的线程，使得线程进入wait状态，释放cpu资源。当队列中有任务时才唤醒对应线程从队列中取出消息进行执行。</li><li>使得在线程不至于一直占用cpu资源。</li></ul><hr><h2 id="如何配置线程池"><a href="#如何配置线程池" class="headerlink" title="如何配置线程池"></a>如何配置线程池</h2><ul><li>CPU密集型任务：尽量使用较小的线程池，一般为CPU核心数+1。 因为CPU密集型任务使得CPU使用率很高，若开过多的线程数，会造成CPU过度切换。</li><li>IO密集型任务：可以使用稍大的线程池，一般为2*CPU核心数。 IO密集型任务CPU使用率并不高，因此可以让CPU在等待IO的时候有其他线程去处理别的任务，充分利用CPU时间。</li></ul><hr><h2 id="线程池的execute-和submit-方法"><a href="#线程池的execute-和submit-方法" class="headerlink" title="线程池的execute()和submit()方法"></a>线程池的execute()和submit()方法</h2><ul><li>execute()，执行一个任务，没有返回值。</li><li>submit()，提交一个线程任务，有返回值。submit(Callable<t> task)能获取到它的返回值，通过future.get()获取（阻塞直到任务执行完）。</t></li></ul><hr><h2 id="获取-Class-对象的-3-种方法"><a href="#获取-Class-对象的-3-种方法" class="headerlink" title="获取 Class 对象的 3 种方法"></a>获取 Class 对象的 3 种方法</h2><h3 id="调用某个对象的-getClass-方法"><a href="#调用某个对象的-getClass-方法" class="headerlink" title="调用某个对象的 getClass()方法"></a>调用某个对象的 getClass()方法</h3><pre><code>Person p = new Person();Class clazz = p.getClass();</code></pre><h3 id="调用某个类的-class-属性来获取该类对应的-Class-对象"><a href="#调用某个类的-class-属性来获取该类对应的-Class-对象" class="headerlink" title="调用某个类的 class 属性来获取该类对应的 Class 对象"></a>调用某个类的 class 属性来获取该类对应的 Class 对象</h3><pre><code>Class clazz = Person.class;</code></pre><h3 id="使用-Class-类中的-forName-静态方法-最安全-性能最好"><a href="#使用-Class-类中的-forName-静态方法-最安全-性能最好" class="headerlink" title="使用 Class 类中的 forName()静态方法(最安全/性能最好)"></a>使用 Class 类中的 forName()静态方法(最安全/性能最好)</h3><pre><code>Class clazz=Class.forName(&quot;类的全路径&quot;); (最常用)</code></pre><hr><h2 id="创建对象的两种方法"><a href="#创建对象的两种方法" class="headerlink" title="创建对象的两种方法"></a>创建对象的两种方法</h2><pre><code>//获取 Person 类的 Class 对象 Class clazz=Class.forName(&quot;reflection.Person&quot;); //使用.newInstane 方法创建对象 Person p=(Person) clazz.newInstance();//获取构造方法并创建对象 Constructor c=clazz.getDeclaredConstructor(String.class,String.class,int.class); //创建对象并设置属性 Person p1=(Person) c.newInstance(&quot;李四&quot;,&quot;男&quot;,20);</code></pre><h3 id="Class-对象的-newInstance"><a href="#Class-对象的-newInstance" class="headerlink" title="Class 对象的 newInstance()"></a>Class 对象的 newInstance()</h3><p>使用 Class 对象的 newInstance()方法来创建该 Class 对象对应类的实例，但是这种方法要求该 Class 对象对应的类有默认的空构造器。</p><h3 id="调用-Constructor-对象的-newInstance"><a href="#调用-Constructor-对象的-newInstance" class="headerlink" title="调用 Constructor 对象的 newInstance()"></a>调用 Constructor 对象的 newInstance()</h3><p>先使用 Class 对象获取指定的 Constructor 对象，再调用 Constructor 对象的 newInstance()方法来创建 Class 对象对应类的实例,通过这种方法可以选定构造方法创建实例。</p><hr><h2 id="集合框架"><a href="#集合框架" class="headerlink" title="集合框架"></a>集合框架</h2><p><img src="/2020/08/01/2020-08-01-zhong-ji-yi-zhan/0001.png" alt></p><hr><h2 id="ArrayList-和-LinkedList-的区别"><a href="#ArrayList-和-LinkedList-的区别" class="headerlink" title="ArrayList 和 LinkedList 的区别"></a>ArrayList 和 LinkedList 的区别</h2><ul><li>数据结构实现：ArrayList 是动态数组的数据结构实现，而 LinkedList 是双向链表的数据结构实现。</li><li>随机访问效率：ArrayList 比 LinkedList 在随机访问的时候效率要高，因为 LinkedList 是线性的数据存储方式，所以需要移动指针从前往后依次查找。</li><li>增加和删除效率：在非首尾的增加和删除操作，LinkedList 要比 ArrayList 效率要高，因为 ArrayList 增删操作要影响数组内的其他数据的下标。</li><li>内存空间占用：LinkedList 比 ArrayList 更占内存，因为 LinkedList 的节点除了存储数据，还存储了两个引用，一个指向前一个元素，一个指向后一个元素。</li><li>线程安全：ArrayList 和 LinkedList 都是不同步的，也就是不保证线程安全；</li><li>综合来说，在需要频繁读取集合中的元素时，更推荐使用 ArrayList，而在插入和删除操作较多时，更推荐使用 LinkedList。</li><li>LinkedList 的双向链表也叫双链表，是链表的一种，它的每个数据结点中都有两个指针，分别指向直接后继和直接前驱。所以，从双向链表中的任意一个结点开始，都可以很方便地访问它的前驱结点和后继结点。</li></ul><hr><h2 id="ArrayList-和-Vector-的区别"><a href="#ArrayList-和-Vector-的区别" class="headerlink" title="ArrayList 和 Vector 的区别"></a>ArrayList 和 Vector 的区别</h2><ul><li>这两个类都实现了 List 接口（List 接口继承了 Collection 接口），他们都是有序集合<ul><li>线程安全：Vector 使用了 Synchronized 来实现线程同步，是线程安全的，而 ArrayList 是非线程安全的。</li><li>性能：ArrayList 在性能方面要优于 Vector。</li><li>扩容：ArrayList 和 Vector 都会根据实际的需要动态的调整容量，只不过在 Vector 扩容每次会增加 1 倍，而 ArrayList 只会增加 50%。</li></ul></li><li>Vector类的所有方法都是同步的。可以由两个线程安全地访问一个Vector对象、但是一个线程访问Vector的话代码要在同步操作上耗费大量的时间。</li><li>Arraylist不是同步的，所以在不需要保证线程安全时时建议使用Arraylist。</li></ul><hr><h2 id="CopyOnWriteArrayList的底层原理"><a href="#CopyOnWriteArrayList的底层原理" class="headerlink" title="CopyOnWriteArrayList的底层原理"></a>CopyOnWriteArrayList的底层原理</h2><ul><li>CopyOnWriteArrayList是线程安全的，写操作的时候会加锁，防止出现并发写入丢失数据的问题</li><li>CopyOnWriteArrayList底层也是数组实现的，在添加元素的时候，会复制一个新的数组，写操作在新数组上进行，读操作在原数组经行，写操作结束之后会把原数组指向新数组</li><li>CopyOnWriteArrayList允许在写操作时来读取数据，大大提高了读的性能，因此适合读多写少的应用场景，但是CopyOnWriteArrayList会比较占内存，同时可能读到的数据不是实时最新的数据，所以不适合实时性要求很高的场景</li></ul><hr><h2 id="HashSet的实现原理"><a href="#HashSet的实现原理" class="headerlink" title="HashSet的实现原理"></a>HashSet的实现原理</h2><p>HashSet 是基于 HashMap 实现的，HashSet的值存放于HashMap的key上，HashMap的value统一为present，因此 HashSet 的实现比较简单，相关 HashSet 的操作，基本上都是直接调用底层 HashMap 的相关方法来完成，HashSet 不允许重复的值。</p><hr><h2 id="HashSet如何检查重复"><a href="#HashSet如何检查重复" class="headerlink" title="HashSet如何检查重复"></a>HashSet如何检查重复</h2><p>当把对象加入HashSet时，HashSet会先计算对象的hashcode值来判断对象加入的位置，同时也会与其他加入的对象的hashcode值作比较，如果没有相符的hashcode，HashSet会假设对象没有重复出现。但是如果发现有相同hashcode值的对象，这时会调用equals（）方法来检查hashcode相等的对象是否真的相同。如果两者相同，HashSet就不会让加入操作成功。</p><hr><h2 id="HashMap-的长度为什么是2的幂次方"><a href="#HashMap-的长度为什么是2的幂次方" class="headerlink" title="HashMap 的长度为什么是2的幂次方"></a>HashMap 的长度为什么是2的幂次方</h2><p>为了能让 HashMap 存取高效，尽量减少hash冲突，也就是要尽量把数据分配均匀，每个链表/红黑树长度大致相同。</p><hr><h2 id="HashMap-与-HashTable-的区别"><a href="#HashMap-与-HashTable-的区别" class="headerlink" title="HashMap 与 HashTable 的区别"></a>HashMap 与 HashTable 的区别</h2><ul><li>线程安全： HashMap 是非线程安全的，HashTable 是线程安全的；HashTable 内部的方法基本都经过 synchronized 修饰。（如果你要保证线程安全的话就使用 ConcurrentHashMap ）；</li><li>效率： 因为线程安全的问题，HashMap 要比 HashTable 效率高一点。另外，HashTable 基本被淘汰，不要在代码中使用它；（如果你要保证线程安全的话就使用 ConcurrentHashMap ）；</li><li>对Null key 和Null value的支持： HashMap 中，null 可以作为键，这样的键只有一个，可以有一个或多个键所对应的值为 null。但是在 HashTable 中 put 进的键值只要有一个 null，直接抛NullPointerException。</li><li>初始容量大小和每次扩充容量大小的不同 ：创建时如果不指定容量初始值，Hashtable 默认的初始大小为11，之后每次扩充，容量变为原来的2n+1。HashMap 默认的初始化大小为16。之后每次扩充，容量变为原来的2倍。创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为2的幂次方大小。也就是说 HashMap 总是使用2的幂作为哈希表的大小.</li><li>底层数据结构： JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间。Hashtable 没有这样的机制。</li><li>推荐使用：在 Hashtable 的类注释可以看到，Hashtable 是保留类不建议使用，推荐在单线程环境下使用 HashMap 替代，如果需要多线程使用则用 ConcurrentHashMap 替代。</li></ul><hr><h2 id="HashMap的put方法的具体流程"><a href="#HashMap的put方法的具体流程" class="headerlink" title="HashMap的put方法的具体流程"></a>HashMap的put方法的具体流程</h2><p><img src="/2020/08/01/2020-08-01-zhong-ji-yi-zhan/0010.png" alt="putVal方法执行流程图"></p><ol><li>判断键值对数组table[i]是否为空或为null，否则执行resize()进行扩容；</li><li>根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向⑥，如果table[i]不为空，转向③；</li><li>判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals；</li><li>判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向⑤；</li><li>遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可；</li><li>插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。</li></ol><hr><h2 id="ConcurrentHashMap-和-Hashtable-的区别"><a href="#ConcurrentHashMap-和-Hashtable-的区别" class="headerlink" title="ConcurrentHashMap 和 Hashtable 的区别"></a>ConcurrentHashMap 和 Hashtable 的区别</h2><p>ConcurrentHashMap 和 Hashtable 的区别主要体现在实现线程安全的方式上不同。</p><ul><li>底层数据结构： JDK1.7的 ConcurrentHashMap 底层采用 分段的数组+链表 实现，JDK1.8 采用的数据结构跟HashMap1.8的结构一样，数组+链表/红黑二叉树。Hashtable 和 JDK1.8 之前的 HashMap 的底层数据结构类似都是采用 数组+链表 的形式，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的；</li><li>实现线程安全的方式：<ul><li>在JDK1.7的时候，ConcurrentHashMap（分段锁） 对整个桶数组进行了分割分段(Segment)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。（默认分配16个Segment，比Hashtable效率提高16倍。） 到了 JDK1.8 的时候已经摒弃了Segment的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并发控制使用 synchronized 和 CAS 来操作。（JDK1.6以后 对 synchronized锁做了很多优化） 整个看起来就像是优化过且线程安全的 HashMap，虽然在JDK1.8中还能看到 Segment 的数据结构，但是已经简化了属性，只是为了兼容旧版本；</li><li>Hashtable(同一把锁) :使用 synchronized 来保证线程安全，效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用 put 添加元素，另一个线程不能使用 put 添加元素，也不能使用 get，竞争会越来越激烈效率越低。</li></ul></li></ul><hr><h2 id="ConcurrentHashMap-底层具体实现知道吗？实现原理是什么？"><a href="#ConcurrentHashMap-底层具体实现知道吗？实现原理是什么？" class="headerlink" title="ConcurrentHashMap 底层具体实现知道吗？实现原理是什么？"></a>ConcurrentHashMap 底层具体实现知道吗？实现原理是什么？</h2><ol><li>ConcurrentHashMap在JDK1.7时是按照分段锁Segment和HashEntry实现线程安全的。每个ConcurrentHashMap里面包含有一个Segment数组，Segment数组里面包含有HashEntry数组，hashEntry是一个链表结构的元素。当对某个 HashEntry 数组的数据进行修改时，必须首先获得对应的 Segment的锁。</li><li>ConcurrentHashMap在JDK1.8时取消了Segment分段锁，采用Node数组和CAS和synchronized来保证线程安全。synchronized只锁定当前链表或红黑二叉树的首节点，这样只要hash不冲突，就不会产生并发，效率又提升N倍。</li></ol><hr><h2 id="IoC"><a href="#IoC" class="headerlink" title="IoC"></a>IoC</h2><p><strong>IoC又叫控制反转，是一种设计思想。IoC的意思就是将原本在程序中手动创建对象的控制权，交给Spring框架来管理。IoC容器实际上就是个Map，以key，value的形式存在，map中存放的就是各种对象。IoC容器解决了各种对象之前复杂的依赖关系，只要加个注解引用就可以。</strong></p><hr><h2 id="AOP"><a href="#AOP" class="headerlink" title="AOP"></a>AOP</h2><p><strong>AOP又叫面向切面编程，将那些与业务无关的公共调用的逻辑（如事务处理，日志管理，权限管理等）封装起来，减少代码的冗余度，也降低模块间的耦合度。Spring AOP是基于动态代理的。当被代理的对象，实现了某个接口，那么就会使用JDK Proxy，否则将会使用Cglib，这个时候Cglib会生成一个被代理对象的子类来代理。</strong></p><hr><h2 id="Spring-AOP-和-AspectJ-AOP-区别"><a href="#Spring-AOP-和-AspectJ-AOP-区别" class="headerlink" title="Spring AOP 和 AspectJ AOP 区别"></a>Spring AOP 和 AspectJ AOP 区别</h2><p><strong>Spring AOP 属于运行时增强，而 AspectJ 是编译时增强。 Spring AOP 基于代理(Proxying)，而 AspectJ 基于字节码操作(Bytecode Manipulation)。当切面太多的话，最好选择 AspectJ ，它比Spring AOP 快很多</strong>。</p><hr><h2 id="SpringMVC-工作原理"><a href="#SpringMVC-工作原理" class="headerlink" title="SpringMVC 工作原理"></a>SpringMVC 工作原理</h2><ol><li>客户端的所有请求都交给前端控制器DispatcherServlet来处理，DispatcherServlet会负责调用系统的其他模块来真正处理用户的请求；</li><li>DispatcherServlet收到请求后，将根据请求的信息（包括URL、请求参数等）以及映射处理器HandlerMapping的配置找到处理该请求的处理器；</li><li>在这个地方Spring会通过适配器处理器HandlerAdapter对该处理器进行封装；</li><li>处理器完成对用户请求的处理后，会返回一个ModelAndView对象给DispatcherServlet；</li><li>DispatcherServlet还要借助视图解析器ViewResolver完成从逻辑视图到真实视图对象的解析工作；</li><li>当得到真正的视图对象后，DispatcherServlet会利用视图对象对模型数据进行渲染。</li></ol><hr><h2 id="spring事务失效场景"><a href="#spring事务失效场景" class="headerlink" title="spring事务失效场景"></a>spring事务失效场景</h2><h3 id="数据库引擎不支持事务"><a href="#数据库引擎不支持事务" class="headerlink" title="数据库引擎不支持事务"></a>数据库引擎不支持事务</h3><p>MySQL 用的不是 InnoDB 引擎，而是用的 MyISAM 存储引擎</p><h3 id="事务方法未被-Spring-容器管理"><a href="#事务方法未被-Spring-容器管理" class="headerlink" title="事务方法未被 Spring 容器管理"></a>事务方法未被 Spring 容器管理</h3><p>如果事务方法所在的类没有加载到 Spring IOC 容器中，也就是说，事务方法所在的类没有被 Spring 容器管理，则Spring事务会失效。</p><p>例如：你的方法所在类没有加@Component或者@Service注解。</p><h3 id="方法没有被-public-修饰"><a href="#方法没有被-public-修饰" class="headerlink" title="方法没有被 public 修饰"></a>方法没有被 public 修饰</h3><p>如果事务所在的方法没有被 public 修饰，此时 Spring 的事务也会失效。</p><h3 id="同一类中方法之间直接的调用"><a href="#同一类中方法之间直接的调用" class="headerlink" title="同一类中方法之间直接的调用"></a>同一类中方法之间直接的调用</h3><p>如果同一个类中有两个方法分别为 A 和 B，方法 A 没有添加事务注解，而方法 B 添加了 @Transactional 事务注解，此时方法 A 直接调用方法 B，则方法 B 的事务会失效。</p><p>因为声明式事务管理（@Transactional）是建立在 AOP 动态代理之上的。其本质是对方法执行前后进行拦截，然后在目标方法开始执行之前创建或者加入一个事务，在执行完目标方法之后根据执行情况提交或者回滚事务。同一类内方法之间的直接调用是不走代理的，这样就无法进行功能的增强处理，所以事务不起作用。</p><h3 id="事务传播类型不支持事务"><a href="#事务传播类型不支持事务" class="headerlink" title="事务传播类型不支持事务"></a>事务传播类型不支持事务</h3><p>如果方法的事务传播类型为不支持事务的传播类型，则该方法的事务在 Spring 中会失效。<br>例如： A 方法的事务传播类型为 NOT_SUPPORTED，不支持事务，此时用带事务的方法 B 去调用 A 方法，则 A 方法的事务失效。</p><h3 id="进行异常捕捉却没有抛出"><a href="#进行异常捕捉却没有抛出" class="headerlink" title="进行异常捕捉却没有抛出"></a>进行异常捕捉却没有抛出</h3><p>比如对某一个新增数据代码段进行 try catch 异常，而 catch 里没有向外抛出异常，此时 spring 事务无法回滚。</p><h3 id="错误的标注异常类型"><a href="#错误的标注异常类型" class="headerlink" title="错误的标注异常类型"></a>错误的标注异常类型</h3><p>如果在 @Transactional 注解中标注的异常类型不是我们抛出的异常类型，则Spring事务的回滚会失效。<br>例如： Spring 中默认回滚的异常类型为 RuntimeException，如果此时你抛出的异常是 Exception，那么Spring 事务中无法捕获到 Exception 异常，则事务回滚会失效。</p><h3 id="开启多线程"><a href="#开启多线程" class="headerlink" title="开启多线程"></a>开启多线程</h3><p>开启一个线程去执行数据库操作，多线程内的方法将不被 spring 事务控制。<br>例如：一个带事务的方法 A 中开启线程去执行同类中的一个 insert 方法，即使这个操作失败了，也不会回滚 A 中的其他数据库操作。</p><hr><h2 id="PostConstruct"><a href="#PostConstruct" class="headerlink" title="@PostConstruct"></a>@PostConstruct</h2><p>@PostConstruct注解的方法在项目启动的时候执行这个方法，也可以理解为在spring容器启动的时候执行，可作为一些数据的常规化加载，比如初始化成员变量之类的。</p><p>Constructor &gt;&gt; @Autowired &gt;&gt; @PostConstruct</p><hr><h2 id="Spring-事务"><a href="#Spring-事务" class="headerlink" title="Spring 事务"></a>Spring 事务</h2><h3 id="Spring-管理事务的方式有几种？"><a href="#Spring-管理事务的方式有几种？" class="headerlink" title="Spring 管理事务的方式有几种？"></a>Spring 管理事务的方式有几种？</h3><ol><li>编程式事务，在代码中硬编码。(不推荐使用)</li><li>声明式事务，在配置文件中配置（推荐使用）</li></ol><p>声明式事务又分为两种：</p><ol><li>基于XML的声明式事务</li><li>基于注解的声明式事务</li></ol><h3 id="Spring-事务中的隔离级别有哪几种"><a href="#Spring-事务中的隔离级别有哪几种" class="headerlink" title="Spring 事务中的隔离级别有哪几种?"></a>Spring 事务中的隔离级别有哪几种?</h3><p>TransactionDefinition 接口中定义了五个表示隔离级别的常量：</p><ul><li>TransactionDefinition.ISOLATION_DEFAULT: 使用后端数据库默认的隔离级别，Mysql 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别.</li><li>TransactionDefinition.ISOLATION_READ_UNCOMMITTED: 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读</li><li>TransactionDefinition.ISOLATION_READ_COMMITTED: 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生</li><li>TransactionDefinition.ISOLATION_REPEATABLE_READ: 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。</li><li>TransactionDefinition.ISOLATION_SERIALIZABLE: 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。</li></ul><h3 id="Spring-事务中哪几种事务传播行为"><a href="#Spring-事务中哪几种事务传播行为" class="headerlink" title="Spring 事务中哪几种事务传播行为?"></a>Spring 事务中哪几种事务传播行为?</h3><p>支持当前事务的情况：</p><ul><li>TransactionDefinition.PROPAGATION_REQUIRED： 如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。</li><li>TransactionDefinition.PROPAGATION_SUPPORTS： 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。</li><li>TransactionDefinition.PROPAGATION_MANDATORY： 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性）</li></ul><p>不支持当前事务的情况：</p><ul><li>TransactionDefinition.PROPAGATION_REQUIRES_NEW： 如果当前存在事务，则把当前事务挂起,创建一个新的事务。</li><li>TransactionDefinition.PROPAGATION_NOT_SUPPORTED： 如果当前存在事务，则把当前事务挂起,以非事务方式运行。</li><li>TransactionDefinition.PROPAGATION_NEVER： 以非事务方式运行，如果当前存在事务，则抛出异常。</li></ul><p>其他情况：</p><ul><li>ransactionDefinition.PROPAGATION_NESTED： 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。</li></ul><hr><h2 id="spring-boot自动装配"><a href="#spring-boot自动装配" class="headerlink" title="spring boot自动装配"></a>spring boot自动装配</h2><p>Spring Boot启动类里面有个@SpringBootApplication注解，这个注解是@Configuration、@EnableAutoConfiguration、@ComponentScan 注解的集合。</p><p>@EnableAutoConfiguration注解里面@Import({AutoConfigurationImportSelector.class}) //加载自动装配类 xxxAutoconfiguration。AutoConfigurationImportSelector 类实现了 ImportSelector接口，也就实现了这个接口中的 selectImports方法，该方法主要用于获取所有符合条件的类的全限定类名，这些类需要被加载到 IoC 容器中。</p><p>不光是这个依赖下的META-INF/spring.factories被读取到，所有 Spring Boot Starter 下的META-INF/spring.factories都会被读取到。但需要过滤，@ConditionalOnXXX。</p><p>Spring Boot 通过@EnableAutoConfiguration开启自动装配，通过 SpringFactoriesLoader 最终加载META-INF/spring.factories中的自动配置类实现自动装配，自动配置类其实就是通过@Conditional按需加载的配置类，想要其生效必须引入spring-boot-starter-xxx包实现起步依赖</p><hr><h2 id="二叉树的遍历规则"><a href="#二叉树的遍历规则" class="headerlink" title="二叉树的遍历规则"></a>二叉树的遍历规则</h2><p>树的遍历顺序大体分为三种：前序遍历（先根遍历、先序遍历），中序遍历（中根遍历），后序遍历（后根遍历）。</p><p><img src="/2020/08/01/2020-08-01-zhong-ji-yi-zhan/qiiji4ye.bmp" alt></p><ul><li><p>前序遍历的规则：</p><ol><li><p>访问根节点</p></li><li><p>前序遍历左子树</p></li><li><p>前序遍历右子树</p><p>前序遍历的输出结果：ABDECF</p></li></ol></li><li><p>中序遍历的规则：</p><ol><li><p>中序遍历左子树</p></li><li><p>访问根节点</p></li><li><p>中序遍历右子树</p><p>中序遍历的输出结果：DBEAFC</p></li></ol></li><li><p>后序遍历二叉树的规则：</p><ol><li><p>后序遍历左子树</p></li><li><p>后序遍历右子树</p></li><li><p>访问根节点</p><p>后序遍历的输出顺序：DEBFCA</p></li></ol></li></ul><hr><h2 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h2><p>举例说明：要排序数组：int[] arr={6,3,8,2,9,1};   </p><p>第一趟排序：</p><p>　　　　第一次排序：6和3比较，6大于3，交换位置：  3  6  8  2  9  1</p><p>　　　　第二次排序：6和8比较，6小于8，不交换位置：3  6  8  2  9  1</p><p>　　　　第三次排序：8和2比较，8大于2，交换位置：  3  6  2  8  9  1</p><p>　　　　第四次排序：8和9比较，8小于9，不交换位置：3  6  2  8  9  1</p><p>　　　　第五次排序：9和1比较：9大于1，交换位置：  3  6  2  8  1  9</p><p>　　　　第一趟总共进行了5次比较， 排序结果：      3  6  2  8  1  9</p><p>第二趟排序：</p><p>　　　　第一次排序：3和6比较，3小于6，不交换位置：3  6  2  8  1  9</p><p>　　　　第二次排序：6和2比较，6大于2，交换位置：  3  2  6  8  1  9</p><p>　　　　第三次排序：6和8比较，6大于8，不交换位置：3  2  6  8  1  9</p><p>　　　　第四次排序：8和1比较，8大于1，交换位置：  3  2  6  1  8  9</p><p>　　　　第二趟总共进行了4次比较， 排序结果：      3  2  6  1  8  9</p><p>第三趟排序：</p><p>　　　　第一次排序：3和2比较，3大于2，交换位置：  2  3  6  1  8  9</p><p>　　　　第二次排序：3和6比较，3小于6，不交换位置：2  3  6  1  8  9</p><p>　　　　第三次排序：6和1比较，6大于1，交换位置：  2  3  1  6  8  9</p><p>　　　　第二趟总共进行了3次比较， 排序结果：         2  3  1  6  8  9</p><p>第四趟排序：</p><p>　　　　第一次排序：2和3比较，2小于3，不交换位置：2  3  1  6  8  9</p><p>　　　　第二次排序：3和1比较，3大于1，交换位置：  2  1  3  6  8  9</p><p>　　　　第二趟总共进行了2次比较， 排序结果：        2  1  3  6  8  9</p><p>第五趟排序：</p><p>　　　　第一次排序：2和1比较，2大于1，交换位置：  1  2  3  6  8  9</p><p>　　　　第二趟总共进行了1次比较， 排序结果：  1  2  3  6  8  9</p><p>最终结果：1  2  3  6  8  9</p><pre><code>for(int i=0;i&lt;arr.length-1;i++){//外层循环控制排序趟数　　for(int j=0;j&lt;arr.length-1-i;j++){//内层循环控制每一趟排序多少次　　　　if(arr[j]&gt;arr[j+1]){　　　　　　int temp=arr[j];　　　　　　arr[j]=arr[j+1];　　　　　　arr[j+1]=temp;　　　　}　　}} </code></pre><hr><h2 id="StringBuffer-和-StringBuilder-的区别"><a href="#StringBuffer-和-StringBuilder-的区别" class="headerlink" title="StringBuffer 和 StringBuilder 的区别"></a>StringBuffer 和 StringBuilder 的区别</h2><p>String 中的对象是不可变的，也就可以理解为常量，线程安全。StringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。StringBuilder 并没有对方法进行加同步锁，所以是非线程安全的。</p><hr><h2 id="与-equals-重要"><a href="#与-equals-重要" class="headerlink" title="== 与 equals(重要)"></a>== 与 equals(重要)</h2><ul><li>== : 它的作用是判断两个对象的地址是不是相等。即，判断两个对象是不是同一个对象(基本数据类型==比较的是值，引用数据类型==比较的是内存地址)。</li><li>equals() : 它的作用也是判断两个对象是否相等。但它一般有两种使用情况：</li></ul><ol><li>情况 1：类没有覆盖 equals() 方法。则通过 equals() 比较该类的两个对象时，等价于通过“==”比较这两个对象。</li><li>情况 2：类覆盖了 equals() 方法。一般，我们都覆盖 equals() 方法来比较两个对象的内容是否相等；若它们的内容相等，则返回 true (即，认为这两个对象相等)。</li></ol><pre><code>public class test1 {    public static void main(String[] args) {        String a = new String(&quot;ab&quot;); // a 为一个引用        String b = new String(&quot;ab&quot;); // b为另一个引用,对象的内容一样        String aa = &quot;ab&quot;; // 放在常量池中        String bb = &quot;ab&quot;; // 从常量池中查找        if (aa == bb) // true            System.out.println(&quot;aa==bb&quot;);        if (a == b) // false，非同一对象            System.out.println(&quot;a==b&quot;);        if (a.equals(b)) // true            System.out.println(&quot;aEQb&quot;);        if (42 == 42.0) { // true            System.out.println(&quot;true&quot;);        }    }}</code></pre><hr><h2 id="IO-流"><a href="#IO-流" class="headerlink" title="IO 流"></a>IO 流</h2><ol><li>Java 中 IO 流分为几种?</li></ol><ul><li>按照流的流向分，可以分为输入流和输出流；</li><li>按照操作单元划分，可以划分为字节流和字符流；</li><li>按照流的角色划分为节点流和处理流。</li></ul><ol start="2"><li>Java I0 流的 40 多个类都是从如下 4 个抽象类基类中派生出来的。</li></ol><ul><li><strong>InputStream/Reader: 所有的输入流的基类，前者是字节输入流，后者是字符输入流。</strong></li><li><strong>OutputStream/Writer: 所有输出流的基类，前者是字节输出流，后者是字符输出流。</strong></li></ul><p><img src="/2020/08/01/2020-08-01-zhong-ji-yi-zhan/ep4ijzng.bmp" alt></p><p><img src="/2020/08/01/2020-08-01-zhong-ji-yi-zhan/755xfuro.bmp" alt></p><hr><h2 id="BIO-NIO-AIO-区别"><a href="#BIO-NIO-AIO-区别" class="headerlink" title="BIO,NIO,AIO 区别"></a>BIO,NIO,AIO 区别</h2><ul><li>BIO (Blocking I/O): 同步阻塞 I/O 模式，数据的读取写入必须阻塞在一个线程内等待其完成。</li><li>NIO (Non-blocking/New I/O): NIO 是一种同步非阻塞的 I/O 模型.</li><li>AIO (Asynchronous I/O): AIO 是异步非阻塞的 IO 模型。异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。</li></ul><hr><h2 id="synchronized和lock的区别"><a href="#synchronized和lock的区别" class="headerlink" title="synchronized和lock的区别"></a>synchronized和lock的区别</h2><p><img src="/2020/08/01/2020-08-01-zhong-ji-yi-zhan/20180904143958577.png" alt></p><p>区别如下：</p><ul><li><p>来源：lock是一个接口，而synchronized是java的一个关键字，synchronized是内置的语言实现；</p></li><li><p>异常是否释放锁：synchronized在发生异常时候会自动释放占有的锁，因此不会出现死锁；而lock发生异常时候，不会主动释放占有的锁，必须手动unlock来释放锁，可能引起死锁的发生。（所以最好将同步代码块用try catch包起来，finally中写入unlock，避免死锁的发生。）</p></li><li><p>是否响应中断：lock等待锁过程中可以用interrupt来中断等待，而synchronized只能等待锁的释放，不能响应中断</p></li><li><p>是否知道获取锁：Lock可以通过trylock来知道有没有获取锁，而synchronized不能；</p></li><li><p>Lock可以提高多个线程进行读操作的效率。（可以通过readwritelock实现读写分离）</p></li><li><p>在性能上来说，如果竞争资源不激烈，两者的性能是差不多的，而当竞争资源非常激烈时（即有大量线程同时竞争），此时Lock的性能要远远优于synchronized。所以说，在具体使用时要根据适当情况选择。</p></li><li><p>synchronized使用Object对象本身的wait 、notify、notifyAll调度机制，而Lock可以使用Condition进行线程之间的调度</p><pre><code>  //Condition定义了等待/通知两种类型的方法  Lock lock=new ReentrantLock();  Condition condition=lock.newCondition();  ...  condition.await();  ...  condition.signal();  condition.signalAll();</code></pre></li></ul><hr><h2 id="封装继承多态"><a href="#封装继承多态" class="headerlink" title="封装继承多态"></a>封装继承多态</h2><h3 id="封装"><a href="#封装" class="headerlink" title="封装"></a>封装</h3><ul><li>将类的某些信息隐藏在类的内部，不允许外部程序进行直接的访问调用。</li><li>通过该类提供的方法来实现对隐藏信息的操作和访问。</li><li>隐藏对象的信息。</li><li>留出访问的对外接口。</li></ul><pre><code>public class Student implements Serializable {    private String name;    public String getName() {        return name;    }    public void setName(String name) {        this.name = name;    }}</code></pre><p><img src="/2020/08/01/2020-08-01-zhong-ji-yi-zhan/960a304e251f95ca446e530854b0ed3666095221.webp" alt></p><h3 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h3><p>继承就是子类继承父类的特征和行为，使得子类对象（实例）具有父类的实例域和方法。当然，如果在父类中拥有私有属性(private修饰)，则子类是不能被继承的。</p><ul><li>只支持单继承，即一个子类只允许有一个父类，但是可以实现多级继承，及子类拥有唯一的父类，而父类还可以再继承。<br>​- 子类可以拥有父类的属性和方法。<br>​- 子类可以拥有自己的属性和方法。<br>​- 子类可以重写覆盖父类的方法。</li></ul><h3 id="多态"><a href="#多态" class="headerlink" title="多态"></a>多态</h3><p>多态是同一个行为具有多个不同表现形式或形态的能力。</p><p>多态的体现形式</p><ul><li>继承</li><li>父类引用指向子类</li><li>重写</li></ul><pre><code>class MultiDemo {    public static void main(String[] args) {        // 多态的引用，就是向上转型        Animals dog = new Dog();        dog.eat();        Animals cat = new Cat();        cat.eat();        // 如果要调用父类中没有的方法，则要向下转型        Dog dogDown = (Dog)dog;        dogDown.watchDoor();    }}class Animals {    public void eat(){        System.out.println(&quot;动物吃饭！&quot;);    }}class Dog extends Animals{    public void eat(){        System.out.println(&quot;狗在吃骨头！&quot;);    }    public void watchDoor(){        System.out.println(&quot;狗看门！&quot;);    }}class Cat extends Animals{    public void eat(){        System.out.println(&quot;猫在吃鱼！&quot;);    }}</code></pre><hr><h2 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h2><ul><li>short类型占16个字节；2位；</li><li>int类型占32个字节；4位；</li><li>long类型占64个字节；8位；</li><li>float类型占32个字节；4位；</li><li>double类型占64个字节；8位；</li><li>char类型占16个字节；2位；</li><li>boolean类型占8个字节；1位；</li><li>byte类型占8个字节；1位。</li></ul><hr><h2 id="JDBC编码"><a href="#JDBC编码" class="headerlink" title="JDBC编码"></a>JDBC编码</h2><pre><code>public static void main(String[] args) throws ClassNotFoundException, SQLException {    //加载驱动    Class.forName(&quot;com.mysql.jdbc.Driver&quot;);  //固定写法 加载驱动    //用户信息和URL    //localhost:mysql默认端口号3306 我是由于改了mysql的端口号    String url=&quot;jdbc:mysql://localhost:3307/user?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=true&quot;;    String username=&quot;root&quot;;    String password=&quot;0000&quot;;//数据库密码    //连接成功，数据库对象  Connection代表数据库    Connection connection = DriverManager.getConnection(url, username, password);    //执行SQL的对象 statement执行SQL的对象    Statement statement=connection.createStatement();    //执行SQL的对象去执行SQL，可能存在返回结果，查看返回结果    String sql=&quot;SELECT * FROM tb_users&quot;;    String sql2=&quot;INSERT INTO tb_users(sname,pwd)  VALUES(&#39;zhangsan&#39;,&#39;123&#39;)&quot;;    int i = statement.executeUpdate(sql2);    System.out.println(&quot;受影响行数&quot;+i);    ResultSet resultSet = statement.executeQuery(sql);  // 返回的结果集    while(resultSet.next()){        //字段要和数据库的字段一致        System.out.println(&quot;id=&quot;+resultSet.getObject(&quot;id&quot;));        System.out.println(&quot;sname=&quot;+resultSet.getObject(&quot;sname&quot;));        System.out.println(&quot;pwd=&quot;+resultSet.getObject(&quot;pwd&quot;));        System.out.println(&quot;=====================================&quot;);    }    //释放连接    resultSet.close();    statement.close();    connection.close();}</code></pre><hr><h2 id="过滤器和拦截器"><a href="#过滤器和拦截器" class="headerlink" title="过滤器和拦截器"></a>过滤器和拦截器</h2><p>Filter过滤器是在Servlet规范定义的，是servlet容器支持的；而拦截器interceptor在spring容器内的，是spring框架支持的。</p><p><img src="/2020/08/01/2020-08-01-zhong-ji-yi-zhan/2371394-20211108170054822-2098663925.png" alt></p><p>过滤器是在请求进入容器后，但请求进入servlet之前进行预处理的。请求结束返回也是，是在servlet处理完后，返回给前端之前。过滤器包裹住servlet，servlet包裹住拦截器。</p><p>过滤器的触发时机是容器后，servlet之前，所以过滤器的doFilter(ServletRequest request, ServletResponse response, FilterChain chain)的入参是ServletRequest ，而不是httpservletrequest。因为过滤器是在httpservlet之前就起作用了。</p><p>拦截器preHandle：拦截器实现HandlerInterceptor 接口，控制器方法执行之前执行preHandle()，其boolean类型的返回值表示是否拦截或放行，返回true为放行，即调用控制器方法；返回false表示拦截，即不调用控制器方法</p><p>拦截器postHandle：控制器方法执行之后执行postHandle()</p><p>拦截器afterComplation：处理完视图和模型数据，渲染视图完毕之后执行afterComplation()</p><p><img src="/2020/08/01/2020-08-01-zhong-ji-yi-zhan/2371394-20211109111408221-407006331.png" alt></p><hr><h2 id="ZooKeeper的工作原理"><a href="#ZooKeeper的工作原理" class="headerlink" title="ZooKeeper的工作原理"></a>ZooKeeper的工作原理</h2><p>Zookeeper的核心是原子广播，这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab协议。Zab协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数Server完成了和 leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和Server具有相同的系统状态。</p><hr><h2 id="zookeeper提供了什么"><a href="#zookeeper提供了什么" class="headerlink" title="zookeeper提供了什么"></a>zookeeper提供了什么</h2><p>简单的说，zookeeper=文件系统+通知机制。</p><hr><h2 id="快速排除线上故障"><a href="#快速排除线上故障" class="headerlink" title="快速排除线上故障"></a>快速排除线上故障</h2><h3 id="CPU过高"><a href="#CPU过高" class="headerlink" title="CPU过高"></a>CPU过高</h3><p>当CPU过高的时候，接口性能会快速下降，同时监控也会开始报警。</p><ol><li>利用 top 命令查询CPU使用率最高的进程拿到PID</li><li>执行命令：printf “%x\n” PID ，将线程 PID 转化为 16 进制为tid:</li><li>执行命令 jstack PID | grep tid -A 30 找到线程堆栈，打印前后30行，可定位哪个类的哪一行代码</li></ol><h3 id="OOM"><a href="#OOM" class="headerlink" title="OOM"></a>OOM</h3><ol><li>下载dump文件</li><li>Eclipse Memory Analyzer(MAT)解析dump文件</li><li>点击查看内存对象统计详情，定位到对象和对应引用</li><li>查看shallow heap与retained heap能发现生成了大量的Object, 无法释放</li></ol>]]></content>
      
      
      <categories>
          
          <category> review </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TCP&amp;UDP</title>
      <link href="/2020/07/28/2020-07-28-tcp-udp/"/>
      <url>/2020/07/28/2020-07-28-tcp-udp/</url>
      
        <content type="html"><![CDATA[<h2 id="TCP-三次握手"><a href="#TCP-三次握手" class="headerlink" title="* TCP 三次握手"></a>* TCP 三次握手</h2><p>为了准确无误地把数据送达目标处，TCP协议采用了三次握手策略。</p><h3 id="TCP-三次握手漫画图解"><a href="#TCP-三次握手漫画图解" class="headerlink" title="TCP 三次握手漫画图解"></a>TCP 三次握手漫画图解</h3><p><img src="/2020/07/28/2020-07-28-tcp-udp/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B.png" alt></p><p><img src="/2020/07/28/2020-07-28-tcp-udp/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B2.png" alt></p><p>简单示意图：</p><ul><li>客户端–发送带有 SYN 标志的数据包–一次握手–服务端</li><li>服务端–发送带有 SYN/ACK 标志的数据包–二次握手–客户端</li><li>客户端–发送带有带有 ACK 标志的数据包–三次握手–服务端</li></ul><h3 id="为什么要三次握手"><a href="#为什么要三次握手" class="headerlink" title="为什么要三次握手"></a>为什么要三次握手</h3><p>三次握手的目的是建立可靠的通信信道，说到通讯，简单来说就是数据的发送与接收，而三次握手最主要的目的就是双方确认自己与对方的发送与接收是正常的。</p><p>第一次握手：Client 什么都不能确认；Server 确认了对方发送正常，自己接收正常</p><p>第二次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：对方发送正常，自己接收正常</p><p>第三次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己发送、接收正常，对方发送、接收正常</p><p>所以三次握手就能确认双发收发功能都正常，缺一不可。</p><h3 id="为什么要传回-SYN"><a href="#为什么要传回-SYN" class="headerlink" title="为什么要传回 SYN"></a>为什么要传回 SYN</h3><p>接收端传回发送端所发送的 SYN 是为了告诉发送端，我接收到的信息确实就是你所发送的信号了。</p><blockquote><p>SYN 是 TCP/IP 建立连接时使用的握手信号。在客户机和服务器之间建立正常的 TCP 网络连接时，客户机首先发出一个 SYN 消息，服务器使用 SYN-ACK 应答表示接收到了这个消息，最后客户机再以 ACK(Acknowledgement[汉译：确认字符 ,在数据通信传输中，接收站发给发送站的一种传输控制字符。它表示确认发来的数据已经接受无误。 ]）消息响应。这样在客户机和服务器之间才能建立起可靠的TCP连接，数据才可以在客户机和服务器之间传递。</p></blockquote><h3 id="传了-SYN-为啥还要传-ACK"><a href="#传了-SYN-为啥还要传-ACK" class="headerlink" title="传了 SYN,为啥还要传 ACK"></a>传了 SYN,为啥还要传 ACK</h3><p>双方通信无误必须是两者互相发送信息都无误。传了 SYN，证明发送方到接收方的通道没有问题，但是接收方到发送方的通道还需要 ACK 信号来进行验证。</p><h2 id="TCP四次挥手"><a href="#TCP四次挥手" class="headerlink" title="* TCP四次挥手"></a>* TCP四次挥手</h2><p><img src="/2020/07/28/2020-07-28-tcp-udp/TCP%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B.png" alt></p><p>断开一个 TCP 连接则需要“四次挥手”：</p><ul><li>客户端-发送一个 FIN，用来关闭客户端到服务器的数据传送</li><li>服务器-收到这个 FIN，它发回一 个 ACK，确认序号为收到的序号加1 。和 SYN 一样，一个 FIN 将占用一个序号</li><li>服务器-关闭与客户端的连接，发送一个FIN给客户端</li><li>客户端-发回 ACK 报文确认，并将确认序号设置为收到序号加1</li></ul><h3 id="为什么要四次挥手"><a href="#为什么要四次挥手" class="headerlink" title="为什么要四次挥手"></a>为什么要四次挥手</h3><p>任何一方都可以在数据传送结束后发出连接释放的通知，待对方确认后进入半关闭状态。当另一方也没有数据再发送的时候，则发出连接释放通知，对方确认后就完全关闭了TCP连接。</p><p>举个例子：A 和 B 打电话，通话即将结束后，A 说“我没啥要说的了”，B回答“我知道了”，但是 B 可能还会有要说的话，A 不能要求 B 跟着自己的节奏结束通话，于是 B 可能又巴拉巴拉说了一通，最后 B 说“我说完了”，A 回答“知道了”，这样通话才算结束。</p><h2 id="TCP-UDP-协议的区别"><a href="#TCP-UDP-协议的区别" class="headerlink" title="* TCP,UDP 协议的区别"></a>* TCP,UDP 协议的区别</h2><p><img src="/2020/07/28/2020-07-28-tcp-udp/tcp-vs-udp.jpg" alt></p><p>UDP 在传送数据之前不需要先建立连接，远地主机在收到 UDP 报文后，不需要给出任何确认。虽然 UDP 不提供可靠交付，但在某些情况下 UDP 确是一种最有效的工作方式（一般用于即时通信），比如： QQ 语音、 QQ 视频 、直播等等</p><p>TCP 提供面向连接的服务。在传送数据之前必须先建立连接，数据传送结束后要释放连接。 TCP 不提供广播或多播服务。由于 TCP 要提供可靠的，面向连接的传输服务（TCP的可靠体现在TCP在传递数据之前，会有三次握手来建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制，在数据传完后，还会断开连接用来节约系统资源），这一难以避免增加了许多开销，如确认，流量控制，计时器以及连接管理等。这不仅使协议数据单元的首部增大很多，还要占用许多处理机资源。TCP 一般用于文件传输、发送和接收邮件、远程登录等场景。</p><h2 id="OSI与TCP-IP各层的结构与功能-都有哪些协议"><a href="#OSI与TCP-IP各层的结构与功能-都有哪些协议" class="headerlink" title="* OSI与TCP/IP各层的结构与功能,都有哪些协议?"></a>* OSI与TCP/IP各层的结构与功能,都有哪些协议?</h2><p><img src="/2020/07/28/2020-07-28-tcp-udp/%E4%BA%94%E5%B1%82%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84.png" alt></p><p>物理层-&gt;数据链路层-&gt;网络层-&gt;运输层-&gt;会话层-&gt;表示层-&gt;应用层</p><h2 id="HTTP-和-HTTPS-的区别"><a href="#HTTP-和-HTTPS-的区别" class="headerlink" title="* HTTP 和 HTTPS 的区别"></a>* HTTP 和 HTTPS 的区别</h2><ul><li>端口 ：HTTP的URL由“http://”起始且默认使用端口80，而HTTPS的URL由“https://”起始且默认使用端口443。</li><li>安全性和资源消耗： HTTP协议运行在TCP之上，所有传输的内容都是明文，客户端和服务器端都无法验证对方的身份。HTTPS是运行在SSL/TLS之上的HTTP协议，SSL/TLS 运行在TCP之上。所有传输的内容都经过加密，加密采用对称加密，但对称加密的密钥用服务器方的证书进行了非对称加密。所以说，HTTP 安全性没有 HTTPS高，但是 HTTPS 比HTTP耗费更多服务器资源。</li></ul><h3 id="对称加密和非对称加密"><a href="#对称加密和非对称加密" class="headerlink" title="对称加密和非对称加密"></a>对称加密和非对称加密</h3><ul><li>对称加密：密钥只有一个，加密解密为同一个密码，且加解密速度快，典型的对称加密算法有DES、AES等；</li><li>非对称加密：密钥成对出现（且根据公钥无法推知私钥，根据私钥也无法推知公钥），加密解密使用不同密钥（公钥加密需要私钥解密，私钥加密需要公钥解密），相对对称加密速度较慢，典型的非对称加密算法有RSA、DSA等。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TCP&amp;UDP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分库分表&amp;读写分离</title>
      <link href="/2020/07/27/2020-07-27-xiao-li-fei-dao-fen-ku-fen-biao-du-xie-fen-chi/"/>
      <url>/2020/07/27/2020-07-27-xiao-li-fei-dao-fen-ku-fen-biao-du-xie-fen-chi/</url>
      
        <content type="html"><![CDATA[<h2 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h2><p>为什么要分库分表（设计高并发系统的时候，数据库层面该如何设计）？用过哪些分库分表中间件？不同的分库分表中间件都有什么优点和缺点？你们具体是如何对数据库如何进行垂直拆分或水平拆分的？</p><h3 id="面试官心理分析"><a href="#面试官心理分析" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>其实这块肯定是扯到高并发了，因为分库分表一定是为了支撑高并发、数据量大两个问题的。而且现在说实话，尤其是互联网类的公司面试，基本上都会来这么一下，分库分表如此普遍的技术问题，不问实在是不行，而如果你不知道那也实在是说不过去！</p><h2 id="为什么要分库分表？（设计高并发系统的时候，数据库层面该如何设计？）"><a href="#为什么要分库分表？（设计高并发系统的时候，数据库层面该如何设计？）" class="headerlink" title="* 为什么要分库分表？（设计高并发系统的时候，数据库层面该如何设计？）"></a>* 为什么要分库分表？（设计高并发系统的时候，数据库层面该如何设计？）</h2>]]></content>
      
      
      <categories>
          
          <category> 小李飞刀 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>海量数据处理</title>
      <link href="/2020/07/26/2020-07-26-xiao-li-fei-dao-hai-liang-shu-ju-chu-li/"/>
      <url>/2020/07/26/2020-07-26-xiao-li-fei-dao-hai-liang-shu-ju-chu-li/</url>
      
        <content type="html"><![CDATA[<h2 id="如何从大量的-URL-中找出相同的-URL？"><a href="#如何从大量的-URL-中找出相同的-URL？" class="headerlink" title="* 如何从大量的 URL 中找出相同的 URL？"></a>* 如何从大量的 URL 中找出相同的 URL？</h2><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>给定 a、b 两个文件，各存放 50 亿个 URL，每个 URL 各占 64B，内存限制是 4G。请找出 a、b 两个文件共同的 URL。</p><h3 id="解答思路"><a href="#解答思路" class="headerlink" title="解答思路"></a>解答思路</h3><p>每个 URL 占 64B，那么 50 亿个 URL占用的空间大小约为 320GB。</p><blockquote><p>5, 000, 000, 000 * 64B ≈ 5GB * 64 = 320GB</p></blockquote><p>由于内存大小只有 4G，因此，我们不可能一次性把所有 URL 加载到内存中处理。对于这种类型的题目，一般采用分治策略，即：把一个文件中的 URL 按照某个特征划分为多个小文件，使得每个小文件大小不超过 4G，这样就可以把这个小文件读到内存中进行处理了。</p><p>思路如下：</p><p>首先遍历文件 a，对遍历到的 URL 求 hash(URL) % 1000 ，根据计算结果把遍历到的 URL 存储到 a0, a1, a2, …, a999，这样每个大小约为 300MB。使用同样的方法遍历文件 b，把文件 b 中的 URL 分别存储到文件 b0, b1, b2, …, b999 中。这样处理过后，所有可能相同的 URL 都在对应的小文件中，即 a0 对应 b0, …, a999 对应 b999，不对应的小文件不可能有相同的 URL。那么接下来，我们只需要求出这 1000 对小文件中相同的 URL 就好了。</p><p>接着遍历 ai( i∈[0,999] )，把 URL 存储到一个 HashSet 集合中。然后遍历 bi 中每个 URL，看在 HashSet 集合中是否存在，若存在，说明这就是共同的 URL，可以把这个 URL 保存到一个单独的文件中。</p><p>方法总结:</p><ul><li>分而治之，进行哈希取余；</li><li>对每个子文件进行 HashSet 统计。</li></ul><hr><h2 id="如何从大量数据中找出高频词？"><a href="#如何从大量数据中找出高频词？" class="headerlink" title="* 如何从大量数据中找出高频词？"></a>* 如何从大量数据中找出高频词？</h2><h3 id="题目描述-1"><a href="#题目描述-1" class="headerlink" title="题目描述"></a>题目描述</h3><p>有一个 1GB 大小的文件，文件里每一行是一个词，每个词的大小不超过 16B，内存大小限制是 1MB，要求返回频数最高的 100 个词(Top 100)。</p><h3 id="解答思路-1"><a href="#解答思路-1" class="headerlink" title="解答思路"></a>解答思路</h3><p>由于内存限制，我们依然无法直接将大文件的所有词一次读到内存中。因此，同样可以采用分治策略，把一个大文件分解成多个小文件，保证每个文件的大小小于 1MB，进而直接将单个小文件读取到内存中进行处理。</p><h3 id="思路如下："><a href="#思路如下：" class="headerlink" title="思路如下："></a>思路如下：</h3><p>首先遍历大文件，对遍历到的每个词x，执行 hash(x) % 5000 ，将结果为 i 的词存放到文件 ai 中。遍历结束后，我们可以得到 5000 个小文件。每个小文件的大小为 200KB 左右。如果有的小文件大小仍然超过 1MB，则采用同样的方式继续进行分解。</p><p>接着统计每个小文件中出现频数最高的 100 个词。最简单的方式是使用 HashMap 来实现。其中 key 为词，value 为该词出现的频率。具体方法是：对于遍历到的词 x，如果在 map 中不存在，则执行 map.put(x, 1) ；若存在，则执行 map.put(x, map.get(x)+1) ，将该词频数加 1。</p><p>上面我们统计了每个小文件单词出现的频数。接下来，我们可以通过维护一个小顶堆来找出所有词中出现频数最高的 100 个。具体方法是：依次遍历每个小文件，构建一个小顶堆，堆大小为 100。如果遍历到的词的出现次数大于堆顶词的出现次数，则用新词替换堆顶的词，然后重新调整为小顶堆，遍历结束后，小顶堆上的词就是出现频数最高的 100 个词。</p><h3 id="方法总结"><a href="#方法总结" class="headerlink" title="方法总结"></a>方法总结</h3><ol><li>分而治之，进行哈希取余；</li><li>使用 HashMap 统计频数；</li><li>求解最大的 TopN 个，用小顶堆；求解最小的 TopN 个，用大顶堆。</li></ol><hr><h2 id="如何找出某一天访问百度网站最多的-IP？"><a href="#如何找出某一天访问百度网站最多的-IP？" class="headerlink" title="* 如何找出某一天访问百度网站最多的 IP？"></a>* 如何找出某一天访问百度网站最多的 IP？</h2><h3 id="题目描述-2"><a href="#题目描述-2" class="headerlink" title="题目描述"></a>题目描述</h3><p>现有海量日志数据保存在一个超大文件中，该文件无法直接读入内存，要求从中提取某天访问百度次数最多的那个 IP。</p><h3 id="解答思路-2"><a href="#解答思路-2" class="headerlink" title="解答思路"></a>解答思路</h3><p>这道题只关心某一天访问百度最多的 IP，因此，可以首先对文件进行一次遍历，把这一天访问百度 IP 的相关信息记录到一个单独的大文件中。接下来采用的方法与上一题一样，大致就是先对 IP 进行哈希映射，接着使用 HashMap 统计重复 IP 的次数，最后计算出重复次数最多的 IP。</p><blockquote><p>注：这里只需要找出出现次数最多的 IP，可以不必使用堆，直接用一个变量 max 即可。</p></blockquote><h3 id="方法总结-1"><a href="#方法总结-1" class="headerlink" title="方法总结"></a>方法总结</h3><ol><li>分而治之，进行哈希取余；</li><li>使用 HashMap 统计频数。</li></ol><hr><h2 id="如何在大量的数据中找出不重复的整数？"><a href="#如何在大量的数据中找出不重复的整数？" class="headerlink" title="* 如何在大量的数据中找出不重复的整数？"></a>* 如何在大量的数据中找出不重复的整数？</h2><h3 id="题目描述-3"><a href="#题目描述-3" class="headerlink" title="题目描述"></a>题目描述</h3><p>在 2.5 亿个整数中找出不重复的整数。注意：内存不足以容纳这 2.5 亿个整数。</p><h3 id="解答思路-3"><a href="#解答思路-3" class="headerlink" title="解答思路"></a>解答思路</h3><h4 id="方法一：分治法"><a href="#方法一：分治法" class="headerlink" title="方法一：分治法"></a>方法一：分治法</h4><p>与前面的题目方法类似，先将 2.5 亿个数划分到多个小文件，用 HashSet/HashMap 找出每个小文件中不重复的整数，再合并每个子结果，即为最终结果。</p><h4 id="方法二：位图法"><a href="#方法二：位图法" class="headerlink" title="方法二：位图法"></a>方法二：位图法</h4><p>位图，就是用一个或多个 bit 来标记某个元素对应的值，而键就是该元素。采用位作为单位来存储数据，可以大大节省存储空间。</p><p>位图通过使用位数组来表示某些元素是否存在。它可以用于快速查找，判重，排序等。不是很清楚？我先举个小例子。</p><p>假设我们要对 [0,7] 中的 5 个元素 (6, 4, 2, 1, 5) 进行排序，可以采用位图法。0~7 范围总共有 8 个数，只需要 8bit，即 1 个字节。首先将每个位都置 0：</p><pre><code>0 0 0 0 0 0 0 0</code></pre><p>然后遍历 5 个元素，首先遇到 6，那么将下标为 6 的位的 0 置为 1；接着遇到 4，把下标为 4 的位 的 0 置为 1：</p><pre><code>0 0 0 0 1 0 1 0</code></pre><p>依次遍历，结束后，位数组是这样的：</p><pre><code>0 1 1 0 1 1 1 0</code></pre><p>每个为 1 的位，它的下标都表示了一个数：</p><pre><code>for i in range(8):    if bits[i] == 1:        print(i)</code></pre><p>这样我们其实就已经实现了排序。</p><p>对于整数相关的算法的求解，位图法是一种非常实用的算法。假设 int 整数占用 4B，即 32bit，那么我们可以表示的整数的个数为 232。</p><p>那么对于这道题，我们用 2 个 bit 来表示各个数字的状态：</p><ul><li>00 表示这个数字没出现过；</li><li>01 表示这个数字出现过一次（即为题目所找的不重复整数）；</li><li>10 表示这个数字出现了多次。</li></ul><p>那么这 2^32 个整数，总共所需内存为 2^32*2b=1GB。因此，当可用内存超过 1GB 时，可以采用位图法。假设内存满足位图法需求，进行下面的操作：</p><p>遍历 2.5 亿个整数，查看位图中对应的位，如果是 00，则变为 01，如果是 01 则变为 10，如果是 10 则保持不变。遍历结束后，查看位图，把对应位是 01 的整数输出即可。</p><h3 id="方法总结-2"><a href="#方法总结-2" class="headerlink" title="方法总结"></a>方法总结</h3><p>判断数字是否重复的问题，位图法是一种非常高效的方法。</p><hr><h2 id="如何在大量的数据中判断一个数是否存在？"><a href="#如何在大量的数据中判断一个数是否存在？" class="headerlink" title="* 如何在大量的数据中判断一个数是否存在？"></a>* 如何在大量的数据中判断一个数是否存在？</h2><h3 id="题目描述-4"><a href="#题目描述-4" class="headerlink" title="题目描述"></a>题目描述</h3><p>给定 40 亿个不重复的没排过序的 unsigned int 型整数，然后再给定一个数，如何快速判断这个数是否在这 40 亿个整数当中？</p><h3 id="解答思路-4"><a href="#解答思路-4" class="headerlink" title="解答思路"></a>解答思路</h3><h4 id="方法一：分治法-1"><a href="#方法一：分治法-1" class="headerlink" title="方法一：分治法"></a>方法一：分治法</h4><p>依然可以用分治法解决，方法与前面类似，就不再次赘述了。</p><h4 id="方法二：位图法-1"><a href="#方法二：位图法-1" class="headerlink" title="方法二：位图法"></a>方法二：位图法</h4><p>40 亿个不重复整数，我们用 40 亿个 bit 来表示，初始位均为 0，那么总共需要内存：4, 000, 000, 000b≈512M。</p><p>我们读取这 40 亿个整数，将对应的 bit 设置为 1。接着读取要查询的数，查看相应位是否为 1，如果为 1 表示存在，如果为 0 表示不存在。</p><h3 id="方法总结-3"><a href="#方法总结-3" class="headerlink" title="方法总结"></a>方法总结</h3><p>判断数字是否存在、判断数字是否重复的问题，位图法是一种非常高效的方法。</p><hr><h2 id="如何查询最热门的查询串？"><a href="#如何查询最热门的查询串？" class="headerlink" title="* 如何查询最热门的查询串？"></a>* 如何查询最热门的查询串？</h2><h3 id="题目描述-5"><a href="#题目描述-5" class="headerlink" title="题目描述"></a>题目描述</h3><p>搜索引擎会通过日志文件把用户每次检索使用的所有查询串都记录下来，每个查询串的长度不超过 255 字节。</p><p>假设目前有 1000w 个记录（这些查询串的重复度比较高，虽然总数是 1000w，但如果除去重复后，则不超过 300w 个）。请统计最热门的 10 个查询串，要求使用的内存不能超过 1G。（一个查询串的重复度越高，说明查询它的用户越多，也就越热门。）</p><h3 id="解答思路-5"><a href="#解答思路-5" class="headerlink" title="解答思路"></a>解答思路</h3><p>每个查询串最长为 255B，1000w 个串需要占用 约 2.55G 内存，因此，我们无法将所有字符串全部读入到内存中处理。</p><h4 id="方法一：分治法-2"><a href="#方法一：分治法-2" class="headerlink" title="方法一：分治法"></a>方法一：分治法</h4><p>分治法依然是一个非常实用的方法。</p><p>划分为多个小文件，保证单个小文件中的字符串能被直接加载到内存中处理，然后求出每个文件中出现次数最多的 10 个字符串；最后通过一个小顶堆统计出所有文件中出现最多的 10 个字符串。</p><p>方法可行，但不是最好，下面介绍其他方法。</p><h4 id="方法二：HashMap-法"><a href="#方法二：HashMap-法" class="headerlink" title="方法二：HashMap 法"></a>方法二：HashMap 法</h4><p>虽然字符串总数比较多，但去重后不超过 300w，因此，可以考虑把所有字符串及出现次数保存在一个 HashMap 中，所占用的空间为 300w*(255+4)≈777M（其中，4表示整数占用的4个字节）。由此可见，1G 的内存空间完全够用。</p><p>思路如下：</p><p>首先，遍历字符串，若不在 map 中，直接存入 map，value 记为 1；若在 map 中，则把对应的 value 加 1，这一步时间复杂度 O(N) 。</p><p>接着遍历 map，构建一个 10 个元素的小顶堆，若遍历到的字符串的出现次数大于堆顶字符串的出现次数，则进行替换，并将堆调整为小顶堆。</p><p>遍历结束后，堆中 10 个字符串就是出现次数最多的字符串。这一步时间复杂度 O(Nlog10) 。</p><h4 id="方法三：前缀树法"><a href="#方法三：前缀树法" class="headerlink" title="方法三：前缀树法"></a>方法三：前缀树法</h4><p>方法二使用了 HashMap 来统计次数，当这些字符串有大量相同前缀时，可以考虑使用前缀树来统计字符串出现的次数，树的结点保存字符串出现次数，0 表示没有出现。</p><p>思路如下：</p><p>在遍历字符串时，在前缀树中查找，如果找到，则把结点中保存的字符串次数加 1，否则为这个字符串构建新结点，构建完成后把叶子结点中字符串的出现次数置为 1。</p><p>最后依然使用小顶堆来对字符串的出现次数进行排序。</p><h3 id="方法总结-4"><a href="#方法总结-4" class="headerlink" title="方法总结"></a>方法总结</h3><p>前缀树经常被用来统计字符串的出现次数。它的另外一个大的用途是字符串查找，判断是否有重复的字符串等。</p><hr><h2 id="如何统计不同电话号码的个数？"><a href="#如何统计不同电话号码的个数？" class="headerlink" title="* 如何统计不同电话号码的个数？"></a>* 如何统计不同电话号码的个数？</h2><h3 id="题目描述-6"><a href="#题目描述-6" class="headerlink" title="题目描述"></a>题目描述</h3><p>已知某个文件内包含一些电话号码，每个号码为 8 位数字，统计不同号码的个数。</p><h3 id="解答思路-6"><a href="#解答思路-6" class="headerlink" title="解答思路"></a>解答思路</h3><p>这道题本质还是求解数据重复的问题，对于这类问题，一般首先考虑位图法。</p><p>对于本题，8 位电话号码可以表示的号码个数为 108 个，即 1 亿个。我们每个号码用一个 bit 来表示，则总共需要 1 亿个 bit，内存占用约 100M。</p><p>思路如下：</p><p>申请一个位图数组，长度为 1 亿，初始化为 0。然后遍历所有电话号码，把号码对应的位图中的位置置为 1。遍历完成后，如果 bit 为 1，则表示这个电话号码在文件中存在，否则不存在。bit 值为 1 的数量即为 不同电话号码的个数。</p><h3 id="方法总结-5"><a href="#方法总结-5" class="headerlink" title="方法总结"></a>方法总结</h3><p>求解数据重复问题，记得考虑位图法。</p><hr><h2 id="如何从-5-亿个数中找出中位数？"><a href="#如何从-5-亿个数中找出中位数？" class="headerlink" title="* 如何从 5 亿个数中找出中位数？"></a>* 如何从 5 亿个数中找出中位数？</h2><h3 id="题目描述-7"><a href="#题目描述-7" class="headerlink" title="题目描述"></a>题目描述</h3><p>从 5 亿个数中找出中位数。数据排序后，位置在最中间的数就是中位数。当样本数为奇数时，中位数为 第 (N+1)/2 个数；当样本数为偶数时，中位数为 第 N/2 个数与第 1+N/2 个数的均值。</p><h3 id="解答思路-7"><a href="#解答思路-7" class="headerlink" title="解答思路"></a>解答思路</h3><p>如果这道题没有内存大小限制，则可以把所有数读到内存中排序后找出中位数。但是最好的排序算法的时间复杂度都为 O(NlogN) 。这里使用其他方法。</p><h4 id="方法一：双堆法"><a href="#方法一：双堆法" class="headerlink" title="方法一：双堆法"></a>方法一：双堆法</h4><p>维护两个堆，一个大顶堆，一个小顶堆。大顶堆中最大的数小于等于小顶堆中最小的数；保证这两个堆中的元素个数的差不超过 1。</p><p>若数据总数为偶数，当这两个堆建好之后，中位数就是这两个堆顶元素的平均值。当数据总数为奇数时，根据两个堆的大小，中位数一定在数据多的堆的堆顶。</p><pre><code>class MedianFinder {    private PriorityQueue&lt;Integer&gt; maxHeap;    private PriorityQueue&lt;Integer&gt; minHeap;    /** initialize your data structure here. */    public MedianFinder() {        maxHeap = new PriorityQueue&lt;&gt;(Comparator.reverseOrder());        minHeap = new PriorityQueue&lt;&gt;(Integer::compareTo);    }    public void addNum(int num) {        if (maxHeap.isEmpty() || maxHeap.peek() &gt; num) {            maxHeap.offer(num);        } else {            minHeap.offer(num);        }        int size1 = maxHeap.size();        int size2 = minHeap.size();        if (size1 - size2 &gt; 1) {            minHeap.offer(maxHeap.poll());        } else if (size2 - size1 &gt; 1) {            maxHeap.offer(minHeap.poll());        }    }    public double findMedian() {        int size1 = maxHeap.size();        int size2 = minHeap.size();        return size1 == size2             ? (maxHeap.peek() + minHeap.peek()) * 1.0 / 2            : (size1 &gt; size2 ? maxHeap.peek() : minHeap.peek());    }}</code></pre><p>以上这种方法，需要把所有数据都加载到内存中。当数据量很大时，就不能这样了，因此，这种方法适用于数据量较小的情况。5 亿个数，每个数字占用 4B，总共需要 2G 内存。如果可用内存不足 2G，就不能使用这种方法了，下面介绍另一种方法。</p><h4 id="方法二：分治法"><a href="#方法二：分治法" class="headerlink" title="方法二：分治法"></a>方法二：分治法</h4><p>分治法的思想是把一个大的问题逐渐转换为规模较小的问题来求解。</p><p>对于这道题，顺序读取这 5 亿个数字，对于读取到的数字 num，如果它对应的二进制中最高位为 1，则把这个数字写到 f1 中，否则写入 f0 中。通过这一步，可以把这 5 亿个数划分为两部分，而且 f0 中的数都大于 f1 中的数（最高位是符号位）。</p><p>划分之后，可以非常容易地知道中位数是在 f0 还是 f1 中。假设 f1 中有 1 亿个数，那么中位数一定在 f0 中，且是在 f0 中，从小到大排列的第 1.5 亿个数与它后面的一个数的平均值。</p><blockquote><p>提示，5 亿数的中位数是第 2.5 亿与右边相邻一个数求平均值。若 f1 有一亿个数，那么中位数就是 f0 中从第 1.5 亿个数开始的两个数求得的平均值。</p></blockquote><p>对于 f0 可以用次高位的二进制继续将文件一分为二，如此划分下去，直到划分后的文件可以被加载到内存中，把数据加载到内存中以后直接排序，找出中位数。</p><blockquote><p>注意，当数据总数为偶数，如果划分后两个文件中的数据有相同个数，那么中位数就是数据较小的文件中的最大值与数据较大的文件中的最小值的平均值。</p></blockquote><h3 id="方法总结-6"><a href="#方法总结-6" class="headerlink" title="方法总结"></a>方法总结</h3><p>分治法，真香！</p><hr><h2 id="如何按照-query-的频度排序？"><a href="#如何按照-query-的频度排序？" class="headerlink" title="* 如何按照 query 的频度排序？"></a>* 如何按照 query 的频度排序？</h2><h3 id="题目描述-8"><a href="#题目描述-8" class="headerlink" title="题目描述"></a>题目描述</h3><p>有 10 个文件，每个文件大小为 1G，每个文件的每一行存放的都是用户的 query，每个文件的 query 都可能重复。要求按照 query 的频度排序。</p><h3 id="解答思路-8"><a href="#解答思路-8" class="headerlink" title="解答思路"></a>解答思路</h3><p>如果 query 的重复度比较大，可以考虑一次性把所有 query 读入内存中处理；如果 query 的重复率不高，那么可用内存不足以容纳所有的 query，这时候就需要采用分治法或其他的方法来解决。</p><h4 id="方法一：HashMap-法"><a href="#方法一：HashMap-法" class="headerlink" title="方法一：HashMap 法"></a>方法一：HashMap 法</h4><p>如果 query 重复率高，说明不同 query 总数比较小，可以考虑把所有的 query 都加载到内存中的 HashMap 中。接着就可以按照 query 出现的次数进行排序。</p><h4 id="方法二：分治法-1"><a href="#方法二：分治法-1" class="headerlink" title="方法二：分治法"></a>方法二：分治法</h4><p>分治法需要根据数据量大小以及可用内存的大小来确定问题划分的规模。对于这道题，可以顺序遍历 10 个文件中的 query，通过 Hash 函数 hash(query) % 10 把这些 query 划分到 10 个小文件中。之后对每个小文件使用 HashMap 统计 query 出现次数，根据次数排序并写入到零外一个单独文件中。</p><p>接着对所有文件按照 query 的次数进行排序，这里可以使用归并排序（由于无法把所有 query 都读入内存，因此需要使用外排序）。</p><h3 id="方法总结-7"><a href="#方法总结-7" class="headerlink" title="方法总结"></a>方法总结</h3><ul><li>内存若够，直接读入进行排序；</li><li>内存不够，先划分为小文件，小文件排好序后，整理使用外排序进行归并。</li></ul><hr><h2 id="如何找出排名前-500-的数？"><a href="#如何找出排名前-500-的数？" class="headerlink" title="* 如何找出排名前 500 的数？"></a>* 如何找出排名前 500 的数？</h2><h3 id="题目描述-9"><a href="#题目描述-9" class="headerlink" title="题目描述"></a>题目描述</h3><p>有 20 个数组，每个数组有 500 个元素，并且有序排列。如何在这 20*500 个数中找出前 500 的数？</p><h3 id="解答思路-9"><a href="#解答思路-9" class="headerlink" title="解答思路"></a>解答思路</h3><p>对于 TopK 问题，最常用的方法是使用堆排序。对本题而言，假设数组降序排列，可以采用以下方法：</p><p>首先建立大顶堆，堆的大小为数组的个数，即为 20，把每个数组最大的值存到堆中。</p><p>接着删除堆顶元素，保存到另一个大小为 500 的数组中，然后向大顶堆插入删除的元素所在数组的下一个元素。</p><p>重复上面的步骤，直到删除完第 500 个元素，也即找出了最大的前 500 个数。</p><blockquote><p>为了在堆中取出一个数据后，能知道它是从哪个数组中取出的，从而可以从这个数组中取下一个值，可以把数组的指针存放到堆中，对这个指针提供比较大小的方法。</p></blockquote><pre><code>import lombok.Data;import java.util.Arrays;import java.util.PriorityQueue;/** * @author https://github.com/yanglbme */@Datapublic class DataWithSource implements Comparable&lt;DataWithSource&gt; {    /**     * 数值     */    private int value;    /**     * 记录数值来源的数组     */    private int source;    /**     * 记录数值在数组中的索引     */    private int index;    public DataWithSource(int value, int source, int index) {        this.value = value;        this.source = source;        this.index = index;    }    /**     *     * 由于 PriorityQueue 使用小顶堆来实现，这里通过修改     * 两个整数的比较逻辑来让 PriorityQueue 变成大顶堆     */    @Override    public int compareTo(DataWithSource o) {        return Integer.compare(o.getValue(), this.value);    }}class Test {    public static int[] getTop(int[][] data) {        int rowSize = data.length;        int columnSize = data[0].length;        // 创建一个columnSize大小的数组，存放结果        int[] result = new int[columnSize];        PriorityQueue&lt;DataWithSource&gt; maxHeap = new PriorityQueue&lt;&gt;();        for (int i = 0; i &lt; rowSize; ++i) {            // 将每个数组的最大一个元素放入堆中            DataWithSource d = new DataWithSource(data[i][0], i, 0);            maxHeap.add(d);        }        int num = 0;        while (num &lt; columnSize) {            // 删除堆顶元素            DataWithSource d = maxHeap.poll();            result[num++] = d.getValue();            if (num &gt;= columnSize) {                break;            }            d.setValue(data[d.getSource()][d.getIndex() + 1]);            d.setIndex(d.getIndex() + 1);            maxHeap.add(d);        }        return result;    }    public static void main(String[] args) {        int[][] data = {                {29, 17, 14, 2, 1},                {19, 17, 16, 15, 6},                {30, 25, 20, 14, 5},        };        int[] top = getTop(data);        System.out.println(Arrays.toString(top)); // [30, 29, 25, 20, 19]    }}</code></pre><h3 id="方法总结-8"><a href="#方法总结-8" class="headerlink" title="方法总结"></a>方法总结</h3><p>求 TopK，不妨考虑一下堆排序？</p><hr><h2 id="总结"><a href="#总结" class="headerlink" title="* 总结"></a>* 总结</h2><ul><li>分治法：hash取值，hashMap计数</li><li>位图法</li></ul>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 海量数据处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring Cloud</title>
      <link href="/2020/07/25/2020-07-25-springcloud/"/>
      <url>/2020/07/25/2020-07-25-springcloud/</url>
      
        <content type="html"><![CDATA[<h2 id="一、业务场景介绍"><a href="#一、业务场景介绍" class="headerlink" title="一、业务场景介绍"></a>一、业务场景介绍</h2><p>先来给大家说一个业务场景，假设咱们现在开发一个电商网站，要实现支付订单的功能，流程如下：</p><ul><li>创建一个订单后，如果用户立刻支付了这个订单，我们需要将订单状态更新为“已支付”</li><li>扣减相应的商品库存</li><li>通知仓储中心，进行发货</li><li>给用户的这次购物增加相应的积分</li></ul><p>针对上述流程，我们需要有订单服务、库存服务、仓储服务、积分服务。整个流程的大体思路如下：</p><ul><li>用户针对一个订单完成支付之后，就会去找订单服务，更新订单状态</li><li>订单服务调用库存服务，完成相应功能</li><li>订单服务调用仓储服务，完成相应功能</li><li>订单服务调用积分服务，完成相应功能</li></ul><p>至此，整个支付订单的业务流程结束</p><p>下图这张图，清晰表明了各服务间的调用过程：</p><p><img src="/2020/07/25/2020-07-25-springcloud/166ebffb48c481a3.png" alt></p><p>好！有了业务场景之后，咱们就一起来看看Spring Cloud微服务架构中，这几个组件如何相互协作，各自发挥的作用以及其背后的原理。</p><h2 id="二、Spring-Cloud核心组件：Eureka"><a href="#二、Spring-Cloud核心组件：Eureka" class="headerlink" title="二、Spring Cloud核心组件：Eureka"></a>二、Spring Cloud核心组件：Eureka</h2><p>咱们来考虑第一个问题：订单服务想要调用库存服务、仓储服务，或者积分服务，怎么调用？</p><ul><li>订单服务压根儿就不知道人家库存服务在哪台机器上啊！他就算想要发起一个请求，都不知道发送给谁，有心无力！</li><li>这时候，就轮到Spring Cloud Eureka出场了。Eureka是微服务架构中的注册中心，专门负责服务的注册与发现。</li></ul><p>咱们来看看下面的这张图，结合图来仔细剖析一下整个流程：</p><p><img src="/2020/07/25/2020-07-25-springcloud/166ebffcb7ce31b8.png" alt></p><p>如上图所示，库存服务、仓储服务、积分服务中都有一个Eureka Client组件，这个组件专门负责将这个服务的信息注册到Eureka Server中。说白了，就是告诉Eureka Server，自己在哪台机器上，监听着哪个端口。而Eureka Server是一个注册中心，里面有一个注册表，保存了各服务所在的机器和端口号</p><p>订单服务里也有一个Eureka Client组件，这个Eureka Client组件会找Eureka Server问一下：库存服务在哪台机器啊？监听着哪个端口啊？仓储服务呢？积分服务呢？然后就可以把这些相关信息从Eureka Server的注册表中拉取到自己本地缓存起来。</p><p>这时如果订单服务想要调用库存服务，不就可以找自己本地的Eureka Client问一下库存服务在哪台机器？监听哪个端口吗？收到响应后，紧接着就可以发送一个请求过去，调用库存服务扣减库存的那个接口！同理，如果订单服务要调用仓储服务、积分服务，也是如法炮制。</p><p>总结一下：</p><ul><li>Eureka Client：负责将这个服务的信息注册到Eureka Server中</li><li>Eureka Server：注册中心，里面有一个注册表，保存了各个服务所在的机器和端口号</li></ul><h2 id="三、Spring-Cloud核心组件：Feign"><a href="#三、Spring-Cloud核心组件：Feign" class="headerlink" title="三、Spring Cloud核心组件：Feign"></a>三、Spring Cloud核心组件：Feign</h2><p>现在订单服务确实知道库存服务、积分服务、仓库服务在哪里了，同时也监听着哪些端口号了。但是新问题又来了：难道订单服务要自己写一大堆代码，跟其他服务建立网络连接，然后构造一个复杂的请求，接着发送请求过去，最后对返回的响应结果再写一大堆代码来处理吗？</p><p>这是上述流程翻译的代码片段，咱们一起来看看，体会一下这种绝望而无助的感受！！！</p><p>友情提示，前方高能：</p><p><img src="/2020/07/25/2020-07-25-springcloud/166ebced960f2024.png" alt></p><p>看完上面那一大段代码，有没有感到后背发凉、一身冷汗？实际上你进行服务间调用时，如果每次都手写代码，代码量比上面那段要多至少几倍，所以这个事压根儿就不是地球人能干的。</p><p>既然如此，那怎么办呢？别急，Feign早已为我们提供好了优雅的解决方案。来看看如果用Feign的话，你的订单服务调用库存服务的代码会变成啥样？</p><p><img src="/2020/07/25/2020-07-25-springcloud/166ebcf01b773dd4.png" alt></p><p>看完上面的代码什么感觉？是不是感觉整个世界都干净了，又找到了活下去的勇气！没有底层的建立连接、构造请求、解析响应的代码，直接就是用注解定义一个 FeignClient接口，然后调用那个接口就可以了。人家Feign Client会在底层根据你的注解，跟你指定的服务建立连接、构造请求、发起靕求、获取响应、解析响应，等等。这一系列脏活累活，人家Feign全给你干了。</p><p>那么问题来了，Feign是如何做到这么神奇的呢？很简单，Feign的一个关键机制就是使用了动态代理。咱们一起来看看下面的图，结合图来分析：</p><ul><li>首先，如果你对某个接口定义了@FeignClient注解，Feign就会针对这个接口创建一个动态代理</li><li>接着你要是调用那个接口，本质就是会调用 Feign创建的动态代理，这是核心中的核心</li><li>Feign的动态代理会根据你在接口上的@RequestMapping等注解，来动态构造出你要请求的服务的地址</li><li>最后针对这个地址，发起请求、解析响应</li></ul><p><img src="/2020/07/25/2020-07-25-springcloud/166ebfff505b2a20.png" alt></p><h2 id="四、Spring-Cloud核心组件：Ribbon"><a href="#四、Spring-Cloud核心组件：Ribbon" class="headerlink" title="四、Spring Cloud核心组件：Ribbon"></a>四、Spring Cloud核心组件：Ribbon</h2><p>说完了Feign，还没完。现在新的问题又来了，如果人家库存服务部署在了5台机器上，如下所示：</p><ul><li>192.168.169:9000</li><li>192.168.170:9000</li><li>192.168.171:9000</li><li>192.168.172:9000</li><li>192.168.173:9000</li></ul><p>这下麻烦了！人家Feign怎么知道该请求哪台机器呢？</p><ul><li>这时Spring Cloud Ribbon就派上用场了。Ribbon就是专门解决这个问题的。它的作用是负载均衡，会帮你在每次请求时选择一台机器，均匀的把请求分发到各个机器上</li><li>Ribbon的负载均衡默认使用的最经典的Round Robin轮询算法。这是啥？简单来说，就是如果订单服务对库存服务发起10次请求，那就先让你请求第1台机器、然后是第2台机器、第3台机器、第4台机器、第5台机器，接着再来—个循环，第1台机器、第2台机器。。。以此类推。</li></ul><p>此外，Ribbon是和Feign以及Eureka紧密协作，完成工作的，具体如下：</p><ul><li>首先Ribbon会从 Eureka Client里获取到对应的服务注册表，也就知道了所有的服务都部署在了哪些机器上，在监听哪些端口号。</li><li>然后Ribbon就可以使用默认的Round Robin算法，从中选择一台机器</li><li>Feign就会针对这台机器，构造并发起请求。</li></ul><p>对上述整个过程，再来一张图，帮助大家更深刻的理解：</p><p><img src="/2020/07/25/2020-07-25-springcloud/166ec001dc155e98.png" alt></p><h2 id="五、Spring-Cloud核心组件：Hystrix"><a href="#五、Spring-Cloud核心组件：Hystrix" class="headerlink" title="五、Spring Cloud核心组件：Hystrix"></a>五、Spring Cloud核心组件：Hystrix</h2><p>在微服务架构里，一个系统会有很多的服务。以本文的业务场景为例：订单服务在一个业务流程里需要调用三个服务。现在假设订单服务自己最多只有100个线程可以处理请求，然后呢，积分服务不幸的挂了，每次订单服务调用积分服务的时候，都会卡住几秒钟，然后抛出—个超时异常。</p><p>咱们一起来分析一下，这样会导致什么问题？</p><ol><li>如果系统处于高并发的场景下，大量请求涌过来的时候，订单服务的100个线程都会卡在请求积分服务这块。导致订单服务没有一个线程可以处理请求</li><li>然后就会导致别人请求订单服务的时候，发现订单服务也挂了，不响应任何请求了</li></ol><p>上面这个，就是微服务架构中恐怖的服务雪崩问题，如下图所示：</p><p><img src="/2020/07/25/2020-07-25-springcloud/166ec0033f64a0a7.png" alt></p><p>如上图，这么多服务互相调用，要是不做任何保护的话，某一个服务挂了，就会引起连锁反应，导致别的服务也挂。比如积分服务挂了，会导致订单服务的线程全部卡在请求积分服务这里，没有一个线程可以工作，瞬间导致订单服务也挂了，别人请求订单服务全部会卡住，无法响应。</p><p>但是我们思考一下，就算积分服务挂了，订单服务也可以不用挂啊！为什么？</p><ul><li>我们结合业务来看：支付订单的时候，只要把库存扣减了，然后通知仓库发货就OK了</li><li>如果积分服务挂了，大不了等他恢复之后，慢慢人肉手工恢复数据！为啥一定要因为一个积分服务挂了，就直接导致订单服务也挂了呢？不可以接受！</li></ul><p>现在问题分析完了，如何解决？</p><p>这时就轮到Hystrix闪亮登场了。Hystrix是隔离、熔断以及降级的一个框架。啥意思呢？说白了，Hystrix会搞很多个小小的线程池，比如订单服务请求库存服务是一个线程池，请求仓储服务是一个线程池，请求积分服务是一个线程池。每个线程池里的线程就仅仅用于请求那个服务。</p><p>打个比方：现在很不幸，积分服务挂了，会咋样？</p><p>当然会导致订单服务里那个用来调用积分服务的线程都卡死不能工作了啊！但由于订单服务调用库存服务、仓储服务的这两个线程池都是正常工作的，所以这两个服务不会受到任何影响。</p><p>这个时候如果别人请求订单服务，订单服务还是可以正常调用库存服务扣减库存，调用仓储服务通知发货。只不过调用积分服务的时候，每次都会报错。但是如果积分服务都挂了，每次调用都要去卡住几秒钟干啥呢？有意义吗？当然没有！所以我们直接对积分服务熔断不就得了，比如在5分钟内请求积分服务直接就返回了，不要去走网络请求卡住几秒钟，这个过程，就是所谓的熔断！</p><p>那人家又说，兄弟，积分服务挂了你就熔断，好歹你干点儿什么啊！别啥都不干就直接返回啊？没问题，咱们就来个降级：每次调用积分服务，你就在数据库里记录一条消息，说给某某用户增加了多少积分，因为积分服务挂了，导致没增加成功！这样等积分服务恢复了，你可以根据这些记录手工加一下积分。这个过程，就是所谓的降级。</p><p>为帮助大家更直观的理解，接下来用一张图，梳理一下Hystrix隔离、熔断和降级的全流程：</p><p><img src="/2020/07/25/2020-07-25-springcloud/166ec004edf94426.png" alt></p><h2 id="六、Spring-Cloud核心组件：Zuul"><a href="#六、Spring-Cloud核心组件：Zuul" class="headerlink" title="六、Spring Cloud核心组件：Zuul"></a>六、Spring Cloud核心组件：Zuul</h2><p>说完了Hystrix，接着给大家说说最后一个组件：Zuul，也就是微服务网关。这个组件是负责网络路由的。不懂网络路由？行，那我给你说说，如果没有Zuul的日常工作会怎样？</p><p>假设你后台部署了几百个服务，现在有个前端兄弟，人家请求是直接从浏览器那儿发过来的。打个比方：人家要请求一下库存服务，你难道还让人家记着这服务的名字叫做inventory-service？部署在5台机器上？就算人家肯记住这一个，你后台可有几百个服务的名称和地址呢？难不成人家请求一个，就得记住一个？你要这样玩儿，那真是友谊的小船，说翻就翻！</p><p>上面这种情况，压根儿是不现实的。所以一般微服务架构中都必然会设计一个网关在里面，像android、ios、pc前端、微信小程序、H5等等，不用去关心后端有几百个服务，就知道有一个网关，所有请求都往网关走，网关会根据请求中的一些特征，将请求转发给后端的各个服务。</p><p>而且有一个网关之后，还有很多好处，比如可以做统一的降级、限流、认证授权、安全，等等。</p><h2 id="七、总结："><a href="#七、总结：" class="headerlink" title="七、总结："></a>七、总结：</h2><p>最后再来总结一下，上述几个Spring Cloud核心组件，在微服务架构中，分别扮演的角色：</p><ul><li>Eureka：各个服务启动时，Eureka Client都会将服务注册到Eureka Server，并且Eureka Client还可以反过来从Eureka Server拉取注册表，从而知道其他服务在哪里</li><li>Ribbon：服务间发起请求的时候，基于Ribbon做负载均衡，从一个服务的多台机器中选择一台</li><li>Feign：基于Feign的动态代理机制，根据注解和选择的机器，拼接请求URL地址，发起请求</li><li>Hystrix：发起请求是通过Hystrix的线程池来走的，不同的服务走不同的线程池，实现了不同服务调用的隔离，避免了服务雪崩的问题</li><li>Zuul：如果前端、移动端要调用后端系统，统一从Zuul网关进入，由Zuul网关转发请求给对应的服务</li></ul><p>文字总结还不够直观？没问题！我们将Spring Cloud的5个核心组件通过一张图串联起来，再来直观的感受一下其底层的架构原理：</p><p><img src="/2020/07/25/2020-07-25-springcloud/166ec006b1536f43.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringCloud </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式系统（分布式锁、分布式事务）</title>
      <link href="/2020/07/24/2020-07-24-xiao-li-fei-dao-fen-bu-shi/"/>
      <url>/2020/07/24/2020-07-24-xiao-li-fei-dao-fen-bu-shi/</url>
      
        <content type="html"><![CDATA[<h2 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h2><p>zookeeper 都有哪些使用场景？</p><h3 id="面试官心理分析"><a href="#面试官心理分析" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>现在聊的 topic 是分布式系统，面试官跟你聊完了 dubbo 相关的一些问题之后，已经确认你对分布式服务框架/RPC框架基本都有一些认知了。那么他可能开始要跟你聊分布式相关的其它问题了。</p><p>分布式锁这个东西，很常用的，你做 Java 系统开发，分布式系统，可能会有一些场景会用到。最常用的分布式锁就是基于 zookeeper 来实现的。</p><p>其实说实话，问这个问题，一般就是看看你是否了解 zookeeper，因为 zookeeper 是分布式系统中很常见的一个基础系统。而且问的话常问的就是说 zookeeper 的使用场景是什么？看你知道不知道一些基本的使用场景。但是其实 zookeeper 挖深了自然是可以问的很深很深的。</p><h2 id="zookeeper-都有哪些使用场景？"><a href="#zookeeper-都有哪些使用场景？" class="headerlink" title="* zookeeper 都有哪些使用场景？"></a>* zookeeper 都有哪些使用场景？</h2><p>大致来说，zookeeper 的使用场景如下，我就举几个简单的，大家能说几个就好了：</p><ul><li>分布式协调</li><li>分布式锁</li><li>元数据/配置信息管理</li><li>HA高可用性</li></ul><h3 id="分布式协调"><a href="#分布式协调" class="headerlink" title="分布式协调"></a>分布式协调</h3><p>这个其实是 zookeeper 很经典的一个用法，简单来说，就好比，你 A 系统发送个请求到 mq，然后 B 系统消息消费之后处理了。那 A 系统如何知道 B 系统的处理结果？用 zookeeper 就可以实现分布式系统之间的协调工作。A 系统发送请求之后可以在 zookeeper 上对某个节点的值注册个监听器，一旦 B 系统处理完了就修改 zookeeper 那个节点的值，A 系统立马就可以收到通知，完美解决。</p><p><img src="/2020/07/24/2020-07-24-xiao-li-fei-dao-fen-bu-shi/zookeeper-distributed-coordination.png" alt></p><h3 id="分布式锁"><a href="#分布式锁" class="headerlink" title="分布式锁"></a>分布式锁</h3><p>举个栗子。对某一个数据连续发出两个修改操作，两台机器同时收到了请求，但是只能一台机器先执行完另外一个机器再执行。那么此时就可以使用 zookeeper 分布式锁，一个机器接收到了请求之后先获取 zookeeper 上的一把分布式锁，就是可以去创建一个 znode，接着执行操作；然后另外一个机器也尝试去创建那个 znode，结果发现自己创建不了，因为被别人创建了，那只能等着，等第一个机器执行完了自己再执行。</p><p><img src="/2020/07/24/2020-07-24-xiao-li-fei-dao-fen-bu-shi/zookeeper-distributed-lock-demo.png" alt></p><h3 id="元数据-配置信息管理"><a href="#元数据-配置信息管理" class="headerlink" title="元数据/配置信息管理"></a>元数据/配置信息管理</h3><p>zookeeper 可以用作很多系统的配置信息的管理，比如 kafka、storm 等等很多分布式系统都会选用 zookeeper 来做一些元数据、配置信息的管理，包括 dubbo 注册中心不也支持 zookeeper 么？</p><p><img src="/2020/07/24/2020-07-24-xiao-li-fei-dao-fen-bu-shi/zookeeper-meta-data-manage.png" alt></p><h3 id="HA高可用性"><a href="#HA高可用性" class="headerlink" title="HA高可用性"></a>HA高可用性</h3><p>这个应该是很常见的，比如 hadoop、hdfs、yarn 等很多大数据系统，都选择基于 zookeeper 来开发 HA 高可用机制，就是一个重要进程一般会做主备两个，主进程挂了立马通过 zookeeper 感知到切换到备用进程。</p><p><img src="/2020/07/24/2020-07-24-xiao-li-fei-dao-fen-bu-shi/zookeeper-active-standby.png" alt></p><hr><h2 id="面试题-1"><a href="#面试题-1" class="headerlink" title="面试题"></a>面试题</h2><p>一般实现分布式锁都有哪些方式？使用 Redis 如何设计分布式锁？使用 zk 来设计分布式锁可以吗？这两种分布式锁的实现方式哪种效率比较高？</p><h3 id="面试官心理分析-1"><a href="#面试官心理分析-1" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>其实一般问问题，都是这么问的，先问问你 zk，然后其实是要过渡到 zk 相关的一些问题里去，比如分布式锁。因为在分布式系统开发中，分布式锁的使用场景还是很常见的。</p><h2 id="一般实现分布式锁都有哪些方式？"><a href="#一般实现分布式锁都有哪些方式？" class="headerlink" title="* 一般实现分布式锁都有哪些方式？"></a>* 一般实现分布式锁都有哪些方式？</h2><h3 id="Redis-分布式锁"><a href="#Redis-分布式锁" class="headerlink" title="Redis 分布式锁"></a>Redis 分布式锁</h3><p>官方叫做 RedLock 算法，是 Redis 官方支持的分布式锁算法。</p><p>这个分布式锁有 3 个重要的考量点：</p><ul><li>互斥（只能有一个客户端获取锁）</li><li>不能死锁</li><li>容错（只要大部分 Redis 节点创建了这把锁就可以）</li></ul><h4 id="Redis-最普通的分布式锁"><a href="#Redis-最普通的分布式锁" class="headerlink" title="Redis 最普通的分布式锁"></a>Redis 最普通的分布式锁</h4><p>第一个最普通的实现方式，就是在 Redis 里使用 SET key value [EX seconds] [PX milliseconds] NX 创建一个 key，这样就算加锁。其中：</p><ul><li>NX：表示只有 key 不存在的时候才会设置成功，如果此时 redis 中存在这个 key，那么设置失败，返回 nil。</li><li>EX seconds：设置 key 的过期时间，精确到秒级。意思是 seconds 秒后锁自动释放，别人创建的时候如果发现已经有了就不能加锁了。</li><li>PX milliseconds：同样是设置 key 的过期时间，精确到毫秒级。</li></ul><p>比如执行以下命令：</p><pre><code>SET resource_name my_random_value PX 30000 NX</code></pre><p>释放锁就是删除 key ，但是一般可以用 lua 脚本删除，判断 value 一样才删除：</p><pre><code>-- 删除锁的时候，找到 key 对应的 value，跟自己传过去的 value 做比较，如果是一样的才删除。if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then    return redis.call(&quot;del&quot;,KEYS[1])else    return 0end</code></pre><p>为啥要用 random_value 随机值呢？因为如果某个客户端获取到了锁，但是阻塞了很长时间才执行完，比如说超过了 30s，此时可能已经自动释放锁了，此时可能别的客户端已经获取到了这个锁，要是你这个时候直接删除 key 的话会有问题，所以得用随机值加上面的 lua 脚本来释放锁。</p><p>但是这样是肯定不行的。因为如果是普通的 Redis 单实例，那就是单点故障。或者是 Redis 普通主从，那 Redis 主从异步复制，如果主节点挂了（key 就没有了），key 还没同步到从节点，此时从节点切换为主节点，别人就可以 set key，从而拿到锁。</p><h4 id="RedLock-算法"><a href="#RedLock-算法" class="headerlink" title="RedLock 算法"></a>RedLock 算法</h4><p>这个场景是假设有一个 Redis cluster，有 5 个 Redis master 实例。然后执行如下步骤获取一把锁：</p><ul><li>获取当前时间戳，单位是毫秒；</li><li>跟上面类似，轮流尝试在每个 master 节点上创建锁，过期时间较短，一般就几十毫秒；</li><li>尝试在大多数节点上建立一个锁，比如 5 个节点就要求是 3 个节点 n / 2 + 1 ；</li><li>客户端计算建立好锁的时间，如果建立锁的时间小于超时时间，就算建立成功了；</li><li>要是锁建立失败了，那么就依次之前建立过的锁删除；</li><li>只要别人建立了一把分布式锁，你就得不断轮询去尝试获取锁。</li></ul><p><img src="/2020/07/24/2020-07-24-xiao-li-fei-dao-fen-bu-shi/redis-redlock.png" alt></p><h3 id="zk-分布式锁"><a href="#zk-分布式锁" class="headerlink" title="zk 分布式锁"></a>zk 分布式锁</h3><p>zk 分布式锁，其实可以做的比较简单，就是某个节点尝试创建临时 znode，此时创建成功了就获取了这个锁；这个时候别的客户端来创建锁会失败，只能注册个监听器监听这个锁。释放锁就是删除这个 znode，一旦释放掉就会通知客户端，然后有一个等待着的客户端就可以再次重新加锁。</p><pre><code>/** * ZooKeeperSession */public class ZooKeeperSession {    private static CountDownLatch connectedSemaphore = new CountDownLatch(1);    private ZooKeeper zookeeper;    private CountDownLatch latch;    public ZooKeeperSession() {        try {            this.zookeeper = new ZooKeeper(&quot;192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181&quot;, 50000, new ZooKeeperWatcher());            try {                connectedSemaphore.await();            } catch (InterruptedException e) {                e.printStackTrace();            }            System.out.println(&quot;ZooKeeper session established......&quot;);        } catch (Exception e) {            e.printStackTrace();        }    }    /**     * 获取分布式锁     *      * @param productId     */    public Boolean acquireDistributedLock(Long productId) {        String path = &quot;/product-lock-&quot; + productId;        try {            zookeeper.create(path, &quot;&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL);            return true;        } catch (Exception e) {            while (true) {                try {                    // 相当于是给node注册一个监听器，去看看这个监听器是否存在                    Stat stat = zk.exists(path, true);                    if (stat != null) {                        this.latch = new CountDownLatch(1);                        this.latch.await(waitTime, TimeUnit.MILLISECONDS);                        this.latch = null;                    }                    zookeeper.create(path, &quot;&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL);                    return true;                } catch (Exception ee) {                    continue;                }            }        }        return true;    }    /**     * 释放掉一个分布式锁     *      * @param productId     */    public void releaseDistributedLock(Long productId) {        String path = &quot;/product-lock-&quot; + productId;        try {            zookeeper.delete(path, -1);            System.out.println(&quot;release the lock for product[id=&quot; + productId + &quot;]......&quot;);        } catch (Exception e) {            e.printStackTrace();        }    }    /**     * 建立 zk session 的 watcher     */    private class ZooKeeperWatcher implements Watcher {        public void process(WatchedEvent event) {            System.out.println(&quot;Receive watched event: &quot; + event.getState());            if (KeeperState.SyncConnected == event.getState()) {                connectedSemaphore.countDown();            }            if (this.latch != null) {                this.latch.countDown();            }        }    }    /**     * 封装单例的静态内部类     */    private static class Singleton {        private static ZooKeeperSession instance;        static {            instance = new ZooKeeperSession();        }        public static ZooKeeperSession getInstance() {            return instance;        }    }    /**     * 获取单例     *      * @return     */    public static ZooKeeperSession getInstance() {        return Singleton.getInstance();    }    /**     * 初始化单例的便捷方法     */    public static void init() {        getInstance();    }}</code></pre><p>也可以采用另一种方式，创建临时顺序节点：</p><p>如果有一把锁，被多个人给竞争，此时多个人会排队，第一个拿到锁的人会执行，然后释放锁；后面的每个人都会去监听排在自己前面的那个人创建的 node 上，一旦某个人释放了锁，排在自己后面的人就会被 ZooKeeper 给通知，一旦被通知了之后，就 ok 了，自己就获取到了锁，就可以执行代码了。</p><pre><code>public class ZooKeeperDistributedLock implements Watcher {    private ZooKeeper zk;    private String locksRoot = &quot;/locks&quot;;    private String productId;    private String waitNode;    private String lockNode;    private CountDownLatch latch;    private CountDownLatch connectedLatch = new CountDownLatch(1);    private int sessionTimeout = 30000;    public ZooKeeperDistributedLock(String productId) {        this.productId = productId;        try {            String address = &quot;192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181&quot;;            zk = new ZooKeeper(address, sessionTimeout, this);            connectedLatch.await();        } catch (IOException e) {            throw new LockException(e);        } catch (KeeperException e) {            throw new LockException(e);        } catch (InterruptedException e) {            throw new LockException(e);        }    }    public void process(WatchedEvent event) {        if (event.getState() == KeeperState.SyncConnected) {            connectedLatch.countDown();            return;        }        if (this.latch != null) {            this.latch.countDown();        }    }    public void acquireDistributedLock() {        try {            if (this.tryLock()) {                return;            } else {                waitForLock(waitNode, sessionTimeout);            }        } catch (KeeperException e) {            throw new LockException(e);        } catch (InterruptedException e) {            throw new LockException(e);        }    }    public boolean tryLock() {        try {             // 传入进去的locksRoot + “/” + productId            // 假设productId代表了一个商品id，比如说1            // locksRoot = locks            // /locks/10000000000，/locks/10000000001，/locks/10000000002            lockNode = zk.create(locksRoot + &quot;/&quot; + productId, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);            // 看看刚创建的节点是不是最小的节点             // locks：10000000000，10000000001，10000000002            List&lt;String&gt; locks = zk.getChildren(locksRoot, false);            Collections.sort(locks);            if(lockNode.equals(locksRoot+&quot;/&quot;+ locks.get(0))){                //如果是最小的节点,则表示取得锁                return true;            }            //如果不是最小的节点，找到比自己小1的节点      int previousLockIndex = -1;            for(int i = 0; i &lt; locks.size(); i++) {        if(lockNode.equals(locksRoot + “/” + locks.get(i))) {                     previousLockIndex = i - 1;            break;        }       }       this.waitNode = locks.get(previousLockIndex);        } catch (KeeperException e) {            throw new LockException(e);        } catch (InterruptedException e) {            throw new LockException(e);        }        return false;    }    private boolean waitForLock(String waitNode, long waitTime) throws InterruptedException, KeeperException {        Stat stat = zk.exists(locksRoot + &quot;/&quot; + waitNode, true);        if (stat != null) {            this.latch = new CountDownLatch(1);            this.latch.await(waitTime, TimeUnit.MILLISECONDS);            this.latch = null;        }        return true;    }    public void unlock() {        try {            // 删除/locks/10000000000节点            // 删除/locks/10000000001节点            System.out.println(&quot;unlock &quot; + lockNode);            zk.delete(lockNode, -1);            lockNode = null;            zk.close();        } catch (InterruptedException e) {            e.printStackTrace();        } catch (KeeperException e) {            e.printStackTrace();        }    }    public class LockException extends RuntimeException {        private static final long serialVersionUID = 1L;        public LockException(String e) {            super(e);        }        public LockException(Exception e) {            super(e);        }    }}</code></pre><h2 id="redis-分布式锁和-zk-分布式锁的对比"><a href="#redis-分布式锁和-zk-分布式锁的对比" class="headerlink" title="* redis 分布式锁和 zk 分布式锁的对比"></a>* redis 分布式锁和 zk 分布式锁的对比</h2><ul><li>redis 分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能。</li><li>zk 分布式锁，获取不到锁，注册个监听器即可，不需要不断主动尝试获取锁，性能开销较小。</li></ul><p>另外一点就是，如果是 Redis 获取锁的那个客户端 出现 bug 挂了，那么只能等待超时时间之后才能释放锁；而 zk 的话，因为创建的是临时 znode，只要客户端挂了，znode 就没了，此时就自动释放锁。</p><p>Redis 分布式锁大家没发现好麻烦吗？遍历上锁，计算时间等等……zk 的分布式锁语义清晰实现简单。</p><p>所以先不分析太多的东西，就说这两点，我个人实践认为 zk 的分布式锁比 Redis 的分布式锁牢靠、而且模型简单易用。</p><hr><h2 id="面试题-2"><a href="#面试题-2" class="headerlink" title="面试题"></a>面试题</h2><p>分布式事务了解吗？你们是如何解决分布式事务问题的？</p><h3 id="面试官心理分析-2"><a href="#面试官心理分析-2" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>只要聊到你做了分布式系统，必问分布式事务，你对分布式事务一无所知的话，确实会很坑，你起码得知道有哪些方案，一般怎么来做，每个方案的优缺点是什么。</p><p>现在面试，分布式系统成了标配，而分布式系统带来的分布式事务也成了标配了。因为你做系统肯定要用事务吧，如果是分布式系统，肯定要用分布式事务吧。先不说你搞过没有，起码你得明白有哪几种方案，每种方案可能有啥坑？比如 TCC 方案的网络问题、XA 方案的一致性问题。</p><h2 id="分布式事务了解吗？你们是如何解决分布式事务问题的？"><a href="#分布式事务了解吗？你们是如何解决分布式事务问题的？" class="headerlink" title="* 分布式事务了解吗？你们是如何解决分布式事务问题的？"></a>* 分布式事务了解吗？你们是如何解决分布式事务问题的？</h2><p>分布式事务的实现主要有以下 6 种方案：</p><ul><li>XA 方案</li><li>TCC 方案</li><li>SAGA 方案</li><li>本地消息表</li><li>可靠消息最终一致性方案</li><li>最大努力通知方案</li></ul><h3 id="两阶段提交方案-XA-方案"><a href="#两阶段提交方案-XA-方案" class="headerlink" title="两阶段提交方案/XA 方案"></a>两阶段提交方案/XA 方案</h3><p>所谓的 XA 方案，即：两阶段提交，有一个事务管理器的概念，负责协调多个数据库（资源管理器）的事务，事务管理器先问问各个数据库你准备好了吗？如果每个数据库都回复 ok，那么就正式提交事务，在各个数据库上执行操作；如果任何其中一个数据库回答不 ok，那么就回滚事务。</p><p>这种分布式事务方案，比较适合单块应用里，跨多个库的分布式事务，而且因为严重依赖于数据库层面来搞定复杂的事务，效率很低，绝对不适合高并发的场景。如果要玩儿，那么基于 Spring + JTA 就可以搞定，自己随便搜个 demo 看看就知道了。</p><p>这个方案，我们很少用，一般来说某个系统内部如果出现跨多个库的这么一个操作，是不合规的。我可以给大家介绍一下， 现在微服务，一个大的系统分成几十个甚至几百个服务。一般来说，我们的规定和规范，是要求每个服务只能操作自己对应的一个数据库。</p><p>如果你要操作别的服务对应的库，不允许直连别的服务的库，违反微服务架构的规范，你随便交叉胡乱访问，几百个服务的话，全体乱套，这样的一套服务是没法管理的，没法治理的，可能会出现数据被别人改错，自己的库被别人写挂等情况。</p><p>如果你要操作别人的服务的库，你必须是通过调用别的服务的接口来实现，绝对不允许交叉访问别人的数据库。</p><p><img src="/2020/07/24/2020-07-24-xiao-li-fei-dao-fen-bu-shi/distributed-transaction-XA.png" alt></p><h3 id="TCC-方案"><a href="#TCC-方案" class="headerlink" title="TCC 方案"></a>TCC 方案</h3><p>TCC 的全称是： Try 、 Confirm 、 Cancel 。</p><ul><li>Try 阶段：这个阶段说的是对各个服务的资源做检测以及对资源进行锁定或者预留。</li><li>Confirm 阶段：这个阶段说的是在各个服务中执行实际的操作。</li><li>Cancel 阶段：如果任何一个服务的业务方法执行出错，那么这里就需要进行补偿，就是执行已经执行成功的业务逻辑的回滚操作。（把那些执行成功的回滚）</li></ul><p>这种方案说实话几乎很少人使用，我们用的也比较少，但是也有使用的场景。因为这个事务回滚实际上是严重依赖于你自己写代码来回滚和补偿了，会造成补偿代码巨大，非常之恶心。</p><p>比如说我们，一般来说跟钱相关的，跟钱打交道的，支付、交易相关的场景，我们会用 TCC，严格保证分布式事务要么全部成功，要么全部自动回滚，严格保证资金的正确性，保证在资金上不会出现问题。</p><p>而且最好是你的各个业务执行的时间都比较短。</p><p>但是说实话，一般尽量别这么搞，自己手写回滚逻辑，或者是补偿逻辑，实在太恶心了，那个业务代码是很难维护的。</p><p><strong>(可以联系实际渠道的退流程)</strong></p><h3 id="Saga-方案"><a href="#Saga-方案" class="headerlink" title="Saga 方案"></a>Saga 方案</h3><p>金融核心等业务可能会选择 TCC 方案，以追求强一致性和更高的并发量，而对于更多的金融核心以上的业务系统 往往会选择补偿事务，补偿事务处理在 30 多年前就提出了 Saga 理论，随着微服务的发展，近些年才逐步受到大家的关注。目前业界比较公认的是采用 Saga 作为长事务的解决方案。</p><h4 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h4><p>业务流程中每个参与者都提交本地事务，若某一个参与者失败，则补偿前面已经成功的参与者。下图左侧是正常的事务流程，当执行到 T3 时发生了错误，则开始执行右边的事务补偿流程，反向执行 T3、T2、T1 的补偿服务 C3、C2、C1，将 T3、T2、T1 已经修改的数据补偿掉。</p><p><img src="/2020/07/24/2020-07-24-xiao-li-fei-dao-fen-bu-shi/distributed-transaction-saga.png" alt></p><h4 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h4><p>对于一致性要求高、短流程、并发高 的场景，如：金融核心系统，会优先考虑 TCC 方案。而在另外一些场景下，我们并不需要这么强的一致性，只需要保证最终一致性即可。</p><p>比如 很多金融核心以上的业务（渠道层、产品层、系统集成层），这些系统的特点是最终一致即可、流程多、流程长、还可能要调用其它公司的服务。这种情况如果选择 TCC 方案开发的话，一来成本高，二来无法要求其它公司的服务也遵循 TCC 模式。同时流程长，事务边界太长，加锁时间长，也会影响并发性能。</p><p>所以 Saga 模式的适用场景是：</p><ul><li>业务流程长、业务流程多；</li><li>参与者包含其它公司或遗留系统服务，无法提供 TCC 模式要求的三个接口。</li></ul><h4 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h4><ul><li>一阶段提交本地事务，无锁，高性能；</li><li>参与者可异步执行，高吞吐；</li><li>补偿服务易于实现，因为一个更新操作的反向操作是比较容易理解的。</li></ul><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul><li>不保证事务的隔离性。</li></ul><h3 id="本地消息表"><a href="#本地消息表" class="headerlink" title="本地消息表"></a>本地消息表</h3><p>本地消息表其实是国外的 ebay 搞出来的这么一套思想。</p><p>这个大概意思是这样的：</p><ul><li>A 系统在自己本地一个事务里操作同时，插入一条数据到消息表；</li><li>接着 A 系统将这个消息发送到 MQ 中去；</li><li>B 系统接收到消息之后，在一个事务里，往自己本地消息表里插入一条数据，同时执行其他的业务操作，如果这个消息已经被处理过了，那么此时这个事务会回滚，这样保证不会重复处理消息；</li><li>B 系统执行成功之后，就会更新自己本地消息表的状态以及 A 系统消息表的状态；</li><li>如果 B 系统处理失败了，那么就不会更新消息表状态，那么此时 A 系统会定时扫描自己的消息表，如果有未处理的消息，会再次发送到 MQ 中去，让 B 再次处理；</li><li>这个方案保证了最终一致性，哪怕 B 事务失败了，但是 A 会不断重发消息，直到 B 那边成功为止。</li></ul><p>这个方案说实话最大的问题就在于严重依赖于数据库的消息表来管理事务啥的，如果是高并发场景咋办呢？咋扩展呢？所以一般确实很少用。</p><p><img src="/2020/07/24/2020-07-24-xiao-li-fei-dao-fen-bu-shi/distributed-transaction-local-message-table" alt></p><h3 id="可靠消息最终一致性方案"><a href="#可靠消息最终一致性方案" class="headerlink" title="可靠消息最终一致性方案"></a>可靠消息最终一致性方案</h3><p>这个的意思，就是干脆不要用本地的消息表了，直接基于 MQ 来实现事务。比如阿里的 RocketMQ 就支持消息事务。</p><p>大概的意思就是：</p><ul><li>A 系统先发送一个 prepared 消息到 mq，如果这个 prepared 消息发送失败那么就直接取消操作别执行了；</li><li>如果这个消息发送成功过了，那么接着执行本地事务，如果成功就告诉 mq 发送确认消息，如果失败就告诉 mq 回滚消息；</li><li>如果发送了确认消息，那么此时 B 系统会接收到确认消息，然后执行本地的事务；</li><li>mq 会自动定时轮询所有 prepared 消息回调你的接口，问你，这个消息是不是本地事务处理失败了，所有没发送确认的消息，是继续重试还是回滚？一般来说这里你就可以查下数据库看之前本地事务是否执行，如果回滚了，那么这里也回滚吧。这个就是避免可能本地事务执行成功了，而确认消息却发送失败了。</li><li>这个方案里，要是系统 B 的事务失败了咋办？重试咯，自动不断重试直到成功，如果实在是不行，要么就是针对重要的资金类业务进行回滚，比如 B 系统本地回滚后，想办法通知系统 A 也回滚；或者是发送报警由人工来手工回滚和补偿。</li><li>这个还是比较合适的，目前国内互联网公司大都是这么玩儿的，要不你就用 RocketMQ 支持的，要不你就自己基于类似 ActiveMQ？RabbitMQ？自己封装一套类似的逻辑出来，总之思路就是这样子的</li></ul><p><img src="/2020/07/24/2020-07-24-xiao-li-fei-dao-fen-bu-shi/distributed-transaction-reliable-message.png" alt></p><h3 id="最大努力通知方案"><a href="#最大努力通知方案" class="headerlink" title="最大努力通知方案"></a>最大努力通知方案</h3><p>这个方案的大致意思就是：</p><ul><li>系统 A 本地事务执行完之后，发送个消息到 MQ；</li><li>这里会有个专门消费 MQ 的最大努力通知服务，这个服务会消费 MQ 然后写入数据库中记录下来，或者是放入个内存队列也可以，接着调用系统 B 的接口；</li><li>要是系统 B 执行成功就 ok 了；要是系统 B 执行失败了，那么最大努力通知服务就定时尝试重新调用系统 B，反复 N 次，最后还是不行就放弃。</li></ul><h2 id="你们公司是如何处理分布式事务的？"><a href="#你们公司是如何处理分布式事务的？" class="headerlink" title="* 你们公司是如何处理分布式事务的？"></a>* 你们公司是如何处理分布式事务的？</h2><p>如果你真的被问到，可以这么说，我们某某特别严格的场景，用的是 TCC 来保证强一致性；然后其他的一些场景基于阿里的 RocketMQ 来实现分布式事务。</p><p>你找一个严格资金要求绝对不能错的场景，你可以说你是用的 TCC 方案；如果是一般的分布式事务场景，订单插入之后要调用库存服务更新库存，库存数据没有资金那么的敏感，可以用可靠消息最终一致性方案。</p><p>友情提示一下，RocketMQ 3.2.6 之前的版本，是可以按照上面的思路来的，但是之后接口做了一些改变，我这里不再赘述了。</p><p>当然如果你愿意，你可以参考可靠消息最终一致性方案来自己实现一套分布式事务，比如基于 RocketMQ 来玩儿。</p>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dubbo</title>
      <link href="/2020/07/23/2020-07-23-xiao-li-fei-dao-dubbo/"/>
      <url>/2020/07/23/2020-07-23-xiao-li-fei-dao-dubbo/</url>
      
        <content type="html"><![CDATA[<h2 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h2><p>说一下的 dubbo 的工作原理？注册中心挂了可以继续通信吗？说说一次 rpc 请求的流程？</p><h3 id="面试官心理分析"><a href="#面试官心理分析" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>MQ、ES、Redis、Dubbo，上来先问你一些思考性的问题、原理，比如 kafka 高可用架构原理、es 分布式架构原理、redis 线程模型原理、Dubbo 工作原理；之后就是生产环境里可能会碰到的一些问题，因为每种技术引入之后生产环境都可能会碰到一些问题；再来点综合的，就是系统设计，比如让你设计一个 MQ、设计一个搜索引擎、设计一个缓存、设计一个 rpc 框架等等。</p><p>那既然开始聊分布式系统了，自然重点先聊聊 dubbo 了，毕竟 dubbo 是目前事实上大部分公司的分布式系统的 rpc 框架标准，基于 dubbo 也可以构建一整套的微服务架构。但是需要自己大量开发。</p><p>当然去年开始 spring cloud 非常火，现在大量的公司开始转向 spring cloud 了，spring cloud 人家毕竟是微服务架构的全家桶式的这么一个东西。但是因为很多公司还在用 dubbo，所以 dubbo 肯定会是目前面试的重点，何况人家 dubbo 现在重启开源社区维护了，捐献给了 apache，未来应该也还是有一定市场和地位的。</p><p>既然聊 dubbo，那肯定是先从 dubbo 原理开始聊了，你先说说 dubbo 支撑 rpc 分布式调用的架构啥的，然后说说一次 rpc 请求 dubbo 是怎么给你完成的，对吧。</p><h2 id="dubbo-工作原理"><a href="#dubbo-工作原理" class="headerlink" title="* dubbo 工作原理"></a>* dubbo 工作原理</h2><ul><li>第一层：service 层，接口层，给服务提供者和消费者来实现的</li><li>第二层：config 层，配置层，主要是对 dubbo 进行各种配置的</li><li>第三层：proxy 层，服务代理层，无论是 consumer 还是 provider，dubbo 都会给你生成代理，代理之间进行网络通信</li><li>第四层：registry 层，服务注册层，负责服务的注册与发现</li><li>第五层：cluster 层，集群层，封装多个服务提供者的路由以及负载均衡，将多个实例组合成一个服务</li><li>第六层：monitor 层，监控层，对 rpc 接口的调用次数和调用时间进行监控</li><li>第七层：protocal 层，远程调用层，封装 rpc 调用</li><li>第八层：exchange 层，信息交换层，封装请求响应模式，同步转异步</li><li>第九层：transport 层，网络传输层，抽象 mina 和 netty 为统一接口</li><li>第十层：serialize 层，数据序列化层</li></ul><ol><li>第一步：provider 向注册中心去注册</li><li>第二步：consumer 从注册中心订阅服务，注册中心会通知 consumer 注册好的服务</li><li>第三步：consumer 调用 provider</li><li>第四步：consumer 和 provider 都异步通知监控中心</li></ol><p><img src="/2020/07/23/2020-07-23-xiao-li-fei-dao-dubbo/dubbo-operating-principle.png" alt></p><h2 id="注册中心挂了可以继续通信吗？"><a href="#注册中心挂了可以继续通信吗？" class="headerlink" title="* 注册中心挂了可以继续通信吗？"></a>* 注册中心挂了可以继续通信吗？</h2><p>可以，因为刚开始初始化的时候，消费者会将提供者的地址等信息拉取到本地缓存，所以注册中心挂了可以继续通信。</p><hr><h2 id="面试题-1"><a href="#面试题-1" class="headerlink" title="面试题"></a>面试题</h2><p>dubbo 支持哪些通信协议？支持哪些序列化协议？说一下 Hessian 的数据结构？PB 知道吗？为什么 PB 的效率是最高的？</p><h3 id="面试官心理分析-1"><a href="#面试官心理分析-1" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>上一个问题，说说 dubbo 的基本工作原理，那是你必须知道的，至少要知道 dubbo 分成哪些层，然后平时怎么发起 rpc 请求的，注册、发现、调用，这些是基本的。</p><p>接着就可以针对底层进行深入的问问了，比如第一步就可以先问问序列化协议这块，就是平时 RPC 的时候怎么走的？</p><h3 id="面试题剖析"><a href="#面试题剖析" class="headerlink" title="面试题剖析"></a>面试题剖析</h3><p>序列化，就是把数据结构或者是一些对象，转换为二进制串的过程，而反序列化是将在序列化过程中所生成的二进制串转换成数据结构或者对象的过程。</p><p><img src="/2020/07/23/2020-07-23-xiao-li-fei-dao-dubbo/serialize-deserialize.png" alt></p><h2 id="dubbo-支持哪些通信协议？"><a href="#dubbo-支持哪些通信协议？" class="headerlink" title="* dubbo 支持哪些通信协议？"></a>* dubbo 支持哪些通信协议？</h2><h3 id="dubbo-协议"><a href="#dubbo-协议" class="headerlink" title="dubbo 协议"></a>dubbo 协议</h3><p>默认就是走 dubbo 协议，单一长连接，进行的是 NIO 异步通信，基于 hessian 作为序列化协议。使用的场景是：传输数据量小（每次请求在 100kb 以内），但是并发量很高。</p><p>为了要支持高并发场景，一般是服务提供者就几台机器，但是服务消费者有上百台，可能每天调用量达到上亿次！此时用长连接是最合适的，就是跟每个服务消费者维持一个长连接就可以，可能总共就 100 个连接。然后后面直接基于长连接 NIO 异步通信，可以支撑高并发请求。</p><p>长连接，通俗点说，就是建立连接过后可以持续发送请求，无须再建立连接。</p><p><img src="/2020/07/23/2020-07-23-xiao-li-fei-dao-dubbo/dubbo-keep-connection.png" alt></p><p>而短连接，每次要发送请求之前，需要先重新建立一次连接。</p><p><img src="/2020/07/23/2020-07-23-xiao-li-fei-dao-dubbo/dubbo-not-keep-connection.png" alt></p><h3 id="rmi-协议"><a href="#rmi-协议" class="headerlink" title="rmi 协议"></a>rmi 协议</h3><p>走 Java 二进制序列化，多个短连接，适合消费者和提供者数量差不多的情况，适用于文件的传输，一般较少用。</p><h3 id="hessian-协议"><a href="#hessian-协议" class="headerlink" title="hessian 协议"></a>hessian 协议</h3><p>走 hessian 序列化协议，多个短连接，适用于提供者数量比消费者数量还多的情况，适用于文件的传输，一般较少用。</p><h3 id="http-协议"><a href="#http-协议" class="headerlink" title="http 协议"></a>http 协议</h3><p>走表单序列化。</p><h3 id="webservice"><a href="#webservice" class="headerlink" title="webservice"></a>webservice</h3><p>走 SOAP 文本序列化。</p><h2 id="dubbo-支持的序列化协议"><a href="#dubbo-支持的序列化协议" class="headerlink" title="* dubbo 支持的序列化协议"></a>* dubbo 支持的序列化协议</h2><p>dubbo 支持 hession、Java 二进制序列化、json、SOAP 文本序列化多种序列化协议。但是 hessian 是其默认的序列化协议。</p><h2 id="说一下-Hessian-的数据结构"><a href="#说一下-Hessian-的数据结构" class="headerlink" title="* 说一下 Hessian 的数据结构"></a>* 说一下 Hessian 的数据结构</h2><p>Hessian 的对象序列化机制有 8 种原始类型：</p><ul><li>原始二进制数据</li><li>boolean</li><li>64-bit date（64 位毫秒值的日期）</li><li>64-bit double</li><li>32-bit int</li><li>64-bit long</li><li>null</li><li>UTF-8 编码的 string</li></ul><p>另外还包括 3 种递归类型：</p><ul><li>list for lists and arrays</li><li>map for maps and dictionaries</li><li>object for objects</li></ul><p>还有一种特殊的类型：</p><ul><li>ref：用来表示对共享对象的引用。</li></ul><h2 id="为什么-PB-的效率是最高的？"><a href="#为什么-PB-的效率是最高的？" class="headerlink" title="* 为什么 PB 的效率是最高的？"></a>* 为什么 PB 的效率是最高的？</h2><p>可能有一些同学比较习惯于 JSON or XML 数据存储格式，对于 Protocol Buffer 还比较陌生。Protocol Buffer 其实是 Google 出品的一种轻量并且高效的结构化数据存储格式，性能比 JSON、XML 要高很多。</p><p>其实 PB 之所以性能如此好，主要得益于两个：第一，它使用 proto 编译器，自动进行序列化和反序列化，速度非常快，应该比 XML 和 JSON 快上了 20~100 倍；第二，它的数据压缩效果好，就是说它序列化后的数据量体积小。因为体积小，传输起来带宽和速度上会有优化。</p><hr><h2 id="面试题-2"><a href="#面试题-2" class="headerlink" title="面试题"></a>面试题</h2><p>dubbo 负载均衡策略和集群容错策略都有哪些？动态代理策略呢？</p><h3 id="面试官心理分析-2"><a href="#面试官心理分析-2" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>继续深问吧，这些都是用 dubbo 必须知道的一些东西，你得知道基本原理，知道序列化是什么协议，还得知道具体用 dubbo 的时候，如何负载均衡，如何高可用，如何动态代理。</p><p>说白了，就是看你对 dubbo 熟悉不熟悉：</p><ul><li>dubbo 工作原理：服务注册、注册中心、消费者、代理通信、负载均衡；</li><li>网络通信、序列化：dubbo 协议、长连接、NIO、hessian 序列化协议；</li><li>负载均衡策略、集群容错策略、动态代理策略：dubbo 跑起来的时候一些功能是如何运转的？怎么做负载均衡？怎么做集群容错？怎么生成动态代理？</li><li>dubbo SPI 机制：你了解不了解 dubbo 的 SPI 机制？如何基于 SPI 机制对 dubbo 进行扩展？</li></ul><h2 id="dubbo-负载均衡策略"><a href="#dubbo-负载均衡策略" class="headerlink" title="* dubbo 负载均衡策略"></a>* dubbo 负载均衡策略</h2><h3 id="RandomLoadBalance"><a href="#RandomLoadBalance" class="headerlink" title="RandomLoadBalance"></a>RandomLoadBalance</h3><p>默认情况下，dubbo 是 RandomLoadBalance ，即随机调用实现负载均衡，可以对 provider 不同实例设置不同的权重，会按照权重来负载均衡，权重越大分配流量越高，一般就用这个默认的就可以了。</p><h3 id="RoundRobinLoadBalance"><a href="#RoundRobinLoadBalance" class="headerlink" title="RoundRobinLoadBalance"></a>RoundRobinLoadBalance</h3><p>这个的话默认就是均匀地将流量打到各个机器上去，但是如果各个机器的性能不一样，容易导致性能差的机器负载过高。所以此时需要调整权重，让性能差的机器承载权重小一些，流量少一些。</p><p>举个栗子。</p><p>跟运维同学申请机器，有的时候，我们运气好，正好公司资源比较充足，刚刚有一批热气腾腾、刚刚做好的虚拟机新鲜出炉，配置都比较高：8 核 + 16G 机器，申请到 2 台。过了一段时间，我们感觉 2 台机器有点不太够，我就去找运维同学说，“哥儿们，你能不能再给我一台机器”，但是这时只剩下一台 4 核 + 8G 的机器。我要还是得要。</p><p>这个时候，可以给两台 8 核 16G 的机器设置权重 4，给剩余 1 台 4 核 8G 的机器设置权重 2。</p><h3 id="LeastActiveLoadBalance"><a href="#LeastActiveLoadBalance" class="headerlink" title="LeastActiveLoadBalance"></a>LeastActiveLoadBalance</h3><p>这个就是自动感知一下，如果某个机器性能越差，那么接收的请求越少，越不活跃，此时就会给不活跃的性能差的机器更少的请求。</p><h3 id="ConsistentHashLoadBalance"><a href="#ConsistentHashLoadBalance" class="headerlink" title="ConsistentHashLoadBalance"></a>ConsistentHashLoadBalance</h3><p>一致性 Hash 算法，相同参数的请求一定分发到一个 provider 上去，provider 挂掉的时候，会基于虚拟节点均匀分配剩余的流量，抖动不会太大。如果你需要的不是随机负载均衡，是要一类请求都到一个节点，那就走这个一致性 Hash 策略。</p><h2 id="dubbo-集群容错策略"><a href="#dubbo-集群容错策略" class="headerlink" title="* dubbo 集群容错策略"></a>* dubbo 集群容错策略</h2><h3 id="Failover-Cluster-模式"><a href="#Failover-Cluster-模式" class="headerlink" title="Failover Cluster 模式"></a>Failover Cluster 模式</h3><p>失败自动切换，自动重试其他机器，默认就是这个，常见于读操作。（失败重试其它机器）</p><p>可以通过以下几种方式配置重试次数：</p><pre><code>&lt;dubbo:service retries=&quot;2&quot; /&gt;</code></pre><p>或者</p><pre><code>&lt;dubbo:reference retries=&quot;2&quot; /&gt;</code></pre><p>或者</p><pre><code>&lt;dubbo:reference&gt;    &lt;dubbo:method name=&quot;findFoo&quot; retries=&quot;2&quot; /&gt;&lt;/dubbo:reference&gt;</code></pre><h3 id="Failfast-Cluster-模式"><a href="#Failfast-Cluster-模式" class="headerlink" title="Failfast Cluster 模式"></a>Failfast Cluster 模式</h3><p>一次调用失败就立即失败，常见于非幂等性的写操作，比如新增一条记录（调用失败就立即失败）</p><h3 id="Failsafe-Cluster-模式"><a href="#Failsafe-Cluster-模式" class="headerlink" title="Failsafe Cluster 模式"></a>Failsafe Cluster 模式</h3><p>出现异常时忽略掉，常用于不重要的接口调用，比如记录日志。</p><p>配置示例如下：</p><pre><code>&lt;dubbo:service cluster=&quot;failsafe&quot; /&gt;</code></pre><p>或者</p><pre><code>&lt;dubbo:reference cluster=&quot;failsafe&quot; /&gt;</code></pre><h3 id="Failback-Cluster-模式"><a href="#Failback-Cluster-模式" class="headerlink" title="Failback Cluster 模式"></a>Failback Cluster 模式</h3><p>失败了后台自动记录请求，然后定时重发，比较适合于写消息队列这种。</p><h3 id="Forking-Cluster-模式"><a href="#Forking-Cluster-模式" class="headerlink" title="Forking Cluster 模式"></a>Forking Cluster 模式</h3><p>并行调用多个 provider，只要一个成功就立即返回。常用于实时性要求比较高的读操作，但是会浪费更多的服务资源，可通过 forks=”2” 来设置最大并行数。</p><h3 id="Broadcast-Cluster-模式"><a href="#Broadcast-Cluster-模式" class="headerlink" title="Broadcast Cluster 模式"></a>Broadcast Cluster 模式</h3><p>逐个调用所有的 provider。任何一个 provider 出错则报错（从 2.1.0 版本开始支持）。通常用于通知所有提供者更新缓存或日志等本地资源信息。</p><h2 id="dubbo-动态代理策略"><a href="#dubbo-动态代理策略" class="headerlink" title="* dubbo 动态代理策略"></a>* dubbo 动态代理策略</h2><p>默认使用 javassist 动态字节码生成，创建代理类。但是可以通过 spi 扩展机制配置自己的动态代理策略。</p><hr><h2 id="面试题-3"><a href="#面试题-3" class="headerlink" title="面试题"></a>面试题</h2><p>dubbo 的 spi 思想是什么？</p><h3 id="面试官心理分析-3"><a href="#面试官心理分析-3" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>继续深入问呗，前面一些基础性的东西问完了，确定你应该都 ok，了解 dubbo 的一些基本东西，那么问个稍微难一点点的问题，就是 spi，先问问你 spi 是啥？然后问问你 dubbo 的 spi 是怎么实现的？</p><p>其实就是看看你对 dubbo 的掌握如何。</p><h2 id="dubbo-的-spi-思想是什么？"><a href="#dubbo-的-spi-思想是什么？" class="headerlink" title="* dubbo 的 spi 思想是什么？"></a>* dubbo 的 spi 思想是什么？</h2><h3 id="spi-是啥？"><a href="#spi-是啥？" class="headerlink" title="spi 是啥？"></a>spi 是啥？</h3><p>spi，简单来说，就是 service provider interface ，说白了是什么意思呢，比如你有个接口，现在这个接口有 3 个实现类，那么在系统运行的时候对这个接口到底选择哪个实现类呢？这就需要 spi 了，需要根据指定的配置或者是默认的配置，去找到对应的实现类加载进来，然后用这个实现类的实例对象。</p><p>举个栗子。</p><p>你有一个接口 A。A1/A2/A3 分别是接口A的不同实现。你通过配置 接口 A = 实现 A2 ，那么在系统实际运行的时候，会加载你的配置，用实现 A2 实例化一个对象来提供服务。</p><p>spi 机制一般用在哪儿？插件扩展的场景，比如说你开发了一个给别人使用的开源框架，如果你想让别人自己写个插件，插到你的开源框架里面，从而扩展某个功能，这个时候 spi 思想就用上了。</p><h3 id="Java-spi-思想的体现"><a href="#Java-spi-思想的体现" class="headerlink" title="Java spi 思想的体现"></a>Java spi 思想的体现</h3><p>spi 经典的思想体现，大家平时都在用，比如说 jdbc。</p><p>Java 定义了一套 jdbc 的接口，但是 Java 并没有提供 jdbc 的实现类。</p><p>但是实际上项目跑的时候，要使用 jdbc 接口的哪些实现类呢？一般来说，我们要根据自己使用的数据库，比如 mysql，你就将 mysql-jdbc-connector.jar 引入进来；oracle，你就将 oracle-jdbc-connector.jar 引入进来。</p><p>在系统跑的时候，碰到你使用 jdbc 的接口，他会在底层使用你引入的那个 jar 中提供的实现类。</p><h3 id="dubbo-的-spi-思想"><a href="#dubbo-的-spi-思想" class="headerlink" title="dubbo 的 spi 思想"></a>dubbo 的 spi 思想</h3><p>dubbo 也用了 spi 思想，不过没有用 jdk 的 spi 机制，是自己实现的一套 spi 机制。</p><pre><code>Protocol protocol = ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension();</code></pre><p>Protocol 接口，在系统运行的时候，，dubbo 会判断一下应该选用这个 Protocol 接口的哪个实现类来实例化对象来使用。</p><p>它会去找一个你配置的 Protocol，将你配置的 Protocol 实现类，加载到 jvm 中来，然后实例化对象，就用你的那个 Protocol 实现类就可以了。</p><p>上面那行代码就是 dubbo 里大量使用的，就是对很多组件，都是保留一个接口和多个实现，然后在系统运行的时候动态根据配置去找到对应的实现类。如果你没配置，那就走默认的实现好了，没问题。</p><pre><code>@SPI(&quot;dubbo&quot;)  public interface Protocol {      int getDefaultPort();      @Adaptive      &lt;T&gt; Exporter&lt;T&gt; export(Invoker&lt;T&gt; invoker) throws RpcException;      @Adaptive      &lt;T&gt; Invoker&lt;T&gt; refer(Class&lt;T&gt; type, URL url) throws RpcException;      void destroy();  }  </code></pre><p>在 dubbo 自己的 jar 里，在 /META_INF/dubbo/internal/com.alibaba.dubbo.rpc.Protocol 文件中：</p><pre><code>dubbo=com.alibaba.dubbo.rpc.protocol.dubbo.DubboProtocolhttp=com.alibaba.dubbo.rpc.protocol.http.HttpProtocolhessian=com.alibaba.dubbo.rpc.protocol.hessian.HessianProtocol</code></pre><p>所以说，这就看到了 dubbo 的 spi 机制默认是怎么玩儿的了，其实就是 Protocol 接口， @SPI(“dubbo”) 说的是，通过 SPI 机制来提供实现类，实现类是通过 dubbo 作为默认 key 去配置文件里找到的，配置文件名称与接口全限定名一样的，通过 dubbo 作为 key 可以找到默认的实现类就是 com.alibaba.dubbo.rpc.protocol.dubbo.DubboProtocol 。</p><p>如果想要动态替换掉默认的实现类，需要使用 @Adaptive 接口，Protocol 接口中，有两个方法加了 @Adaptive 注解，就是说那俩接口会被代理实现。</p><p>啥意思呢？</p><p>比如这个 Protocol 接口搞了俩 @Adaptive 注解标注了方法，在运行的时候会针对 Protocol 生成代理类，这个代理类的那俩方法里面会有代理代码，代理代码会在运行的时候动态根据 url 中的 protocol 来获取那个 key，默认是 dubbo，你也可以自己指定，你如果指定了别的 key，那么就会获取别的实现类的实例了。</p><h2 id="如何自己扩展-dubbo-中的组件"><a href="#如何自己扩展-dubbo-中的组件" class="headerlink" title="* 如何自己扩展 dubbo 中的组件"></a>* 如何自己扩展 dubbo 中的组件</h2><p>下面来说说怎么来自己扩展 dubbo 中的组件。</p><p>自己写个工程，要是那种可以打成 jar 包的，里面的 src/main/resources 目录下，搞一个 META-INF/services ，里面放个文件叫： com.alibaba.dubbo.rpc.Protocol ，文件里搞一个 my=com.bingo.MyProtocol 。自己把 jar 弄到 nexus 私服里去。</p><p>然后自己搞一个 dubbo provider 工程，在这个工程里面依赖你自己搞的那个 jar，然后在 spring 配置文件里给个配置：</p><pre><code>&lt;dubbo:protocol name=”my” port=”20000” /&gt;</code></pre><p>provider 启动的时候，就会加载到我们 jar 包里的 my=com.bingo.MyProtocol 这行配置里，接着会根据你的配置使用你定义好的 MyProtocol 了，这个就是简单说明一下，你通过上述方式，可以替换掉大量的 dubbo 内部的组件，就是扔个你自己的 jar 包，然后配置一下即可。</p><p><img src="/2020/07/23/2020-07-23-xiao-li-fei-dao-dubbo/dubbo-spi.png" alt></p><p>dubbo 里面提供了大量的类似上面的扩展点，就是说，你如果要扩展一个东西，只要自己写个 jar，让你的 consumer 或者是 provider 工程，依赖你的那个 jar，在你的 jar 里指定目录下配置好接口名称对应的文件，里面通过 key=实现类 。</p><p>然后对于对应的组件，类似 <a href="dubbo:protocol" target="_blank" rel="noopener">dubbo:protocol</a> 用你的那个 key 对应的实现类来实现某个接口，你可以自己去扩展 dubbo 的各种功能，提供你自己的实现。</p><blockquote><p>(可以描述自己的日志输出和语言过滤器)</p></blockquote><p>语言过滤器：</p><p>src/main/resources/META-INF/dubbo/com.alibaba.dubbo.rpc.Filter</p><pre><code>DubboContextLanguageFilter=com.xxx.infrastructure.filter.DubboContextLanguageFilter</code></pre><pre><code>@Activate(group = Constants.PROVIDER)public class DubboContextLanguageFilter implements Filter {    private final Logger log = LoggerFactory.getLogger(this.getClass());    @Override    public Result invoke(Invoker&lt;?&gt; invoker, Invocation invocation) throws RpcException {        try {            String language = RpcContext.getContext().getAttachment(&quot;session:org.springframework.web.servlet.i18n.SessionLocaleResolver.LOCALE&quot;);            if (&quot;en&quot;.equals(language)) {                LocaleContextHolder.setDefaultLocale(new Locale(&quot;en&quot;, &quot;US&quot;));            } else {                LocaleContextHolder.setDefaultLocale(new Locale(&quot;zh&quot;, &quot;CN&quot;));            }        } catch (Throwable t) {            log.error(&quot;[DubboContextEnterFilter - invoke - language fail] 获取并设置language失败！&quot;, t);        }        return invoker.invoke(invocation);    }}</code></pre><hr><h2 id="面试题-4"><a href="#面试题-4" class="headerlink" title="面试题"></a>面试题</h2><p>如何基于 dubbo 进行服务治理、服务降级、失败重试以及超时重试？</p><h3 id="面试官心理分析-4"><a href="#面试官心理分析-4" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>服务治理，这个问题如果问你，其实就是看看你有没有服务治理的思想，因为这个是做过复杂微服务的人肯定会遇到的一个问题。</p><p>服务降级，这个是涉及到复杂分布式系统中必备的一个话题，因为分布式系统互相来回调用，任何一个系统故障了，你不降级，直接就全盘崩溃？那就太坑爹了吧。</p><p>失败重试，分布式系统中网络请求如此频繁，要是因为网络问题不小心失败了一次，是不是要重试？</p><p>超时重试，跟上面一样，如果不小心网络慢一点，超时了，如何重试？</p><h2 id="服务治理"><a href="#服务治理" class="headerlink" title="服务治理"></a>服务治理</h2><h3 id="1-调用链路自动生成"><a href="#1-调用链路自动生成" class="headerlink" title="1. 调用链路自动生成"></a>1. 调用链路自动生成</h3><p>一个大型的分布式系统，或者说是用现在流行的微服务架构来说吧，分布式系统由大量的服务组成。那么这些服务之间互相是如何调用的？调用链路是啥？说实话，几乎到后面没人搞的清楚了，因为服务实在太多了，可能几百个甚至几千个服务。</p><p>那就需要基于 dubbo 做的分布式系统中，对各个服务之间的调用自动记录下来，然后自动将各个服务之间的依赖关系和调用链路生成出来，做成一张图，显示出来，大家才可以看到对吧。</p><p><img src="/2020/07/23/2020-07-23-xiao-li-fei-dao-dubbo/dubbo-service-invoke-road.png" alt></p><h3 id="2-服务访问压力以及时长统计"><a href="#2-服务访问压力以及时长统计" class="headerlink" title="2. 服务访问压力以及时长统计"></a>2. 服务访问压力以及时长统计</h3><p>需要自动统计各个接口和服务之间的调用次数以及访问延时，而且要分成两个级别。</p><ul><li>一个级别是接口粒度，就是每个服务的每个接口每天被调用多少次，TP50/TP90/TP99，三个档次的请求延时分别是多少；</li><li>第二个级别是从源头入口开始，一个完整的请求链路经过几十个服务之后，完成一次请求，每天全链路走多少次，全链路请求延时的 TP50/TP90/TP99，分别是多少。</li></ul><p>这些东西都搞定了之后，后面才可以来看当前系统的压力主要在哪里，如何来扩容和优化啊。</p><h3 id="3-其它"><a href="#3-其它" class="headerlink" title="3. 其它"></a>3. 其它</h3><ul><li>服务分层（避免循环依赖）</li><li>调用链路失败监控和报警</li><li>服务鉴权</li><li>每个服务的可用性的监控（接口调用成功率？几个 9？99.99%，99.9%，99%）</li></ul><h2 id="服务降级"><a href="#服务降级" class="headerlink" title="服务降级"></a>服务降级</h2><p>比如说服务 A 调用服务 B，结果服务 B 挂掉了，服务 A 重试几次调用服务 B，还是不行，那么直接降级，走一个备用的逻辑，给用户返回响应。</p><p>举个栗子，我们有接口 HelloService 。 HelloServiceImpl 有该接口的具体实现。</p><pre><code>public interface HelloService {   void sayHello();}public class HelloServiceImpl implements HelloService {    public void sayHello() {        System.out.println(&quot;hello world......&quot;);    }}</code></pre><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;    xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:dubbo=&quot;http://code.alibabatech.com/schema/dubbo&quot;    xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans        http://www.springframework.org/schema/beans/spring-beans.xsd        http://code.alibabatech.com/schema/dubbo        http://code.alibabatech.com/schema/dubbo/dubbo.xsd&quot;&gt;    &lt;dubbo:application name=&quot;dubbo-provider&quot; /&gt;    &lt;dubbo:registry address=&quot;zookeeper://127.0.0.1:2181&quot; /&gt;    &lt;dubbo:protocol name=&quot;dubbo&quot; port=&quot;20880&quot; /&gt;    &lt;dubbo:service interface=&quot;com.zhss.service.HelloService&quot; ref=&quot;helloServiceImpl&quot; timeout=&quot;10000&quot; /&gt;    &lt;bean id=&quot;helloServiceImpl&quot; class=&quot;com.zhss.service.HelloServiceImpl&quot; /&gt;&lt;/beans&gt;&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;    xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;    xmlns:dubbo=&quot;http://code.alibabatech.com/schema/dubbo&quot;    xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans        http://www.springframework.org/schema/beans/spring-beans.xsd        http://code.alibabatech.com/schema/dubbo        http://code.alibabatech.com/schema/dubbo/dubbo.xsd&quot;&gt;    &lt;dubbo:application name=&quot;dubbo-consumer&quot;  /&gt;    &lt;dubbo:registry address=&quot;zookeeper://127.0.0.1:2181&quot; /&gt;    &lt;dubbo:reference id=&quot;fooService&quot; interface=&quot;com.test.service.FooService&quot;  timeout=&quot;10000&quot; check=&quot;false&quot; mock=&quot;return null&quot;&gt;    &lt;/dubbo:reference&gt;&lt;/beans&gt;</code></pre><p>我们调用接口失败的时候，可以通过 mock 统一返回 null。</p><p>mock 的值也可以修改为 true，然后再跟接口同一个路径下实现一个 Mock 类，命名规则是 “接口名称+ Mock ” 后缀。然后在 Mock 类里实现自己的降级逻辑。</p><pre><code>public class HelloServiceMock implements HelloService {    public void sayHello() {        // 降级逻辑    }}</code></pre><h2 id="失败重试和超时重试"><a href="#失败重试和超时重试" class="headerlink" title="失败重试和超时重试"></a>失败重试和超时重试</h2><p>所谓失败重试，就是 consumer 调用 provider 要是失败了，比如抛异常了，此时应该是可以重试的，或者调用超时了也可以重试。配置如下：</p><pre><code>&lt;dubbo:reference id=&quot;xxxx&quot; interface=&quot;xx&quot; check=&quot;true&quot; async=&quot;false&quot; retries=&quot;3&quot; timeout=&quot;2000&quot;/&gt;</code></pre><p>举个栗子。</p><p>某个服务的接口，要耗费 5s，你这边不能干等着，你这边配置了 timeout 之后，我等待 2s，还没返回，我直接就撤了，不能干等你。</p><p>可以结合你们公司具体的场景来说说你是怎么设置这些参数的：</p><ul><li>timeout ：一般设置为 200ms ，我们认为不能超过 200ms 还没返回。</li><li>retries ：设置 retries，一般是在读请求的时候，比如你要查询个数据，你可以设置个 retries，如果第一次没读到，报错，重试指定的次数，尝试再次读取。</li></ul><hr><h2 id="面试题-5"><a href="#面试题-5" class="headerlink" title="面试题"></a>面试题</h2><p>分布式服务接口的幂等性如何设计（比如不能重复扣款）？</p><h3 id="面试官心理分析-5"><a href="#面试官心理分析-5" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>从这个问题开始，面试官就已经进入了实际的生产问题的面试了。</p><p>一个分布式系统中的某个接口，该如何保证幂等性？这个事儿其实是你做分布式系统的时候必须要考虑的一个生产环境的技术问题。啥意思呢？</p><p>你看，假如你有个服务提供一些接口供外部调用，这个服务部署在了 5 台机器上，接着有个接口就是付款接口。然后人家用户在前端上操作的时候，不知道为啥，总之就是一个订单不小心发起了两次支付请求，然后这俩请求分散在了这个服务部署的不同的机器上，好了，结果一个订单扣款扣两次。</p><p>或者是订单系统调用支付系统进行支付，结果不小心因为网络超时了，然后订单系统走了前面我们看到的那个重试机制，咔嚓给你重试了一把，好，支付系统收到一个支付请求两次，而且因为负载均衡算法落在了不同的机器上，尴尬了。。。</p><p>所以你肯定得知道这事儿，否则你做出来的分布式系统恐怕容易埋坑。</p><h2 id="分布式服务接口的幂等性如何设计（比如不能重复扣款）？"><a href="#分布式服务接口的幂等性如何设计（比如不能重复扣款）？" class="headerlink" title="* 分布式服务接口的幂等性如何设计（比如不能重复扣款）？"></a>* 分布式服务接口的幂等性如何设计（比如不能重复扣款）？</h2><p>这个不是技术问题，这个没有通用的一个方法，这个应该结合业务来保证幂等性。</p><p>所谓幂等性，就是说一个接口，多次发起同一个请求，你这个接口得保证结果是准确的，比如不能多扣款、不能多插入一条数据、不能将统计值多加了 1。这就是幂等性。</p><p>其实保证幂等性主要是三点：</p><ul><li>对于每个请求必须有一个唯一的标识，举个栗子：订单支付请求，肯定得包含订单 id，一个订单 id 最多支付一次，对吧。</li><li>每次处理完请求之后，必须有一个记录标识这个请求处理过了。常见的方案是在 mysql 中记录个状态啥的，比如支付之前记录一条这个订单的支付流水。</li><li>每次接收请求需要进行判断，判断之前是否处理过。比如说，如果有一个订单已经支付了，就已经有了一条支付流水，那么如果重复发送这个请求，则此时先插入支付流水，orderId 已经存在了，唯一键约束生效，报错插入不进去的。然后你就不用再扣款了。</li></ul><p>实际运作过程中，你要结合自己的业务来，比如说利用 Redis，用 orderId 作为唯一键。只有成功插入这个支付流水，才可以执行实际的支付扣款。</p><p>要求是支付一个订单，必须插入一条支付流水，order_id 建一个唯一键 unique key 。你在支付一个订单之前，先插入一条支付流水，order_id 就已经进去了。你就可以写一个标识到 Redis 里面去， set order_id payed ，下一次重复请求过来了，先查 Redis 的 order_id 对应的 value，如果是 payed 就说明已经支付过了，你就别重复支付了。</p><hr><h2 id="面试题-6"><a href="#面试题-6" class="headerlink" title="面试题"></a>面试题</h2><p>分布式服务接口请求的顺序性如何保证？</p><h3 id="面试官心理分析-6"><a href="#面试官心理分析-6" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>其实分布式系统接口的调用顺序，也是个问题，一般来说是不用保证顺序的。但是有时候可能确实是需要严格的顺序保证。给大家举个例子，你服务 A 调用服务 B，先插入再删除。好，结果俩请求过去了，落在不同机器上，可能插入请求因为某些原因执行慢了一些，导致删除请求先执行了，此时因为没数据所以啥效果也没有；结果这个时候插入请求过来了，好，数据插入进去了，那就尴尬了。</p><p>本来应该是 “先插入 -&gt; 再删除”，这条数据应该没了，结果现在 “先删除 -&gt; 再插入”，数据还存在，最后你死都想不明白是怎么回事。</p><p>所以这都是分布式系统一些很常见的问题。</p><h2 id="分布式服务接口请求的顺序性如何保证？"><a href="#分布式服务接口请求的顺序性如何保证？" class="headerlink" title="* 分布式服务接口请求的顺序性如何保证？"></a>* 分布式服务接口请求的顺序性如何保证？</h2><p>首先，一般来说，个人建议是，你们从业务逻辑上设计的这个系统最好是不需要这种顺序性的保证，因为一旦引入顺序性保障，比如使用分布式锁，会导致系统复杂度上升，而且会带来效率低下，热点数据压力过大等问题。</p><p>下面我给个我们用过的方案吧，简单来说，首先你得用 Dubbo 的一致性 hash 负载均衡策略，将比如某一个订单 id 对应的请求都给分发到某个机器上去，接着就是在那个机器上，因为可能还是多线程并发执行的，你可能得立即将某个订单 id 对应的请求扔一个内存队列里去，强制排队，这样来确保他们的顺序性。</p><p><img src="/2020/07/23/2020-07-23-xiao-li-fei-dao-dubbo/distributed-system-request-sequence.png" alt></p><p>但是这样引发的后续问题就很多，比如说要是某个订单对应的请求特别多，造成某台机器成热点怎么办？解决这些问题又要开启后续一连串的复杂技术方案…… 曾经这类问题弄的我们头疼不已，所以，还是建议什么呢？</p><p>最好是比如说刚才那种，一个订单的插入和删除操作，能不能合并成一个操作，就是一个删除，或者是其它什么，避免这种问题的产生。</p><hr><h2 id="面试题-7"><a href="#面试题-7" class="headerlink" title="面试题"></a>面试题</h2><p>如何自己设计一个类似 Dubbo 的 RPC 框架？</p><h3 id="面试官心理分析-7"><a href="#面试官心理分析-7" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>说实话，就这问题，其实就跟问你如何自己设计一个 MQ 一样的道理，就考两个：</p><ul><li>你有没有对某个 rpc 框架原理有非常深入的理解。</li><li>你能不能从整体上来思考一下，如何设计一个 rpc 框架，考考你的系统设计能力。</li></ul><h2 id="如何自己设计一个类似-Dubbo-的-RPC-框架？"><a href="#如何自己设计一个类似-Dubbo-的-RPC-框架？" class="headerlink" title="* 如何自己设计一个类似 Dubbo 的 RPC 框架？"></a>* 如何自己设计一个类似 Dubbo 的 RPC 框架？</h2><p>其实问到你这问题，你起码不能认怂，因为是知识的扫盲，那我不可能给你深入讲解什么 kafka 源码剖析，dubbo 源码剖析，何况我就算讲了，你要真的消化理解和吸收，起码个把月以后了。</p><p>所以我给大家一个建议，遇到这类问题，起码从你了解的类似框架的原理入手，自己说说参照 dubbo 的原理，你来设计一下，举个例子，dubbo 不是有那么多分层么？而且每个分层是干啥的，你大概是不是知道？那就按照这个思路大致说一下吧，起码你不能懵逼，要比那些上来就懵，啥也说不出来的人要好一些。</p><p>举个栗子，我给大家说个最简单的回答思路：</p><ul><li>上来你的服务就得去注册中心注册吧，你是不是得有个注册中心，保留各个服务的信息，可以用 zookeeper 来做，对吧。</li><li>然后你的消费者需要去注册中心拿对应的服务信息吧，对吧，而且每个服务可能会存在于多台机器上。</li><li>接着你就该发起一次请求了，咋发起？当然是基于动态代理了，你面向接口获取到一个动态代理，这个动态代理就是接口在本地的一个代理，然后这个代理会找到服务对应的机器地址。</li><li>然后找哪个机器发送请求？那肯定得有个负载均衡算法了，比如最简单的可以随机轮询是不是。</li><li>接着找到一台机器，就可以跟它发送请求了，第一个问题咋发送？你可以说用 netty 了，nio 方式；第二个问题发送啥格式数据？你可以说用 hessian 序列化协议了，或者是别的，对吧。然后请求过去了。</li><li>服务器那边一样的，需要针对你自己的服务生成一个动态代理，监听某个网络端口了，然后代理你本地的服务代码。接收到请求的时候，就调用对应的服务代码，对吧。</li></ul><p>这就是一个最最基本的 rpc 框架的思路，先不说你有多牛逼的技术功底，哪怕这个最简单的思路你先给出来行不行？</p><hr><h2 id="分布式系统-CAP-定理-P-代表什么含义"><a href="#分布式系统-CAP-定理-P-代表什么含义" class="headerlink" title="* 分布式系统 CAP 定理 P 代表什么含义"></a>* 分布式系统 CAP 定理 P 代表什么含义</h2><p>作者之前在看 CAP 定理时抱有很大的疑惑，CAP 定理的定义是指在分布式系统中三者只能满足其二，也就是存在分布式 CA 系统的。作者在网络上查阅了很多关于 CAP 文章，虽然这些文章对于 P 的解释五花八门，但总结下来这些观点大多都是指 P 是不可缺少的，也就是说在分布式系统只能是 AP 或者 CP，这种理论与我之前所认识的理论（存在分布式 CA 系统）是冲突的，所以才有了疑惑。</p><h3 id="什么是-CAP-定理（CAP-theorem）"><a href="#什么是-CAP-定理（CAP-theorem）" class="headerlink" title="什么是 CAP 定理（CAP theorem）"></a>什么是 CAP 定理（CAP theorem）</h3><p>在理论计算机科学中，CAP 定理（CAP theorem），又被称作布鲁尔定理（Brewer’s theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点：</p><ul><li>一致性（Consistency） （等同于所有节点访问同一份最新的数据副本）</li><li>可用性（Availability）（每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据）</li><li>分区容错性（Partition tolerance）（以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在 C 和 A 之间做出选择。）</li></ul><h3 id="分区容错性（Partition-tolerance）"><a href="#分区容错性（Partition-tolerance）" class="headerlink" title="分区容错性（Partition tolerance）"></a>分区容错性（Partition tolerance）</h3><p>理解 CAP 理论的最简单方式是想象两个节点分处分区两侧。允许至少一个节点更新状态会导致数据不一致，即丧失了 C 性质。如果为了保证数据一致性，将分区一侧的节点设置为不可用，那么又丧失了 A 性质。除非两个节点可以互相通信，才能既保证 C 又保证 A，这又会导致丧失 P 性质。</p><ul><li>P 指的是分区容错性，分区现象产生后需要容错，容错是指在 A 与 C 之间选择。如果分布式系统没有分区现象（没有出现不一致不可用情况） 本身就没有分区 ，既然没有分区则就更没有分区容错性 P。</li><li>无论我设计的系统是 AP 还是 CP 系统如果没有出现不一致不可用。 则该系统就处于 CA 状态</li><li>P 的体现前提是得有分区情况存在</li></ul>]]></content>
      
      
      <categories>
          
          <category> 小李飞刀 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Dubbo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>布隆过滤器</title>
      <link href="/2020/07/22/2020-07-22-bu-long-guo-lu-qi/"/>
      <url>/2020/07/22/2020-07-22-bu-long-guo-lu-qi/</url>
      
        <content type="html"><![CDATA[<h2 id="1-什么是布隆过滤器？"><a href="#1-什么是布隆过滤器？" class="headerlink" title="1.什么是布隆过滤器？"></a>1.什么是布隆过滤器？</h2><p>首先，我们需要了解布隆过滤器的概念。</p><p>布隆过滤器（Bloom Filter）是一个叫做 Bloom 的老哥于1970年提出的。我们可以把它看作由二进制向量（或者说位数组）和一系列随机映射函数（哈希函数）两部分组成的数据结构。相比于我们平时常用的的 List、Map 、Set 等数据结构，它占用空间更少并且效率更高，但是缺点是其返回的结果是概率性的，而不是非常准确的。理论情况下添加到集合中的元素越多，误报的可能性就越大。并且，存放在布隆过滤器的数据不容易删除。</p><p><img src="/2020/07/22/2020-07-22-bu-long-guo-lu-qi/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8-bit%E6%95%B0%E7%BB%84.png" alt></p><p>位数组中的每个元素都只占用 1 bit ，并且每个元素只能是 0 或者 1。这样申请一个 100w 个元素的位数组只占用 1000000Bit / 8 = 125000 Byte = 125000/1024 kb ≈ 122kb 的空间。</p><p>总结：一个名叫 Bloom 的人提出了一种来检索元素是否在给定大集合中的数据结构，这种数据结构是高效且性能很好的，但缺点是具有一定的错误识别率和删除难度。并且，理论情况下，添加到集合中的元素越多，误报的可能性就越大。</p><h2 id="2-布隆过滤器的原理介绍"><a href="#2-布隆过滤器的原理介绍" class="headerlink" title="2.布隆过滤器的原理介绍"></a>2.布隆过滤器的原理介绍</h2><p>当一个元素加入布隆过滤器中的时候，会进行如下操作：</p><ul><li>使用布隆过滤器中的哈希函数对元素值进行计算，得到哈希值（有几个哈希函数得到几个哈希值）。</li><li>根据得到的哈希值，在位数组中把对应下标的值置为 1。</li></ul><p>当我们需要判断一个元素是否存在于布隆过滤器的时候，会进行如下操作：</p><ul><li>对给定元素再次进行相同的哈希计算；</li><li>得到值之后判断位数组中的每个元素是否都为 1，如果值都为 1，那么说明这个值在布隆过滤器中，如果存在一个值不为 1，说明该元素不在布隆过滤器中。</li></ul><p>举个简单的例子：</p><p><img src="/2020/07/22/2020-07-22-bu-long-guo-lu-qi/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8-hash%E8%BF%90%E7%AE%97.png" alt></p><p>如图所示，当字符串存储要加入到布隆过滤器中时，该字符串首先由多个哈希函数生成不同的哈希值，然后在对应的位数组的下表的元素设置为 1（当位数组初始化时 ，所有位置均为0）。当第二次存储相同字符串时，因为先前的对应位置已设置为1，所以很容易知道此值已经存在（去重非常方便）。</p><p>如果我们需要判断某个字符串是否在布隆过滤器中时，只需要对给定字符串再次进行相同的哈希计算，得到值之后判断位数组中的每个元素是否都为 1，如果值都为 1，那么说明这个值在布隆过滤器中，如果存在一个值不为 1，说明该元素不在布隆过滤器中。</p><p>不同的字符串可能哈希出来的位置相同，这种情况我们可以适当增加位数组大小或者调整我们的哈希函数。</p><p>综上，我们可以得出：布隆过滤器说某个元素存在，小概率会误判。布隆过滤器说某个元素不在，那么这个元素一定不在。</p><h2 id="3-布隆过滤器使用场景"><a href="#3-布隆过滤器使用场景" class="headerlink" title="3.布隆过滤器使用场景"></a>3.布隆过滤器使用场景</h2><ul><li>判断给定数据是否存在：比如判断一个数字是否在于包含大量数字的数字集中（数字集很大，5亿以上！）、 防止缓存穿透（判断请求的数据是否有效避免直接绕过缓存请求数据库）等等、邮箱的垃圾邮件过滤、黑名单功能等等。</li><li>去重：比如爬给定网址的时候对已经爬取过的 URL 去重。</li></ul><h2 id="4-通过-Java-编程手动实现布隆过滤器"><a href="#4-通过-Java-编程手动实现布隆过滤器" class="headerlink" title="4.通过 Java 编程手动实现布隆过滤器"></a>4.通过 Java 编程手动实现布隆过滤器</h2><p>我们上面已经说了布隆过滤器的原理，知道了布隆过滤器的原理之后就可以自己手动实现一个了。</p><p>如果你想要手动实现一个的话，你需要：</p><ul><li>一个合适大小的位数组保存数据</li><li>几个不同的哈希函数</li><li>添加元素到位数组（布隆过滤器）的方法实现</li><li>判断给定元素是否存在于位数组（布隆过滤器）的方法实现。</li></ul><pre><code>import java.util.BitSet;public class MyBloomFilter {    /**     * 位数组的大小     */    private static final int DEFAULT_SIZE = 2 &lt;&lt; 24;    /**     * 通过这个数组可以创建 6 个不同的哈希函数     */    private static final int[] SEEDS = new int[]{3, 13, 46, 71, 91, 134};    /**     * 位数组。数组中的元素只能是 0 或者 1     */    private BitSet bits = new BitSet(DEFAULT_SIZE);    /**     * 存放包含 hash 函数的类的数组     */    private SimpleHash[] func = new SimpleHash[SEEDS.length];    /**     * 初始化多个包含 hash 函数的类的数组，每个类中的 hash 函数都不一样     */    public MyBloomFilter() {        // 初始化多个不同的 Hash 函数        for (int i = 0; i &lt; SEEDS.length; i++) {            func[i] = new SimpleHash(DEFAULT_SIZE, SEEDS[i]);        }    }    /**     * 添加元素到位数组     */    public void add(Object value) {        for (SimpleHash f : func) {            bits.set(f.hash(value), true);        }    }    /**     * 判断指定元素是否存在于位数组     */    public boolean contains(Object value) {        boolean ret = true;        for (SimpleHash f : func) {            ret = ret &amp;&amp; bits.get(f.hash(value));        }        return ret;    }    /**     * 静态内部类。用于 hash 操作！     */    public static class SimpleHash {        private int cap;        private int seed;        public SimpleHash(int cap, int seed) {            this.cap = cap;            this.seed = seed;        }        /**         * 计算 hash 值         */        public int hash(Object value) {            int h;            return (value == null) ? 0 : Math.abs(seed * (cap - 1) &amp; ((h = value.hashCode()) ^ (h &gt;&gt;&gt; 16)));        }    }}</code></pre><p>测试：</p><pre><code>String value1 = &quot;https://javaguide.cn/&quot;;String value2 = &quot;https://github.com/Snailclimb&quot;;MyBloomFilter filter = new MyBloomFilter();System.out.println(filter.contains(value1));System.out.println(filter.contains(value2));filter.add(value1);filter.add(value2);System.out.println(filter.contains(value1));System.out.println(filter.contains(value2));</code></pre><p>Output:</p><pre><code>falsefalsetruetrue</code></pre><p>测试：</p><pre><code>Integer value1 = 13423;Integer value2 = 22131;MyBloomFilter filter = new MyBloomFilter();System.out.println(filter.contains(value1));System.out.println(filter.contains(value2));filter.add(value1);filter.add(value2);System.out.println(filter.contains(value1));System.out.println(filter.contains(value2));</code></pre><p>Output:</p><pre><code>falsefalsetruetrue</code></pre><h2 id="5-利用Google开源的-Guava中自带的布隆过滤器"><a href="#5-利用Google开源的-Guava中自带的布隆过滤器" class="headerlink" title="5.利用Google开源的 Guava中自带的布隆过滤器"></a>5.利用Google开源的 Guava中自带的布隆过滤器</h2><p>自己实现的目的主要是为了让自己搞懂布隆过滤器的原理，Guava 中布隆过滤器的实现算是比较权威的，所以实际项目中我们不需要手动实现一个布隆过滤器。</p><p>首先我们需要在项目中引入 Guava 的依赖：</p><pre><code>&lt;dependency&gt;    &lt;groupId&gt;com.google.guava&lt;/groupId&gt;    &lt;artifactId&gt;guava&lt;/artifactId&gt;    &lt;version&gt;28.0-jre&lt;/version&gt;&lt;/dependency&gt;</code></pre><p>实际使用如下：</p><p>我们创建了一个最多存放 最多 1500个整数的布隆过滤器，并且我们可以容忍误判的概率为百分之（0.01）</p><pre><code>// 创建布隆过滤器对象BloomFilter&lt;Integer&gt; filter = BloomFilter.create(        Funnels.integerFunnel(),        1500,        0.01);// 判断指定元素是否存在System.out.println(filter.mightContain(1));System.out.println(filter.mightContain(2));// 将元素添加进布隆过滤器filter.put(1);filter.put(2);System.out.println(filter.mightContain(1));System.out.println(filter.mightContain(2));</code></pre><p>在我们的示例中，当mightContain（） 方法返回true时，我们可以99％确定该元素在过滤器中，当过滤器返回false时，我们可以100％确定该元素不存在于过滤器中。</p><p>Guava 提供的布隆过滤器的实现还是很不错的（想要详细了解的可以看一下它的源码实现），但是它有一个重大的缺陷就是只能单机使用（另外，容量扩展也不容易），而现在互联网一般都是分布式的场景。为了解决这个问题，我们就需要用到 Redis 中的布隆过滤器了。</p><h2 id="6-Redis-中的布隆过滤器"><a href="#6-Redis-中的布隆过滤器" class="headerlink" title="6.Redis 中的布隆过滤器"></a>6.Redis 中的布隆过滤器</h2><h3 id="6-1介绍"><a href="#6-1介绍" class="headerlink" title="6.1介绍"></a>6.1介绍</h3><p>Redis v4.0 之后有了 Module（模块/插件） 功能，Redis Modules 让 Redis 可以使用外部模块扩展其功能 。布隆过滤器就是其中的 Module。详情可以查看 Redis 官方对 Redis Modules 的介绍 ：<a href="https://redis.io/modules。" target="_blank" rel="noopener">https://redis.io/modules。</a></p><p>另外，官网推荐了一个 RedisBloom 作为 Redis 布隆过滤器的 Module,地址：<a href="https://github.com/RedisBloom/RedisBloom。其他还有：" target="_blank" rel="noopener">https://github.com/RedisBloom/RedisBloom。其他还有：</a></p><ul><li>redis-lua-scaling-bloom-filter （lua 脚本实现）：<a href="https://github.com/erikdubbelboer/redis-lua-scaling-bloom-filter" target="_blank" rel="noopener">https://github.com/erikdubbelboer/redis-lua-scaling-bloom-filter</a></li><li>pyreBloom（Python中的快速Redis 布隆过滤器） ：<a href="https://github.com/seomoz/pyreBloom" target="_blank" rel="noopener">https://github.com/seomoz/pyreBloom</a></li></ul><p>RedisBloom 提供了多种语言的客户端支持，包括：Python、Java、JavaScript 和 PHP。</p><h3 id="6-2使用Docker安装"><a href="#6-2使用Docker安装" class="headerlink" title="6.2使用Docker安装"></a>6.2使用Docker安装</h3><p>如果我们需要体验 Redis 中的布隆过滤器非常简单，通过 Docker 就可以了！我们直接在 Google 搜索docker redis bloomfilter 然后在排除广告的第一条搜素结果就找到了我们想要的答案（这是我平常解决问题的一种方式，分享一下），具体地址：<a href="https://hub.docker.com/r/redislabs/rebloom/" target="_blank" rel="noopener">https://hub.docker.com/r/redislabs/rebloom/</a> （介绍的很详细 ）。</p><p>具体操作如下：</p><pre><code>➜  ~ docker run -p 6379:6379 --name redis-redisbloom redislabs/rebloom:latest➜  ~ docker exec -it redis-redisbloom bashroot@21396d02c252:/data# redis-cli127.0.0.1:6379&gt; </code></pre><h3 id="6-3常用命令一览"><a href="#6-3常用命令一览" class="headerlink" title="6.3常用命令一览"></a>6.3常用命令一览</h3><blockquote><p>注意： key:布隆过滤器的名称，item : 添加的元素。</p></blockquote><ul><li>BF.ADD：将元素添加到布隆过滤器中，如果该过滤器尚不存在，则创建该过滤器。格式：BF.ADD {key} {item}。</li><li>BF.MADD : 将一个或多个元素添加到“布隆过滤器”中，并创建一个尚不存在的过滤器。该命令的操作方式BF.ADD与之相同，只不过它允许多个输入并返回多个值。格式：BF.MADD {key} {item} [item …] 。</li><li>*BF.EXISTS * : 确定元素是否在布隆过滤器中存在。格式：BF.EXISTS {key} {item}。</li><li>BF.MEXISTS ： 确定一个或者多个元素是否在布隆过滤器中存在格式：BF.MEXISTS {key} {item} [item …]。</li></ul><p>另外，BF.RESERVE 命令需要单独介绍一下：</p><p>这个命令的格式如下：</p><ul><li>BF.RESERVE {key} {error_rate} {capacity} [EXPANSION expansion]。</li></ul><p>下面简单介绍一下每个参数的具体含义：</p><ol><li>key：布隆过滤器的名称</li><li>error_rate :误报的期望概率。这应该是介于0到1之间的十进制值。例如，对于期望的误报率0.1％（1000中为1），error_rate应该设置为0.001。该数字越接近零，则每个项目的内存消耗越大，并且每个操作的CPU使用率越高。</li><li>capacity: 过滤器的容量。当实际存储的元素个数超过这个值之后，性能将开始下降。实际的降级将取决于超出限制的程度。随着过滤器元素数量呈指数增长，性能将线性下降。</li></ol><p>可选参数：</p><ol start="4"><li>expansion：如果创建了一个新的子过滤器，则其大小将是当前过滤器的大小乘以expansion。默认扩展值为2。这意味着每个后续子过滤器将是前一个子过滤器的两倍。</li></ol><h3 id="6-4实际使用"><a href="#6-4实际使用" class="headerlink" title="6.4实际使用"></a>6.4实际使用</h3><pre><code>127.0.0.1:6379&gt; BF.ADD myFilter java(integer) 1127.0.0.1:6379&gt; BF.ADD myFilter javaguide(integer) 1127.0.0.1:6379&gt; BF.EXISTS myFilter java(integer) 1127.0.0.1:6379&gt; BF.EXISTS myFilter javaguide(integer) 1127.0.0.1:6379&gt; BF.EXISTS myFilter github(integer) 0</code></pre>]]></content>
      
      
      <categories>
          
          <category> 中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 布隆过滤器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis哨兵集群实现高可用</title>
      <link href="/2020/07/21/2020-07-21-redis-shao-bing/"/>
      <url>/2020/07/21/2020-07-21-redis-shao-bing/</url>
      
        <content type="html"><![CDATA[<h2 id="哨兵的介绍"><a href="#哨兵的介绍" class="headerlink" title="哨兵的介绍"></a>哨兵的介绍</h2><p>sentinel，中文名是哨兵。哨兵是 Redis 集群架构中非常重要的一个组件，主要有以下功能：</p><ul><li>集群监控：负责监控 Redis master 和 slave 进程是否正常工作。</li><li>消息通知：如果某个 Redis 实例有故障，那么哨兵负责发送消息作为报警通知给管理员。</li><li>故障转移：如果 master node 挂掉了，会自动转移到 slave node 上。</li><li>配置中心：如果故障转移发生了，通知 client 客户端新的 master 地址。</li></ul><p>哨兵用于实现 Redis 集群的高可用，本身也是分布式的，作为一个哨兵集群去运行，互相协同工作。</p><ul><li>故障转移时，判断一个 master node 是否宕机了，需要大部分的哨兵都同意才行，涉及到了分布式选举的问题。</li><li>即使部分哨兵节点挂掉了，哨兵集群还是能正常工作的，因为如果一个作为高可用机制重要组成部分的故障转移系统本身是单点的，那就很坑爹了。</li></ul><h2 id="哨兵的核心知识"><a href="#哨兵的核心知识" class="headerlink" title="哨兵的核心知识"></a>哨兵的核心知识</h2><ul><li>哨兵至少需要 3 个实例，来保证自己的健壮性。</li><li>哨兵 + Redis 主从的部署架构，是不保证数据零丢失的，只能保证 Redis 集群的高可用性。</li><li>对于哨兵 + Redis 主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练。</li></ul><p>哨兵集群必须部署 2 个以上节点，如果哨兵集群仅仅部署了 2 个哨兵实例，quorum = 1。</p><pre><code>+----+         +----+| M1 |---------| R1 || S1 |         | S2 |+----+         +----+</code></pre><p>配置 quorum=1 ，如果 master 宕机， s1 和 s2 中只要有 1 个哨兵认为 master 宕机了，就可以进行切换，同时 s1 和 s2 会选举出一个哨兵来执行故障转移。但是同时这个时候，需要 majority，也就是大多数哨兵都是运行的。</p><pre><code>2 个哨兵，majority=23 个哨兵，majority=24 个哨兵，majority=25 个哨兵，majority=3...</code></pre><p>如果此时仅仅是 M1 进程宕机了，哨兵 s1 正常运行，那么故障转移是 OK 的。但是如果是整个 M1 和 S1 运行的机器宕机了，那么哨兵只有 1 个，此时就没有 majority 来允许执行故障转移，虽然另外一台机器上还有一个 R1，但是故障转移不会执行。</p><p>经典的 3 节点哨兵集群是这样的：</p><pre><code>       +----+       | M1 |       | S1 |       +----+          |+----+    |    +----+| R2 |----+----| R3 || S2 |         | S3 |+----+         +----+</code></pre><p>配置 quorum=2 ，如果 M1 所在机器宕机了，那么三个哨兵还剩下 2 个，S2 和 S3 可以一致认为 master 宕机了，然后选举出一个来执行故障转移，同时 3 个哨兵的 majority 是 2，所以还剩下的 2 个哨兵运行着，就可以允许执行故障转移。</p><h2 id="Redis-哨兵主备切换的数据丢失问题"><a href="#Redis-哨兵主备切换的数据丢失问题" class="headerlink" title="Redis 哨兵主备切换的数据丢失问题"></a>Redis 哨兵主备切换的数据丢失问题</h2><h3 id="导致数据丢失的两种情况"><a href="#导致数据丢失的两种情况" class="headerlink" title="导致数据丢失的两种情况"></a>导致数据丢失的两种情况</h3><p>主备切换的过程，可能会导致数据丢失：</p><ul><li><p>异步复制导致的数据丢失</p><p>  因为 master-&gt;slave 的复制是异步的，所以可能有部分数据还没复制到 slave，master 就宕机了，此时这部分数据就丢失了。</p><p>  <img src="/2020/07/21/2020-07-21-redis-shao-bing/async-replication-data-lose-case.png" alt></p></li><li><p>脑裂导致的数据丢失</p><p>  脑裂，也就是说，某个 master 所在机器突然脱离了正常的网络，跟其他 slave 机器不能连接，但是实际上 master 还运行着。此时哨兵可能就会认为 master 宕机了，然后开启选举，将其他 slave 切换成了 master。这个时候，集群里就会有两个 master ，也就是所谓的脑裂。</p><p>  此时虽然某个 slave 被切换成了 master，但是可能 client 还没来得及切换到新的 master，还继续向旧 master 写数据。因此旧 master 再次恢复的时候，会被作为一个 slave 挂到新的 master 上去，自己的数据会清空，重新从新的 master 复制数据。而新的 master 并没有后来 client 写入的数据，因此，这部分数据也就丢失了。</p><p>  <img src="/2020/07/21/2020-07-21-redis-shao-bing/redis-cluster-split-brain.png" alt></p></li></ul><h3 id="数据丢失问题的解决方案"><a href="#数据丢失问题的解决方案" class="headerlink" title="数据丢失问题的解决方案"></a>数据丢失问题的解决方案</h3><p>进行如下配置：</p><pre><code>min-slaves-to-write 1min-slaves-max-lag 10</code></pre><p>表示，要求至少有 1 个 slave，数据复制和同步的延迟不能超过 10 秒。</p><p>如果说一旦所有的 slave，数据复制和同步的延迟都超过了 10 秒钟，那么这个时候，master 就不会再接收任何请求了。</p><ul><li><p>减少异步复制数据的丢失</p><p>  有了 min-slaves-max-lag 这个配置，就可以确保说，一旦 slave 复制数据和 ack 延时太长，就认为可能 master 宕机后损失的数据太多了，那么就拒绝写请求，这样可以把 master 宕机时由于部分数据未同步到 slave 导致的数据丢失降低的可控范围内。</p></li><li><p>减少脑裂的数据丢失</p><p>  如果一个 master 出现了脑裂，跟其他 slave 丢了连接，那么上面两个配置可以确保说，如果不能继续给指定数量的 slave 发送数据，而且 slave 超过 10 秒没有给自己 ack 消息，那么就直接拒绝客户端的写请求。因此在脑裂场景下，最多就丢失 10 秒的数据。</p></li></ul><h2 id="sdown-和-odown-转换机制"><a href="#sdown-和-odown-转换机制" class="headerlink" title="sdown 和 odown 转换机制"></a>sdown 和 odown 转换机制</h2><ul><li>sdown 是主观宕机，就一个哨兵如果自己觉得一个 master 宕机了，那么就是主观宕机</li><li>odown 是客观宕机，如果 quorum 数量的哨兵都觉得一个 master 宕机了，那么就是客观宕机</li></ul><p>sdown 达成的条件很简单，如果一个哨兵 ping 一个 master，超过了 is-master-down-after-milliseconds 指定的毫秒数之后，就主观认为 master 宕机了；如果一个哨兵在指定时间内，收到了 quorum 数量的其它哨兵也认为那个 master 是 sdown 的，那么就认为是 odown 了。</p><h2 id="哨兵集群的自动发现机制"><a href="#哨兵集群的自动发现机制" class="headerlink" title="哨兵集群的自动发现机制"></a>哨兵集群的自动发现机制</h2><p>哨兵互相之间的发现，是通过 Redis 的 pub/sub 系统实现的，每个哨兵都会往 <strong>sentinel</strong>:hello 这个 channel 里发送一个消息，这时候所有其他哨兵都可以消费到这个消息，并感知到其他的哨兵的存在。</p><p>每隔两秒钟，每个哨兵都会往自己监控的某个 master+slaves 对应的 <strong>sentinel</strong>:hello channel 里发送一个消息，内容是自己的 host、ip 和 runid 还有对这个 master 的监控配置。</p><p>每个哨兵也会去监听自己监控的每个 master+slaves 对应的 <strong>sentinel</strong>:hello channel，然后去感知到同样在监听这个 master+slaves 的其他哨兵的存在。</p><p>每个哨兵还会跟其他哨兵交换对 master 的监控配置，互相进行监控配置的同步。</p><h2 id="slave-配置的自动纠正"><a href="#slave-配置的自动纠正" class="headerlink" title="slave 配置的自动纠正"></a>slave 配置的自动纠正</h2><p>哨兵会负责自动纠正 slave 的一些配置，比如 slave 如果要成为潜在的 master 候选人，哨兵会确保 slave 复制现有 master 的数据；如果 slave 连接到了一个错误的 master 上，比如故障转移之后，那么哨兵会确保它们连接到正确的 master 上。</p><h2 id="slave-gt-master-选举算法"><a href="#slave-gt-master-选举算法" class="headerlink" title="slave-&gt;master 选举算法"></a>slave-&gt;master 选举算法</h2><p>如果一个 master 被认为 odown 了，而且 majority 数量的哨兵都允许主备切换，那么某个哨兵就会执行主备切换操作，此时首先要选举一个 slave 来，会考虑 slave 的一些信息：</p><ul><li>跟 master 断开连接的时长</li><li>slave 优先级</li><li>复制 offset</li><li>run id</li></ul><p>如果一个 slave 跟 master 断开连接的时间已经超过了 down-after-milliseconds 的 10 倍，外加 master 宕机的时长，那么 slave 就被认为不适合选举为 master。</p><pre><code>(down-after-milliseconds * 10) + milliseconds_since_master_is_in_SDOWN_state</code></pre><p>接下来会对 slave 进行排序：</p><ul><li>按照 slave 优先级进行排序，slave priority 越低，优先级就越高。</li><li>如果 slave priority 相同，那么看 replica offset，哪个 slave 复制了越多的数据，offset 越靠后，优先级就越高。</li><li>如果上面两个条件都相同，那么选择一个 run id 比较小的那个 slave。</li></ul><h2 id="quorum-和-majority"><a href="#quorum-和-majority" class="headerlink" title="quorum 和 majority"></a>quorum 和 majority</h2><p>每次一个哨兵要做主备切换，首先需要 quorum 数量的哨兵认为 odown，然后选举出一个哨兵来做切换，这个哨兵还需要得到 majority 哨兵的授权，才能正式执行切换。</p><p>如果 quorum &lt; majority，比如 5 个哨兵，majority 就是 3，quorum 设置为 2，那么就 3 个哨兵授权就可以执行切换。</p><p>但是如果 quorum &gt;= majority，那么必须 quorum 数量的哨兵都授权，比如 5 个哨兵，quorum 是 5，那么必须 5 个哨兵都同意授权，才能执行切换。</p><h2 id="configuration-epoch"><a href="#configuration-epoch" class="headerlink" title="configuration epoch"></a>configuration epoch</h2><p>哨兵会对一套 Redis master+slaves 进行监控，有相应的监控的配置。</p><p>执行切换的那个哨兵，会从要切换到的新 master（salve-&gt;master）那里得到一个 configuration epoch，这就是一个 version 号，每次切换的 version 号都必须是唯一的。</p><p>如果第一个选举出的哨兵切换失败了，那么其他哨兵，会等待 failover-timeout 时间，然后接替继续执行切换，此时会重新获取一个新的 configuration epoch，作为新的 version 号。</p><h2 id="configuration-传播"><a href="#configuration-传播" class="headerlink" title="configuration 传播"></a>configuration 传播</h2><p>哨兵完成切换之后，会在自己本地更新生成最新的 master 配置，然后同步给其他的哨兵，就是通过之前说的 pub/sub 消息机制。</p><p>这里之前的 version 号就很重要了，因为各种消息都是通过一个 channel 去发布和监听的，所以一个哨兵完成一次新的切换之后，新的 master 配置是跟着新的 version 号的。其他的哨兵都是根据版本号的大小来更新自己的 master 配置的。</p>]]></content>
      
      
      <categories>
          
          <category> 中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis主从架构</title>
      <link href="/2020/07/20/2020-07-20-redis-zhu-cong-jia-gou/"/>
      <url>/2020/07/20/2020-07-20-redis-zhu-cong-jia-gou/</url>
      
        <content type="html"><![CDATA[<p>单机的 Redis，能够承载的 QPS 大概就在上万到几万不等。对于缓存来说，一般都是用来支撑读高并发的。因此架构做成主从(master-slave)架构，一主多从，主负责写，并且将数据复制到其它的 slave 节点，从节点负责读。所有的读请求全部走从节点。这样也可以很轻松实现水平扩容，支撑读高并发。</p><p><img src="/2020/07/20/2020-07-20-redis-zhu-cong-jia-gou/redis-master-slave.png" alt></p><p>Redis replication -&gt; 主从架构 -&gt; 读写分离 -&gt; 水平扩容支撑读高并发</p><h2 id="Redis-replication-的核心机制"><a href="#Redis-replication-的核心机制" class="headerlink" title="Redis replication 的核心机制"></a>Redis replication 的核心机制</h2><ul><li>Redis 采用异步方式复制数据到 slave 节点，不过 Redis2.8 开始，slave node 会周期性地确认自己每次复制的数据量；</li><li>一个 master node 是可以配置多个 slave node 的；</li><li>slave node 也可以连接其他的 slave node；</li><li>slave node 做复制的时候，不会 block master node 的正常工作；</li><li>slave node 在做复制的时候，也不会 block 对自己的查询操作，它会用旧的数据集来提供服务；但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了；</li><li>slave node 主要用来进行横向扩容，做读写分离，扩容的 slave node 可以提高读的吞吐量。</li></ul><p>注意，如果采用了主从架构，那么建议必须开启 master node 的持久化，不建议用 slave node 作为 master node 的数据热备，因为那样的话，如果你关掉 master 的持久化，可能在 master 宕机重启的时候数据是空的，然后可能一经过复制， slave node 的数据也丢了。</p><p>另外，master 的各种备份方案，也需要做。万一本地的所有文件丢失了，从备份中挑选一份 rdb 去恢复 master，这样才能确保启动的时候，是有数据的，即使采用了后续讲解的高可用机制，slave node 可以自动接管 master node，但也可能 sentinel 还没检测到 master failure，master node 就自动重启了，还是可能导致上面所有的 slave node 数据被清空。</p><h2 id="Redis-主从复制的核心原理"><a href="#Redis-主从复制的核心原理" class="headerlink" title="Redis 主从复制的核心原理"></a>Redis 主从复制的核心原理</h2><p>当启动一个 slave node 的时候，它会发送一个 PSYNC 命令给 master node。</p><p>如果这是 slave node 初次连接到 master node，那么会触发一次 full resynchronization 全量复制。此时 master 会启动一个后台线程，开始生成一份 RDB 快照文件，同时还会将从客户端 client 新收到的所有写命令缓存在内存中。 RDB 文件生成完毕后， master 会将这个 RDB 发送给 slave，slave 会先写入本地磁盘，然后再从本地磁盘加载到内存中，接着 master 会将内存中缓存的写命令发送到 slave，slave 也会同步这些数据。slave node 如果跟 master node 有网络故障，断开了连接，会自动重连，连接之后 master node 仅会复制给 slave 部分缺少的数据。</p><p><img src="/2020/07/20/2020-07-20-redis-zhu-cong-jia-gou/redis-master-slave-replication.png" alt></p><h2 id="主从复制的断点续传"><a href="#主从复制的断点续传" class="headerlink" title="主从复制的断点续传"></a>主从复制的断点续传</h2><p>从 Redis2.8 开始，就支持主从复制的断点续传，如果主从复制过程中，网络连接断掉了，那么可以接着上次复制的地方，继续复制下去，而不是从头开始复制一份。</p><p>master node 会在内存中维护一个 backlog，master 和 slave 都会保存一个 replica offset 还有一个 master run id，offset 就是保存在 backlog 中的。如果 master 和 slave 网络连接断掉了，slave 会让 master 从上次 replica offset 开始继续复制，如果没有找到对应的 offset，那么就会执行一次 resynchronization 。</p><blockquote><p>如果根据 host+ip 定位 master node，是不靠谱的，如果 master node 重启或者数据出现了变化，那么 slave node 应该根据不同的 run id 区分。</p></blockquote><h2 id="无磁盘化复制"><a href="#无磁盘化复制" class="headerlink" title="无磁盘化复制"></a>无磁盘化复制</h2><p>master 在内存中直接创建 RDB ，然后发送给 slave，不会在自己本地落地磁盘了。只需要在配置文件中开启 repl-diskless-sync yes 即可。</p><pre><code>repl-diskless-sync yes# 等待 5s 后再开始复制，因为要等更多 slave 重新连接过来repl-diskless-sync-delay 5</code></pre><h2 id="过期-key-处理"><a href="#过期-key-处理" class="headerlink" title="过期 key 处理"></a>过期 key 处理</h2><p>slave 不会过期 key，只会等待 master 过期 key。如果 master 过期了一个 key，或者通过 LRU 淘汰了一个 key，那么会模拟一条 del 命令发送给 slave。</p><h2 id="复制的完整流程"><a href="#复制的完整流程" class="headerlink" title="复制的完整流程"></a>复制的完整流程</h2><p>slave node 启动时，会在自己本地保存 master node 的信息，包括 master node 的 host 和 ip ，但是复制流程没开始。</p><p>slave node 内部有个定时任务，每秒检查是否有新的 master node 要连接和复制，如果发现，就跟 master node 建立 socket 网络连接。然后 slave node 发送 ping 命令给 master node。如果 master 设置了 requirepass，那么 slave node 必须发送 masterauth 的口令过去进行认证。master node 第一次执行全量复制，将所有数据发给 slave node。而在后续，master node 持续将写命令，异步复制给 slave node。</p><p><img src="/2020/07/20/2020-07-20-redis-zhu-cong-jia-gou/redis-master-slave-replication-detail.png" alt></p><h2 id="全量复制"><a href="#全量复制" class="headerlink" title="全量复制"></a>全量复制</h2><ul><li><p>master 执行 bgsave ，在本地生成一份 rdb 快照文件。</p></li><li><p>master node 将 rdb 快照文件发送给 slave node，如果 rdb 复制时间超过 60秒（repl-timeout），那么 slave node 就会认为复制失败，可以适当调大这个参数(对于千兆网卡的机器，一般每秒传输 100MB，6G 文件，很可能超过 60s)</p></li><li><p>master node 在生成 rdb 时，会将所有新的写命令缓存在内存中，在 slave node 保存了 rdb 之后，再将新的写命令复制给 slave node。</p></li><li><p>如果在复制期间，内存缓冲区持续消耗超过 64MB，或者一次性超过 256MB，那么停止复制，复制失败。</p><pre><code>  client-output-buffer-limit slave 256MB 64MB 60</code></pre></li><li><p>slave node 接收到 rdb 之后，清空自己的旧数据，然后重新加载 rdb 到自己的内存中，同时基于旧的数据版本对外提供服务。</p></li><li><p>如果 slave node 开启了 AOF，那么会立即执行 BGREWRITEAOF，重写 AOF。</p></li></ul><h2 id="增量复制"><a href="#增量复制" class="headerlink" title="增量复制"></a>增量复制</h2><ul><li>如果全量复制过程中，master-slave 网络连接断掉，那么 slave 重新连接 master 时，会触发增量复制。</li><li>master 直接从自己的 backlog 中获取部分丢失的数据，发送给 slave node，默认 backlog 就是 1MB。</li><li>master 就是根据 slave 发送的 psync 中的 offset 来从 backlog 中获取数据的。</li></ul><h2 id="heartbeat"><a href="#heartbeat" class="headerlink" title="heartbeat"></a>heartbeat</h2><p>主从节点互相都会发送 heartbeat 信息。</p><p>master 默认每隔 10秒 发送一次 heartbeat，slave node 每隔 1秒 发送一个 heartbeat。</p><h2 id="异步复制"><a href="#异步复制" class="headerlink" title="异步复制"></a>异步复制</h2><p>master 每次接收到写命令之后，先在内部写入数据，然后异步发送给 slave node。</p><h2 id="Redis-如何才能做到高可用"><a href="#Redis-如何才能做到高可用" class="headerlink" title="Redis 如何才能做到高可用"></a>Redis 如何才能做到高可用</h2><p>如果系统在 365 天内，有 99.99% 的时间，都是可以哗哗对外提供服务的，那么就说系统是高可用的。</p><p>一个 slave 挂掉了，是不会影响可用性的，还有其它的 slave 在提供相同数据下的相同的对外的查询服务。</p><p>但是，如果 master node 死掉了，会怎么样？没法写数据了，写缓存的时候，全部失效了。slave node 还有什么用呢，没有 master 给它们复制数据了，系统相当于不可用了。</p><p>Redis 的高可用架构，叫做 failover 故障转移，也可以叫做主备切换。</p><p>master node 在故障时，自动检测，并且将某个 slave node 自动切换为 master node 的过程，叫做主备切换。这个过程，实现了 Redis 的主从架构下的高可用。</p>]]></content>
      
      
      <categories>
          
          <category> 中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis</title>
      <link href="/2020/07/19/2020-07-19-xiao-li-fei-dao-redis/"/>
      <url>/2020/07/19/2020-07-19-xiao-li-fei-dao-redis/</url>
      
        <content type="html"><![CDATA[<h2 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h2><p>项目中缓存是如何使用的？为什么要用缓存？缓存使用不当会造成什么后果？</p><h3 id="面试官心理分析"><a href="#面试官心理分析" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>这个问题，互联网公司必问，要是一个人连缓存都不太清楚，那确实比较尴尬。</p><p>只要问到缓存，上来第一个问题，肯定是先问问你项目哪里用了缓存？为啥要用？不用行不行？如果用了以后可能会有什么不良的后果？</p><p>这就是看看你对缓存这个东西背后有没有思考，如果你就是傻乎乎的瞎用，没法给面试官一个合理的解答，那面试官对你印象肯定不太好，觉得你平时思考太少，就知道干活儿。</p><blockquote><p>面试题剖析:<br>项目中缓存是如何使用的？这个，需要结合自己项目的业务来。</p></blockquote><h2 id="为什么要用缓存？"><a href="#为什么要用缓存？" class="headerlink" title="* 为什么要用缓存？"></a>* 为什么要用缓存？</h2><p>用缓存，主要有两个用途：高性能、高并发。</p><h3 id="高性能"><a href="#高性能" class="headerlink" title="高性能"></a>高性能</h3><p>假设这么个场景，你有个操作，一个请求过来，吭哧吭哧你各种乱七八糟操作 mysql，半天查出来一个结果，耗时 600ms。但是这个结果可能接下来几个小时都不会变了，或者变了也可以不用立即反馈给用户。那么此时咋办？</p><p>缓存啊，折腾 600ms 查出来的结果，扔缓存里，一个 key 对应一个 value，下次再有人查，别走 mysql 折腾 600ms 了，直接从缓存里，通过一个 key 查出来一个 value，2ms 搞定。性能提升 300 倍。</p><p>就是说对于一些需要复杂操作耗时查出来的结果，且确定后面不怎么变化，但是有很多读请求，那么直接将查询出来的结果放在缓存中，后面直接读缓存就好。</p><h3 id="高并发"><a href="#高并发" class="headerlink" title="高并发"></a>高并发</h3><p>mysql 这么重的数据库，压根儿设计不是让你玩儿高并发的，虽然也可以玩儿，但是天然支持不好。mysql 单机支撑到 2000QPS 也开始容易报警了。</p><p>所以要是你有个系统，高峰期一秒钟过来的请求有 1万，那一个 mysql 单机绝对会死掉。你这个时候就只能上缓存，把很多数据放缓存，别放 mysql。缓存功能简单，说白了就是 key-value 式操作，单机支撑的并发量轻松一秒几万十几万，支撑高并发 so easy。单机承载并发量是 mysql 单机的几十倍。</p><p><strong>缓存是走内存的，内存天然就支撑高并发。</strong></p><h2 id="用了缓存之后会有什么不良后果？"><a href="#用了缓存之后会有什么不良后果？" class="headerlink" title="* 用了缓存之后会有什么不良后果？"></a>* 用了缓存之后会有什么不良后果？</h2><p>常见的缓存问题有以下几个：</p><ul><li>缓存与数据库双写不一致</li><li>缓存雪崩、缓存穿透、缓存击穿</li><li>缓存并发竞争</li></ul><hr><h2 id="面试题-1"><a href="#面试题-1" class="headerlink" title="面试题"></a>面试题</h2><p>Redis 和 Memcached 有什么区别？Redis 的线程模型是什么？为什么 Redis 单线程却能支撑高并发？</p><h3 id="面试官心理分析-1"><a href="#面试官心理分析-1" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>这个是问 Redis 的时候，最基本的问题吧，Redis 最基本的一个内部原理和特点，就是 Redis 实际上是个单线程工作模型，你要是这个都不知道，那后面玩儿 Redis 的时候，出了问题岂不是什么都不知道？</p><p>还有可能面试官会问问你 Redis 和 Memcached 的区别，但是 Memcached 是早些年各大互联网公司常用的缓存方案，但是现在近几年基本都是 Redis，没什么公司用 Memcached 了。</p><h2 id="Redis-和-Memcached-有啥区别？"><a href="#Redis-和-Memcached-有啥区别？" class="headerlink" title="* Redis 和 Memcached 有啥区别？"></a>* Redis 和 Memcached 有啥区别？</h2><h3 id="Redis-支持复杂的数据结构"><a href="#Redis-支持复杂的数据结构" class="headerlink" title="Redis 支持复杂的数据结构"></a>Redis 支持复杂的数据结构</h3><p>Redis 相比 Memcached 来说，拥有更多的数据结构，能支持更丰富的数据操作。如果需要缓存能够支持更复杂的结构和操作， Redis 会是不错的选择。</p><h3 id="Redis-原生支持集群模式"><a href="#Redis-原生支持集群模式" class="headerlink" title="Redis 原生支持集群模式"></a>Redis 原生支持集群模式</h3><p>在 Redis3.x 版本中，便能支持 cluster 模式，而 Memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据。</p><h3 id="性能对比"><a href="#性能对比" class="headerlink" title="性能对比"></a>性能对比</h3><p>由于 Redis 只使用单核，而 Memcached 可以使用多核，所以平均每一个核上 Redis 在存储小数据时比 Memcached 性能更高。而在 100k 以上的数据中，Memcached 性能要高于 Redis。虽然 Redis 最近也在存储大数据的性能上进行优化，但是比起 Memcached，还是稍有逊色。</p><h2 id="Redis-的线程模型-重要，灵魂之处"><a href="#Redis-的线程模型-重要，灵魂之处" class="headerlink" title="* Redis 的线程模型(重要，灵魂之处)"></a>* Redis 的线程模型(重要，灵魂之处)</h2><p>Redis 内部使用<strong>文件事件处理器</strong> file event handler ，<strong>这个文件事件处理器是单线程的，所以 Redis 才叫做单线程的模型</strong>。<strong>它采用 IO 多路复用机制同时监听多个 socket，将产生事件的 socket 压入内存队列中，事件分派器根据 socket 上的事件类型来选择对应的事件处理器进行处理</strong>。</p><p>文件事件处理器的结构包含 4 个部分：</p><ul><li>多个 socket</li><li>IO 多路复用程序</li><li>文件事件分派器</li><li>事件处理器（连接应答处理器、命令请求处理器、命令回复处理器）</li></ul><p><strong>多个 socket 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 socket，会将产生事件的 socket 放入队列中排队，事件分派器每次从队列中取出一个 socket，根据 socket 的事件类型交给对应的事件处理器进行处理。</strong></p><p>来看客户端与 Redis 的一次通信过程：</p><p><img src="/2020/07/19/2020-07-19-xiao-li-fei-dao-redis/redis-single-thread-model.png" alt></p><p>要明白，通信是通过 socket 来完成的，不懂的同学可以先去看一看 socket 网络编程。</p><p>首先，Redis 服务端进程初始化的时候，会将 server socket 的 AE_READABLE 事件与连接应答处理器关联。</p><p>客户端 socket01 向 Redis 进程的 server socket 请求建立连接，此时 server socket 会产生一个 AE_READABLE 事件，IO 多路复用程序监听到 server socket 产生的事件后，将该 socket 压入队列中。文件事件分派器从队列中获取 socket，交给连接应答处理器。连接应答处理器会创建一个能与客户端通信的 socket01，并将该 socket01 后面产生的 AE_READABLE 事件与命令请求处理器关联。</p><p>假设此时客户端发送了一个 set key value 请求，此时 Redis 中的 socket01 会产生 AE_READABLE 事件，IO 多路复用程序将 socket01 压入队列，此时事件分派器从队列中获取到 socket01 产生的 AE_READABLE 事件，由于前面 socket01 的 AE_READABLE 事件已经与命令请求处理器关联，因此事件分派器将事件交给命令请求处理器来处理。命令请求处理器读取 socket01 的 key value 并在自己内存中完成 key value 的设置。操作完成后，它会将 socket01 的 AE_WRITABLE 事件与命令回复处理器关联。</p><p>如果此时客户端准备好接收返回结果了，那么 Redis 中的 socket01 会产生一个 AE_WRITABLE 事件，同样压入队列中，事件分派器找到相关联的命令回复处理器，由命令回复处理器对 socket01 输入本次操作的一个结果，比如 ok ，之后解除 socket01 的 AE_WRITABLE 事件与命令回复处理器的关联。</p><h2 id="为啥-Redis-单线程模型也能效率这么高？"><a href="#为啥-Redis-单线程模型也能效率这么高？" class="headerlink" title="* 为啥 Redis 单线程模型也能效率这么高？"></a>* 为啥 Redis 单线程模型也能效率这么高？</h2><ul><li>纯内存操作。</li><li>核心是基于非阻塞的 IO 多路复用机制。</li><li>C 语言实现，一般来说，C 语言实现的程序“距离”操作系统更近，执行速度相对会更快。</li><li>单线程反而避免了多线程的频繁上下文切换问题，预防了多线程可能产生的竞争问题。</li></ul><hr><h2 id="面试题-2"><a href="#面试题-2" class="headerlink" title="面试题"></a>面试题</h2><p>Redis 都有哪些数据类型？分别在哪些场景下使用比较合适？</p><h3 id="面试官心理分析-2"><a href="#面试官心理分析-2" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>除非是面试官感觉看你简历，是工作 3 年以内的比较初级的同学，可能对技术没有很深入的研究，面试官才会问这类问题。否则，在宝贵的面试时间里，面试官实在不想多问。</p><p>其实问这个问题，主要有两个原因：</p><ul><li>看看你到底有没有全面的了解 Redis 有哪些功能，一般怎么来用，啥场景用什么，就怕你别就会最简单的 KV 操作；</li><li>看看你在实际项目里都怎么玩儿过 Redis。</li></ul><p>要是你回答的不好，没说出几种数据类型，也没说什么场景，你完了，面试官对你印象肯定不好，觉得你平时就是做个简单的 set 和 get。</p><h2 id="Redis-都有哪些数据类型？分别在哪些场景下使用比较合适？"><a href="#Redis-都有哪些数据类型？分别在哪些场景下使用比较合适？" class="headerlink" title="* Redis 都有哪些数据类型？分别在哪些场景下使用比较合适？"></a>* Redis 都有哪些数据类型？分别在哪些场景下使用比较合适？</h2><p>Redis 主要有以下几种数据类型：</p><ul><li>Strings</li><li>Hashes</li><li>Lists</li><li>Sets</li><li>Sorted Sets</li></ul><blockquote><p>Redis 除了这 5 种数据类型之外，还有 Bitmaps、HyperLogLogs、Streams 等。</p></blockquote><h3 id="Strings"><a href="#Strings" class="headerlink" title="Strings"></a>Strings</h3><p>这是最简单的类型，就是普通的 set 和 get，做简单的 KV 缓存。</p><pre><code>set key1 value1</code></pre><h3 id="Hashes"><a href="#Hashes" class="headerlink" title="Hashes"></a>Hashes</h3><p>这个是类似 map 的一种结构，这个一般就是可以将结构化的数据，比如一个对象（前提是这个对象没嵌套其他的对象）给缓存在 Redis 里，然后每次读写缓存的时候，可以就操作 hash 里的某个字段。</p><pre><code>hset person name bingohset person age 20hset person id 1hget person name</code></pre><pre><code>person = {    &quot;name&quot;: &quot;bingo&quot;,    &quot;age&quot;: 20,    &quot;id&quot;: 1}</code></pre><h3 id="Lists"><a href="#Lists" class="headerlink" title="Lists"></a>Lists</h3><p>Lists 是有序列表，这个可以玩儿出很多花样。</p><p>比如可以通过 list 存储一些列表型的数据结构，类似粉丝列表、文章的评论列表之类的东西。</p><p>比如可以通过 lrange 命令，读取某个闭区间内的元素，可以基于 list 实现分页查询，这个是很棒的一个功能，基于 Redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西，性能高，就一页一页走。</p><pre><code># 0开始位置，-1结束位置，结束位置为-1时，表示列表的最后一个位置，即查看所有。lrange mylist 0 -1</code></pre><p>比如可以搞个简单的消息队列，从 list 头怼进去，从 list 尾巴那里弄出来。</p><pre><code>lpush mylist 1lpush mylist 2lpush mylist 3 4 5# 1rpop mylist</code></pre><h3 id="Sets"><a href="#Sets" class="headerlink" title="Sets"></a>Sets</h3><p>Sets 是无序集合，自动去重。</p><p>直接基于 set 将系统里需要去重的数据扔进去，自动就给去重了，如果你需要对一些数据进行快速的全局去重，你当然也可以基于 jvm 内存里的 HashSet 进行去重，但是如果你的某个系统部署在多台机器上呢？得基于 Redis 进行全局的 set 去重。</p><p>可以基于 set 玩儿交集、并集、差集的操作，比如交集吧，可以把两个人的粉丝列表整一个交集，看看俩人的共同好友是谁？对吧。</p><p>把两个大 V 的粉丝都放在两个 set 中，对两个 set 做交集。</p><pre><code>#-------操作一个set-------# 添加元素sadd mySet 1# 查看全部元素smembers mySet# 判断是否包含某个值sismember mySet 3# 删除某个/些元素srem mySet 1srem mySet 2 4# 查看元素个数scard mySet# 随机删除一个元素spop mySet#-------操作多个set-------# 将一个set的元素移动到另外一个setsmove yourSet mySet 2# 求两set的交集sinter yourSet mySet# 求两set的并集sunion yourSet mySet# 求在yourSet中而不在mySet中的元素sdiff yourSet mySet</code></pre><h3 id="Sorted-Sets"><a href="#Sorted-Sets" class="headerlink" title="Sorted Sets"></a>Sorted Sets</h3><p>Sorted Sets 是排序的 set，去重但可以排序，写进去的时候给一个分数，自动根据分数排序。</p><pre><code>zadd board 85 zhangsanzadd board 72 lisizadd board 96 wangwuzadd board 63 zhaoliu# 获取排名前三的用户（默认是升序，所以需要 rev 改为降序）zrevrange board 0 3# 获取某用户的排名zrank board zhaoliu</code></pre><hr><h2 id="面试题-3"><a href="#面试题-3" class="headerlink" title="面试题"></a>面试题</h2><p>Redis 的过期策略都有哪些？内存淘汰机制都有哪些？手写一下 LRU 代码实现？</p><h3 id="面试官心理分析-3"><a href="#面试官心理分析-3" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>如果你连这个问题都不知道，上来就懵了，回答不出来，那线上你写代码的时候，想当然的认为写进 Redis 的数据就一定会存在，后面导致系统各种 bug，谁来负责？</p><h3 id="面试题剖析"><a href="#面试题剖析" class="headerlink" title="面试题剖析"></a>面试题剖析</h3><ul><li><p>往 Redis 写入的数据怎么没了？</p><p>  可能有同学会遇到，在生产环境的 Redis 经常会丢掉一些数据，写进去了，过一会儿可能就没了。我的天，同学，你问这个问题就说明 Redis 你就没用对啊。Redis 是缓存，你给当存储了是吧？</p><p>  啥叫缓存？用内存当缓存。内存是无限的吗，内存是很宝贵而且是有限的，磁盘是廉价而且是大量的。可能一台机器就几十个 G 的内存，但是可以有几个 T 的硬盘空间。Redis 主要是基于内存来进行高性能、高并发的读写操作的。</p><p>  那既然内存是有限的，比如 Redis 就只能用 10G，你要是往里面写了 20G 的数据，会咋办？当然会干掉 10G 的数据，然后就保留 10G 的数据了。那干掉哪些数据？保留哪些数据？当然是干掉不常用的数据，保留常用的数据了。</p></li><li><p>数据明明过期了，怎么还占用着内存？</p><p>  这是由 Redis 的过期策略来决定。</p></li></ul><h2 id="Redis-过期策略"><a href="#Redis-过期策略" class="headerlink" title="* Redis 过期策略"></a>* Redis 过期策略</h2><p><strong>Redis 过期策略是：定期删除+惰性删除。</strong></p><p>所谓定期删除，指的是 Redis 默认是每隔 100ms 就随机抽取一些设置了过期时间的 key，检查其是否过期，如果过期就删除。</p><p>假设 Redis 里放了 10w 个 key，都设置了过期时间，你每隔几百毫秒，就检查 10w 个 key，那 Redis 基本上就死了，cpu 负载会很高的，消耗在你的检查过期 key 上了。注意，这里可不是每隔 100ms 就遍历所有的设置过期时间的 key，那样就是一场性能上的灾难。实际上 Redis 是每隔 100ms 随机抽取一些 key 来检查和删除的。</p><p>但是问题是，定期删除可能会导致很多过期 key 到了时间并没有被删除掉，那咋整呢？所以就是惰性删除了。这就是说，在你获取某个 key 的时候，Redis 会检查一下 ，这个 key 如果设置了过期时间那么是否过期了？如果过期了此时就会删除，不会给你返回任何东西。</p><blockquote><p>获取 key 的时候，如果此时 key 已经过期，就删除，不会返回任何东西。</p></blockquote><p>但是实际上这还是有问题的，如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期 key 堆积在内存里，导致 Redis 内存块耗尽了，咋整？</p><p>答案是：走内存淘汰机制。</p><h2 id="内存淘汰机制"><a href="#内存淘汰机制" class="headerlink" title="* 内存淘汰机制"></a>* 内存淘汰机制</h2><p>Redis 内存淘汰机制有以下几个：</p><ul><li>noeviction: 当内存不足以容纳新写入数据时，新写入操作会报错，这个一般没人用吧，实在是太恶心了。</li><li>allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的）。</li><li>allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个 key，这个一般没人用吧，为啥要随机，肯定是把最近最少使用的 key 给干掉啊。</li><li>volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 key（这个一般不太合适）。</li><li>volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个 key。</li><li>volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 key 优先移除。</li></ul><h2 id="手写一个-LRU-算法"><a href="#手写一个-LRU-算法" class="headerlink" title="* 手写一个 LRU 算法"></a>* 手写一个 LRU 算法</h2><p>你可以现场手写最原始的 LRU 算法，那个代码量太大了，似乎不太现实。</p><p>不求自己纯手工从底层开始打造出自己的 LRU，但是起码要知道如何利用已有的 JDK 数据结构实现一个 Java 版的 LRU。</p><pre><code>class LRUCache&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; {    private final int CACHE_SIZE;    /**     * 传递进来最多能缓存多少数据     *     * @param cacheSize 缓存大小     */    public LRUCache(int cacheSize) {        // true 表示让 linkedHashMap 按照访问顺序来进行排序，最近访问的放在头部，最老访问的放在尾部。        super((int) Math.ceil(cacheSize / 0.75) + 1, 0.75f, true);        CACHE_SIZE = cacheSize;    }    /**     * 钩子方法，通过put新增键值对的时候，若该方法返回true     * 便移除该map中最老的键和值     */    @Override    protected boolean removeEldestEntry(Map.Entry&lt;K, V&gt; eldest) {        // 当 map中的数据量大于指定的缓存个数的时候，就自动删除最老的数据。        return size() &gt; CACHE_SIZE;    }}</code></pre><hr><h2 id="面试题-4"><a href="#面试题-4" class="headerlink" title="面试题"></a>面试题</h2><p>如何保证 redis 的高并发和高可用？redis 的主从复制原理能介绍一下么？redis 的哨兵原理能介绍一下么？</p><h3 id="面试官心理分析-4"><a href="#面试官心理分析-4" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>其实问这个问题，主要是考考你，redis 单机能承载多高并发？如果单机扛不住如何扩容扛更多的并发？redis 会不会挂？既然 redis 会挂那怎么保证 redis 是高可用的？</p><p>其实针对的都是项目中你肯定要考虑的一些问题，如果你没考虑过，那确实你对生产系统中的问题思考太少。</p><h2 id="如何保证-redis-的高并发和高可用？"><a href="#如何保证-redis-的高并发和高可用？" class="headerlink" title="* 如何保证 redis 的高并发和高可用？"></a>* 如何保证 redis 的高并发和高可用？</h2><p>如果你用 redis 缓存技术的话，肯定要考虑如何用 redis 来加多台机器，保证 redis 是高并发的，还有就是如何让 redis 保证自己不是挂掉以后就直接死掉了，即 redis 高可用。</p><p>redis 实现高并发主要依靠主从架构，一主多从，一般来说，很多项目其实就足够了，单主用来写入数据，单机几万 QPS，多从用来查询数据，多个从实例可以提供每秒 10w 的 QPS。</p><p>如果想要在实现高并发的同时，容纳大量的数据，那么就需要 redis 集群，使用 redis 集群之后，可以提供每秒几十万的读写并发。</p><p>redis 高可用，如果是做主从架构部署，那么加上哨兵就可以了，就可以实现，任何一个实例宕机，可以进行主备切换。</p><p>由于此节内容较多，因此，会分为两个小节进行讲解。</p><ul><li><a href="https://adbo.gitee.io/2020/07/20/2020-07-20-redis-zhu-cong-jia-gou/">redis 主从架构</a></li><li><a href="https://adbo.gitee.io/2020/07/21/2020-07-21-redis-shao-bing/">redis 基于哨兵实现高可用</a></li></ul><p><img src="/2020/07/19/2020-07-19-xiao-li-fei-dao-redis/m2x109fl.bmp" alt></p><hr><h2 id="面试题-5"><a href="#面试题-5" class="headerlink" title="面试题"></a>面试题</h2><p>Redis 的持久化有哪几种方式？不同的持久化机制都有什么优缺点？持久化机制具体底层是如何实现的？</p><h3 id="面试官心理分析-5"><a href="#面试官心理分析-5" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>Redis 如果仅仅只是将数据缓存在内存里面，如果 Redis 宕机了再重启，内存里的数据就全部都弄丢了啊。你必须得用 Redis 的持久化机制，将数据写入内存的同时，异步的慢慢的将数据写入磁盘文件里，进行持久化。</p><p>如果 Redis 宕机重启，自动从磁盘上加载之前持久化的一些数据就可以了，也许会丢失少许数据，但是至少不会将所有数据都弄丢。</p><p>这个其实一样，针对的都是 Redis 的生产环境可能遇到的一些问题，就是 Redis 要是挂了再重启，内存里的数据不就全丢了？能不能重启的时候把数据给恢复了？</p><h3 id="面试题剖析-1"><a href="#面试题剖析-1" class="headerlink" title="面试题剖析"></a>面试题剖析</h3><p>持久化主要是做灾难恢复、数据恢复，也可以归类到高可用的一个环节中去，比如你 Redis 整个挂了，然后 Redis 就不可用了，你要做的事情就是让 Redis 变得可用，尽快变得可用。</p><p>重启 Redis，尽快让它对外提供服务，如果没做数据备份，这时候 Redis 启动了，也不可用啊，数据都没了。</p><p>很可能说，大量的请求过来，缓存全部无法命中，在 Redis 里根本找不到数据，这个时候就死定了，出现缓存雪崩问题。所有请求没有在 Redis 命中，就会去 mysql 数据库这种数据源头中去找，一下子 mysql 承接高并发，然后就挂了…</p><p>如果你把 Redis 持久化做好，备份和恢复方案做到企业级的程度，那么即使你的 Redis 故障了，也可以通过备份数据，快速恢复，一旦恢复立即对外提供服务。</p><h2 id="Redis-持久化的两种方式"><a href="#Redis-持久化的两种方式" class="headerlink" title="* Redis 持久化的两种方式"></a>* Redis 持久化的两种方式</h2><ul><li>RDB：RDB 持久化机制，是对 Redis 中的数据执行周期性的持久化。</li><li>AOF：AOF 机制对每条写入命令作为日志，以 append-only 的模式写入一个日志文件中，在 Redis 重启的时候，可以通过回放 AOF 日志中的写入指令来重新构建整个数据集。</li></ul><p>通过 RDB 或 AOF，都可以将 Redis 内存中的数据给持久化到磁盘上面来，然后可以将这些数据备份到别的地方去，比如说阿里云等云服务。</p><p>如果 Redis 挂了，服务器上的内存和磁盘上的数据都丢了，可以从云服务上拷贝回来之前的数据，放到指定的目录中，然后重新启动 Redis，Redis 就会自动根据持久化数据文件中的数据，去恢复内存中的数据，继续对外提供服务。</p><p><strong>如果同时使用 RDB 和 AOF 两种持久化机制，那么在 Redis 重启的时候，会使用 AOF 来重新构建数据，因为 AOF 中的数据更加完整。</strong></p><h2 id="Redis不同的持久化机制都有什么优缺点？"><a href="#Redis不同的持久化机制都有什么优缺点？" class="headerlink" title="* Redis不同的持久化机制都有什么优缺点？"></a>* Redis不同的持久化机制都有什么优缺点？</h2><h3 id="RDB-优缺点"><a href="#RDB-优缺点" class="headerlink" title="RDB 优缺点"></a>RDB 优缺点</h3><ul><li>RDB 会生成多个数据文件，每个数据文件都代表了某一个时刻中 Redis 的数据，这种多个数据文件的方式，非常适合做冷备，可以将这种完整的数据文件发送到一些远程的安全存储上去，比如说 Amazon 的 S3 云服务上去，在国内可以是阿里云的 ODPS 分布式存储上，以预定好的备份策略来定期备份 Redis 中的数据。</li><li>RDB 对 Redis 对外提供的读写服务，影响非常小，可以让 Redis 保持高性能，因为 Redis 主进程只需要 fork 一个子进程，让子进程执行磁盘 IO 操作来进行 RDB 持久化即可。</li><li>相对于 AOF 持久化机制来说，直接基于 RDB 数据文件来重启和恢复 Redis 进程，更加快速。</li><li>如果想要在 Redis 故障时，尽可能少的丢失数据，那么 RDB 没有 AOF 好。一般来说，RDB 数据快照文件，都是每隔 5 分钟，或者更长时间生成一次，这个时候就得接受一旦 Redis 进程宕机，那么会丢失最近 5 分钟的数据。</li><li>RDB 每次在 fork 子进程来执行 RDB 快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒。</li></ul><h3 id="AOF-优缺点"><a href="#AOF-优缺点" class="headerlink" title="AOF 优缺点"></a>AOF 优缺点</h3><ul><li>AOF 可以更好的保护数据不丢失，一般 AOF 会每隔 1 秒，通过一个后台线程执行一次 fsync 操作，最多丢失 1 秒钟的数据。</li><li>AOF 日志文件以 append-only 模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修复。</li><li>AOF 日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在 rewrite log 的时候，会对其中的指令进行压缩，创建出一份需要恢复数据的最小日志出来。在创建新日志文件的时候，老的日志文件还是照常写入。当新的 merge 后的日志文件 ready 的时候，再交换新老日志文件即可。</li><li>AOF 日志文件的命令通过可读较强的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用 flushall 命令清空了所有数据，只要这个时候后台 rewrite 还没有发生，那么就可以立即拷贝 AOF 文件，将最后一条 flushall 命令给删了，然后再将该 AOF 文件放回去，就可以通过恢复机制，自动恢复所有数据。</li><li>对于同一份数据来说，AOF 日志文件通常比 RDB 数据快照文件更大。</li><li>AOF 开启后，支持的写 QPS 会比 RDB 支持的写 QPS 低，因为 AOF 一般会配置成每秒 fsync 一次日志文件，当然，每秒一次 fsync ，性能也还是很高的。（如果实时写入，那么 QPS 会大降，Redis 性能会大大降低）</li><li>以前 AOF 发生过 bug，就是通过 AOF 记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似 AOF 这种较为复杂的基于命令日志 / merge / 回放的方式，比基于 RDB 每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有 bug。不过 AOF 就是为了避免 rewrite 过程导致的 bug，因此每次 rewrite 并不是基于旧的指令日志进行 merge 的，而是基于当时内存中的数据进行指令的重新构建，这样健壮性会好很多。</li></ul><h2 id="RDB-和-AOF-到底该如何选择"><a href="#RDB-和-AOF-到底该如何选择" class="headerlink" title="* RDB 和 AOF 到底该如何选择"></a>* RDB 和 AOF 到底该如何选择</h2><ul><li>不要仅仅使用 RDB，因为那样会导致你丢失很多数据；</li><li>也不要仅仅使用 AOF，因为那样有两个问题：第一，你通过 AOF 做冷备，没有 RDB 做冷备来的恢复速度更快；第二，RDB 每次简单粗暴生成数据快照，更加健壮，可以避免 AOF 这种复杂的备份和恢复机制的 bug；</li><li>Redis 支持同时开启开启两种持久化方式，我们可以综合使用 AOF 和 RDB 两种持久化机制，用 AOF 来保证数据不丢失，作为数据恢复的第一选择; 用 RDB 来做不同程度的冷备，在 AOF 文件都丢失或损坏不可用的时候，还可以使用 RDB 来进行快速的数据恢复。</li><li>如果同时使用 RDB 和 AOF 两种持久化机制，那么在 Redis 重启的时候，会使用 AOF 来重新构建数据，因为 AOF 中的数据更加完整。</li></ul><hr><h2 id="面试题-6"><a href="#面试题-6" class="headerlink" title="面试题"></a>面试题</h2><p>Redis 集群模式的工作原理能说一下么？在集群模式下，Redis 的 key 是如何寻址的？分布式寻址都有哪些算法？了解一致性 hash 算法吗？</p><h3 id="面试官心理分析-6"><a href="#面试官心理分析-6" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>在前几年，Redis 如果要搞几个节点，每个节点存储一部分的数据，得借助一些中间件来实现，比如说有 codis ，或者 twemproxy ，都有。有一些 Redis 中间件，你读写 Redis 中间件，Redis 中间件负责将你的数据分布式存储在多台机器上的 Redis 实例中。</p><p>这两年，Redis 不断在发展，Redis 也不断有新的版本，现在的 Redis 集群模式，可以做到在多台机器上，部署多个 Redis 实例，每个实例存储一部分的数据，同时每个 Redis 主实例可以挂 Redis 从实例，自动确保说，如果 Redis 主实例挂了，会自动切换到 Redis 从实例上来。</p><p>现在 Redis 的新版本，大家都是用 Redis cluster 的，也就是 Redis 原生支持的 Redis 集群模式，那么面试官肯定会就 Redis cluster 对你来个几连炮。要是你没用过 Redis cluster，正常，以前很多人用 codis 之类的客户端来支持集群，但是起码你得研究一下 Redis cluster 吧。</p><p>如果你的数据量很少，主要是承载高并发高性能的场景，比如你的缓存一般就几个 G，单机就足够了，可以使用 replication，一个 master 多个 slaves，要几个 slave 跟你要求的读吞吐量有关，然后自己搭建一个 sentinel 集群去保证 Redis 主从架构的高可用性。</p><p>Redis cluster，主要是针对海量数据+高并发+高可用的场景。Redis cluster 支撑 N 个 Redis master node，每个 master node 都可以挂载多个 slave node。这样整个 Redis 就可以横向扩容了。如果你要支撑更大数据量的缓存，那就横向扩容更多的 master 节点，每个 master 节点就能存放更多的数据了。</p><h2 id="Redis-集群模式的工作原理能说一下么？"><a href="#Redis-集群模式的工作原理能说一下么？" class="headerlink" title="* Redis 集群模式的工作原理能说一下么？"></a>* Redis 集群模式的工作原理能说一下么？</h2><h3 id="Redis-cluster-介绍"><a href="#Redis-cluster-介绍" class="headerlink" title="Redis cluster 介绍"></a>Redis cluster 介绍</h3><ul><li>自动将数据进行分片，每个 master 上放一部分数据</li><li>提供内置的高可用支持，部分 master 不可用时，还是可以继续工作的</li></ul><p>在 Redis cluster 架构下，每个 Redis 要放开两个端口号，比如一个是 6379，另外一个就是 加1w 的端口号，比如 16379。</p><p>16379 端口号是用来进行节点间通信的，也就是 cluster bus 的东西，cluster bus 的通信，用来进行故障检测、配置更新、故障转移授权。cluster bus 用了另外一种二进制的协议， gossip 协议，用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间。</p><h3 id="节点间的内部通信机制"><a href="#节点间的内部通信机制" class="headerlink" title="节点间的内部通信机制"></a>节点间的内部通信机制</h3><h4 id="基本通信原理"><a href="#基本通信原理" class="headerlink" title="基本通信原理"></a>基本通信原理</h4><p>集群元数据的维护有两种方式：集中式、Gossip 协议。Redis cluster 节点间采用 gossip 协议进行通信。</p><p>集中式是将集群元数据（节点信息、故障等等）几种存储在某个节点上。集中式元数据集中存储的一个典型代表，就是大数据领域的 storm 。它是分布式的大数据实时计算引擎，是集中式的元数据存储的结构，底层基于 zookeeper（分布式协调的中间件）对所有元数据进行存储维护。</p><p><img src="/2020/07/19/2020-07-19-xiao-li-fei-dao-redis/zookeeper-centralized-storage.png" alt></p><p>Redis 维护集群元数据采用另一个方式， gossip 协议，所有节点都持有一份元数据，不同的节点如果出现了元数据的变更，就不断将元数据发送给其它的节点，让其它节点也进行元数据的变更。</p><p><img src="/2020/07/19/2020-07-19-xiao-li-fei-dao-redis/redis-gossip.png" alt></p><p>集中式的好处在于，元数据的读取和更新，时效性非常好，一旦元数据出现了变更，就立即更新到集中式的存储中，其它节点读取的时候就可以感知到；不好在于，所有的元数据的更新压力全部集中在一个地方，可能会导致元数据的存储有压力。</p><p>gossip 好处在于，元数据的更新比较分散，不是集中在一个地方，更新请求会陆陆续续打到所有节点上去更新，降低了压力；不好在于，元数据的更新有延时，可能导致集群中的一些操作会有一些滞后。</p><ul><li>10000 端口：每个节点都有一个专门用于节点间通信的端口，就是自己提供服务的端口号+10000，比如 7001，那么用于节点间通信的就是 17001 端口。每个节点每隔一段时间都会往另外几个节点发送 ping 消息，同时其它几个节点接收到 ping 之后返回 pong 。</li><li>交换的信息：信息包括故障信息，节点的增加和删除，hash slot 信息等等。</li></ul><h4 id="gossip-协议"><a href="#gossip-协议" class="headerlink" title="gossip 协议"></a>gossip 协议</h4><p>gossip 协议包含多种消息，包含 ping , pong , meet , fail 等等。</p><ul><li><p>meet：某个节点发送 meet 给新加入的节点，让新节点加入集群中，然后新节点就会开始与其它节点进行通信。</p><pre><code>  Redis-trib.rb add-node</code></pre><p>  其实内部就是发送了一个 gossip meet 消息给新加入的节点，通知那个节点去加入我们的集群。</p></li><li><p>ping：每个节点都会频繁给其它节点发送 ping，其中包含自己的状态还有自己维护的集群元数据，互相通过 ping 交换元数据。</p></li><li><p>pong：返回 ping 和 meeet，包含自己的状态和其它信息，也用于信息广播和更新。</p></li><li><p>fail：某个节点判断另一个节点 fail 之后，就发送 fail 给其它节点，通知其它节点说，某个节点宕机啦。</p></li></ul><h4 id="ping-消息深入"><a href="#ping-消息深入" class="headerlink" title="ping 消息深入"></a>ping 消息深入</h4><p>ping 时要携带一些元数据，如果很频繁，可能会加重网络负担。</p><p>每个节点每秒会执行 10 次 ping，每次会选择 5 个最久没有通信的其它节点。当然如果发现某个节点通信延时达到了 cluster_node_timeout / 2 ，那么立即发送 ping，避免数据交换延时过长，落后的时间太长了。比如说，两个节点之间都 10 分钟没有交换数据了，那么整个集群处于严重的元数据不一致的情况，就会有问题。所以 cluster_node_timeout 可以调节，如果调得比较大，那么会降低 ping 的频率。</p><p>每次 ping，会带上自己节点的信息，还有就是带上 1/10 其它节点的信息，发送出去，进行交换。至少包含 3 个其它节点的信息，最多包含 总节点数减 2 个其它节点的信息。</p><h3 id="Redis-cluster-的高可用与主备切换原理"><a href="#Redis-cluster-的高可用与主备切换原理" class="headerlink" title="Redis cluster 的高可用与主备切换原理"></a>Redis cluster 的高可用与主备切换原理</h3><p>Redis cluster 的高可用的原理，几乎跟哨兵是类似的。</p><h5 id="从节点过滤"><a href="#从节点过滤" class="headerlink" title="从节点过滤"></a>从节点过滤</h5><p>对宕机的 master node，从其所有的 slave node 中，选择一个切换成 master node。</p><p>检查每个 slave node 与 master node 断开连接的时间，如果超过了 cluster-node-timeout * cluster-slave-validity-factor ，那么就没有资格切换成 master 。</p><h4 id="从节点选举"><a href="#从节点选举" class="headerlink" title="从节点选举"></a>从节点选举</h4><p>每个从节点，都根据自己对 master 复制数据的 offset，来设置一个选举时间，offset 越大（复制数据越多）的从节点，选举时间越靠前，优先进行选举。</p><p>所有的 master node 开始 slave 选举投票，给要进行选举的 slave 进行投票，如果大部分 master node （N/2 + 1） 都投票给了某个从节点，那么选举通过，那个从节点可以切换成 master。</p><p>从节点执行主备切换，从节点切换为主节点。</p><h4 id="与哨兵比较"><a href="#与哨兵比较" class="headerlink" title="与哨兵比较"></a>与哨兵比较</h4><p>整个流程跟哨兵相比，非常类似，所以说，Redis cluster 功能强大，直接集成了 replication 和 sentinel 的功能。</p><h2 id="分布式寻址算法"><a href="#分布式寻址算法" class="headerlink" title="* 分布式寻址算法"></a>* 分布式寻址算法</h2><ul><li>hash 算法（大量缓存重建）</li><li>一致性 hash 算法（自动缓存迁移）+ 虚拟节点（自动负载均衡）</li><li>Redis cluster 的 hash slot 算法</li></ul><h3 id="hash-算法"><a href="#hash-算法" class="headerlink" title="hash 算法"></a>hash 算法</h3><p>来了一个 key，首先计算 hash 值，然后对节点数取模。然后打在不同的 master 节点上。一旦某一个 master 节点宕机，所有请求过来，都会基于最新的剩余 master 节点数去取模，尝试去取数据。这会导致大部分的请求过来，全部无法拿到有效的缓存，导致大量的流量涌入数据库。</p><p><img src="/2020/07/19/2020-07-19-xiao-li-fei-dao-redis/hash.png" alt></p><h3 id="一致性-hash-算法"><a href="#一致性-hash-算法" class="headerlink" title="一致性 hash 算法"></a>一致性 hash 算法</h3><p>一致性 hash 算法将整个 hash 值空间组织成一个虚拟的圆环，整个空间按顺时针方向组织，下一步将各个 master 节点（使用服务器的 ip 或主机名）进行 hash。这样就能确定每个节点在其哈希环上的位置。</p><p>来了一个 key，首先计算 hash 值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，遇到的第一个 master 节点就是 key 所在位置。</p><p>在一致性哈希算法中，如果一个节点挂了，受影响的数据仅仅是此节点到环空间前一个节点（沿着逆时针方向行走遇到的第一个节点）之间的数据，其它不受影响。增加一个节点也同理。</p><p>燃鹅，一致性哈希算法在节点太少时，容易因为节点分布不均匀而造成缓存热点的问题。为了解决这种热点问题，一致性 hash 算法引入了虚拟节点机制，即对每一个节点计算多个 hash，每个计算结果位置都放置一个虚拟节点。这样就实现了数据的均匀分布，负载均衡。</p><p><img src="/2020/07/19/2020-07-19-xiao-li-fei-dao-redis/consistent-hashing-algorithm.png" alt></p><h3 id="Redis-cluster-的-hash-slot-算法"><a href="#Redis-cluster-的-hash-slot-算法" class="headerlink" title="Redis cluster 的 hash slot 算法"></a>Redis cluster 的 hash slot 算法</h3><p>Redis cluster 有固定的 16384 个 hash slot，对每个 key 计算 CRC16 值，然后对 16384 取模，可以获取 key 对应的 hash slot。</p><p>Redis cluster 中每个 master 都会持有部分 slot，比如有 3 个 master，那么可能每个 master 持有 5000 多个 hash slot。hash slot 让 node 的增加和移除很简单，增加一个 master，就将其他 master 的 hash slot 移动部分过去，减少一个 master，就将它的 hash slot 移动到其他 master 上去。移动 hash slot 的成本是非常低的。客户端的 api，可以对指定的数据，让他们走同一个 hash slot，通过 hash tag 来实现。</p><p>任何一台机器宕机，另外两个节点，不影响的。因为 key 找的是 hash slot，不是机器。</p><p><img src="/2020/07/19/2020-07-19-xiao-li-fei-dao-redis/hash-slot.png" alt></p><hr><h2 id="面试题-7"><a href="#面试题-7" class="headerlink" title="面试题"></a>面试题</h2><p>了解什么是 Redis 的雪崩、穿透和击穿？Redis 崩溃之后会怎么样？系统该如何应对这种情况？如何处理 Redis 的穿透？</p><h3 id="面试官心理分析-7"><a href="#面试官心理分析-7" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>其实这是问到缓存必问的，因为缓存雪崩和穿透，是缓存最大的两个问题，要么不出现，一旦出现就是致命性的问题，所以面试官一定会问你。</p><h2 id="缓存雪崩-重要"><a href="#缓存雪崩-重要" class="headerlink" title="* 缓存雪崩(重要)"></a>* 缓存雪崩(重要)</h2><p>对于系统 A，假设每天高峰期每秒 5000 个请求，本来缓存在高峰期可以扛住每秒 4000 个请求，但是缓存机器意外发生了全盘宕机。缓存挂了，此时 1 秒 5000 个请求全部落数据库，数据库必然扛不住，它会报一下警，然后就挂了。此时，如果没有采用什么特别的方案来处理这个故障，DBA 很着急，重启数据库，但是数据库立马又被新的流量给打死了。</p><p>这就是缓存雪崩。</p><p><img src="/2020/07/19/2020-07-19-xiao-li-fei-dao-redis/redis-caching-avalanche.png" alt></p><p>大约在 3 年前，国内比较知名的一个互联网公司，曾因为缓存事故，导致雪崩，后台系统全部崩溃，事故从当天下午持续到晚上凌晨 3~4 点，公司损失了几千万。</p><p>缓存雪崩的事前事中事后的解决方案如下：</p><ul><li>事前：Redis 高可用，主从+哨兵，Redis cluster，避免全盘崩溃。</li><li>事中：本地 ehcache 缓存 + hystrix 限流&amp;降级，避免 MySQL 被打死。</li><li>事后：Redis 持久化，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。</li></ul><p><img src="/2020/07/19/2020-07-19-xiao-li-fei-dao-redis/redis-caching-avalanche-solution.png" alt></p><p>用户发送一个请求，系统 A 收到请求后，先查本地 ehcache 缓存，如果没查到再查 Redis。如果 ehcache 和 Redis 都没有，再查数据库，将数据库中的结果，写入 ehcache 和 Redis 中。</p><p>限流组件，可以设置每秒的请求，有多少能通过组件，剩余的未通过的请求，怎么办？走降级！可以返回一些默认的值，或者友情提示，或者空值。</p><p>好处：</p><ul><li>数据库绝对不会死，限流组件确保了每秒只有多少个请求能通过。</li><li>只要数据库不死，就是说，对用户来说，2/5 的请求都是可以被处理的。</li><li>只要有 2/5 的请求可以被处理，就意味着你的系统没死，对用户来说，可能就是点击几次刷不出来页面，但是多点几次，就可以刷出来了。</li></ul><h2 id="缓存穿透-重要"><a href="#缓存穿透-重要" class="headerlink" title="* 缓存穿透(重要)"></a>* 缓存穿透(重要)</h2><p>对于系统A，假设一秒 5000 个请求，结果其中 4000 个请求是黑客发出的恶意攻击。</p><p>黑客发出的那 4000 个攻击，缓存中查不到，每次你去数据库里查，也查不到。</p><p>举个栗子。数据库 id 是从 1 开始的，结果黑客发过来的请求 id 全部都是负数。这样的话，缓存中不会有，请求每次都“视缓存于无物”，直接查询数据库。这种恶意攻击场景的缓存穿透就会直接把数据库给打死。</p><p><img src="/2020/07/19/2020-07-19-xiao-li-fei-dao-redis/redis-caching-penetration.png" alt></p><ul><li><p>缓存无效 key : 如果缓存和数据库都查不到某个 key 的数据就写一个到 redis 中去并设置过期时间，具体命令如下：SET key value EX 10086。这种方式可以解决请求的 key 变化不频繁的情况，如果黑客恶意攻击，每次构建不同的请求key，会导致 redis 中缓存大量无效的 key 。很明显，这种方案并不能从根本上解决此问题。如果非要用这种方式来解决穿透问题的话，尽量将无效的 key 的过期时间设置短一点比如 1 分钟。</p></li><li><p>布隆过滤器：布隆过滤器是一个非常神奇的数据结构，通过它我们可以非常方便地判断一个给定数据是否存在与海量数据中。具体是这样做的：把所有可能存在的请求的值都存放在布隆过滤器中，当用户请求过来，我会先判断用户发来的请求的值是否存在于布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端，存在的话才会走下面的流程</p><p>  <img src="/2020/07/19/2020-07-19-xiao-li-fei-dao-redis/k0ml0thj.bmp" alt></p><p>  <a href="https://adbo.gitee.io/2020/07/22/2020-07-22-bu-long-guo-lu-qi/">布隆过滤器详解</a></p></li></ul><h2 id="缓存击穿-重要"><a href="#缓存击穿-重要" class="headerlink" title="* 缓存击穿(重要)"></a>* 缓存击穿(重要)</h2><p>缓存击穿，就是说某个 key 非常热点，访问非常频繁，处于集中式高并发访问的情况，当这个 key 在失效的瞬间，大量的请求就击穿了缓存，直接请求数据库，就像是在一道屏障上凿开了一个洞。</p><p>不同场景下的解决方式可如下：</p><ul><li>若缓存的数据是基本不会发生更新的，则可尝试将该热点数据设置为永不过期。</li><li>若缓存的数据更新不频繁，且缓存刷新的整个流程耗时较少的情况下，则可以采用基于 Redis、zookeeper 等分布式中间件的分布式互斥锁，或者本地互斥锁以保证仅少量的请求能请求数据库并重新构建缓存，其余线程则在锁释放后能访问到新缓存。</li><li>若缓存的数据更新频繁或者在缓存刷新的流程耗时较长的情况下，可以利用定时线程在缓存过期前主动地重新构建缓存或者延后缓存的过期时间，以保证所有的请求能一直访问到对应的缓存。</li></ul><hr><h2 id="面试题-8"><a href="#面试题-8" class="headerlink" title="面试题"></a>面试题</h2><p>如何保证缓存与数据库的双写一致性？</p><h3 id="面试官心理分析-8"><a href="#面试官心理分析-8" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>你只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？</p><h3 id="面试题剖析-2"><a href="#面试题剖析-2" class="headerlink" title="面试题剖析"></a>面试题剖析</h3><p>一般来说，如果允许缓存可以稍微的跟数据库偶尔有不一致的情况，也就是说如果你的系统不是严格要求 “缓存+数据库” 必须保持一致性的话，最好不要做这个方案，即：读请求和写请求串行化，串到一个内存队列里去。</p><p>串行化可以保证一定不会出现不一致的情况，但是它也会导致系统的吞吐量大幅度降低，用比正常情况下多几倍的机器去支撑线上的一个请求。</p><h2 id="如何保证缓存与数据库的双写一致性？"><a href="#如何保证缓存与数据库的双写一致性？" class="headerlink" title="* 如何保证缓存与数据库的双写一致性？"></a>* 如何保证缓存与数据库的双写一致性？</h2><h3 id="Cache-Aside-Pattern"><a href="#Cache-Aside-Pattern" class="headerlink" title="Cache Aside Pattern"></a>Cache Aside Pattern</h3><p>最经典的缓存+数据库读写的模式，就是 Cache Aside Pattern。</p><ul><li>读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。</li><li>更新的时候，先更新数据库，然后再删除缓存。</li></ul><p>为什么是删除缓存，而不是更新缓存？</p><p>原因很简单，很多时候，在复杂点的缓存场景，缓存不单单是数据库中直接取出来的值。</p><p>比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据并进行运算，才能计算出缓存最新的值的。</p><p>另外更新缓存的代价有时候是很高的。是不是说，每次修改数据库的时候，都一定要将其对应的缓存更新一份？也许有的场景是这样，但是对于比较复杂的缓存数据计算的场景，就不是这样了。如果你频繁修改一个缓存涉及的多个表，缓存也频繁更新。但是问题在于，这个缓存到底会不会被频繁访问到？</p><p>举个栗子，一个缓存涉及的表的字段，在 1 分钟内就修改了 20 次，或者是 100 次，那么缓存更新 20 次、100 次；但是这个缓存在 1 分钟内只被读取了 1 次，有大量的冷数据。实际上，如果你只是删除缓存的话，那么在 1 分钟内，这个缓存不过就重新计算一次而已，开销大幅度降低。用到缓存才去算缓存。</p><p>其实删除缓存，而不是更新缓存，就是一个 lazy 计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算。像 mybatis，hibernate，都有懒加载思想。查询一个部门，部门带了一个员工的 list，没有必要说每次查询部门，都把里面的 1000 个员工的数据也同时查出来啊。80% 的情况，查这个部门，就只是要访问这个部门的信息就可以了。先查部门，同时要访问里面的员工，那么这个时候只有在你要访问里面的员工的时候，才会去数据库里面查询 1000 个员工。</p><h3 id="最初级的缓存不一致问题及解决方案"><a href="#最初级的缓存不一致问题及解决方案" class="headerlink" title="最初级的缓存不一致问题及解决方案"></a>最初级的缓存不一致问题及解决方案</h3><p>问题：先更新数据库，再删除缓存。如果删除缓存失败了，那么会导致数据库中是新数据，缓存中是旧数据，数据就出现了不一致。</p><p><img src="/2020/07/19/2020-07-19-xiao-li-fei-dao-redis/redis-junior-inconsistent.png" alt></p><p>解决思路：先删除缓存，再更新数据库。如果数据库更新失败了，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致。因为读的时候缓存没有，所以去读了数据库中的旧数据，然后更新到缓存中。</p><h3 id="比较复杂的数据不一致问题分析"><a href="#比较复杂的数据不一致问题分析" class="headerlink" title="比较复杂的数据不一致问题分析"></a>比较复杂的数据不一致问题分析</h3><p>数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改。一个请求过来，去读缓存，发现缓存空了，去查询数据库，查到了修改前的旧数据，放到了缓存中。随后数据变更的程序完成了数据库的修改。完了，数据库和缓存中的数据不一样了…</p><p>为什么上亿流量高并发场景下，缓存会出现这个问题？</p><p>只有在对一个数据在并发的进行读写的时候，才可能会出现这种问题。其实如果说你的并发量很低的话，特别是读并发很低，每天访问量就 1 万次，那么很少的情况下，会出现刚才描述的那种不一致的场景。但是问题是，如果每天的是上亿的流量，每秒并发读是几万，每秒只要有数据更新的请求，就可能会出现上述的数据库+缓存不一致的情况。</p><p><strong>解决方案如下：</strong></p><p>更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个 jvm 内部队列中。读取数据的时候，如果发现数据不在缓存中，那么将重新执行“读取数据+更新缓存”的操作，根据唯一标识路由之后，也发送到同一个 jvm 内部队列中。</p><p>一个队列对应一个工作线程，每个工作线程串行拿到对应的操作，然后一条一条的执行。这样的话，一个数据变更的操作，先删除缓存，然后再去更新数据库，但是还没完成更新。此时如果一个读请求过来，没有读到缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成。</p><p>这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可。</p><p>待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中。</p><p>如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回；如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值。</p><p>高并发的场景下，该解决方案要注意的问题：</p><ul><li>读请求长时阻塞</li></ul><p>由于读请求进行了非常轻度的异步化，所以一定要注意读超时的问题，每个读请求必须在超时时间范围内返回。</p><p>该解决方案，最大的风险点在于说，可能数据更新很频繁，导致队列中积压了大量更新操作在里面，然后读请求会发生大量的超时，最后导致大量的请求直接走数据库。务必通过一些模拟真实的测试，看看更新数据的频率是怎样的。</p><p>另外一点，因为一个队列中，可能会积压针对多个数据项的更新操作，因此需要根据自己的业务情况进行测试，可能需要部署多个服务，每个服务分摊一些数据的更新操作。如果一个内存队列里居然会挤压 100 个商品的库存修改操作，每个库存修改操作要耗费 10ms 去完成，那么最后一个商品的读请求，可能等待 10 * 100 = 1000ms = 1s 后，才能得到数据，这个时候就导致读请求的长时阻塞。</p><p>一定要做根据实际业务系统的运行情况，去进行一些压力测试，和模拟线上环境，去看看最繁忙的时候，内存队列可能会挤压多少更新操作，可能会导致最后一个更新操作对应的读请求，会 hang 多少时间，如果读请求在 200ms 返回，如果你计算过后，哪怕是最繁忙的时候，积压 10 个更新操作，最多等待 200ms，那还可以的。</p><p>如果一个内存队列中可能积压的更新操作特别多，那么你就要加机器，让每个机器上部署的服务实例处理更少的数据，那么每个内存队列中积压的更新操作就会越少。</p><p>其实根据之前的项目经验，一般来说，数据的写频率是很低的，因此实际上正常来说，在队列中积压的更新操作应该是很少的。像这种针对读高并发、读缓存架构的项目，一般来说写请求是非常少的，每秒的 QPS 能到几百就不错了。</p><p>我们来实际粗略测算一下。</p><p>如果一秒有 500 的写操作，如果分成 5 个时间片，每 200ms 就 100 个写操作，放到 20 个内存队列中，每个内存队列，可能就积压 5 个写操作。每个写操作性能测试后，一般是在 20ms 左右就完成，那么针对每个内存队列的数据的读请求，也就最多 hang 一会儿，200ms 以内肯定能返回了。</p><p>经过刚才简单的测算，我们知道，单机支撑的写 QPS 在几百是没问题的，如果写 QPS 扩大了 10 倍，那么就扩容机器，扩容 10 倍的机器，每个机器 20 个队列。</p><ul><li>读请求并发量过高</li></ul><p>这里还必须做好压力测试，确保恰巧碰上上述情况的时候，还有一个风险，就是突然间大量读请求会在几十毫秒的延时 hang 在服务上，看服务能不能扛的住，需要多少机器才能扛住最大的极限情况的峰值。</p><p>但是因为并不是所有的数据都在同一时间更新，缓存也不会同一时间失效，所以每次可能也就是少数数据的缓存失效了，然后那些数据对应的读请求过来，并发量应该也不会特别大。</p><ul><li>多服务实例部署的请求路由</li></ul><p>可能这个服务部署了多个实例，那么必须保证说，执行数据更新操作，以及执行缓存更新操作的请求，都通过 Nginx 服务器路由到相同的服务实例上。</p><p>比如说，对同一个商品的读写请求，全部路由到同一台机器上。可以自己去做服务间的按照某个请求参数的 hash 路由，也可以用 Nginx 的 hash 路由功能等等。</p><ul><li>热点商品的路由问题，导致请求的倾斜</li></ul><p>万一某个商品的读写请求特别高，全部打到相同的机器的相同的队列里面去了，可能会造成某台机器的压力过大。就是说，因为只有在商品数据更新的时候才会清空缓存，然后才会导致读写并发，所以其实要根据业务系统去看，如果更新频率不是太高的话，这个问题的影响并不是特别大，但是的确可能某些机器的负载会高一些。</p><hr><h2 id="面试题-9"><a href="#面试题-9" class="headerlink" title="面试题"></a>面试题</h2><p>Redis 的并发竞争问题是什么？如何解决这个问题？了解 Redis 事务的 CAS 方案吗？</p><h3 id="面试官心理分析-9"><a href="#面试官心理分析-9" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>这个也是线上非常常见的一个问题，就是多客户端同时并发写一个 key，可能本来应该先到的数据后到了，导致数据版本错了；或者是多客户端同时获取一个 key，修改值之后再写回去，只要顺序错了，数据就错了。</p><p>而且 Redis 自己就有天然解决这个问题的 CAS 类的乐观锁方案。</p><h2 id="Redis-的并发竞争问题是什么？如何解决这个问题？"><a href="#Redis-的并发竞争问题是什么？如何解决这个问题？" class="headerlink" title="* Redis 的并发竞争问题是什么？如何解决这个问题？"></a>* Redis 的并发竞争问题是什么？如何解决这个问题？</h2><p>某个时刻，多个系统实例都去更新某个 key。可以基于 zookeeper 实现分布式锁。每个系统通过 zookeeper 获取分布式锁，确保同一时间，只能有一个系统实例在操作某个 key，别人都不允许读和写。</p><p><img src="/2020/07/19/2020-07-19-xiao-li-fei-dao-redis/zookeeper-distributed-lock.png" alt></p><p>你要写入缓存的数据，都是从 mysql 里查出来的，都得写入 mysql 中，写入 mysql 中的时候必须保存一个时间戳，从 mysql 查出来的时候，时间戳也查出来。</p><p>每次要写之前，先判断一下当前这个 value 的时间戳是否比缓存里的 value 的时间戳要新。如果是的话，那么可以写，否则，就不能用旧的数据覆盖新的数据。</p><hr><h2 id="面试题-10"><a href="#面试题-10" class="headerlink" title="面试题"></a>面试题</h2><p>生产环境中的 Redis 是怎么部署的？</p><h3 id="面试官心理分析-10"><a href="#面试官心理分析-10" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>看看你了解不了解你们公司的 Redis 生产集群的部署架构，如果你不了解，那么确实你就很失职了，你的 Redis 是主从架构？集群架构？用了哪种集群方案？有没有做高可用保证？有没有开启持久化机制确保可以进行数据恢复？线上 Redis 给几个 G 的内存？设置了哪些参数？压测后你们 Redis 集群承载多少 QPS？</p><p>兄弟，这些你必须是门儿清的，否则你确实是没好好思考过。</p><h2 id="生产环境中的-Redis-是怎么部署的？"><a href="#生产环境中的-Redis-是怎么部署的？" class="headerlink" title="* 生产环境中的 Redis 是怎么部署的？"></a>* 生产环境中的 Redis 是怎么部署的？</h2><p>Redis cluster，10 台机器，5 台机器部署了 Redis 主实例，另外 5 台机器部署了 Redis 的从实例，每个主实例挂了一个从实例，5 个节点对外提供读写服务，每个节点的读写高峰 QPS 可能可以达到每秒 5 万，5 台机器最多是 25 万读写请求每秒。</p><p>机器是什么配置？32G 内存+ 8 核 CPU + 1T 磁盘，但是分配给 Redis 进程的是 10g 内存，一般线上生产环境，Redis 的内存尽量不要超过 10g，超过 10g 可能会有问题。</p><p>5 台机器对外提供读写，一共有 50g 内存。</p><p>因为每个主实例都挂了一个从实例，所以是高可用的，任何一个主实例宕机，都会自动故障迁移，Redis 从实例会自动变成主实例继续提供读写服务。</p><p>你往内存里写的是什么数据？每条数据的大小是多少？商品数据，每条数据是 10kb。100 条数据是 1mb，10 万条数据是 1g。常驻内存的是 200 万条商品数据，占用内存是 20g，仅仅不到总内存的 50%。目前高峰期每秒就是 3500 左右的请求量。</p><p>其实大型的公司，会有基础架构的 team 负责缓存集群的运维。</p><p><img src="/2020/07/19/2020-07-19-xiao-li-fei-dao-redis/hjevsx78.bmp" alt></p>]]></content>
      
      
      <categories>
          
          <category> 小李飞刀 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>消息队列</title>
      <link href="/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/"/>
      <url>/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/</url>
      
        <content type="html"><![CDATA[<h2 id="消息队列面试场景"><a href="#消息队列面试场景" class="headerlink" title="消息队列面试场景"></a>消息队列面试场景</h2><p>面试官：你好。</p><p>候选人：你好。</p><p>（面试官在你的简历上面看到了，呦，有个亮点，你在项目里用过 MQ ，比如说你用过 ActiveMQ ）</p><p>面试官：你在系统里用过消息队列吗？（面试官在随和的语气中展开了面试）</p><p>候选人：用过的（此时感觉没啥）</p><p>面试官：那你说一下你们在项目里是怎么用消息队列的？</p><p>候选人：巴拉巴拉，“我们啥啥系统发送个啥啥消息到队列，别的系统来消费啥啥的。比如我们有个订单系统，订单系统每次下一个新的订单的时候，就会发送一条消息到 ActiveMQ 里面去，后台有个库存系统负责获取消息然后更新库存。”</p><p>（部分同学在这里会进入一个误区，就是你仅仅就是知道以及回答你们是怎么用这个消息队列的，用这个消息队列来干了个什么事情？）</p><p>面试官：那你们为什么使用消息队列啊？你的订单系统不发送消息到 MQ ，直接订单系统调用库存系统一个接口，咔嚓一下，直接就调用成功，库存不也就更新了。</p><p>候选人：额。。。（楞了一下，为什么？我没怎么仔细想过啊，老大让用就用了），硬着头皮胡言乱语了几句。</p><p>（面试官此时听你楞了一下，然后听你胡言乱语了几句，开始心里觉得有点儿那什么了，怀疑你之前就压根儿没思考过这问题）</p><p>面试官：那你说说用消息队列都有什么优点和缺点？</p><p>（面试官此时心里想的是，你的 MQ 在项目里为啥要用，你没怎么考虑过，那我稍微简单点儿，我问问你消息队列你之前有没有考虑过如果用的话，优点和缺点分别是啥？）</p><p>候选人：这个。。。（确实平时没怎么考虑过这个问题啊。。。胡言乱语了）</p><p>（面试官此时心里已经更觉得你这哥儿们不行，平时都没什么思考）</p><p>面试官： Kafka 、 ActiveMQ 、 RabbitMQ 、 RocketMQ 都有什么区别？</p><p>（面试官问你这个问题，就是说，绕过比较虚的话题，直接看看你对各种 MQ 中间件是否了解，是否做过功课，是否做过调研）</p><p>候选人：我们就用过 ActiveMQ ，所以别的没用过。。。区别，也不太清楚。。。</p><p>（面试官此时更是觉得你这哥儿们平时就是瞎用，根本就没什么思考，觉得不行）</p><p>面试官：那你们是如何保证消息队列的高可用啊？</p><p>候选人：这个。。。我平时就是简单走 API 调用一下，不太清楚消息队列怎么部署的。。。</p><p>面试官：如何保证消息不被重复消费啊？如何保证消费的时候是幂等的啊？</p><p>候选人：啥？（ MQ 不就是写入&amp;消费就可以了，哪来这么多问题）</p><p>面试官：如何保证消息的可靠性传输啊？要是消息丢失了怎么办啊？</p><p>候选人：我们没怎么丢过消息啊。。。</p><p>面试官：那如何保证消息的顺序性？</p><p>候选人：顺序性？什么意思？我为什么要保证消息的顺序性？它不是本来就有顺序吗？</p><p>面试官：如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？有几百万消息持续积压几小时，说说怎么解决？</p><p>候选人：不是，我这平时没遇到过这些问题啊，就是简单用用，知道 MQ 的一些功能。</p><p>面试官：如果让你写一个消息队列，该如何进行架构设计啊？说一下你的思路。</p><p>候选人：。。。。。我还是走吧。。。。</p><hr><p>这其实是面试官的一种面试风格，就是说面试官的问题不是发散的，而是从一个小点慢慢铺开。比如说面试官可能会跟你聊聊高并发话题，就这个话题里面跟你聊聊缓存、 MQ 等等东西，由浅入深，一步步深挖。</p><p>其实上面是一个非常典型的关于消息队列的技术考察过程，好的面试官一定是从你做过的某一个点切入，然后层层展开深入考察，一个接一个问，直到把这个技术点刨根问底，问到最底层。</p><hr><h2 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h2><ol><li>为什么使用消息队列？</li><li>消息队列有什么优点和缺点？</li><li>Kafka、ActiveMQ、RabbitMQ、RocketMQ 都有什么区别，以及适合哪些场景？</li></ol><h3 id="面试官心理分析"><a href="#面试官心理分析" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>其实面试官主要是想看看：</p><ol><li><p>第一，你知不知道你们系统里为什么要用消息队列这个东西？</p><p> 不少候选人，说自己项目里用了 Redis、MQ，但是其实他并不知道自己为什么要用这个东西。其实说白了，就是为了用而用，或者是别人设计的架构，他从头到尾都没思考过。</p><p> 没有对自己的架构问过为什么的人，一定是平时没有思考的人，面试官对这类候选人印象通常很不好。因为面试官担心你进了团队之后只会木头木脑的干呆活儿，不会自己思考。</p></li><li><p>第二，你既然用了消息队列这个东西，你知不知道用了有什么好处&amp;坏处？</p><p> 你要是没考虑过这个，那你盲目弄个 MQ 进系统里，后面出了问题你是不是就自己溜了给公司留坑？你要是没考虑过引入一个技术可能存在的弊端和风险，面试官把这类候选人招进来了，基本可能就是挖坑型选手。就怕你干 1 年挖一堆坑，自己跳槽了，给公司留下无穷后患。</p></li><li><p>第三，既然你用了 MQ，可能是某一种 MQ，那么你当时做没做过调研？</p><p> 你别傻乎乎的自己拍脑袋看个人喜好就瞎用了一个 MQ，比如 Kafka，甚至都从没调研过业界流行的 MQ 到底有哪几种。每一个 MQ 的优点和缺点是什么。每一个 MQ 没有绝对的好坏，但是就是看用在哪个场景可以扬长避短，利用其优势，规避其劣势。</p><p> 如果是一个不考虑技术选型的候选人招进了团队，leader 交给他一个任务，去设计个什么系统，他在里面用一些技术，可能都没考虑过选型，最后选的技术可能并不一定合适，一样是留坑。</p></li></ol><blockquote><p>面试题剖析</p></blockquote><h2 id="为什么使用消息队列"><a href="#为什么使用消息队列" class="headerlink" title="* 为什么使用消息队列"></a>* 为什么使用消息队列</h2><p>其实就是问问你消息队列都有哪些使用场景，然后你项目里具体是什么场景，说说你在这个场景里用消息队列是什么？</p><p>面试官问你这个问题，期望的一个回答是说，你们公司有个什么业务场景，这个业务场景有个什么技术挑战，如果不用 MQ 可能会很麻烦，但是你现在用了 MQ 之后带给了你很多的好处。</p><p>先说一下消息队列常见的使用场景吧，其实场景有很多，但是比较核心的有 3 个：<strong>解耦、异步、削峰</strong>。</p><h3 id="解耦"><a href="#解耦" class="headerlink" title="解耦"></a>解耦</h3><p>看这么个场景。A 系统发送数据到 BCD 三个系统，通过接口调用发送。如果 E 系统也要这个数据呢？那如果 C 系统现在不需要了呢？A 系统负责人几乎崩溃……</p><p><img src="/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/mq-1.png" alt></p><p>在这个场景中，A 系统跟其它各种乱七八糟的系统严重耦合，A 系统产生一条比较关键的数据，很多系统都需要 A 系统将这个数据发送过来。A 系统要时时刻刻考虑 BCDE 四个系统如果挂了该咋办？要不要重发，要不要把消息存起来？头发都白了啊！</p><p>如果使用 MQ，A 系统产生一条数据，发送到 MQ 里面去，哪个系统需要数据自己去 MQ 里面消费。如果新系统需要数据，直接从 MQ 里消费即可；如果某个系统不需要这条数据了，就取消对 MQ 消息的消费即可。这样下来，A 系统压根儿不需要去考虑要给谁发送数据，不需要维护这个代码，也不需要考虑人家是否调用成功、失败超时等情况。</p><p><img src="/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/mq-2.png" alt></p><p>总结：通过一个 MQ，Pub/Sub 发布订阅消息这么一个模型，A 系统就跟其它系统彻底解耦了。</p><p>面试技巧：你需要去考虑一下你负责的系统中是否有类似的场景，就是一个系统或者一个模块，调用了多个系统或者模块，互相之间的调用很复杂，维护起来很麻烦。但是其实这个调用是不需要直接同步调用接口的，如果用 MQ 给它异步化解耦，也是可以的，你就需要去考虑在你的项目里，是不是可以运用这个 MQ 去进行系统的解耦。在简历中体现出来这块东西，用 MQ 作解耦。</p><h3 id="异步"><a href="#异步" class="headerlink" title="异步"></a>异步</h3><p>再来看一个场景，A 系统接收一个请求，需要在自己本地写库，还需要在 BCD 三个系统写库，自己本地写库要 3ms，BCD 三个系统分别写库要 300ms、450ms、200ms。最终请求总延时是 3 + 300 + 450 + 200 = 953ms，接近 1s，用户感觉搞个什么东西，慢死了慢死了。用户通过浏览器发起请求，等待个 1s，这几乎是不可接受的。</p><p><img src="/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/mq-3.png" alt></p><p>一般互联网类的企业，对于用户直接的操作，一般要求是每个请求都必须在 200 ms 以内完成，对用户几乎是无感知的。</p><p>如果使用 MQ，那么 A 系统连续发送 3 条消息到 MQ 队列中，假如耗时 5ms，A 系统从接受一个请求到返回响应给用户，总时长是 3 + 5 = 8ms，对于用户而言，其实感觉上就是点个按钮，8ms 以后就直接返回了，爽！网站做得真好，真快！</p><p><img src="/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/mq-4.png" alt></p><h3 id="削峰"><a href="#削峰" class="headerlink" title="削峰"></a>削峰</h3><p>每天 0:00 到 12:00，A 系统风平浪静，每秒并发请求数量就 50 个。结果每次一到 12:00 ~ 13:00 ，每秒并发请求数量突然会暴增到 5k+ 条。但是系统是直接基于 MySQL 的，大量的请求涌入 MySQL，每秒钟对 MySQL 执行约 5k 条 SQL。</p><p>一般的 MySQL，扛到每秒 2k 个请求就差不多了，如果每秒请求到 5k 的话，可能就直接把 MySQL 给打死了，导致系统崩溃，用户也就没法再使用系统了。</p><p>但是高峰期一过，到了下午的时候，就成了低峰期，可能也就 1w 的用户同时在网站上操作，每秒中的请求数量可能也就 50 个请求，对整个系统几乎没有任何的压力。</p><p><img src="/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/mq-5.png" alt></p><p>如果使用 MQ，每秒 5k 个请求写入 MQ，A 系统每秒钟最多处理 2k 个请求，因为 MySQL 每秒钟最多处理 2k 个。A 系统从 MQ 中慢慢拉取请求，每秒钟就拉取 2k 个请求，不要超过自己每秒能处理的最大请求数量就 ok，这样下来，哪怕是高峰期的时候，A 系统也绝对不会挂掉。而 MQ 每秒钟 5k 个请求进来，就 2k 个请求出去，结果就导致在中午高峰期（1 个小时），可能有几十万甚至几百万的请求积压在 MQ 中。</p><p><img src="/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/mq-6.png" alt></p><p>这个短暂的高峰期积压是 ok 的，因为高峰期过了之后，每秒钟就 50 个请求进 MQ，但是 A 系统依然会按照每秒 2k 个请求的速度在处理。所以说，只要高峰期一过，A 系统就会快速将积压的消息给解决掉。</p><h2 id="消息队列有什么优缺点"><a href="#消息队列有什么优缺点" class="headerlink" title="* 消息队列有什么优缺点"></a>* 消息队列有什么优缺点</h2><p>优点上面已经说了，就是在特殊场景下有其对应的好处，解耦、异步、削峰。</p><p>缺点有以下几个：</p><ul><li><p>系统可用性降低</p><p>  系统引入的外部依赖越多，越容易挂掉。本来你就是 A 系统调用 BCD 三个系统的接口就好了，ABCD 四个系统还好好的，没啥问题，你偏加个 MQ 进来，万一 MQ 挂了咋整？MQ 一挂，整套系统崩溃，你不就完了？</p></li><li><p>系统复杂度提高</p><p>  硬生生加个 MQ 进来，你怎么保证消息没有重复消费？怎么处理消息丢失的情况？怎么保证消息传递的顺序性？头大头大，问题一大堆，痛苦不已。</p></li><li><p>一致性问题</p><p>  A 系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是 BCD 三个系统那里，BD 两个系统写库成功了，结果 C 系统写库失败了，咋整？你这数据就不一致了。</p></li></ul><p>所以消息队列实际是一种非常复杂的架构，你引入它有很多好处，但是也得针对它带来的坏处做各种额外的技术方案和架构来规避掉，做好之后，你会发现，妈呀，系统复杂度提升了一个数量级，也许是复杂了 10 倍。但是关键时刻，用，还是得用的。</p><h2 id="Kafka、ActiveMQ、RabbitMQ、RocketMQ-有什么优缺点？"><a href="#Kafka、ActiveMQ、RabbitMQ、RocketMQ-有什么优缺点？" class="headerlink" title="* Kafka、ActiveMQ、RabbitMQ、RocketMQ 有什么优缺点？"></a>* Kafka、ActiveMQ、RabbitMQ、RocketMQ 有什么优缺点？</h2><p><img src="/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/1595136879.jpg" alt></p><p>综上，各种对比之后，有如下建议：</p><p>一般的业务系统要引入 MQ，最早大家都用 ActiveMQ，但是现在确实大家用的不多了，没经过大规模吞吐量场景的验证，社区也不是很活跃，所以大家还是算了吧，我个人不推荐用这个了；</p><p>后来大家开始用 RabbitMQ，但是确实 erlang 语言阻止了大量的 Java 工程师去深入研究和掌控它，对公司而言，几乎处于不可控的状态，但是确实人家是开源的，比较稳定的支持，活跃度也高；</p><p>不过现在确实越来越多的公司会去用 RocketMQ，确实很不错，毕竟是阿里出品，但社区可能有突然黄掉的风险（目前 RocketMQ 已捐给 Apache，但 GitHub 上的活跃度其实不算高）对自己公司技术实力有绝对自信的，推荐用 RocketMQ，否则回去老老实实用 RabbitMQ 吧，人家有活跃的开源社区，绝对不会黄。</p><p>所以中小型公司，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；大型公司，基础架构研发实力较强，用 RocketMQ 是很好的选择。</p><p>如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。</p><hr><h2 id="面试题-1"><a href="#面试题-1" class="headerlink" title="面试题"></a>面试题</h2><p>如何保证消息队列的高可用？</p><h3 id="面试官心理分析-1"><a href="#面试官心理分析-1" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>如果有人问到你 MQ 的知识，高可用是必问的。上一讲提到，MQ 会导致系统可用性降低。所以只要你用了 MQ，接下来问的一些要点肯定就是围绕着 MQ 的那些缺点怎么来解决了。</p><p>要是你傻乎乎的就干用了一个 MQ，各种问题从来没考虑过，那你就杯具了，面试官对你的感觉就是，只会简单使用一些技术，没任何思考，马上对你的印象就不太好了。这样的同学招进来要是做个 20k 薪资以内的普通小弟还凑合，要是做薪资 20k+ 的高工，那就惨了，让你设计个系统，里面肯定一堆坑，出了事故公司受损失，团队一起背锅。</p><blockquote><p>面试题剖析<br>这个问题这么问是很好的，因为不能问你 Kafka 的高可用性怎么保证？ActiveMQ 的高可用性怎么保证？一个面试官要是这么问就显得很没水平，人家可能用的就是 RabbitMQ，没用过 Kafka，你上来问人家 Kafka 干什么？这不是摆明了刁难人么。<br>所以有水平的面试官，问的是 MQ 的高可用性怎么保证？这样就是你用过哪个 MQ，你就说说你对那个 MQ 的高可用性的理解。</p></blockquote><h2 id="如何保证消息队列的高可用？"><a href="#如何保证消息队列的高可用？" class="headerlink" title="* 如何保证消息队列的高可用？"></a>* 如何保证消息队列的高可用？</h2><h3 id="RabbitMQ-的高可用性"><a href="#RabbitMQ-的高可用性" class="headerlink" title="RabbitMQ 的高可用性"></a>RabbitMQ 的高可用性</h3><p>RabbitMQ 是比较有代表性的，因为是基于主从（非分布式）做高可用性的，我们就以 RabbitMQ 为例子讲解第一种 MQ 的高可用性怎么实现。</p><p>RabbitMQ 有三种模式：单机模式、普通集群模式、镜像集群模式。</p><h4 id="单机模式"><a href="#单机模式" class="headerlink" title="单机模式"></a>单机模式</h4><p>单机模式，就是 Demo 级别的，一般就是你本地启动了玩玩儿的😄，没人生产用单机模式。</p><h4 id="普通集群模式（无高可用性）"><a href="#普通集群模式（无高可用性）" class="headerlink" title="普通集群模式（无高可用性）"></a>普通集群模式（无高可用性）</h4><p>普通集群模式，意思就是在多台机器上启动多个 RabbitMQ 实例，每个机器启动一个。你创建的 queue，只会放在一个 RabbitMQ 实例上，但是每个实例都同步 queue 的元数据（元数据可以认为是 queue 的一些配置信息，通过元数据，可以找到 queue 所在实例）。你消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从 queue 所在实例上拉取数据过来。</p><p><img src="/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/mq-7.png" alt></p><p>这种方式确实很麻烦，也不怎么好，没做到所谓的分布式，就是个普通集群。因为这导致你要么消费者每次随机连接一个实例然后拉取数据，要么固定连接那个 queue 所在实例消费数据，前者有数据拉取的开销，后者导致单实例性能瓶颈。</p><p>而且如果那个放 queue 的实例宕机了，会导致接下来其他实例就无法从那个实例拉取，如果你开启了消息持久化，让 RabbitMQ 落地存储消息的话，消息不一定会丢，得等这个实例恢复了，然后才可以继续从这个 queue 拉取数据。</p><p>所以这个事儿就比较尴尬了，这就没有什么所谓的高可用性，这方案主要是提高吞吐量的，就是说让集群中多个节点来服务某个 queue 的读写操作。</p><h4 id="镜像集群模式（高可用性）"><a href="#镜像集群模式（高可用性）" class="headerlink" title="镜像集群模式（高可用性）"></a>镜像集群模式（高可用性）</h4><p>这种模式，才是所谓的 RabbitMQ 的高可用模式。跟普通集群模式不一样的是，在镜像集群模式下，你创建的 queue，无论元数据还是 queue 里的消息都会存在于多个实例上，就是说，每个 RabbitMQ 节点都有这个 queue 的一个完整镜像，包含 queue 的全部数据的意思。然后每次你写消息到 queue 的时候，都会自动把消息同步到多个实例的 queue 上。</p><p><img src="/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/mq-8.png" alt></p><p>那么如何开启这个镜像集群模式呢？其实很简单，RabbitMQ 有很好的管理控制台，就是在后台新增一个策略，这个策略是镜像集群模式的策略，指定的时候是可以要求数据同步到所有节点的，也可以要求同步到指定数量的节点，再次创建 queue 的时候，应用这个策略，就会自动将数据同步到其他的节点上去了。</p><p>这样的话，好处在于，你任何一个机器宕机了，没事儿，其它机器（节点）还包含了这个 queue 的完整数据，别的 consumer 都可以到其它节点上去消费数据。坏处在于，第一，这个性能开销也太大了吧，消息需要同步到所有机器上，导致网络带宽压力和消耗很重！第二，这么玩儿，不是分布式的，就没有扩展性可言了，如果某个 queue 负载很重，你加机器，新增的机器也包含了这个 queue 的所有数据，并没有办法线性扩展你的 queue。你想，如果这个 queue 的数据量很大，大到这个机器上的容量无法容纳了，此时该怎么办呢？</p><h3 id="Kafka-的高可用性"><a href="#Kafka-的高可用性" class="headerlink" title="Kafka 的高可用性"></a>Kafka 的高可用性</h3><p>Kafka 一个最基本的架构认识：由多个 broker 组成，每个 broker 是一个节点；你创建一个 topic，这个 topic 可以划分为多个 partition，每个 partition 可以存在于不同的 broker 上，每个 partition 就放一部分数据。</p><p><strong>这就是天然的分布式消息队列，就是说一个 topic 的数据，是分散放在多个机器上的，每个机器就放一部分数据。</strong></p><p>实际上 RabbitMQ 之类的，并不是分布式消息队列，它就是传统的消息队列，只不过提供了一些集群、HA(High Availability, 高可用性) 的机制而已，因为无论怎么玩儿，RabbitMQ 一个 queue 的数据都是放在一个节点里的，镜像集群下，也是每个节点都放这个 queue 的完整数据。</p><p>Kafka 0.8 以前，是没有 HA 机制的，就是任何一个 broker 宕机了，那个 broker 上的 partition 就废了，没法写也没法读，没有什么高可用性可言。</p><p>比如说，我们假设创建了一个 topic，指定其 partition 数量是 3 个，分别在三台机器上。但是，如果第二台机器宕机了，会导致这个 topic 的 1/3 的数据就丢了，因此这个是做不到高可用的。</p><p><img src="/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/kafka-before.png" alt></p><p>Kafka 0.8 以后，提供了 HA 机制，就是 replica（复制品） 副本机制。每个 partition 的数据都会同步到其它机器上，形成自己的多个 replica 副本。所有 replica 会选举一个 leader 出来，那么生产和消费都跟这个 leader 打交道，然后其他 replica 就是 follower。写的时候，leader 会负责把数据同步到所有 follower 上去，读的时候就直接读 leader 上的数据即可。只能读写 leader？很简单，要是你可以随意读写每个 follower，那么就要 care 数据一致性的问题，系统复杂度太高，很容易出问题。Kafka 会均匀地将一个 partition 的所有 replica 分布在不同的机器上，这样才可以提高容错性。</p><p><img src="/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/kafka-after.png" alt></p><p>这么搞，就有所谓的高可用性了，因为如果某个 broker 宕机了，没事儿，那个 broker上面的 partition 在其他机器上都有副本的。如果这个宕机的 broker 上面有某个 partition 的 leader，那么此时会从 follower 中重新选举一个新的 leader 出来，大家继续读写那个新的 leader 即可。这就有所谓的高可用性了。</p><p>写数据的时候，生产者就写 leader，然后 leader 将数据落地写本地磁盘，接着其他 follower 自己主动从 leader 来 pull 数据。一旦所有 follower 同步好数据了，就会发送 ack 给 leader，leader 收到所有 follower 的 ack 之后，就会返回写成功的消息给生产者。（当然，这只是其中一种模式，还可以适当调整这个行为）</p><p>消费的时候，只会从 leader 去读，但是只有当一个消息已经被所有 follower 都同步成功返回 ack 的时候，这个消息才会被消费者读到。</p><p>看到这里，相信你大致明白了 Kafka 是如何保证高可用机制的了，对吧？不至于一无所知，现场还能给面试官画画图。要是遇上面试官确实是 Kafka 高手，深挖了问，那你只能说不好意思，太深入的你没研究过。</p><p><img src="/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/e314ee45gy1g05zgr67bbj20gp0b3aba.jpg" alt></p><hr><h2 id="面试题-2"><a href="#面试题-2" class="headerlink" title="面试题"></a>面试题</h2><p>如何保证消息不被重复消费？或者说，如何保证消息消费的幂等性？</p><h3 id="面试官心理分析-2"><a href="#面试官心理分析-2" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>其实这是很常见的一个问题，这俩问题基本可以连起来问。既然是消费消息，那肯定要考虑会不会重复消费？能不能避免重复消费？或者重复消费了也别造成系统异常可以吗？</p><blockquote><p>面试题剖析:<br>这个是 MQ 领域的基本问题，其实本质上还是问你使用消息队列如何保证幂等性，这个是你架构里要考虑的一个问题。</p></blockquote><h2 id="如何保证消息消费的幂等性？"><a href="#如何保证消息消费的幂等性？" class="headerlink" title="* 如何保证消息消费的幂等性？"></a>* 如何保证消息消费的幂等性？</h2><p>回答这个问题，首先你别听到重复消息这个事儿，就一无所知吧，你先大概说一说可能会有哪些重复消费的问题。</p><p>首先，比如 RabbitMQ、RocketMQ、Kafka，都有可能会出现消息重复消费的问题，正常。因为这问题通常不是 MQ 自己保证的，是由我们开发来保证的。挑一个 Kafka 来举个例子，说说怎么重复消费吧。</p><p>Kafka 实际上有个 offset 的概念，就是每个消息写进去，都有一个 offset，代表消息的序号，然后 consumer 消费了数据之后，每隔一段时间（定时定期），会把自己消费过的消息的 offset 提交一下，表示“我已经消费过了，下次我要是重启啥的，你就让我继续从上次消费到的 offset 来继续消费吧”。</p><p>但是凡事总有意外，比如我们之前生产经常遇到的，就是你有时候重启系统，看你怎么重启了，如果碰到点着急的，直接 kill 进程了，再重启。这会导致 consumer 有些消息处理了，但是没来得及提交 offset，尴尬了。重启之后，少数消息会再次消费一次。</p><p>举个栗子:</p><p>有这么个场景。数据 1/2/3 依次进入 kafka，kafka 会给这三条数据每条分配一个 offset，代表这条数据的序号，我们就假设分配的 offset 依次是 152/153/154。消费者从 kafka 去消费的时候，也是按照这个顺序去消费。假如当消费者消费了 offset=153 的这条数据，刚准备去提交 offset 到 zookeeper，此时消费者进程被重启了。那么此时消费过的数据 1/2 的 offset 并没有提交，kafka 也就不知道你已经消费了 offset=153 这条数据。那么重启之后，消费者会找 kafka 说，嘿，哥儿们，你给我接着把上次我消费到的那个地方后面的数据继续给我传递过来。由于之前的 offset 没有提交成功，那么数据 1/2 会再次传过来，如果此时消费者没有去重的话，那么就会导致重复消费。</p><p><img src="/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/mq-10.png" alt></p><p>如果消费者干的事儿是拿一条数据就往数据库里写一条，会导致说，你可能就把数据 1/2 在数据库里插入了 2 次，那么数据就错啦。</p><p>其实重复消费不可怕，可怕的是你没考虑到重复消费之后，怎么保证幂等性。</p><p>举个例子吧。假设你有个系统，消费一条消息就往数据库里插入一条数据，要是你一个消息重复两次，你不就插入了两条，这数据不就错了？但是你要是消费到第二次的时候，自己判断一下是否已经消费过了，若是就直接扔了，这样不就保留了一条数据，从而保证了数据的正确性。</p><p>一条数据重复出现两次，数据库里就只有一条数据，这就保证了系统的幂等性。</p><p>幂等性，通俗点说，就一个数据，或者一个请求，给你重复来多次，你得确保对应的数据是不会改变的，不能出错。</p><p>所以第二个问题来了，怎么保证消息队列消费的幂等性？</p><p>其实还是得结合业务来思考，我这里给几个思路：</p><ul><li>比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。</li><li>比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。</li><li>比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。</li><li>比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。</li></ul><p><img src="/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/mq-11.png" alt></p><p>当然，如何保证 MQ 的消费是幂等性的，需要结合具体的业务来看。</p><hr><h2 id="面试题-3"><a href="#面试题-3" class="headerlink" title="面试题"></a>面试题</h2><p>如何保证消息的可靠性传输？或者说，如何处理消息丢失的问题？</p><h3 id="面试官心理分析-3"><a href="#面试官心理分析-3" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>这个是肯定的，用 MQ 有个基本原则，就是数据不能多一条，也不能少一条，不能多，就是前面说的重复消费和幂等性问题。不能少，就是说这数据别搞丢了。那这个问题你必须得考虑一下。</p><p>如果说你这个是用 MQ 来传递非常核心的消息，比如说计费、扣费的一些消息，那必须确保这个 MQ 传递过程中绝对不会把计费消息给弄丢。</p><blockquote><p>面试题剖析:<br>数据的丢失问题，可能出现在生产者、MQ、消费者中，咱们从 RabbitMQ 和 Kafka 分别来分析一下吧。</p></blockquote><h2 id="如何处理消息丢失的问题？"><a href="#如何处理消息丢失的问题？" class="headerlink" title="* 如何处理消息丢失的问题？"></a>* 如何处理消息丢失的问题？</h2><h3 id="RabbitMQ"><a href="#RabbitMQ" class="headerlink" title="RabbitMQ"></a>RabbitMQ</h3><p><img src="/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/rabbitmq-message-lose.png" alt></p><h4 id="生产者弄丢了数据"><a href="#生产者弄丢了数据" class="headerlink" title="生产者弄丢了数据"></a>生产者弄丢了数据</h4><p>生产者将数据发送到 RabbitMQ 的时候，可能数据就在半路给搞丢了，因为网络问题啥的，都有可能。</p><p>此时可以选择用 RabbitMQ 提供的事务功能，就是生产者发送数据之前开启 RabbitMQ 事务 channel.txSelect ，然后发送消息，如果消息没有成功被 RabbitMQ 接收到，那么生产者会收到异常报错，此时就可以回滚事务 channel.txRollback ，然后重试发送消息；如果收到了消息，那么可以提交事务 channel.txCommit 。</p><pre><code>// 开启事务channel.txSelecttry {    // 这里发送消息} catch (Exception e) {    channel.txRollback    // 这里再次重发这条消息}// 提交事务channel.txCommit</code></pre><p>但是问题是，RabbitMQ 事务机制（同步）一搞，基本上吞吐量会下来，因为太耗性能。</p><p>所以一般来说，如果你要确保说写 RabbitMQ 的消息别丢，可以开启 confirm 模式，在生产者那里设置开启 confirm 模式之后，你每次写的消息都会分配一个唯一的 id，然后如果写入了 RabbitMQ 中，RabbitMQ 会给你回传一个 ack 消息，告诉你说这个消息 ok 了。如果 RabbitMQ 没能处理这个消息，会回调你的一个 nack 接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息 id 的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。</p><p>事务机制和 confirm 机制最大的不同在于，事务机制是同步的，你提交一个事务之后会阻塞在那儿，但是 confirm 机制是异步的，你发送个消息之后就可以发送下一个消息，然后那个消息 RabbitMQ 接收了之后会异步回调你的一个接口通知你这个消息接收到了。</p><p>所以一般在生产者这块避免数据丢失，都是用 confirm 机制的。</p><h4 id="RabbitMQ-弄丢了数据"><a href="#RabbitMQ-弄丢了数据" class="headerlink" title="RabbitMQ 弄丢了数据"></a>RabbitMQ 弄丢了数据</h4><p>就是 RabbitMQ 自己弄丢了数据，这个你必须开启 RabbitMQ 的持久化，就是消息写入之后会持久化到磁盘，哪怕是 RabbitMQ 自己挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。除非极其罕见的是，RabbitMQ 还没持久化，自己就挂了，可能导致少量数据丢失，但是这个概率较小。</p><p>设置持久化有两个步骤：</p><ul><li><p>创建 queue 的时候将其设置为持久化</p><p>  这样就可以保证 RabbitMQ 持久化 queue 的元数据，但是它是不会持久化 queue 里的数据的。</p></li><li><p>第二个是发送消息的时候将消息的 deliveryMode 设置为 2</p><p>  就是将消息设置为持久化的，此时 RabbitMQ 就会将消息持久化到磁盘上去。</p></li></ul><p>必须要同时设置这两个持久化才行，RabbitMQ 哪怕是挂了，再次重启，也会从磁盘上重启恢复 queue，恢复这个 queue 里的数据。</p><p>注意，哪怕是你给 RabbitMQ 开启了持久化机制，也有一种可能，就是这个消息写到了 RabbitMQ 中，但是还没来得及持久化到磁盘上，结果不巧，此时 RabbitMQ 挂了，就会导致内存里的一点点数据丢失。</p><p><strong>所以，持久化可以跟生产者那边的 confirm 机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者 ack 了，所以哪怕是在持久化到磁盘之前，RabbitMQ 挂了，数据丢了，生产者收不到 ack ，你也是可以自己重发的。</strong></p><h4 id="消费端弄丢了数据"><a href="#消费端弄丢了数据" class="headerlink" title="消费端弄丢了数据"></a>消费端弄丢了数据</h4><p>RabbitMQ 如果丢失了数据，主要是因为你消费的时候，刚消费到，还没处理，结果进程挂了，比如重启了，那么就尴尬了，RabbitMQ 认为你都消费了，这数据就丢了。</p><p>这个时候得用 RabbitMQ 提供的 ack 机制，简单来说，就是你必须关闭 RabbitMQ 的自动 ack ，可以通过一个 api 来调用就行，然后每次你自己代码里确保处理完的时候，再在程序里 ack 一把。这样的话，如果你还没处理完，不就没有 ack 了？那 RabbitMQ 就认为你还没处理完，这个时候 RabbitMQ 会把这个消费分配给别的 consumer 去处理，消息是不会丢的。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p><img src="/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/rabbitmq-message-lose-solution.png" alt></p><h3 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h3><h4 id="消费端弄丢了数据-1"><a href="#消费端弄丢了数据-1" class="headerlink" title="消费端弄丢了数据"></a>消费端弄丢了数据</h4><p>唯一可能导致消费者弄丢数据的情况，就是说，你消费到了这个消息，然后消费者那边自动提交了 offset，让 Kafka 以为你已经消费好了这个消息，但其实你才刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。</p><p>这不是跟 RabbitMQ 差不多吗，大家都知道 Kafka 会自动提交 offset，那么只要关闭自动提交 offset，在处理完之后自己手动提交 offset，就可以保证数据不会丢。但是此时确实还是可能会有重复消费，比如你刚处理完，还没提交 offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。</p><h4 id="Kafka-弄丢了数据"><a href="#Kafka-弄丢了数据" class="headerlink" title="Kafka 弄丢了数据"></a>Kafka 弄丢了数据</h4><p>这块比较常见的一个场景，就是 Kafka 某个 broker 宕机，然后重新选举 partition 的 leader。大家想想，要是此时其他的 follower 刚好还有些数据没有同步，结果此时 leader 挂了，然后选举某个 follower 成 leader 之后，不就少了一些数据？这就丢了一些数据啊。</p><p>生产环境也遇到过，我们也是，之前 Kafka 的 leader 机器宕机了，将 follower 切换为 leader 之后，就会发现说这个数据就丢了。</p><p>所以此时一般是要求起码设置如下 4 个参数：</p><ul><li>给 topic 设置 replication.factor 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。</li><li>在 Kafka 服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。</li><li>在 producer 端设置 acks=all ：这个是要求每条数据，必须是写入所有 replica 之后，才能认为是写成功了。</li><li>在 producer 端设置 retries=MAX （很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了。</li></ul><p>我们生产环境就是按照上述要求配置的，这样配置之后，至少在 Kafka broker 端就可以保证在 leader 所在 broker 发生故障，进行 leader 切换时，数据不会丢失。</p><h4 id="生产者会不会弄丢数据？"><a href="#生产者会不会弄丢数据？" class="headerlink" title="生产者会不会弄丢数据？"></a>生产者会不会弄丢数据？</h4><p>如果按照上述的思路设置了 acks=all ，一定不会丢，要求是，你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。</p><hr><h2 id="面试题-4"><a href="#面试题-4" class="headerlink" title="面试题"></a>面试题</h2><p>如何保证消息的顺序性？</p><h3 id="面试官心理分析-4"><a href="#面试官心理分析-4" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>其实这个也是用 MQ 的时候必问的话题，第一看看你了不了解顺序这个事儿？第二看看你有没有办法保证消息是有顺序的？这是生产系统中常见的问题。</p><blockquote><p>面试题剖析:<br>保证消息是有顺序</p></blockquote><h2 id="如何保证消息的顺序性？"><a href="#如何保证消息的顺序性？" class="headerlink" title="* 如何保证消息的顺序性？"></a>* 如何保证消息的顺序性？</h2><p>我举个例子，我们以前做过一个 mysql binlog 同步的系统，压力还是非常大的，日同步数据要达到上亿，就是说数据从一个 mysql 库原封不动地同步到另一个 mysql 库里面去（mysql -&gt; mysql）。常见的一点在于说比如大数据 team，就需要同步一个 mysql 库过来，对公司的业务系统的数据做各种复杂的操作。</p><p>你在 mysql 里增删改一条数据，对应出来了增删改 3 条 binlog 日志，接着这三条 binlog 发送到 MQ 里面，再消费出来依次执行，起码得保证人家是按照顺序来的吧？不然本来是：增加、修改、删除；你愣是换了顺序给执行成删除、修改、增加，不全错了么。</p><p>本来这个数据同步过来，应该最后这个数据被删除了；结果你搞错了这个顺序，最后这个数据保留下来了，数据同步就出错了。</p><p>先看看顺序会错乱的俩场景：</p><ul><li><p>RabbitMQ：</p><p>  一个 queue，多个 consumer。比如，生产者向 RabbitMQ 里发送了三条数据，顺序依次是 data1/data2/data3，压入的是 RabbitMQ 的一个内存队列。有三个消费者分别从 MQ 中消费这三条数据中的一条，结果消费者2先执行完操作，把 data2 存入数据库，然后是 data1/data3。这不明显乱了。</p><p>  <img src="/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/rabbitmq-order-01.png" alt></p></li><li><p>Kafka：</p><p>  比如说我们建了一个 topic，有三个 partition。生产者在写的时候，其实可以指定一个 key，比如说我们指定了某个订单 id 作为 key，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个 partition 中的数据一定是有顺序的。</p><p>  消费者从 partition 中取出来数据的时候，也一定是有顺序的。到这里，顺序还是 ok 的，没有错乱。接着，我们在消费者里可能会搞多个线程来并发处理消息。因为如果消费者是单线程消费处理，而处理比较耗时的话，比如处理一条消息耗时几十 ms，那么 1 秒钟只能处理几十条消息，这吞吐量太低了。而多个线程并发跑的话，顺序可能就乱掉了。</p><p>  <img src="/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/kafka-order-01.png" alt></p></li></ul><p>解决方案:</p><ul><li><p>RabbitMQ</p><p>  拆分多个 queue，每个 queue 一个 consumer，就是多一些 queue 而已，确实是麻烦点；或者就一个 queue 但是对应一个 consumer，然后这个 consumer 内部用内存队列做排队，然后分发给底层不同的 worker 来处理。</p><p>  <img src="/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/rabbitmq-order-02.png" alt></p></li><li><p>Kafka</p><p>  一个 topic，一个 partition，一个 consumer，内部单线程消费，单线程吞吐量太低，一般不会用这个。</p><p>  写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性。</p><p>  <img src="/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/kafka-order-02.png" alt></p></li></ul><hr><h2 id="面试题-5"><a href="#面试题-5" class="headerlink" title="面试题"></a>面试题</h2><p>如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？有几百万消息持续积压几小时，说说怎么解决？</p><h3 id="面试官心理分析-5"><a href="#面试官心理分析-5" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>你看这问法，其实本质针对的场景，都是说，可能你的消费端出了问题，不消费了；或者消费的速度极其慢。接着就坑爹了，可能你的消息队列集群的磁盘都快写满了，都没人消费，这个时候怎么办？或者是这整个就积压了几个小时，你这个时候怎么办？或者是你积压的时间太长了，导致比如 RabbitMQ 设置了消息过期时间后就没了怎么办？</p><p>所以就这事儿，其实线上挺常见的，一般不出，一出就是大 case。一般常见于，举个例子，消费端每次消费之后要写 mysql，结果 mysql 挂了，消费端 hang 那儿了，不动了；或者是消费端出了个什么岔子，导致消费速度极其慢。</p><blockquote><p>面试题剖析<br>关于这个事儿，我们一个一个来梳理吧，先假设一个场景，我们现在消费端出故障了，然后大量消息在 mq 里积压，现在出事故了，慌了。</p></blockquote><h2 id="消息堆积怎么办？"><a href="#消息堆积怎么办？" class="headerlink" title="* 消息堆积怎么办？"></a>* 消息堆积怎么办？</h2><h3 id="大量消息在-mq-里积压了几个小时了还没解决"><a href="#大量消息在-mq-里积压了几个小时了还没解决" class="headerlink" title="大量消息在 mq 里积压了几个小时了还没解决"></a>大量消息在 mq 里积压了几个小时了还没解决</h3><p>几千万条数据在 MQ 里积压了七八个小时，从下午 4 点多，积压到了晚上 11 点多。这个是我们真实遇到过的一个场景，确实是线上故障了，这个时候要不然就是修复 consumer 的问题，让它恢复消费速度，然后傻傻的等待几个小时消费完毕。这个肯定不能在面试的时候说吧。</p><p>一个消费者一秒是 1000 条，一秒 3 个消费者是 3000 条，一分钟就是 18 万条。所以如果你积压了几百万到上千万的数据，即使消费者恢复了，也需要大概 1 小时的时间才能恢复过来。</p><p>一般这个时候，只能临时紧急扩容了，具体操作步骤和思路如下：</p><ul><li>先修复 consumer 的问题，确保其恢复消费速度，然后将现有 consumer 都停掉。</li><li>新建一个 topic，partition 是原来的 10 倍，临时建立好原先 10 倍的 queue 数量。</li><li>然后写一个临时的分发数据的 consumer 程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的 10 倍数量的 queue。</li><li>接着临时征用 10 倍的机器来部署 consumer，每一批 consumer 消费一个临时 queue 的数据。这种做法相当于是临时将 queue 资源和 consumer 资源扩大 10 倍，以正常的 10 倍速度来消费数据。</li><li>等快速消费完积压数据之后，得恢复原先部署的架构，重新用原先的 consumer 机器来消费消息。</li></ul><h3 id="mq-中的消息过期失效了"><a href="#mq-中的消息过期失效了" class="headerlink" title="mq 中的消息过期失效了"></a>mq 中的消息过期失效了</h3><p>假设你用的是 RabbitMQ，RabbtiMQ 是可以设置过期时间的，也就是 TTL。如果消息在 queue 中积压超过一定的时间就会被 RabbitMQ 给清理掉，这个数据就没了。那这就是第二个坑了。这就不是说数据会大量积压在 mq 里，而是大量的数据会直接搞丢。</p><p>这个情况下，就不是说要增加 consumer 消费积压的消息，因为实际上没啥积压，而是丢了大量的消息。我们可以采取一个方案，就是批量重导，这个我们之前线上也有类似的场景干过。就是大量积压的时候，我们当时就直接丢弃数据了，然后等过了高峰期以后，比如大家一起喝咖啡熬夜到晚上12点以后，用户都睡觉了。这个时候我们就开始写程序，将丢失的那批数据，写个临时程序，一点一点的查出来，然后重新灌入 mq 里面去，把白天丢的数据给他补回来。也只能是这样了。</p><p>假设 1 万个订单积压在 mq 里面，没有处理，其中 1000 个订单都丢了，你只能手动写程序把那 1000 个订单给查出来，手动发到 mq 里去再补一次。</p><h3 id="mq-都快写满了"><a href="#mq-都快写满了" class="headerlink" title="mq 都快写满了"></a>mq 都快写满了</h3><p>如果消息积压在 mq 里，你很长时间都没有处理掉，此时导致 mq 都快写满了，咋办？这个还有别的办法吗？没有，谁让你第一个方案执行的太慢了，你临时写程序，接入数据来消费，消费一个丢弃一个，都不要了，快速消费掉所有的消息。然后走第二个方案，到了晚上再补数据吧。</p><p><img src="/2020/07/18/2020-07-18-xiao-li-fei-dao-xiao-xi-dui-lie/hjevsx78.bmp" alt></p><hr><h2 id="面试题-6"><a href="#面试题-6" class="headerlink" title="面试题"></a>面试题</h2><p>如果让你写一个消息队列，该如何进行架构设计？说一下你的思路。</p><h3 id="面试官心理分析-6"><a href="#面试官心理分析-6" class="headerlink" title="面试官心理分析"></a>面试官心理分析</h3><p>其实聊到这个问题，一般面试官要考察两块：</p><ul><li>你有没有对某一个消息队列做过较为深入的原理的了解，或者从整体了解把握住一个消息队列的架构原理。</li><li>看看你的设计能力，给你一个常见的系统，就是消息队列系统，看看你能不能从全局把握一下整体架构设计，给出一些关键点出来。</li></ul><p>说实话，问类似问题的时候，大部分人基本都会蒙，因为平时从来没有思考过类似的问题，大多数人就是平时埋头用，从来不去思考背后的一些东西。类似的问题，比如，如果让你来设计一个 Spring 框架你会怎么做？如果让你来设计一个 Dubbo 框架你会怎么做？如果让你来设计一个 MyBatis 框架你会怎么做？</p><blockquote><p>面试题剖析<br>其实回答这类问题，说白了，不求你看过那技术的源码，起码你要大概知道那个技术的基本原理、核心组成部分、基本架构构成，然后参照一些开源的技术把一个系统设计出来的思路说一下就好。</p></blockquote><h2 id="如何设计一个消息队列？"><a href="#如何设计一个消息队列？" class="headerlink" title="* 如何设计一个消息队列？"></a>* 如何设计一个消息队列？</h2><p>比如说这个消息队列系统，我们从以下几个角度来考虑一下：</p><ul><li>首先这个 mq 得支持可伸缩性吧，就是需要的时候快速扩容，就可以增加吞吐量和容量，那怎么搞？设计个分布式的系统呗，参照一下 kafka 的设计理念，broker -&gt; topic -&gt; partition，每个 partition 放一个机器，就存一部分数据。如果现在资源不够了，简单啊，给 topic 增加 partition，然后做数据迁移，增加机器，不就可以存放更多数据，提供更高的吞吐量了？</li><li>其次你得考虑一下这个 mq 的数据要不要落地磁盘吧？那肯定要了，落磁盘才能保证别进程挂了数据就丢了。那落磁盘的时候怎么落啊？顺序写，这样就没有磁盘随机读写的寻址开销，磁盘顺序读写的性能是很高的，这就是 kafka 的思路。</li><li>其次你考虑一下你的 mq 的可用性啊？这个事儿，具体参考之前可用性那个环节讲解的 kafka 的高可用保障机制。多副本 -&gt; leader &amp; follower -&gt; broker 挂了重新选举 leader 即可对外服务。</li><li>能不能支持数据 0 丢失啊？可以的，参考我们之前说的那个 kafka 数据零丢失方案。</li></ul><p>mq 肯定是很复杂的，面试官问你这个问题，其实是个开放题，他就是看看你有没有从架构角度整体构思和设计的思维以及能力。确实这个问题可以刷掉一大批人，因为大部分人平时不思考这些东西。</p>]]></content>
      
      
      <categories>
          
          <category> 小李飞刀 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 消息队列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>事务隔离</title>
      <link href="/2020/06/08/2020-06-08-mysql-08/"/>
      <url>/2020/06/08/2020-06-08-mysql-08/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><p><strong>begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动。</strong>如果你想要马上启动一个事务，可以使用 start transaction with consistent snapshot 这个命令。</p><p>在 MySQL 里，有两个“视图”的概念：</p><ul><li>一个是 view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是 create view … ，而它的查询方法与表一样。</li><li>另一个是 InnoDB 在实现 MVCC 时用到的一致性读视图，即 consistent read view，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现。</li></ul><p>“快照”在 MVCC 里是怎么工作的？</p><p>在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。</p><p>这时，你会说这看上去不太现实啊。如果一个库有 100G，那么我启动一个事务，MySQL 就要拷贝 100G 的数据出来，这个过程得多慢啊。可是，我平时的事务执行起来很快啊。</p><p>实际上，我们并不需要拷贝出这 100G 的数据。我们先来看看这个快照是怎么实现的。</p><p>InnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。</p><p>而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。</p><p>也就是说，数据表中的一行记录，其实可能有多个版本 (row)，每个版本有自己的 row trx_id。</p><p><img src="/2020/06/08/2020-06-08-mysql-08/68d08d277a6f7926a41cc5541d3dfced.png" alt="行状态变更图"></p><p>图中虚线框里是同一行数据的 4 个版本，当前最新版本是 V4，k 的值是 22，它是被 transaction id 为 25 的事务更新的，因此它的 row trx_id 也是 25。</p><p>你可能会问，前面的文章不是说，语句更新会生成 undo log（回滚日志）吗？那么，undo log 在哪呢？</p><p>实际上，图 2 中的三个虚线箭头，就是 undo log；而 V1、V2、V3 并不是物理上真实存在的，而是每次需要的时候根据当前版本和 undo log 计算出来的。比如，需要 V2 的时候，就是通过 V4 依次执行 U3、U2 算出来。</p><p>明白了多版本和 row trx_id 的概念后，我们再来想一下，InnoDB 是怎么定义那个“100G”的快照的。</p><p>按照可重复读的定义，一个事务启动的时候，能够看到所有已经提交的事务结果。但是之后，这个事务执行期间，其他事务的更新对它不可见。</p><p>因此，一个事务只需要在启动的时候声明说，“以我启动的时刻为准，如果一个数据版本是在我启动之前生成的，就认；如果是我启动以后才生成的，我就不认，我必须要找到它的上一个版本”。</p><p>当然，如果“上一个版本”也不可见，那就得继续往前找。还有，如果是这个事务自己更新的数据，它自己还是要认的。</p><p>在实现上， InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。</p><p>数组里面事务 ID 的最小值记为低水位，当前系统里面已经创建过的事务 ID 的最大值加 1 记为高水位。</p><p>这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。</p><p>而数据版本的可见性规则，就是基于数据的 row trx_id 和这个一致性视图的对比结果得到的。</p><p>这个视图数组把所有的 row trx_id 分成了几种不同的情况。</p><p><img src="/2020/06/08/2020-06-08-mysql-08/882114aaf55861832b4270d44507695e.png" alt="数据版本可见性规则"></p><p>这样，对于当前事务的启动瞬间来说，一个数据版本的 row trx_id，有以下几种可能：</p><ol><li>如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；</li><li>如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；</li><li>如果落在黄色部分，那就包括两种情况:a.若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见；b.若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。</li></ol><p>比如，对于图1中的数据来说，如果有一个事务，它的低水位是 18，那么当它访问这一行数据时，就会从 V4 通过 U3 计算出 V3，所以在它看来，这一行的值是 11。</p><p>你看，有了这个声明后，系统里面随后发生的更新，是不是就跟这个事务看到的内容无关了呢？因为之后的更新，生成的版本一定属于上面的 2 或者 3(a) 的情况，而对它来说，这些新的数据版本是不存在的，所以这个事务的快照，就是“静态”的了。</p><p>这里，我们不妨做如下假设：</p><ol><li>事务 A 开始前，系统里面只有一个活跃事务 ID 是 99；</li><li>事务 A、B、C 的版本号分别是 100、101、102，且当前系统里只有这四个事务；</li><li>三个事务开始前，(1,1）这一行数据的 row trx_id 是 90。</li></ol><p>这样，事务 A 的视图数组就是[99,100], 事务 B 的视图数组是[99,100,101], 事务 C 的视图数组是[99,100,101,102]。</p><p>为了简化分析，我先把其他干扰语句去掉，只画出跟事务 A 查询逻辑有关的操作：</p><p><img src="/2020/06/08/2020-06-08-mysql-08/9416c310e406519b7460437cb0c5c149.png" alt="事务 A 查询数据逻辑图"></p><p>从图中可以看到，第一个有效更新是事务 C，把数据从 (1,1) 改成了 (1,2)。这时候，这个数据的最新版本的 row trx_id 是 102，而 90 这个版本已经成为了历史版本。</p><p>第二个有效更新是事务 B，把数据从 (1,2) 改成了 (1,3)。这时候，这个数据的最新版本（即 row trx_id）是 101，而 102 又成为了历史版本。</p><p>你可能注意到了，在事务 A 查询的时候，其实事务 B 还没有提交，但是它生成的 (1,3) 这个版本已经变成当前版本了。但这个版本对事务 A 必须是不可见的，否则就变成脏读了。</p><p>好，现在事务 A 要来读数据了，它的视图数组是[99,100]。当然了，读数据都是从当前版本读起的。所以，事务 A 查询语句的读数据流程是这样的：</p><ul><li>找到 (1,3) 的时候，判断出 row trx_id=101，比高水位大，处于红色区域，不可见；</li><li>接着，找到上一个历史版本，一看 row trx_id=102，比高水位大，处于红色区域，不可见；</li><li>再往前找，终于找到了（1,1)，它的 row trx_id=90，比低水位小，处于绿色区域，可见。</li></ul><p>这样执行下来，虽然期间这一行数据被修改过，但是事务 A 不论在什么时候查询，看到这行数据的结果都是一致的，所以我们称之为一致性读。</p><p>这个判断规则是从代码逻辑直接转译过来的，但是正如你所见，用于人肉分析可见性很麻烦。</p><p>所以，我来给你翻译一下。一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况：</p><ol><li>版本未提交，不可见；</li><li>版本已提交，但是是在视图创建后提交的，不可见；</li><li>版本已提交，而且是在视图创建前提交的，可见。</li></ol><p>现在，我们用这个规则来判断图 4 中的查询结果，事务 A 的查询语句的视图数组是在事务 A 启动的时候生成的，这时候：</p><ol><li>(1,3) 还没提交，属于情况 1，不可见；</li><li>(1,2) 虽然提交了，但是是在视图数组创建之后提交的，属于情况 2，不可见；</li><li>(1,1) 是在视图数组创建之前提交的，可见。</li></ol><p>事务 B 的 update 语句，如果按照一致性读，好像结果不对哦？</p><p>是的，如果事务 B 在更新之前查询一次数据，这个查询返回的 k 的值确实是 1。</p><p>但是，当它要去更新数据的时候，就不能再在历史版本上更新了，否则事务 C 的更新就丢失了。因此，事务 B 此时的 set k=k+1 是在（1,2）的基础上进行的操作。</p><p>所以，这里就用到了这样一条规则：<strong>更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）</strong>。</p><p>因此，在更新的时候，当前读拿到的数据是 (1,2)，更新后生成了新版本的数据 (1,3)，这个新版本的 row trx_id 是 101。</p><p>所以，在执行事务 B 查询语句的时候，一看自己的版本号是 101，最新数据的版本号也是 101，是自己的更新，可以直接使用，所以查询得到的 k 的值是 3。</p><p>这里我们提到了一个概念，叫作当前读。其实，<strong>除了 update 语句外，select 语句如果加锁，也是当前读</strong>。</p><p>所以，如果把事务 A 的查询语句 select * from t where id=1 修改一下，加上 lock in share mode 或 for update，也都可以读到版本号是 101 的数据，返回的 k 的值是 3。下面这两个 select 语句，就是分别加了读锁（S 锁，共享锁）和写锁（X 锁，排他锁）。</p><pre><code>mysql&gt; select k from t where id=1 lock in share mode;mysql&gt; select k from t where id=1 for update;</code></pre><p>可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。</p><p>而读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是：</p><ul><li>在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；</li><li>在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。</li></ul><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ol><li><p>InnoDB 的行数据有多个版本，每个数据版本有自己的 row trx_id，每个事务或者语句有自己的一致性视图。普通查询语句是一致性读，一致性读会根据 row trx_id 和一致性视图确定数据版本的可见性。</p></li><li><p>对于可重复读，查询只承认在事务启动前就已经提交完成的数据；</p></li><li><p>对于读提交，查询只承认在语句启动前就已经提交完成的数据；</p></li><li><p>而当前读，总是读取已经提交完成的最新版本。</p></li><li><p>更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。</p></li><li><p>begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>行锁</title>
      <link href="/2020/06/07/2020-06-07-mysql-07/"/>
      <url>/2020/06/07/2020-06-07-mysql-07/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><p>MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一。</p><p>行锁就是针对数据表中行记录的锁。这很好理解，比如事务 A 更新了一行，而这时候事务 B 也要更新同一行，则必须等事务 A 的操作完成后才能进行更新。</p><p>在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。</p><p>如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。</p><h2 id="死锁和死锁检测"><a href="#死锁和死锁检测" class="headerlink" title="死锁和死锁检测"></a>死锁和死锁检测</h2><p>当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。这里用数据库中的行锁举个例子。</p><p><img src="/2020/06/07/2020-06-07-mysql-07/4d0eeec7b136371b79248a0aed005a52.jpg" alt></p><p>这时候，事务 A 在等待事务 B 释放 id=2 的行锁，而事务 B 在等待事务 A 释放 id=1 的行锁。 事务 A 和事务 B 在互相等待对方的资源释放，就是进入了死锁状态。</p><p>当出现死锁以后，有两种策略：</p><ul><li>一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。</li><li>另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。</li></ul><p>在 InnoDB 中，innodb_lock_wait_timeout 的默认值是 50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。</p><p>但是，我们又不可能直接把这个时间设置成一个很小的值，比如 1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤。</p><p>所以，正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且 innodb_deadlock_detect 的默认值本身就是 on。主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。</p><p>你可以想象一下这个过程：每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。</p><p>每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n) 的操作。假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的 CPU 资源。因此，你就会看到 CPU 利用率很高，但是每秒却执行不了几个事务。</p><p>根据上面的分析，我们来讨论一下，怎么解决由这种热点行更新导致的性能问题呢？问题的症结在于，死锁检测要耗费大量的 CPU 资源。</p><p>一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关掉死锁检测意味着可能会出现大量的超时，这是业务有损的。</p><p>另一个思路是控制并发度。根据上面的分析，你会发现如果并发能够控制住，比如同一行同时最多只有 10 个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。一个直接的想法就是，在客户端做并发控制。但是，你会很快发现这个方法不太可行，因为客户端很多。我见过一个应用，有 600 个客户端，这样即使每个客户端控制到只有 5 个并发线程，汇总到数据库服务端以后，峰值并发数也可能要达到 3000。</p><p>因此，这个并发控制要做在数据库服务端。如果你有中间件，可以考虑在中间件实现；如果你的团队有能修改 MySQL 源码的人，也可以做在 MySQL 里面。基本思路就是，对于相同行的更新，在进入引擎之前排队。这样在 InnoDB 内部就不会有大量的死锁检测工作了。</p><p>可能你会问，如果团队里暂时没有数据库方面的专家，不能实现这样的方案，能不能从设计上优化这个问题呢？</p><p>你可以考虑通过将一行改成逻辑上的多行来减少锁冲突。还是以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。</p><p>这个方案看上去是无损的，但其实这类方案需要根据业务逻辑做详细设计。如果账户余额可能会减少，比如退票逻辑，那么这时候就需要考虑当一部分行记录变成 0 的时候，代码要有特殊处理。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ol><li>两阶段锁：在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放， 而是要等到事务结束时才释放。建议：如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。</li><li>死锁：当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态。解决方案：</li></ol><ul><li>通过参数 innodb_lock_wait_timeout 根据实际业务场景来设置超时时间，InnoDB引擎默认值是50s。</li><li>发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑（默认是开启状态）。</li></ul><ol start="3"><li>如何解决热点行更新导致的性能问题？</li></ol><ul><li>如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关闭掉。一般不建议采用</li><li>控制并发度，对应相同行的更新，在进入引擎之前排队。这样在InnoDB内部就不会有大量的死锁检测工作了。</li><li>将热更新的行数据拆分成逻辑上的多行来减少锁冲突，但是业务复杂度可能会大大提高。</li></ul><ol start="4"><li><strong>innodb行级锁是通过锁索引记录实现的，如果更新的列没建索引是会锁住整个表的。</strong></li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>全局锁和表锁</title>
      <link href="/2020/06/06/2020-06-06-mysql-06/"/>
      <url>/2020/06/06/2020-06-06-mysql-06/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><p>根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类。</p><h2 id="全局锁"><a href="#全局锁" class="headerlink" title="全局锁"></a>全局锁</h2><p>顾名思义，全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。</p><p>全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都 select 出来存成文本。</p><p>以前有一种做法，是通过 FTWRL 确保不会有其他线程对数据库做更新，然后对整个库做备份。注意，在备份过程中整个库完全处于只读状态。</p><p>但是让整库都只读，听上去就很危险：</p><ul><li>如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆；</li><li>如果你在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致主从延迟。</li></ul><p>看来加全局锁不太好。但是细想一下，备份为什么要加锁呢？我们来看一下不加锁会有什么问题。</p><p>不加锁的话，备份系统备份的得到的库不是一个逻辑时间点，这个视图是逻辑不一致的。</p><p>官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数–single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。</p><p>你一定在疑惑，有了这个功能，为什么还需要 FTWRL 呢？一致性读是好，但前提是引擎要支持这个隔离级别。比如，对于 MyISAM 这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用 FTWRL 命令了。</p><p>所以，single-transaction 方法只适用于所有的表使用事务引擎的库。如果有的表使用了不支持事务的引擎，那么备份就只能通过 FTWRL 方法。这往往是 DBA 要求业务开发人员使用 InnoDB 替代 MyISAM 的原因之一。</p><p>你也许会问，既然要全库只读，为什么不使用 set global readonly=true 的方式呢？确实 readonly 方式也可以让全库进入只读状态，但我还是会建议你用 FTWRL 方式，主要有两个原因：</p><ol><li>在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大，我不建议你使用。</li><li>在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。</li></ol><p>业务的更新不只是增删改数据（DML)，还有可能是加字段等修改表结构的操作（DDL）。不论是哪种方法，一个库被全局锁上以后，你要对里面任何一个表做加字段操作，都是会被锁住的。</p><h2 id="表级锁"><a href="#表级锁" class="headerlink" title="表级锁"></a>表级锁</h2><p>MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。</p><p>表锁的语法是 lock tables … read/write。与 FTWRL 类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。</p><p>举个例子, 如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。连写 t1 都不允许，自然也不能访问其他表。</p><p>在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。而对于 InnoDB 这种支持行锁的引擎，一般不使用 lock tables 命令来控制并发，毕竟锁住整个表的影响面还是太大。</p><p>另一类表级的锁是 MDL（metadata lock)。MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。</p><p>因此，在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。</p><ul><li>读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。</li><li>读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。</li></ul><p><img src="/2020/06/06/2020-06-06-mysql-06/7cf6a3bf90d72d1f0fc156ececdfb0ce.jpg" alt></p><p>我们可以看到 session A 先启动，这时候会对表 t 加一个 MDL 读锁。由于 session B 需要的也是 MDL 读锁，因此可以正常执行。</p><p>之后 session C 会被 blocked，是因为 session A 的 MDL 读锁还没有释放，而 session C 需要 MDL 写锁，因此只能被阻塞。</p><p>如果只有 session C 自己被阻塞还没什么关系，但是之后所有要在表 t 上新申请 MDL 读锁的请求也会被 session C 阻塞。前面我们说了，所有对表的增删改查操作都需要先申请 MDL 读锁，就都被锁住，等于这个表现在完全不可读写了。</p><p>如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新 session 再请求的话，这个库的线程很快就会爆满。</p><p>你现在应该知道了，事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。</p><p>如何安全地给小表加字段？</p><p>首先我们要解决长事务，事务不提交，就会一直占着 MDL 锁。在 MySQL 的 information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。</p><p>但考虑一下这个场景。如果你要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而你不得不加个字段，你该怎么做呢？</p><p>这时候 kill 可能未必管用，因为新的请求马上就来了。比较理想的机制是，在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者 DBA 再通过重试命令重复这个过程。</p><p>MariaDB 已经合并了 AliSQL 的这个功能，所以这两个开源分支目前都支持 DDL NOWAIT/WAIT n 这个语法。</p><pre><code>ALTER TABLE tbl_name NOWAIT add column ...ALTER TABLE tbl_name WAIT N add column ... </code></pre><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ol><li>根据加锁范围：MySQL里面的锁可以分为：全局锁、表级锁、行级锁</li><li>全局锁：<ul><li>对整个数据库实例加锁。</li><li>MySQL提供加全局读锁的方法：Flush tables with read lock(FTWRL)</li><li>这个命令可以使整个库处于只读状态。使用该命令之后，数据更新语句、数据定义语句和更新类事务的提交语句等操作都会被阻塞。</li><li>使用场景：全库逻辑备份。</li><li>风险：1.如果在主库备份，在备份期间不能更新，业务停摆；2.如果在从库备份，备份期间不能执行主库同步的binlog，导致主从延迟</li><li>官方自带的逻辑备份工具mysqldump，当mysqldump使用参数–single-transaction的时候，会启动一个事务，确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是可以正常更新的。一致性读是好，但是前提是引擎要支持这个隔离级别。</li><li>如果要全库只读，为什么不使用set global readonly=true的方式？1.在有些系统中，readonly的值会被用来做其他逻辑，比如判断主备库。所以修改global变量的方式影响太大；2.在异常处理机制上有差异。如果执行FTWRL命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个库长时间处于不可写状态，风险较高。</li></ul></li><li>表级锁：<ul><li>MySQL里面表级锁有两种，一种是表锁，一种是元数据锁(meta data lock,MDL)</li><li>表锁的语法是:lock tables … read/write</li><li>可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。</li><li>对于InnoDB这种支持行锁的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面还是太大。</li><li>MDL：不需要显式使用，在访问一个表的时候会被自动加上。</li><li>MDL的作用：保证读写的正确性。</li><li>在对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。</li><li>读锁之间不互斥。读写锁之间，写锁之间是互斥的，用来保证变更表结构操作的安全性。</li><li>MDL 会直到事务提交才会释放，在做表结构变更的时候，一定要小心不要导致锁住线上查询和更新。</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>索引（下）</title>
      <link href="/2020/06/05/2020-06-05-mysql-05/"/>
      <url>/2020/06/05/2020-06-05-mysql-05/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="覆盖索引"><a href="#覆盖索引" class="headerlink" title="覆盖索引"></a>覆盖索引</h2><p>如果执行的语句是 select ID from T where k between 3 and 5，这时只需要查 ID 的值，而 ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引 k 已经“覆盖了”我们的查询需求，我们称为覆盖索引。</p><p>由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。</p><h2 id="最左前缀原则"><a href="#最左前缀原则" class="headerlink" title="最左前缀原则"></a>最左前缀原则</h2><p>如果为每一种查询都设计一个索引，索引是不是太多了。如果我现在要按照市民的身份证号去查他的家庭地址呢？虽然这个查询需求在业务中出现的概率不高，但总不能让它走全表扫描吧？反过来说，单独为一个不频繁的请求创建一个（身份证号，地址）的索引又感觉有点浪费。应该怎么做呢？</p><p>B+ 树这种索引结构，可以利用索引的“最左前缀”，来定位记录。</p><p>为了直观地说明这个概念，我们用（name，age）这个联合索引来分析。</p><p><img src="/2020/06/05/2020-06-05-mysql-05/89f74c631110cfbc83298ef27dcd6370.jpg" alt></p><p>当你的逻辑需求是查到所有名字是“张三”的人时，可以快速定位到 ID4，然后向后遍历得到所有需要的结果。</p><p>如果你要查的是所有名字第一个字是“张”的人，你的 SQL 语句的条件是”where name like ‘张 %’”。这时，你也能够用上这个索引，查找到第一个符合条件的记录是 ID3，然后向后遍历，直到不满足条件为止。</p><p>可以看到，不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。</p><p>基于上面对最左前缀索引的说明，一个问题：在建立联合索引的时候，如何安排索引内的字段顺序。</p><p>这里评估标准是，索引的复用能力。因为可以支持最左前缀，所以当已经有了 (a,b) 这个联合索引后，一般就不需要单独在 a 上建立索引了。因此，第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。</p><p>那么，如果既有联合查询，又有基于 a、b 各自的查询呢？查询条件里面只有 b 的语句，是无法使用 (a,b) 这个联合索引的，这时候你不得不维护另外一个索引，也就是说你需要同时维护 (a,b)、(b) 这两个索引。</p><p>这时候，我们要考虑的原则就是空间了。比如上面这个市民表的情况，name 字段是比 age 字段大的 ，那我就建议你创建一个（name,age) 的联合索引和一个 (age) 的单字段索引。</p><h2 id="索引下推"><a href="#索引下推" class="headerlink" title="索引下推"></a>索引下推</h2><p>我们还是以市民表的联合索引（name, age）为例。如果现在有一个需求：检索出表中“名字第一个字是张，而且年龄是 10 岁的所有男孩”。那么，SQL 语句是这么写的：</p><pre><code>mysql&gt; select * from tuser where name like &#39;张%&#39; and age=10 and ismale=1;</code></pre><p>你已经知道了前缀索引规则，所以这个语句在搜索索引树的时候，只能用 “张”，找到第一个满足条件的记录 ID3。当然，这还不错，总比全表扫描要好。</p><p>然后呢？当然是判断其他条件是否满足。</p><p>在 MySQL 5.6 之前，只能从 ID3 开始一个个回表。到主键索引上找出数据行，再对比字段值。</p><p>而 MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。</p><p>是这两个过程的执行流程图。</p><p><img src="/2020/06/05/2020-06-05-mysql-05/b32aa8b1f75611e0759e52f5915539ac.jpg" alt="无索引下推执行流程"></p><p><img src="/2020/06/05/2020-06-05-mysql-05/76e385f3df5a694cc4238c7b65acfe1b.jpg" alt="索引下推执行流程"></p><p>这两个图里面，每一个虚线箭头表示回表一次。</p><p>InnoDB 在 (name,age) 索引内部就判断了 age 是否等于 10，对于不等于 10 的记录，直接判断并跳过。在我们的这个例子中，只需要对 ID4、ID5 这两条记录回表取数据判断，就只需要回表 2 次。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>回表：回到主键索引树搜索的过程，称为回表</li><li>覆盖索引：某索引已经覆盖了查询需求，称为覆盖索引，例如：select ID from T where k between 3 and 5 在引擎内部使用覆盖索引在索引K上其实读了三个记录，R3~R5(对应的索引k上的记录项)，但对于MySQL的Server层来说，它就是找引擎拿到了两条记录，因此MySQL认为扫描行数是2</li><li>最左前缀原则：B+Tree这种索引结构，可以利用索引的”最左前缀”来定位记录</li><li>只要满足最左前缀，就可以利用索引来加速检索。</li><li>最左前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左M个字符。第一原则是：如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。</li><li>索引下推：在MySQL5.6之前，只能从根据最左前缀查询到ID开始一个个回表。到主键索引上找出数据行，再对比字段值。MySQL5.6引入的索引下推优化，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。</li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>索引（上）</title>
      <link href="/2020/06/04/2020-06-04-mysql-04/"/>
      <url>/2020/06/04/2020-06-04-mysql-04/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="索引的常见模型"><a href="#索引的常见模型" class="headerlink" title="索引的常见模型"></a>索引的常见模型</h2><p>索引的出现是为了提高查询效率，但是实现索引的方式却有很多种，所以这里也就引入了索引模型的概念。可以用于提高读写效率的数据结构很多，这里我先给你介绍三种常见、也比较简单的数据结构，它们分别是哈希表、有序数组和搜索树。</p><p>哈希表是一种以键 - 值（key-value）存储数据的结构，我们只要输入待查找的键即 key，就可以找到其对应的值即 Value。哈希的思路很简单，把值放在数组里，用一个哈希函数把 key 换算成一个确定的位置，然后把 value 放在数组的这个位置。</p><p>不可避免地，多个 key 值经过哈希函数的换算，会出现同一个值的情况。处理这种情况的一种方法是，拉出一个链表。</p><p>所以，哈希表这种结构适用于只有等值查询的场景，比如 Memcached 及其他一些 NoSQL 引擎。</p><p>而有序数组在等值查询和范围查询场景中的性能就都非常优秀。还是上面这个根据身份证号查名字的例子，如果我们使用有序数组来实现的话，示意图如下所示：</p><p><img src="/2020/06/04/2020-06-04-mysql-04/bfc907a92f99cadf5493cf0afac9ca49.png" alt></p><p>这里我们假设身份证号没有重复，这个数组就是按照身份证号递增的顺序保存的。这时候如果你要查 ID_card_n2 对应的名字，用二分法就可以快速得到，这个时间复杂度是 O(log(N))。</p><p>同时很显然，这个索引结构支持范围查询。你要查身份证号在[ID_card_X, ID_card_Y]区间的 User，可以先用二分法找到 ID_card_X（如果不存在 ID_card_X，就找到大于 ID_card_X 的第一个 User），然后向右遍历，直到查到第一个大于 ID_card_Y 的身份证号，退出循环。</p><p>如果仅仅看查询效率，有序数组就是最好的数据结构了。但是，在需要更新数据的时候就麻烦了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高。</p><p>所以，有序数组索引只适用于静态存储引擎，比如你要保存的是 2017 年某个城市的所有人口信息，这类不会再修改的数据。</p><p>二叉搜索树也是课本里的经典数据结构了。还是上面根据身份证号查名字的例子，如果我们用二叉搜索树来实现的话，示意图如下所示：</p><p><img src="/2020/06/04/2020-06-04-mysql-04/04fb9d24065635a6a637c25ba9ddde68.png" alt></p><p>二叉搜索树的特点是：父节点左子树所有结点的值小于父节点的值，右子树所有结点的值大于父节点的值。这样如果你要查 ID_card_n2 的话，按照图中的搜索顺序就是按照 UserA -&gt; UserC -&gt; UserF -&gt; User2 这个路径得到。这个时间复杂度是 O(log(N))。</p><p>当然为了维持 O(log(N)) 的查询复杂度，你就需要保持这棵树是平衡二叉树。为了做这个保证，更新的时间复杂度也是 O(log(N))。</p><p>树可以有二叉，也可以有多叉。多叉树就是每个节点有多个儿子，儿子之间的大小保证从左到右递增。二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。</p><p>你可以想象一下一棵 100 万节点的平衡二叉树，树高 20。一次查询可能需要访问 20 个数据块。在机械硬盘时代，从磁盘随机读一个数据块需要 10 ms 左右的寻址时间。也就是说，对于一个 100 万行的表，如果使用二叉树来存储，单独访问一个行可能需要 20 个 10 ms 的时间，这个查询可真够慢的。</p><p>为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N 叉”树。这里，“N 叉”树中的“N”取决于数据块的大小。</p><p>以 InnoDB 的一个整数字段索引为例，这个 N 差不多是 1200。这棵树高是 4 的时候，就可以存 1200 的 3 次方个值，这已经 17 亿了。考虑到树根的数据块总是在内存中的，一个 10 亿行的表上一个整数字段的索引，查找一个值最多只需要访问 3 次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。</p><p>N 叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了。</p><p>在 MySQL 中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。</p><h2 id="InnoDB-的索引模型"><a href="#InnoDB-的索引模型" class="headerlink" title="InnoDB 的索引模型"></a>InnoDB 的索引模型</h2><p>在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。又因为前面我们提到的，InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。</p><p><strong>每一个索引在 InnoDB 里面对应一棵 B+ 树。</strong></p><p>假设，我们有一个主键列为 ID 的表，表中有字段 k，并且在 k 上有索引。</p><p>表中 R1~R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6)，两棵树的示例示意图如下。</p><p><img src="/2020/06/04/2020-06-04-mysql-04/dcda101051f28502bd5c4402b292e38d.png" alt></p><p>从图中不难看出，根据叶子节点的内容，索引类型分为主键索引和非主键索引。</p><p><strong>主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）。</strong></p><p><strong>非主键索引的叶子节点内容是主键的值。</strong>在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。</p><p>根据上面的索引结构说明，我们来讨论一个问题：基于主键索引和普通索引的查询有什么区别？</p><ul><li><p>如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索 ID 这棵 B+ 树；</p></li><li><p>如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。</p></li></ul><p>也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。</p><h2 id="索引维护"><a href="#索引维护" class="headerlink" title="索引维护"></a>索引维护</h2><p>B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。以上面这个图为例，如果插入新的行 ID 值为 700，则只需要在 R5 的记录后面插入一个新记录。如果新插入的 ID 值为 400，就相对麻烦了，需要逻辑上挪动后面的数据，空出位置。</p><p>而更糟的情况是，如果 R5 所在的数据页已经满了，根据 B+ 树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响。</p><p>除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约 50%。</p><p>当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程。</p><p>自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： NOT NULL PRIMARY KEY AUTO_INCREMENT。</p><p>插入新记录的时候可以不指定 ID 的值，系统会获取当前 ID 最大值加 1 作为下一条记录的 ID 值。</p><p>也就是说，自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。</p><p>而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。</p><p>除了考虑性能外，我们还可以从存储空间的角度来看。假设你的表中确实有一个唯一字段，比如字符串类型的身份证号，那应该用身份证号做主键，还是用自增字段做主键呢？</p><p>由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的叶子节点占用约 20 个字节，而如果用整型做主键，则只要 4 个字节，如果是长整型（bigint）则是 8 个字节。</p><p><strong>显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。</strong></p><p>所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。</p><p>有没有什么场景适合用业务字段直接做主键的呢？还是有的。比如，有些业务的场景需求是这样的：</p><ol><li>只有一个索引；</li><li>该索引必须是唯一索引。</li></ol><p>一定看出来了，这就是典型的 KV 场景。</p><p>由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。</p><p>这时候我们就要优先考虑上一段提到的“尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>B+ 树能够很好地配合磁盘的读写特性，减少单次查询的磁盘访问次数。</li><li>由于 InnoDB 是索引组织表，一般情况下建议创建一个自增主键，这样非主键索引占用的空间最小。</li><li>索引的作用：提高数据查询效率</li><li>常见索引模型：哈希表、有序数组、搜索树</li><li>哈希表：键 - 值(key - value)。</li><li>哈希思路：把值放在数组里，用一个哈希函数把key换算成一个确定的位置，然后把value放在数组的这个位置</li><li>哈希冲突的处理办法：链表</li><li>哈希表适用场景：只有等值查询的场景</li><li>有序数组：按顺序存储。查询用二分法就可以快速查询，时间复杂度是：O(log(N))</li><li>有序数组查询效率高，更新效率低</li><li>有序数组的适用场景：静态存储引擎。</li><li>二叉搜索树：每个节点的左儿子小于父节点，父节点又小于右儿子</li><li>二叉搜索树：查询时间复杂度O(log(N))，更新时间复杂度O(log(N))</li><li>数据库存储大多不适用二叉树，因为树高过高，会适用N叉树</li><li>InnoDB中的索引模型：B+Tree</li><li>索引类型：主键索引、非主键索引主键索引的叶子节点存的是整行的数据(聚簇索引)，非主键索引的叶子节点内容是主键的值(二级索引)</li><li>主键索引和普通索引的区别：主键索引只要搜索ID这个B+Tree即可拿到数据。普通索引先搜索索引拿到主键值，再到主键索引树搜索一次(回表)</li><li>一个数据页满了，按照B+Tree算法，新增加一个数据页，叫做页分裂，会导致性能下降。空间利用率降低大概50%。当相邻的两个数据页利用率很低的时候会做数据页合并，合并的过程是分裂过程的逆过程。</li><li>从性能和存储空间方面考量，自增主键往往是更合理的选择。</li><li>索引是排好序的快速查找的数据结构。</li><li>Hash结构：查询速度快，查询时间复杂度为O(1)。但不支持范围查询和排序。</li><li>二叉树：查询时间复杂度O(log(N))，但容易造成左倾或右倾两边不平衡情况，导致树高过高，磁盘IO成本高。</li><li>平衡二叉树：自动调节两边平衡。但每个节点只有两个子节点，当数据过多的时候，树高也过高，磁IO成本也相对高。</li><li>B树： 每个节点都存储key和data，要找到具体的数据，需要进行一次中序遍历按序来扫，导致查询效率不稳定。并且查询的最小单位是页，每页默认16K，数据页的大小有限，节点包含数据信息，会导致查询的数据变少，IO读写次数变多。</li><li>B+树：只有叶子节点存储data。每个数据的查询效率稳定相等。非叶子节点不带有数据信息，一次性读入内存的需要查找的关键字也就越多，相对IO读写次数就降低了。</li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>事务隔离</title>
      <link href="/2020/06/03/2020-06-03-mysql-03/"/>
      <url>/2020/06/03/2020-06-03-mysql-03/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="隔离性与隔离级别"><a href="#隔离性与隔离级别" class="headerlink" title="隔离性与隔离级别"></a>隔离性与隔离级别</h2><p>提到事务，你肯定会想到 ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性），今天我们就来说说其中 I，也就是“隔离性”。</p><p>当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概念。</p><p>在谈隔离级别之前，你首先要知道，你隔离得越严实，效率就会越低。因此很多时候，我们都要在二者之间寻找一个平衡点。SQL 标准的事务隔离级别包括：读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）。</p><ul><li>读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。</li><li>读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。</li><li>可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。</li><li>串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。</li></ul><p>其中“读提交”和“可重复读”比较难理解，所以我用一个例子说明这几种隔离级别。假设数据表 T 中只有一列，其中一行的值为 1，下面是按照时间顺序执行两个事务的行为。</p><pre><code>mysql&gt; create table T(c int) engine=InnoDB;insert into T(c) values(1);</code></pre><p><img src="/2020/06/03/2020-06-03-mysql-03/2020060301.png" alt></p><p>我们来看看在不同的隔离级别下，事务 A 会有哪些不同的返回结果，也就是图里面 V1、V2、V3 的返回值分别是什么。</p><ul><li>若隔离级别是“读未提交”， 则 V1 的值就是 2。这时候事务 B 虽然还没有提交，但是结果已经被 A 看到了。因此，V2、V3 也都是 2。</li><li>若隔离级别是“读提交”，则 V1 是 1，V2 的值是 2。事务 B 的更新在提交后才能被 A 看到。所以， V3 的值也是 2。</li><li>若隔离级别是“可重复读”，则 V1、V2 是 1，V3 是 2。之所以 V2 还是 1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的。</li><li>若隔离级别是“串行化”，则在事务 B 执行“将 1 改成 2”的时候，会被锁住。直到事务 A 提交后，事务 B 才可以继续执行。所以从 A 的角度看， V1、V2 值是 1，V3 的值是 2。</li></ul><p>在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在“读提交”隔离级别下，这个视图是在每个 SQL 语句开始执行的时候创建的。这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。</p><p>我们可以看到在不同的隔离级别下，数据库行为是有所不同的。Oracle 数据库的默认隔离级别其实就是“读提交”，因此对于一些从 Oracle 迁移到 MySQL 的应用，为保证数据库隔离级别的一致，你一定要记得将 MySQL 的隔离级别设置为“读提交”。</p><p>配置的方式是，将启动参数 transaction-isolation 的值设置成 READ-COMMITTED。你可以用 show variables 来查看当前的值。</p><pre><code>mysql&gt; show variables like &#39;transaction_isolation&#39;;+-----------------------+----------------+| Variable_name | Value |+-----------------------+----------------+| transaction_isolation | READ-COMMITTED |+-----------------------+----------------+</code></pre><p>总结来说，存在即合理，每种隔离级别都有自己的使用场景，你要根据自己的业务情况来定。</p><h2 id="事务隔离的实现"><a href="#事务隔离的实现" class="headerlink" title="事务隔离的实现"></a>事务隔离的实现</h2><p>理解了事务的隔离级别，我们再来看看事务隔离具体是怎么实现的。这里我们展开说明“可重复读”。</p><p>在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。</p><p>假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。</p><p><img src="/2020/06/03/2020-06-03-mysql-03/2020060302.png" alt></p><p>当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view。如图中看到的，在视图 A、B、C 里面，这一个记录的值分别是 1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。</p><p>同时你会发现，即使现在有另外一个事务正在将 4 改成 5，这个事务跟 read-view A、B、C 对应的事务是不会冲突的。</p><p>你一定会问，回滚日志总不能一直保留吧，什么时候删除呢？答案是，在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。</p><p>什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的 read-view 的时候。</p><p>基于上面的说明，我们来讨论一下为什么建议你尽量不要使用长事务。</p><p>长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。</p><p>在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。我见过数据只有 20GB，而回滚段有 200GB 的库。最终只好为了清理回滚段，重建整个库。</p><p>除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库。</p><h2 id="事务的启动方式"><a href="#事务的启动方式" class="headerlink" title="事务的启动方式"></a>事务的启动方式</h2><p>如前面所述，长事务有这些潜在风险，我当然是建议你尽量避免。其实很多时候业务开发同学并不是有意使用长事务，通常是由于误用所致。MySQL 的事务启动方式有以下几种：</p><ol><li>显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。</li><li>set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。</li></ol><p>有些客户端连接框架会默认连接成功后先执行一个 set autocommit=0 的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。</p><p>因此，我会建议你总是使用 set autocommit=1, 通过显式语句的方式来启动事务。</p><p>但是有的开发同学会纠结“多一次交互”的问题。对于一个需要频繁使用事务的业务，第二种方式每个事务在开始时都不需要主动执行一次 “begin”，减少了语句的交互次数。如果你也有这个顾虑，我建议你使用 commit work and chain 语法。</p><p>你可以在 information_schema 库的 innodb_trx 这个表中查询长事务，比如下面这个语句，用于查找持续时间超过 60s 的事务。</p><pre><code>select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))&gt;60</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>事务的特性：原子性、一致性、隔离性、持久性</li><li>多事务同时执行的时候，可能会出现的问题：脏读、不可重复读、幻读</li><li>事务隔离级别：读未提交、读提交、可重复读、串行化</li><li>不同事务隔离级别的区别：<ul><li>读未提交：一个事务还未提交，它所做的变更就可以被别的事务看到</li><li>读提交：一个事务提交之后，它所做的变更才可以被别的事务看到</li><li>可重复读：一个事务执行过程中看到的数据是一致的。未提交的更改对其他事务是不可见的</li><li>串行化：对应一个记录会加读写锁，出现冲突的时候，后访问的事务必须等前一个事务执行完成才能继续执行</li></ul></li><li>配置方法：启动参数transaction-isolation</li><li>事务隔离的实现：每条记录在更新的时候都会同时记录一条回滚操作。同一条记录在系统中可以存在多个版本，这就是数据库的多版本并发控制（MVCC）。</li><li>回滚日志什么时候删除？系统会判断当没有事务需要用到这些回滚日志的时候，回滚日志会被删除。</li><li>什么时候不需要了？当系统里么有比这个回滚日志更早的read-view的时候。</li><li>为什么尽量不要使用长事务。长事务意味着系统里面会存在很老的事务视图，在这个事务提交之前，回滚记录都要保留，这会导致大量占用存储空间。除此之外，长事务还占用锁资源，可能会拖垮库。</li><li>事务启动方式：一、显式启动事务语句，begin或者start transaction,提交commit，回滚rollback；二、set autocommit=0，该命令会把这个线程的自动提交关掉。这样只要执行一个select语句，事务就启动，并不会自动提交，直到主动执行commit或rollback或断开连接。</li><li>建议使用方法一，如果考虑多一次交互问题，可以使用commit work and chain语法。在autocommit=1的情况下用begin显式启动事务，如果执行commit则提交事务。如果执行commit work and chain则提交事务并自动启动下一个事务。</li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日志系统</title>
      <link href="/2020/06/02/2020-06-02-mysql-02/"/>
      <url>/2020/06/02/2020-06-02-mysql-02/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="前序"><a href="#前序" class="headerlink" title="前序"></a>前序</h2><p><img src="/2020/06/02/2020-06-02-mysql-02/2020060201.png" alt="MySQL 的逻辑架构图"></p><pre><code>mysql&gt; update T set c=c+1 where ID=2;</code></pre><p>首先，可以确定的说，查询语句的那一套流程，更新语句也是同样会走一遍。</p><p>你执行语句前要先连接数据库，这是连接器的工作。</p><p>前面我们说过，在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表 T 上所有缓存结果都清空。这也就是我们一般不建议使用查询缓存的原因。</p><p>接下来，分析器会通过词法和语法解析知道这是一条更新语句。优化器决定要使用 ID 这个索引。然后，执行器负责具体执行，找到这一行，然后更新。</p><p>与查询流程不一样的是，更新流程还涉及两个重要的日志模块，它们正是我们今天要讨论的主角：redo log（重做日志）和 binlog（归档日志）。如果接触 MySQL，那这两个词肯定是绕不过的，我后面的内容里也会不断地和你强调。不过话说回来，redo log 和 binlog 在设计上有很多有意思的地方，这些设计思路也可以用到你自己的程序里。</p><h2 id="重做日志：redo-log"><a href="#重做日志：redo-log" class="headerlink" title="重做日志：redo log"></a>重做日志：redo log</h2><p>不知道你还记不记得《孔乙己》这篇文章，酒店掌柜有一个粉板，专门用来记录客人的赊账记录。如果赊账的人不多，那么他可以把顾客名和账目写在板上。但如果赊账的人多了，粉板总会有记不下的时候，这个时候掌柜一定还有一个专门记录赊账的账本。</p><p>如果有人要赊账或者还账的话，掌柜一般有两种做法：</p><ul><li>一种做法是直接把账本翻出来，把这次赊的账加上去或者扣除掉；</li><li>另一种做法是先在粉板上记下这次的账，等打烊以后再把账本翻出来核算。</li></ul><p>在生意红火柜台很忙时，掌柜一定会选择后者，因为前者操作实在是太麻烦了。首先，你得找到这个人的赊账总额那条记录。你想想，密密麻麻几十页，掌柜要找到那个名字，可能还得带上老花镜慢慢找，找到之后再拿出算盘计算，最后再将结果写回到账本上。</p><p>这整个过程想想都麻烦。相比之下，还是先在粉板上记一下方便。你想想，如果掌柜没有粉板的帮助，每次记账都得翻账本，效率是不是低得让人难以忍受？</p><p>同样，在 MySQL 里也有这个问题，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。为了解决这个问题，MySQL 的设计者就用了类似酒店掌柜粉板的思路来提升更新效率。</p><p>而粉板和账本配合的整个过程，其实就是 MySQL 里经常说到的 WAL 技术，WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘，也就是先写粉板，等不忙的时候再写账本。</p><p>具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log（粉板）里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事。</p><p>如果今天赊账的不多，掌柜可以等打烊后再整理。但如果某天赊账的特别多，粉板写满了，又怎么办呢？这个时候掌柜只好放下手中的活儿，把粉板中的一部分赊账记录更新到账本中，然后把这些记录从粉板上擦掉，为记新账腾出空间。</p><p>与此类似，InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示。</p><p><img src="/2020/06/02/2020-06-02-mysql-02/2020060202.png" alt></p><p>write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。</p><p>write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。</p><p>有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。</p><p>要理解 crash-safe 这个概念，可以想想我们前面赊账记录的例子。只要赊账记录记在了粉板上或写在了账本上，之后即使掌柜忘记了，比如突然停业几天，恢复生意后依然可以通过账本和粉板上的数据明确赊账账目。</p><h2 id="归档日志：binlog"><a href="#归档日志：binlog" class="headerlink" title="归档日志：binlog"></a>归档日志：binlog</h2><p>前面我们讲过，MySQL 整体来看，其实就有两块：一块是 Server 层，它主要做的是 MySQL 功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。上面我们聊到的粉板 redo log 是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，称为 binlog（归档日志）。</p><p>我想你肯定会问，为什么会有两份日志呢？</p><p>因为最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。</p><p>这两种日志有以下三点不同。</p><ul><li><strong>redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。</strong></li><li><strong>redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。</strong></li><li><strong>redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。</strong></li></ul><h2 id="update语句执行流程"><a href="#update语句执行流程" class="headerlink" title="update语句执行流程"></a>update语句执行流程</h2><pre><code>mysql&gt; update T set c=c+1 where ID=2;</code></pre><p>有了对这两个日志的概念性理解，我们再来看执行器和 InnoDB 引擎在执行这个简单的 update 语句时的内部流程。</p><ol><li>执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。</li><li>执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。</li><li>引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。</li><li>执行器生成这个操作的 binlog，并把 binlog 写入磁盘。</li><li>执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。</li></ol><p>update 语句的执行流程图，图中浅色框表示是在 InnoDB 内部执行的，深色框表示是在执行器中执行的。</p><p><img src="/2020/06/02/2020-06-02-mysql-02/2020060203.png" alt="update 语句执行流程"></p><p>最后三步看上去有点“绕”，将 redo log 的写入拆成了两个步骤：prepare 和 commit，这就是”两阶段提交”。</p><h2 id="两阶段提交"><a href="#两阶段提交" class="headerlink" title="两阶段提交"></a>两阶段提交</h2><p>为什么必须有“两阶段提交”呢？这是为了让两份日志之间的逻辑一致。要说明这个问题，我们得从文章开头的那个问题说起：怎样让数据库恢复到半个月内任意一秒的状态？</p><p>前面我们说过了，binlog 会记录所有的逻辑操作，并且是采用“追加写”的形式。如果你的 DBA 承诺说半个月内可以恢复，那么备份系统中一定会保存最近半个月的所有 binlog，同时系统会定期做整库备份。这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备。</p><p>当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做：</p><ul><li>首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库；</li><li>然后，从备份的时间点开始，将备份的 binlog 依次取出来，重放到中午误删表之前的那个时刻。</li></ul><p>这样你的临时库就跟误删之前的线上库一样了，然后你可以把表数据从临时库取出来，按需要恢复到线上库去。</p><p>好了，说完了数据恢复过程，我们回来说说，为什么日志需要“两阶段提交”。这里不妨用反证法来进行解释。</p><p>由于 redo log 和 binlog 是两个独立的逻辑，如果不用两阶段提交，要么就是先写完 redo log 再写 binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。</p><p>仍然用前面的 update 语句来做例子。假设当前 ID=2 的行，字段 c 的值是 0，再假设执行 update 语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了 crash，会出现什么情况呢？</p><ol><li><p>先写 redo log 后写 binlog</p><p> 假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。由于我们前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。</p></li><li><p>先写 binlog 后写 redo log</p><p> 如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。</p></li></ol><p>可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。</p><p>你可能会说，这个概率是不是很低，平时也没有什么动不动就需要恢复临时库的场景呀？</p><p>其实不是的，不只是误操作后需要用这个过程来恢复数据。当你需要扩容的时候，也就是需要再多搭建一些备库来增加系统的读能力的时候，现在常见的做法也是用全量备份加上应用 binlog 来实现的，这个“不一致”就会导致你的线上出现主从数据库不一致的情况。</p><p>简单说，redo log 和 binlog 都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>redo log 是 InnoDB 引擎特有的；binlog是 MySQL 的 Server 层实现的，所有引擎都可以使用。</li><li>redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。</li><li>redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。</li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基础架构</title>
      <link href="/2020/06/01/2020-06-01-mysql-01/"/>
      <url>/2020/06/01/2020-06-01-mysql-01/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文章仅用于本人学习笔记记录<br>来源《MySQL实战45课》<br>微信：A20991212A（如本文档内容侵权了您的权益，请您通过微信联系到我）</p></blockquote><h2 id="基础架构"><a href="#基础架构" class="headerlink" title="基础架构"></a>基础架构</h2><p><img src="/2020/06/01/2020-06-01-mysql-01/2020060101.png" alt="MySQL 的逻辑架构图"></p><p>大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。</p><p>Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。</p><p>而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。</p><p>也就是说，执行 create table 建表的时候，如果不指定引擎类型，默认使用的就是 InnoDB。不过，也可以通过指定存储引擎的类型来选择别的引擎，比如在 create table 语句中使用 engine=memory, 来指定使用内存引擎创建表。不同存储引擎的表数据存取方式不同，支持的功能也不同。</p><h2 id="连接器"><a href="#连接器" class="headerlink" title="连接器"></a>连接器</h2><p>第一步，你会先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令一般是这么写的：</p><pre><code>mysql -h$ip -P$port -u$user -p</code></pre><p>输完命令之后，你就需要在交互对话里面输入密码。虽然密码也可以直接跟在 -p 后面写在命令行中，但这样可能会导致你的密码泄露。如果你连的是生产服务器，强烈建议你不要这么做。</p><p>连接命令中的 mysql 是客户端工具，用来跟服务端建立连接。在完成经典的 TCP 握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。</p><ul><li>如果用户名或密码不对，你就会收到一个”Access denied for user”的错误，然后客户端程序结束执行。</li><li>如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。</li></ul><p>这就意味着，<strong>一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。</strong></p><p>连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在 show processlist 命令中看到它。文本中这个图是 show processlist 的结果，其中的 Command 列显示为“Sleep”的这一行，就表示现在系统里面有一个空闲连接。</p><p><img src="/2020/06/01/2020-06-01-mysql-01/2020060102.png" alt></p><p>客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值是 8 小时。</p><p>如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒： Lost connection to MySQL server during query。这时候如果你要继续，就需要重连，然后再执行请求了。</p><p>数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。</p><p>建立连接的过程通常是比较复杂的，所以我建议你在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。</p><p>但是全部使用长连接后，你可能会发现，有些时候 MySQL 占用内存涨得特别快，这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。</p><p>怎么解决这个问题呢？你可以考虑以下两种方案。</p><ol><li>定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。</li><li>如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。</li></ol><h2 id="查询缓存"><a href="#查询缓存" class="headerlink" title="查询缓存"></a>查询缓存</h2><p>连接建立完成后，你就可以执行 select 语句了。执行逻辑就会来到第二步：查询缓存。</p><p>MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。如果你的查询能够直接在这个缓存中找到 key，那么这个 value 就会被直接返回给客户端。</p><p>如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。你可以看到，如果查询命中缓存，MySQL 不需要执行后面的复杂操作，就可以直接返回结果，这个效率会很高。</p><p>但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。</p><p><strong>查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。</strong>因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。</p><p>好在 MySQL 也提供了这种“按需使用”的方式。你可以将参数 query_cache_type 设置成 DEMAND，这样对于默认的 SQL 语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用 SQL_CACHE 显式指定，像下面这个语句一样：</p><pre><code>mysql&gt; select SQL_CACHE * from T where ID=10；</code></pre><p><strong>需要注意的是，MySQL 8.0 版本直接将查询缓存的整块功能删掉了，也就是说 8.0 开始彻底没有这个功能了。</strong></p><h2 id="分析器"><a href="#分析器" class="headerlink" title="分析器"></a>分析器</h2><p>如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL 需要知道你要做什么，因此需要对 SQL 语句做解析。</p><p>分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么，代表什么。</p><p>MySQL 从你输入的”select”这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名 T”，把字符串“ID”识别成“列 ID”。</p><p>做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。</p><p>如果你的语句不对，就会收到“You have an error in your SQL syntax”的错误提醒，比如下面这个语句 select 少打了开头的字母“s”。</p><pre><code>mysql&gt; elect * from t where ID=1;ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#39;elect * from t where ID=1&#39; at line 1</code></pre><p>一般语法错误会提示第一个出现错误的位置，所以你要关注的是紧接“use near”的内容。</p><h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><p>经过了分析器，MySQL 就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。</p><p>优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。比如你执行下面这样的语句，这个语句是执行两个表的 join：</p><pre><code>mysql&gt; select * from t1 join t2 using(ID)  where t1.c=10 and t2.d=20;</code></pre><ul><li>既可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20。</li><li>也可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。</li></ul><p>这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。</p><h2 id="执行器"><a href="#执行器" class="headerlink" title="执行器"></a>执行器</h2><p>MySQL 通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。</p><p>开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示 (在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限)。</p><pre><code>mysql&gt; select * from T where ID=10;ERROR 1142 (42000): SELECT command denied to user &#39;b&#39;@&#39;localhost&#39; for table &#39;T&#39;</code></pre><p>如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。</p><p>比如我们这个例子中的表 T 中，ID 字段没有索引，那么执行器的执行流程是这样的：</p><ol><li>调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中；</li><li>调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。</li><li>执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。</li></ol><p>至此，这个语句就执行完成了。</p><p>对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。</p><p>你会在数据库的慢查询日志中看到一个 rows_examined 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。</p><p>在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟 rows_examined 并不是完全相同的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>mysql分为Server层 和 引擎层</li><li>Server层：连接器、查询缓存、分析器、优化器、执行器等 以及所有内置的函数（eg:日期、时间、数学和加密函数等）。所有跨存储引擎的功能都在这一层实现，eg：存储过程、触发器、视图</li><li>连接器：一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。</li><li>查询缓存：mysql拿到一个查询后，先查询缓存，（缓存保存形式KV（K为查询的语句，V为查询的结果）） 。但建议关闭查询缓存（show variables like ‘%query_cache_type%’; set GLOBAL query_cache_type=’OFF’;）。失效频繁，而且对于压力大的数据库，命中率非常低。mysql8.0版本取消了查询缓存的整个功能模块。</li><li>分析器：<ul><li>词法分析：识别出里面的字符串分别是什么代表什么。</li><li>语法分析: 根据词法分析的结果和语法规则，判断是否满足Mysql语法。</li></ul></li><li>优化器：分析器知道要做什么，优化器在便利有多个索引时，决定使用哪个索引；在一个语句有多表关联（join）时，决定各个表的连接顺序。</li><li>执行器：优化器知道怎么做了，执行器进行执行语句。开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限</li></ol>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OAuth2.0</title>
      <link href="/2020/05/19/2020-05-19-oauth2-0/"/>
      <url>/2020/05/19/2020-05-19-oauth2-0/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本篇主要记录和总结工作中用到授权认证的OAuth2.0<br>如果有人再问你OAuth2.0,你就把这篇文章扔给他</p></blockquote><h2 id="流程原理"><a href="#流程原理" class="headerlink" title="流程原理"></a>流程原理</h2><p>使用授权码模式完成OAuth2.0授权的过程需要以下四个步骤：</p><ul><li>client请求授权服务端，获取Authorization Code；</li><li>client通过Authorization Code再次请求授权服务端，获取Access Token和Refresh Token；</li><li>client通过Refresh Token刷新获取Access Token；</li><li>client通过服务端返回的Access Token调取服务端接口。</li></ul><p><img src="/2020/05/19/2020-05-19-oauth2-0/auth.jpg" alt></p><h2 id="数据库的表结构设计"><a href="#数据库的表结构设计" class="headerlink" title="数据库的表结构设计"></a>数据库的表结构设计</h2><h3 id="t-partner-interfaces"><a href="#t-partner-interfaces" class="headerlink" title="t_partner_interfaces"></a>t_partner_interfaces</h3><p>用于记录后台分配给用户的权限</p><pre><code>CREATE TABLE `t_partner_interfaces` (  `id` bigint(11) unsigned NOT NULL AUTO_INCREMENT,  `partner_no` varchar(20) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;合作伙伴编号&#39;,  `interfaces` varchar(255) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;接口标志（多个用，分隔）&#39;,  `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,  `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,  PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT=&#39;合作伙伴接口表&#39;;</code></pre><h3 id="t-auth-info"><a href="#t-auth-info" class="headerlink" title="t_auth_info"></a>t_auth_info</h3><p>用于记录用户的clientId、clientSecret、redirectUri等</p><pre><code>CREATE TABLE `t_auth_info` (  `id` bigint(11) unsigned NOT NULL AUTO_INCREMENT,  `partner_no` varchar(20) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;合作伙伴编号&#39;,  `client_id` varchar(64) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;唯一的clientId&#39;,  `client_secret` varchar(64) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;密钥&#39;,  `redirect_uri` varchar(255) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;回调URI（多个用，分隔）&#39;,  `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,  `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,  PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT=&#39;授权信息表&#39;;</code></pre><h3 id="t-auth-token"><a href="#t-auth-token" class="headerlink" title="t_auth_token"></a>t_auth_token</h3><p>用户记录授权完成后的授权信息</p><pre><code>CREATE TABLE `t_auth_token` (  `id` bigint(11) unsigned NOT NULL AUTO_INCREMENT,  `partner_no` varchar(20) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;合作伙伴编号&#39;,  `access_token` varchar(128) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;唯一的Access Token&#39;,  `refresh_token` varchar(128) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;唯一的Refresh Token&#39;,  `access_expires_in` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#39;accessToken过期时间&#39;,  `refresh_expires_in` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#39;refreshToken过期时间&#39;,  `scope` varchar(255) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;accessToken可访问范围&#39;,  `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,  `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,  PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT=&#39;令牌表&#39;;</code></pre><h2 id="授权服务端主要接口的代码实现"><a href="#授权服务端主要接口的代码实现" class="headerlink" title="授权服务端主要接口的代码实现"></a>授权服务端主要接口的代码实现</h2><h3 id="获取code接口"><a href="#获取code接口" class="headerlink" title="获取code接口"></a>获取code接口</h3><pre><code>@ApiOperation( &quot;授权获取code&quot;)@GetMapping(&quot;/authorize&quot;)public ResultData authorize(HttpServletRequest request) {    try {        oAuthService.auth(request);    } catch (IllegalArgumentException | IllegalStateException e) {        return ResultData.fail(e.getMessage());    } catch (Exception e) {        LOGGER.error(&quot;Exception 请求授权接口异常&quot;, e);        return ResultData.fail(&quot;系统繁忙，请稍候重试&quot;);    }    return ResultData.succeed();}</code></pre><pre><code>public void auth(HttpServletRequest request) {    oAuthDomain.auth(request);}</code></pre><pre><code>@Component@Scope(DefaultListableBeanFactory.SCOPE_PROTOTYPE)public class OAuthDomain {    public void auth(HttpServletRequest request) {        // 客户端ID        String clientId = request.getParameter(&quot;client_id&quot;);        // 权限范围        String scope = request.getParameter(&quot;scope&quot;);        // 回调URL        String redirectUri = request.getParameter(&quot;redirect_uri&quot;);        // state，用于防止CSRF攻击（非必填）        String state = request.getParameter(&quot;state&quot;);        Assert.hasText(clientId, ErrorCodeEnum.INVALID_REQUEST.getErrorDescription());        Assert.hasText(scope, ErrorCodeEnum.INVALID_REQUEST.getErrorDescription());        Assert.hasText(redirectUri, ErrorCodeEnum.INVALID_REQUEST.getErrorDescription());        // 1 校验clientId        AuthInfoDO authInfoDO = authInfoDAO.getByClientId(clientId);        Assert.notNull(authInfoDO, ErrorCodeEnum.INVALID_CLIENT.getErrorDescription());        // 2 校验scope范围        String partnerNo = authInfoDO.getPartnerNo();        // 2.1 TODO 校验系统的scope范围        // 2.2 校验用户的scope范围        PartnerDO partnerDO = partnerDAO.getByPartnerNo(partnerNo);        Assert.state(PartnerStatusEnum.OPEN.getStatus().equals(partnerDO.getStatus()), ErrorCodeEnum.INVALID_SCOPE.getErrorDescription());        PartnerInterfacesDO partnerInterfacesDO = partnerInterfacesDAO.getByPartnerNo(partnerNo);        Assert.notNull(partnerInterfacesDO, ErrorCodeEnum.INVALID_SCOPE.getErrorDescription());        String interfaces = partnerInterfacesDO.getInterfaces();        List interfacesList = Arrays.asList(interfaces.split(&quot;,&quot;));        List scopeList = Arrays.asList(scope.split(&quot;,&quot;));        Assert.state(interfacesList.containsAll(scopeList), ErrorCodeEnum.INVALID_SCOPE.getErrorDescription());        // 3 校验redirectUri是否存在        List redirectUriList = Arrays.asList(authInfoDO.getRedirectUri().split(&quot;,&quot;));        Assert.state(redirectUriList.contains(redirectUri), ErrorCodeEnum.INVALID_REDIRECT_URI.getErrorDescription());        // 4 生成Authorization Code        String code = createAuthorizationCode(clientId, scope);        LOGGER.info(&quot;客户端ID：&quot; + clientId + &quot;生成的code：&quot; + code);        // 5 保存Authorization Code, scope到redis        redisTemplate.opsForValue().set(&quot;code:&quot; + partnerNo, code, ExpireEnum.AUTHORIZATION_CODE.getTime(), ExpireEnum.AUTHORIZATION_CODE.getTimeUnit());        redisTemplate.opsForValue().set(&quot;scope:&quot; + partnerNo, scope);        // 6 设置返回参数        String params = &quot;?code=&quot; + code;        params = StringUtils.isNotBlank(state) ? params.concat(&quot;&amp;state=&quot;).concat(state) : params;        String uri = redirectUri.concat(params);        // 7 开子线程，请求重定向地址        ObjectU.execute(() -&gt; {            request(uri);        });    }}</code></pre><h3 id="code获取Access-Token"><a href="#code获取Access-Token" class="headerlink" title="code获取Access Token"></a>code获取Access Token</h3><pre><code>@ApiOperation( &quot;code获取token&quot;)@GetMapping(value = &quot;/token&quot;)public ResultData&lt;Map&lt;String,Object&gt;&gt; token(HttpServletRequest request){    try {        return ResultData.succeed(oAuthService.token(request));    } catch (IllegalArgumentException | IllegalStateException e) {        return ResultData.fail(e.getMessage());    } catch (Exception e) {        LOGGER.error(&quot;Exception 请求授权接口异常&quot;, e);        return ResultData.fail(&quot;系统繁忙，请稍候重试&quot;);    }}</code></pre><pre><code>public Map&lt;String,Object&gt; token(HttpServletRequest request) {    // 1 返回token    Map&lt;String, Object&gt; result = oAuthDomain.token(request);    // 2 保存token相关信息    String clientId = request.getParameter(&quot;client_id&quot;);    AuthInfoDO authInfoDO = authInfoDAO.getByClientId(clientId);    AuthTokenDO authTokenDO = createAuthTokenDO(result, authInfoDO.getPartnerNo(), TokenEnum.ACCESS.getType());    AuthToken authToken = authTokenFactory.get(authTokenDO);    authToken.saveOrUpdate();    return result;}</code></pre><pre><code>public Map&lt;String,Object&gt; token(HttpServletRequest request) {    Map&lt;String,Object&gt; result = new HashMap&lt;&gt;(8);    // 授权方式    String grantType = request.getParameter(&quot;grant_type&quot;);    // 前面获取的Authorization Code    String code = request.getParameter(&quot;code&quot;);    // 客户端ID    String clientId = request.getParameter(&quot;client_id&quot;);    // 接入的客户端的密钥    String clientSecret = request.getParameter(&quot;client_secret&quot;);    Assert.hasText(grantType, ErrorCodeEnum.INVALID_REQUEST.getErrorDescription());    Assert.hasText(code, ErrorCodeEnum.INVALID_REQUEST.getErrorDescription());    Assert.hasText(clientId, ErrorCodeEnum.INVALID_REQUEST.getErrorDescription());    Assert.hasText(clientSecret, ErrorCodeEnum.INVALID_REQUEST.getErrorDescription());    // 1 校验授权方式    Assert.state(GrantTypeEnum.AUTHORIZATION_CODE.getType().equals(grantType), ErrorCodeEnum.UNSUPPORTED_GRANT_TYPE.getErrorDescription());    // 2 校验clientId    AuthInfoDO authInfoDO = authInfoDAO.getByClientId(clientId);    Assert.notNull(authInfoDO, ErrorCodeEnum.INVALID_CLIENT.getErrorDescription());    // 3 校验clientSecret    String localClientSecret = authInfoDO.getClientSecret();    Assert.state(clientSecret.equals(localClientSecret), ErrorCodeEnum.INVALID_CLIENT.getErrorDescription());    // 4 校验code    String partnerNo = authInfoDO.getPartnerNo();    String authCode = redisTemplate.opsForValue().get(&quot;code:&quot; + partnerNo);    Assert.hasText(authCode, ErrorCodeEnum.INVALID_GRANT.getErrorDescription());    Assert.state(code.equals(authCode), ErrorCodeEnum.INVALID_GRANT.getErrorDescription());    // 5 生成accessToken    String accessToken = createAccessToken(partnerNo, clientId);    // 6 生成refreshToken    String refreshToken = createRefreshToken(partnerNo, accessToken);    String scope = redisTemplate.opsForValue().get(&quot;scope:&quot; + partnerNo);    // 7 返回数据    result.put(&quot;access_token&quot;, accessToken);    result.put(&quot;refresh_token&quot;, refreshToken);    result.put(&quot;expires_in&quot;, ExpireEnum.ACCESS_TOKEN.getTime() * 3600);    result.put(&quot;scope&quot;, scope);    return result;}</code></pre><h3 id="refreshToken刷新获取accessToken"><a href="#refreshToken刷新获取accessToken" class="headerlink" title="refreshToken刷新获取accessToken"></a>refreshToken刷新获取accessToken</h3><pre><code>@ApiOperation( &quot;refreshToken获取accessToken&quot;)@GetMapping(value = &quot;/refreshToken&quot;)public ResultData&lt;Map&lt;String,Object&gt;&gt; refreshToken(HttpServletRequest request){    try {        return ResultData.succeed(oAuthService.refreshToken(request));    } catch (IllegalArgumentException | IllegalStateException e) {        return ResultData.fail(e.getMessage());    } catch (Exception e) {        LOGGER.error(&quot;Exception 请求授权接口异常&quot;, e);        return ResultData.fail(&quot;系统繁忙，请稍候重试&quot;);    }}</code></pre><pre><code>public Map&lt;String, Object&gt; refreshToken(HttpServletRequest request) {    // 1 返回token    Map&lt;String, Object&gt; result = oAuthDomain.refreshToken(request);    // 2 保存token相关信息    String refreshToken = request.getParameter(&quot;refresh_token&quot;);    AuthTokenDO localAuthTokenDO = authTokenDAO.getByRefreshToken(refreshToken);    AuthTokenDO authTokenDO = createAuthTokenDO(result, localAuthTokenDO.getPartnerNo(), TokenEnum.REFRESH.getType());    AuthToken authToken = authTokenFactory.get(authTokenDO);    authToken.saveOrUpdate();    return result;}</code></pre><pre><code>public Map&lt;String, Object&gt; refreshToken(HttpServletRequest request) {    Map&lt;String,Object&gt; result = new HashMap&lt;&gt;(8);    // 获取refreshToken    String refreshToken = request.getParameter(&quot;refresh_token&quot;);    // 1 校验refreshToken是否真实有效    Assert.hasText(refreshToken, ErrorCodeEnum.INVALID_REQUEST.getErrorDescription());    AuthTokenDO authTokenDO = authTokenDAO.getByRefreshToken(refreshToken);    Assert.notNull(authTokenDO, ErrorCodeEnum.INVALID_GRANT.getErrorDescription());    // 2 校验refreshToken是否过期    Timestamp refreshExpiresIn = authTokenDO.getRefreshExpiresIn();    Assert.state(LocalDateTime.now().isBefore(refreshExpiresIn.toLocalDateTime()), ErrorCodeEnum.EXPIRED_TOKEN.getErrorDescription());    // 3 生成新的Access Token    String partnerNo = authTokenDO.getPartnerNo();    AuthInfoDO authInfoDO = authInfoDAO.getByPartnerNo(partnerNo);    String newAccessToken = createAccessToken(partnerNo, authInfoDO.getClientId());    // 4 返回数据    result.put(&quot;access_token&quot;, newAccessToken);    result.put(&quot;refresh_token&quot;, refreshToken);    result.put(&quot;expires_in&quot;, ExpireEnum.ACCESS_TOKEN.getTime() * 3600);    result.put(&quot;scope&quot;, authTokenDO.getScope());    return result;}</code></pre><h3 id="枚举"><a href="#枚举" class="headerlink" title="枚举"></a>枚举</h3><pre><code>public enum ErrorCodeEnum {    INVALID_REQUEST(&quot;invalid_request&quot;,&quot;请求缺少某个必需参数，包含一个不支持的参数或参数值，或者格式不正确。&quot;),    INVALID_CLIENT(&quot;invalid_client&quot;,&quot;请求的client_id或client_secret参数无效。&quot;),    INVALID_GRANT(&quot;invalid_grant&quot;,&quot;请求的Authorization Code、Access Token、Refresh Token等信息是无效的。&quot;),    UNSUPPORTED_GRANT_TYPE(&quot;unsupported_grant_type&quot;,&quot;不支持的grant_type。&quot;),    INVALID_SCOPE(&quot;invalid_scope&quot;,&quot;请求的scope参数是无效的、未知的、格式不正确的，或所请求的权限范围超过了数据拥有者所授予的权限范围。&quot;),    EXPIRED_TOKEN(&quot;expired_token&quot;,&quot;请求的Access Token或Refresh Token已过期。&quot;),    REDIRECT_URI_MISMATCH(&quot;redirect_uri_mismatch&quot;,&quot;请求的redirect_uri所在的域名与开发者注册应用时所填写的域名不匹配。&quot;),    INVALID_REDIRECT_URI(&quot;invalid_redirect_uri&quot;,&quot;请求的回调URL不在白名单中。&quot;),    UNKNOWN_ERROR(&quot;unknown_error&quot;,&quot;程序发生未知异常，请联系管理员解决。&quot;);    /**     * 错误码     */    private String error;    /**     * 错误描述信息     */    private String errorDescription;    ErrorCodeEnum(String error, String errorDescription) {        this.error = error;        this.errorDescription = errorDescription;    }    public String getError() {        return error;    }    public String getErrorDescription() {        return errorDescription;    }}</code></pre><pre><code>/** * 过期时间相关枚举 * @author CFL */public enum ExpireEnum {    // Authorization Code的有效期为10分钟    AUTHORIZATION_CODE(10L, TimeUnit.MINUTES),    // Access Token的有效期为12小时    ACCESS_TOKEN(12L, TimeUnit.HOURS),    // Refresh Token的有效期为24小时    REFRESH_TOKEN(24L,TimeUnit.HOURS)    ;    /**     * 过期时间     */    private Long time;    /**     * 时间单位     */    private TimeUnit timeUnit;    ExpireEnum(Long time, TimeUnit timeUnit) {        this.time = time;        this.timeUnit = timeUnit;    }    public Long getTime() {        return time;    }    public TimeUnit getTimeUnit() {        return timeUnit;    }}</code></pre><pre><code>/** * 授权方式 */public enum GrantTypeEnum {    // 授权码模式    AUTHORIZATION_CODE(&quot;authorization_code&quot;);    private String type;    GrantTypeEnum(String type) {        this.type = type;    }    public String getType() {        return type;    }}</code></pre><pre><code>public enum TokenEnum {    ACCESS((byte)0, &quot;accessToken&quot;),    REFRESH((byte)1, &quot;refreshToken&quot;);    private Byte type;    private String remark;    TokenEnum(Byte type, String remark) {        this.type = type;        this.remark = remark;    }    public Byte getType() {        return type;    }    public String getRemark() {        return remark;    }}</code></pre><h3 id="拦截器"><a href="#拦截器" class="headerlink" title="拦截器"></a>拦截器</h3><pre><code>@Componentpublic class AuthAccessTokenInterceptor extends HandlerInterceptorAdapter {    @Override    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {        String accessToken = request.getHeader(&quot;accessToken&quot;);        String requestURI = request.getRequestURI();        if (StringUtils.isNotBlank(accessToken)) {            AuthTokenDO authTokenDO = authTokenDAO.getByAccessToken(accessToken);            if (Objects.nonNull(authTokenDO)) {                String scope = authTokenDO.getScope();                if (StringUtils.isBlank(scope)) {                    return generateErrorResponse(response, ErrorCodeEnum.INVALID_SCOPE);                }                List&lt;String&gt; scopeList = Arrays.asList(scope.split(&quot;,&quot;));                String uri = requestURI.substring(requestURI.lastIndexOf(&quot;/&quot;) + 1);                if (!scopeList.contains(uri)) {                    return generateErrorResponse(response, ErrorCodeEnum.INVALID_SCOPE);                }                ApiPermissionDO apiPermissionDO = apiPermissionDAO.getByApiSource(uri);                if (Objects.isNull(apiPermissionDO) || apiPermissionDO.getStatus() == 0) {                    return generateErrorResponse(response, ErrorCodeEnum.INVALID_SCOPE);                }                PartnerDO partnerDO = partnerDAO.getByPartnerNo(authTokenDO.getPartnerNo());                if (Objects.isNull(partnerDO) || partnerDO.getStatus() == 1) {                    return generateErrorResponse(response, ErrorCodeEnum.INVALID_SCOPE);                }                LocalDateTime accessExpiresIn = authTokenDO.getAccessExpiresIn().toLocalDateTime();                request.setAttribute(&quot;partner_no&quot;, authTokenDO.getPartnerNo());                return LocalDateTime.now().isBefore(accessExpiresIn) || generateErrorResponse(response, ErrorCodeEnum.EXPIRED_TOKEN);            } else {                return generateErrorResponse(response, ErrorCodeEnum.INVALID_GRANT);            }        } else {            return generateErrorResponse(response, ErrorCodeEnum.INVALID_REQUEST);        }    }    private boolean generateErrorResponse(HttpServletResponse response, ErrorCodeEnum errorCodeEnum) throws Exception {        response.setCharacterEncoding(&quot;UTF-8&quot;);        response.setHeader(&quot;Content-type&quot;, &quot;application/json;charset=UTF-8&quot;);        Map&lt;String,String&gt; result = new HashMap&lt;&gt;(2);        result.put(&quot;statusCode&quot;, errorCodeEnum.getError());        result.put(&quot;message&quot;,errorCodeEnum.getErrorDescription());        response.getWriter().write(JSON.toJSON(result).toString());        return false;    }}</code></pre><pre><code>public class AuthUtil {    public static String getPartnerNo() {        ServletRequestAttributes servletRequestAttributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes();        if (Objects.isNull(servletRequestAttributes)){            return null;        }        HttpServletRequest request = servletRequestAttributes.getRequest();        String partnerNo = (String) request.getAttribute(&quot;partner_no&quot;);        return partnerNo;    }}</code></pre><pre><code>@Configurationpublic class InterceptorConfig implements WebMvcConfigurer {    @Autowired    private AuthAccessTokenInterceptor authAccessTokenInterceptor;    @Override    public void addInterceptors(InterceptorRegistry registry) {        registry.addInterceptor(authAccessTokenInterceptor)                .addPathPatterns(&quot;/api/**&quot;);    }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OAuth2 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>消息队列</title>
      <link href="/2020/05/18/2020-05-18-rocketmq/"/>
      <url>/2020/05/18/2020-05-18-rocketmq/</url>
      
        <content type="html"><![CDATA[<h2 id="消息队列能用来干什么？"><a href="#消息队列能用来干什么？" class="headerlink" title="消息队列能用来干什么？"></a>消息队列能用来干什么？</h2><h3 id="异步"><a href="#异步" class="headerlink" title="异步"></a>异步</h3><p><img src="/2020/05/18/2020-05-18-rocketmq/wvtrl7al.bmp" alt></p><p>我们省略中间的网络通信时间消耗，假如购票系统处理需要 150ms ，短信系统处理需要 200ms ，那么整个处理流程的时间消耗就是 150ms + 200ms = 350ms。</p><p>当然，乍看没什么问题。可是仔细一想你就感觉有点问题，我用户购票在购票系统的时候其实就已经完成了购买，而我现在通过同步调用非要让整个请求拉长时间，而短息系统这玩意又不是很有必要，它仅仅是一个辅助功能增强用户体验感而已。我现在整个调用流程就有点 头重脚轻 的感觉了，购票是一个不太耗时的流程，而我现在因为同步调用，非要等待发送短信这个比较耗时的操作才返回结果。那我如果再加一个发送邮件呢？</p><p><img src="/2020/05/18/2020-05-18-rocketmq/jm46n892.bmp" alt></p><p>这样整个系统的调用链又变长了，整个时间就变成了550ms。</p><p>在中间也加了个类似于服务员的中间件——消息队列。这个时候我们就可以把模型给改造了。</p><p><img src="/2020/05/18/2020-05-18-rocketmq/m9fwc9bd.bmp" alt></p><p>这样，我们在将消息存入消息队列之后我们就可以直接返回了，所以整个耗时只是 150ms + 10ms = 160ms。</p><blockquote><p>但是你需要注意的是，整个流程的时长是没变的。</p></blockquote><h3 id="解耦"><a href="#解耦" class="headerlink" title="解耦"></a>解耦</h3><p>回到最初同步调用的过程，我们写个伪代码简单概括一下。</p><p><img src="/2020/05/18/2020-05-18-rocketmq/0j0afz3c.bmp" alt></p><p>那么第二步，我们又添加了一个发送邮件，我们就得重新去修改代码，如果我们又加一个需求：用户购买完还需要给他加积分，这个时候我们是不是又得改代码？</p><p><img src="/2020/05/18/2020-05-18-rocketmq/5qm3gmj9.bmp" alt></p><p>如果你觉得还行，那么我这个时候不要发邮件这个服务了呢，我是不是又得改代码，又得重启应用？</p><p><img src="/2020/05/18/2020-05-18-rocketmq/e1rs93rp.bmp" alt></p><p>这样改来改去是不是很麻烦，那么 此时我们就用一个消息队列在中间进行解耦 。你需要注意的是，我们后面的发送短信、发送邮件、添加积分等一些操作都依赖于上面的 result ，这东西抽象出来就是购票的处理结果呀，比如订单号，用户账号等等，也就是说我们后面的一系列服务都是需要同样的消息来进行处理。既然这样，我们是不是可以通过 “广播消息” 来实现。</p><p>上面所讲的“广播”并不是真正的广播，而是接下来的系统作为消费者去 订阅 特定的主题。比如我们这里的主题就可以叫做 订票 ，我们购买系统作为一个生产者去生产这条消息放入消息队列，然后消费者订阅了这个主题，会从消息队列中拉取消息并消费。就比如我们刚刚画的那张图，你会发现，在生产者这边我们只需要关注 生产消息到指定主题中 ，而 消费者只需要关注从指定主题中拉取消息 就行了。</p><p><img src="/2020/05/18/2020-05-18-rocketmq/y3v1tztk.bmp" alt></p><blockquote><p>如果没有消息队列，每当一个新的业务接入，我们都要在主系统调用新接口、或者当我们取消某些业务，我们也得在主系统删除某些接口调用。有了消息队列，我们只需要关心消息是否送达了队列，至于谁希望订阅，接下来收到消息如何处理，是下游的事情，无疑极大地减少了开发和联调的工作量。</p></blockquote><h3 id="削峰"><a href="#削峰" class="headerlink" title="削峰"></a>削峰</h3><p>我们再次回到一开始我们使用同步调用系统的情况，并且思考一下，如果此时有大量用户请求购票整个系统会变成什么样？</p><p><img src="/2020/05/18/2020-05-18-rocketmq/0x81yqn5.bmp" alt></p><p>如果，此时有一万的请求进入购票系统，我们知道运行我们主业务的服务器配置一般会比较好，所以这里我们假设购票系统能承受这一万的用户请求，那么也就意味着我们同时也会出现一万调用发短信服务的请求。而对于短信系统来说并不是我们的主要业务，所以我们配备的硬件资源并不会太高，那么你觉得现在这个短信系统能承受这一万的峰值么，且不说能不能承受，系统会不会 直接崩溃 了？</p><p>短信业务又不是我们的主业务，我们能不能 折中处理 呢？如果我们把购买完成的信息发送到消息队列中，而短信系统 尽自己所能地去消息队列中取消息和消费消息 ，即使处理速度慢一点也无所谓，只要我们的系统没有崩溃就行了。</p><p>留得江山在，还怕没柴烧？你敢说每次发送验证码的时候是一发你就收到了的么？</p><h2 id="消息队列会带来副作用吗？"><a href="#消息队列会带来副作用吗？" class="headerlink" title="消息队列会带来副作用吗？"></a>消息队列会带来副作用吗？</h2><p>没有哪一门技术是“银弹”，消息队列也有它的副作用。</p><p>比如，本来好好的两个系统之间的调用，我中间加了个消息队列，如果消息队列挂了怎么办呢？是不是 <strong>降低了系统的可用性</strong> ？</p><p>那这样是不是要保证HA(高可用)？是不是要搞集群？那么我 <strong>整个系统的复杂度是不是上升了</strong> ？</p><p>抛开上面的问题不讲，万一我发送方发送失败了，然后执行重试，这样就可能产生重复的消息。</p><p>或者我消费端处理失败了，请求重发，这样也会产生重复的消息。</p><p>对于一些微服务来说，消费重复消息会带来更大的麻烦，比如增加积分，这个时候我加了多次是不是对其他用户不公平？</p><p>那么，又 <strong>如何解决重复消费消息的问题</strong> 呢？</p><p>如果我们此时的消息需要保证严格的顺序性怎么办呢？比如生产者生产了一系列的有序消息(对一个id为1的记录进行删除增加修改)，但是我们知道在发布订阅模型中，对于主题是无顺序的，那么这个时候就会导致对于消费者消费消息的时候没有按照生产者的发送顺序消费，比如这个时候我们消费的顺序为修改删除增加，如果该记录涉及到金额的话是不是会出大事情？</p><p>那么，又 <strong>如何解决消息的顺序消费问题</strong> 呢？</p><p>就拿我们上面所讲的分布式系统来说，用户购票完成之后是不是需要增加账户积分？在同一个系统中我们一般会使用事务来进行解决，如果用 Spring 的话我们在上面伪代码中加入 @Transactional 注解就好了。但是在不同系统中如何保证事务呢？总不能这个系统我扣钱成功了你那积分系统积分没加吧？或者说我这扣钱明明失败了，你那积分系统给我加了积分。</p><p>那么，又如何 <strong>解决分布式事务问题</strong> 呢？</p><p>我们刚刚说了，消息队列可以进行削峰操作，那如果我的消费者如果消费很慢或者生产者生产消息很快，这样是不是会将消息堆积在消息队列中？</p><p>那么，又如何 <strong>解决消息堆积的问题</strong> 呢？</p><p>可用性降低，复杂度上升，又带来一系列的重复消费，顺序消费，分布式事务，消息堆积的问题，这消息队列还怎么用啊😵？</p><p><img src="/2020/05/18/2020-05-18-rocketmq/bkatoagh.bmp" alt></p><h2 id="RocketMQ是什么？"><a href="#RocketMQ是什么？" class="headerlink" title="RocketMQ是什么？"></a>RocketMQ是什么？</h2><p>RocketMQ 是一个 队列模型 的消息中间件，具有<strong>高性能、高可靠、高实时、分布式</strong> 的特点。它是一个采用 Java 语言开发的分布式的消息系统，由阿里巴巴团队开发，在2016年底贡献给 Apache，成为了 Apache 的一个顶级项目。 在阿里内部，RocketMQ 很好地服务了集团大大小小上千个应用，在每年的双十一当天，更有不可思议的万亿级消息通过 RocketMQ 流转。</p><p>你只要知道 RocketMQ 很快、很牛、而且经历过双十一的实践就行了！</p><h3 id="队列模型和主题模型"><a href="#队列模型和主题模型" class="headerlink" title="队列模型和主题模型"></a>队列模型和主题模型</h3><h4 id="队列模型"><a href="#队列模型" class="headerlink" title="队列模型"></a>队列模型</h4><p>就像我们理解队列一样，消息中间件的队列模型就真的只是一个队列。。。</p><p><img src="/2020/05/18/2020-05-18-rocketmq/16ef3834ae653469.jpg" alt></p><p>在一开始我跟你提到了一个 “广播” 的概念，也就是说如果我们此时我们需要将一个消息发送给多个消费者(比如此时我需要将信息发送给短信系统和邮件系统)，这个时候单个队列即不能满足需求了。</p><p>当然你可以让 Producer 生产消息放入多个队列中，然后每个队列去对应每一个消费者。问题是可以解决，创建多个队列并且复制多份消息是会很影响资源和性能的。而且，这样子就会导致生产者需要知道具体消费者个数然后去复制对应数量的消息队列，这就违背我们消息中间件的 解耦 这一原则。</p><h4 id="主题模型"><a href="#主题模型" class="headerlink" title="主题模型"></a>主题模型</h4><p>那么有没有好的方法去解决这一个问题呢？有，那就是 主题模型 或者可以称为 发布订阅模型 。</p><p>在主题模型中，消息的生产者称为 发布者(Publisher) ，消息的消费者称为 订阅者(Subscriber) ，存放消息的容器称为 主题(Topic) 。</p><p>其中，发布者将消息发送到指定主题中，订阅者需要 提前订阅主题 才能接受特定主题的消息。</p><p><img src="/2020/05/18/2020-05-18-rocketmq/16ef3837887d9a54sds.jpg" alt></p><h3 id="RocketMQ中的消息模型"><a href="#RocketMQ中的消息模型" class="headerlink" title="RocketMQ中的消息模型"></a>RocketMQ中的消息模型</h3><p>其实对于主题模型的实现来说每个消息中间件的底层设计都是不一样的，就比如 Kafka 中的 分区 ，RocketMQ 中的 队列 ，RabbitMQ 中的 Exchange 。我们可以理解为 主题模型/发布订阅模型 就是一个标准，那些中间件只不过照着这个标准去实现而已。</p><p><img src="/2020/05/18/2020-05-18-rocketmq/16ef383d3e8c9788.jpg" alt></p><p>我们可以看到在整个图中有 Producer Group 、Topic 、Consumer Group 三个角色，我来分别介绍一下他们。</p><ul><li>Producer Group 生产者组： 代表某一类的生产者，比如我们有多个秒杀系统作为生产者，这多个合在一起就是一个 Producer Group 生产者组，它们一般生产相同的消息。</li><li>Consumer Group 消费者组： 代表某一类的消费者，比如我们有多个短信系统作为消费者，这多个合在一起就是一个 Consumer Group 消费者组，它们一般消费相同的消息。</li><li>Topic 主题： 代表一类消息，比如订单消息，物流消息等等。</li></ul><p>你可以看到图中生产者组中的生产者会向主题发送消息，而 主题中存在多个队列，生产者每次生产消息之后是指定主题中的某个队列发送消息的。</p><p>每个主题中都有多个队列(这里还不涉及到 Broker)，集群消费模式下，一个消费者集群多台机器共同消费一个 topic 的多个队列，一个队列只会被一个消费者消费。如果某个消费者挂掉，分组内其它消费者会接替挂掉的消费者继续消费。就像上图中 Consumer1 和 Consumer2 分别对应着两个队列，而 Consuer3 是没有队列对应的，所以一般来讲要控制 消费者组中的消费者个数和主题中队列个数相同 。</p><p>每个消费组在每个队列上维护一个消费位置 ，为什么呢？</p><p>因为我们刚刚画的仅仅是一个消费者组，我们知道在发布订阅模式中一般会涉及到多个消费者组，而每个消费者组在每个队列中的消费位置都是不同的。如果此时有多个消费者组，那么消息被一个消费者组消费完之后是不会删除的(因为其它消费者组也需要呀)，它仅仅是为每个消费者组维护一个 消费位移(offset) ，每次消费者组消费完会返回一个成功的响应，然后队列再把维护的消费位移加一，这样就不会出现刚刚消费过的消息再一次被消费了。</p><p><img src="/2020/05/18/2020-05-18-rocketmq/16ef3857fefaa079.jpg" alt></p><p>可能你还有一个问题，为什么一个主题中需要维护多个队列 ？</p><p>答案是 提高并发能力 。的确，每个主题中只存在一个队列也是可行的。你想一下，如果每个主题中只存在一个队列，这个队列中也维护着每个消费者组的消费位置，这样也可以做到 发布订阅模式 。如下图。</p><p><img src="/2020/05/18/2020-05-18-rocketmq/16ef38600cdb6d4b.jpg" alt></p><p>但是，这样我生产者是不是只能向一个队列发送消息？又因为需要维护消费位置所以一个队列只能对应一个消费者组中的消费者，这样是不是其他的 Consumer 就没有用武之地了？从这两个角度来讲，并发度一下子就小了很多。</p><p>所以总结来说，RocketMQ 通过使用在一个 Topic 中配置多个队列并且每个队列维护每个消费者组的消费位置 实现了 主题模式/发布订阅模式 。</p><h2 id="RocketMQ的架构图"><a href="#RocketMQ的架构图" class="headerlink" title="RocketMQ的架构图"></a>RocketMQ的架构图</h2><p>RocketMQ 技术架构中有四大角色 NameServer 、Broker 、Producer 、Consumer 。我来向大家分别解释一下这四个角色是干啥的。</p><h3 id="Broker"><a href="#Broker" class="headerlink" title="Broker"></a>Broker</h3><p>Broker： 主要负责消息的存储、投递和查询以及服务高可用保证。说白了就是消息队列服务器嘛，生产者生产消息到 Broker ，消费者从 Broker 拉取消息并消费。</p><p>这里，我还得普及一下关于 Broker 、Topic 和 队列的关系。上面我讲解了 Topic 和队列的关系——一个 Topic 中存在多个队列，那么这个 Topic 和队列存放在哪呢？</p><p>一个 Topic 分布在多个 Broker上，一个 Broker 可以配置多个 Topic ，它们是多对多的关系。 </p><p>如果某个 Topic 消息量很大，应该给它多配置几个队列(上文中提到了提高并发能力)，并且 尽量多分布在不同 Broker 上，以减轻某个 Broker 的压力 。</p><p>Topic 消息量都比较均匀的情况下，如果某个 broker 上的队列越多，则该 broker 压力越大。</p><p><img src="/2020/05/18/2020-05-18-rocketmq/16ef38687488a5a4.jpg" alt></p><blockquote><p>所以说我们需要配置多个Broker。</p></blockquote><h3 id="NameServer"><a href="#NameServer" class="headerlink" title="NameServer"></a>NameServer</h3><p>NameServer： 不知道你们有没有接触过 ZooKeeper 和 Spring Cloud 中的 Eureka ，它其实也是一个 注册中心 ，主要提供两个功能：Broker管理 和 路由信息管理 。说白了就是 Broker 会将自己的信息注册到 NameServer 中，此时 NameServer 就存放了很多 Broker 的信息(Broker的路由表)，消费者和生产者就从 NameServer 中获取路由表然后照着路由表的信息和对应的 Broker 进行通信(生产者和消费者定期会向 NameServer 去查询相关的 Broker 的信息)。</p><h3 id="Producer"><a href="#Producer" class="headerlink" title="Producer"></a>Producer</h3><p>Producer： 消息发布的角色，支持分布式集群方式部署。说白了就是生产者。</p><h3 id="Consumer"><a href="#Consumer" class="headerlink" title="Consumer"></a>Consumer</h3><p>Consumer： 消息消费的角色，支持分布式集群方式部署。支持以push推，pull拉两种模式对消息进行消费。同时也支持集群方式和广播方式的消费，它提供实时消息订阅机制。说白了就是消费者。</p><h3 id="架构总结"><a href="#架构总结" class="headerlink" title="架构总结"></a>架构总结</h3><p><img src="/2020/05/18/2020-05-18-rocketmq/16ef386c6d1e8bdb.png" alt></p><p>嗯？你可能会发现一个问题，这老家伙 NameServer 干啥用的，这不多余吗？直接 Producer 、Consumer 和 Broker 直接进行生产消息，消费消息不就好了么？</p><p>但是，我们上文提到过 Broker 是需要保证高可用的，如果整个系统仅仅靠着一个 Broker 来维持的话，那么这个 Broker 的压力会不会很大？所以我们需要使用多个 Broker 来保证 负载均衡 。</p><p>如果说，我们的消费者和生产者直接和多个 Broker 相连，那么当 Broker 修改的时候必定会牵连着每个生产者和消费者，这样就会产生耦合问题，而 NameServer 注册中心就是用来解决这个问题的。</p><p>当然，RocketMQ 中的技术架构肯定不止前面那么简单，因为上面图中的四个角色都是需要做集群的。我给出一张官网的架构图，大家尝试理解一下。</p><p><img src="/2020/05/18/2020-05-18-rocketmq/16ef386fa3be1e53.jpg" alt></p><p>第一、我们的 Broker 做了集群并且还进行了主从部署 ，由于消息分布在各个 Broker 上，一旦某个 Broker 宕机，则该Broker 上的消息读写都会受到影响。所以 Rocketmq 提供了 master/slave 的结构，salve 定时从 master 同步数据(同步刷盘或者异步刷盘)，如果 master 宕机，则 slave 提供消费服务，但是不能写入消息 (后面我还会提到哦)。</p><p>第二、为了保证 HA ，我们的 NameServer 也做了集群部署，但是请注意它是 去中心化 的。也就意味着它没有主节点，你可以很明显地看出 NameServer 的所有节点是没有进行 Info Replicate 的，在 RocketMQ 中是通过 单个Broker和所有NameServer保持长连接 ，并且在每隔30秒 Broker 会向所有 Nameserver 发送心跳，心跳包含了自身的 Topic 配置信息，这个步骤就对应这上面的 Routing Info 。</p><p>第三、在生产者需要向 Broker 发送消息的时候，需要先从 NameServer 获取关于 Broker 的路由信息，然后通过 轮询 的方法去向每个队列中生产数据以达到 负载均衡 的效果。</p><p>第四、消费者通过 NameServer 获取所有 Broker 的路由信息后，向 Broker 发送 Pull 请求来获取消息数据。Consumer 可以以两种模式启动—— 广播（Broadcast）和集群（Cluster）。广播模式下，一条消息会发送给 同一个消费组中的所有消费者 ，集群模式下消息只会发送给一个消费者。</p><h2 id="如何解决-顺序消费、重复消费"><a href="#如何解决-顺序消费、重复消费" class="headerlink" title="如何解决 顺序消费、重复消费"></a>如何解决 顺序消费、重复消费</h2><h3 id="顺序消费"><a href="#顺序消费" class="headerlink" title="顺序消费"></a>顺序消费</h3><p>在上面的技术架构介绍中，我们已经知道了 RocketMQ 在主题上是无序的(多个队列)、它只有在队列层面才是保证有序 的。</p><p>这又扯到两个概念—— 普通顺序 和 严格顺序 。</p><p>所谓普通顺序是指 消费者通过 同一个消费队列收到的消息是有顺序的 ，不同消息队列收到的消息则可能是无顺序的。普通顺序消息在 Broker 重启情况下不会保证消息顺序性 (短暂时间) 。</p><p>所谓严格顺序是指 消费者收到的 所有消息 均是有顺序的。严格顺序消息 即使在异常情况下也会保证消息的顺序性 。</p><p>但是，严格顺序看起来虽好，实现它可会付出巨大的代价。如果你使用严格顺序模式，Broker 集群中只要有一台机器不可用，则整个集群都不可用。你还用啥？现在主要场景也就在 binlog 同步。</p><p>一般而言，我们的 MQ 都是能容忍短暂的乱序，所以推荐使用普通顺序模式。</p><p>那么，我们现在使用了 普通顺序模式 ，我们从上面学习知道了在 Producer 生产消息的时候会进行轮询(取决你的负载均衡策略)来向同一主题的不同消息队列发送消息。那么如果此时我有几个消息分别是同一个订单的创建、支付、发货，在轮询的策略下这 三个消息会被发送到不同队列 ，因为在不同的队列此时就无法使用 RocketMQ 带来的队列有序特性来保证消息有序性了。</p><p><img src="/2020/05/18/2020-05-18-rocketmq/16ef3874585e096e.jpg" alt></p><p>那么，怎么解决呢？</p><p>其实很简单，我们需要处理的仅仅是将同一语义下的消息放入同一个队列(比如这里是同一个订单)，那我们就可以使用 Hash取模法 来保证同一个订单在同一个队列中就行了。</p><h3 id="重复消费"><a href="#重复消费" class="headerlink" title="重复消费"></a>重复消费</h3><p>emmm，就两个字—— 幂等 。在编程中一个幂等 操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。比如说，这个时候我们有一个订单的处理积分的系统，每当来一个消息的时候它就负责为创建这个订单的用户的积分加上相应的数值。可是有一次，消息队列发送给订单系统 FrancisQ 的订单信息，其要求是给 FrancisQ 的积分加上 500。但是积分系统在收到 FrancisQ 的订单信息处理完成之后返回给消息队列处理成功的信息的时候出现了网络波动(当然还有很多种情况，比如Broker意外重启等等)，这条回应没有发送成功。</p><p>那么，消息队列没收到积分系统的回应会不会尝试重发这个消息？问题就来了，我再发这个消息，万一它又给 FrancisQ 的账户加上 500 积分怎么办呢？</p><p>所以我们需要给我们的消费者实现 幂等 ，也就是对同一个消息的处理结果，执行多少次都不变。</p><p>那么如何给业务实现幂等呢？这个还是需要结合具体的业务的。你可以使用 写入 Redis 来保证，因为 Redis 的 key 和 value 就是天然支持幂等的。当然还有使用 数据库插入法 ，基于数据库的唯一键来保证重复数据不会被插入多条。</p><p>不过最主要的还是需要 根据特定场景使用特定的解决方案 ，你要知道你的消息消费是否是完全不可重复消费还是可以忍受重复消费的，然后再选择强校验和弱校验的方式。毕竟在 CS 领域还是很少有技术银弹的说法。</p><p>而在整个互联网领域，幂等不仅仅适用于消息队列的重复消费问题，这些实现幂等的方法，也同样适用于，在其他场景中来解决重复请求或者重复调用的问题 。比如将HTTP服务设计成幂等的，解决前端或者APP重复提交表单数据的问题 ，也可以将一个微服务设计成幂等的，解决 RPC 框架自动重试导致的 重复调用问题 。</p><h2 id="分布式事务"><a href="#分布式事务" class="headerlink" title="分布式事务"></a>分布式事务</h2><p>如何解释分布式事务呢？事务大家都知道吧？要么都执行要么都不执行 。在同一个系统中我们可以轻松地实现事务，但是在分布式架构中，我们有很多服务是部署在不同系统之间的，而不同服务之间又需要进行调用。比如此时我下订单然后增加积分，如果保证不了分布式事务的话，就会出现A系统下了订单，但是B系统增加积分失败或者A系统没有下订单，B系统却增加了积分。前者对用户不友好，后者对运营商不利，这是我们都不愿意见到的。</p><p>那么，如何去解决这个问题呢？</p><p>如今比较常见的分布式事务实现有 2PC、TCC 和事务消息(half 半消息机制)。每一种实现都有其特定的使用场景，但是也有各自的问题，都不是完美的解决方案。</p><p>在 RocketMQ 中使用的是 事务消息加上事务反查机制 来解决分布式事务问题的。我画了张图，大家可以对照着图进行理解。</p><p><img src="/2020/05/18/2020-05-18-rocketmq/16ef38798d7a987f.jpg" alt></p><p>在第一步发送的 half 消息 ，它的意思是 在事务提交之前，对于消费者来说，这个消息是不可见的 。</p><blockquote><p>那么，如何做到写入消息但是对用户不可见呢？RocketMQ事务消息的做法是：如果消息是half消息，将备份原消息的主题与消息消费队列，然后 改变主题 为RMQ_SYS_TRANS_HALF_TOPIC。由于消费组未订阅该主题，故消费端无法消费half类型的消息，然后RocketMQ会开启一个定时任务，从Topic为RMQ_SYS_TRANS_HALF_TOPIC中拉取消息进行消费，根据生产者组获取一个服务提供者发送回查事务状态请求，根据事务状态来决定是提交或回滚消息。</p></blockquote><p>你可以试想一下，如果没有从第5步开始的 事务反查机制 ，如果出现网路波动第4步没有发送成功，这样就会产生 MQ 不知道是不是需要给消费者消费的问题，他就像一个无头苍蝇一样。在 RocketMQ 中就是使用的上述的事务反查来解决的，而在 Kafka 中通常是直接抛出一个异常让用户来自行解决。</p><p>你还需要注意的是，在 MQ Server 指向系统B的操作已经和系统A不相关了，也就是说在消息队列中的分布式事务是——本地事务和存储消息到消息队列才是同一个事务。这样也就产生了事务的最终一致性，因为整个过程是异步的，每个系统只要保证它自己那一部分的事务就行了。</p><h2 id="消息堆积问题"><a href="#消息堆积问题" class="headerlink" title="消息堆积问题"></a>消息堆积问题</h2><p>在上面我们提到了消息队列一个很重要的功能——削峰 。那么如果这个峰值太大了导致消息堆积在队列中怎么办呢？</p><p>其实这个问题可以将它广义化，因为产生消息堆积的根源其实就只有两个——生产者生产太快或者消费者消费太慢。</p><p>我们可以从多个角度去思考解决这个问题，当流量到峰值的时候是因为生产者生产太快，我们可以使用一些 限流降级 的方法，当然你也可以增加多个消费者实例去水平扩展增加消费能力来匹配生产的激增。如果消费者消费过慢的话，我们可以先检查 是否是消费者出现了大量的消费错误 ，或者打印一下日志查看是否是哪一个线程卡死，出现了锁资源不释放等等的问题。</p><blockquote><p>当然，最快速解决消息堆积问题的方法还是增加消费者实例，不过 同时你还需要增加每个主题的队列数量 。<br>别忘了在 RocketMQ 中，一个队列只会被一个消费者消费 ，如果你仅仅是增加消费者实例就会出现我一开始给你画架构图的那种情况。</p></blockquote><p><img src="/2020/05/18/2020-05-18-rocketmq/16ef387d939ab66d.jpg" alt></p><h2 id="回溯消费"><a href="#回溯消费" class="headerlink" title="回溯消费"></a>回溯消费</h2><p>回溯消费是指 Consumer 已经消费成功的消息，由于业务上需求需要重新消费，在RocketMQ 中， Broker 在向Consumer 投递成功消息后，消息仍然需要保留 。并且重新消费一般是按照时间维度，例如由于 Consumer 系统故障，恢复后需要重新消费1小时前的数据，那么 Broker 要提供一种机制，可以按照时间维度来回退消费进度。RocketMQ 支持按照时间回溯消费，时间维度精确到毫秒。</p><h2 id="RocketMQ-的刷盘机制"><a href="#RocketMQ-的刷盘机制" class="headerlink" title="RocketMQ 的刷盘机制"></a>RocketMQ 的刷盘机制</h2><p>上面我讲了那么多的 RocketMQ 的架构和设计原理，你有没有好奇</p><p>在 Topic 中的 队列是以什么样的形式存在的？</p><p>队列中的消息又是如何进行存储持久化的呢？</p><p>我在上文中提到的 同步刷盘 和 异步刷盘 又是什么呢？它们会给持久化带来什么样的影响呢？</p><h3 id="同步刷盘和异步刷盘"><a href="#同步刷盘和异步刷盘" class="headerlink" title="同步刷盘和异步刷盘"></a>同步刷盘和异步刷盘</h3><p><img src="/2020/05/18/2020-05-18-rocketmq/16ef387fba311cda.jpg" alt></p><p>如上图所示，在同步刷盘中需要等待一个刷盘成功的 ACK ，同步刷盘对 MQ 消息可靠性来说是一种不错的保障，但是 性能上会有较大影响 ，一般地适用于金融等特定业务场景。</p><p>而异步刷盘往往是开启一个线程去异步地执行刷盘操作。消息刷盘采用后台异步线程提交的方式进行， 降低了读写延迟 ，提高了 MQ 的性能和吞吐量，一般适用于如发验证码等对于消息保证要求不太高的业务场景。</p><p>一般地，异步刷盘只有在 Broker 意外宕机的时候会丢失部分数据，你可以设置 Broker 的参数 FlushDiskType 来调整你的刷盘策略(ASYNC_FLUSH 或者 SYNC_FLUSH)。</p><h3 id="同步复制和异步复制"><a href="#同步复制和异步复制" class="headerlink" title="同步复制和异步复制"></a>同步复制和异步复制</h3><p>上面的同步刷盘和异步刷盘是在单个结点层面的，而同步复制和异步复制主要是指的 Borker 主从模式下，主节点返回消息给客户端的时候是否需要同步从节点。</p><ul><li>同步复制： 也叫 “同步双写”，也就是说，只有消息同步双写到主从结点上时才返回写入成功 。</li><li>异步复制： 消息写入主节点之后就直接返回写入成功 。</li></ul><p>然而，很多事情是没有完美的方案的，就比如我们进行消息写入的节点越多就更能保证消息的可靠性，但是随之的性能也会下降，所以需要程序员根据特定业务场景去选择适应的主从复制方案。</p><p>那么，异步复制会不会也像异步刷盘那样影响消息的可靠性呢？</p><p>答案是不会的，因为两者就是不同的概念，对于消息可靠性是通过不同的刷盘策略保证的，而像异步同步复制策略仅仅是影响到了 可用性 。为什么呢？其主要原因是 RocketMQ 是不支持自动主从切换的，当主节点挂掉之后，生产者就不能再给这个主节点生产消息了。</p><p>比如这个时候采用异步复制的方式，在主节点还未发送完需要同步的消息的时候主节点挂掉了，这个时候从节点就少了一部分消息。但是此时生产者无法再给主节点生产消息了，消费者可以自动切换到从节点进行消费(仅仅是消费)，所以在主节点挂掉的时间只会产生主从结点短暂的消息不一致的情况，降低了可用性，而当主节点重启之后，从节点那部分未来得及复制的消息还会继续复制。</p><p>在单主从架构中，如果一个主节点挂掉了，那么也就意味着整个系统不能再生产了。那么这个可用性的问题能否解决呢？一个主从不行那就多个主从的呗，别忘了在我们最初的架构图中，每个 Topic 是分布在不同 Broker 中的。</p><p><img src="/2020/05/18/2020-05-18-rocketmq/16ef38687488a5a4.jpg" alt></p><p>但是这种复制方式同样也会带来一个问题，那就是无法保证 严格顺序 。在上文中我们提到了如何保证的消息顺序性是通过将一个语义的消息发送在同一个队列中，使用 Topic 下的队列来保证顺序性的。如果此时我们主节点A负责的是订单A的一系列语义消息，然后它挂了，这样其他节点是无法代替主节点A的，如果我们任意节点都可以存入任何消息，那就没有顺序性可言了。</p><p>而在 RocketMQ 中采用了 Dledger 解决这个问题。他要求在写入消息的时候，要求至少消息复制到半数以上的节点之后，才给客⼾端返回写⼊成功，并且它是⽀持通过选举来动态切换主节点的。这里我就不展开说明了，读者可以自己去了解。</p><blockquote><p>也不是说 Dledger 是个完美的方案，至少在 Dledger 选举过程中是无法提供服务的，而且他必须要使用三个节点或以上，如果多数节点同时挂掉他也是无法保证可用性的，而且要求消息复制板书以上节点的效率和直接异步复制还是有一定的差距的。</p></blockquote><h2 id="存储机制"><a href="#存储机制" class="headerlink" title="存储机制"></a>存储机制</h2><p>但是，在 Topic 中的 队列是以什么样的形式存在的？队列中的消息又是如何进行存储持久化的呢？ 还未解决，其实这里涉及到了 RocketMQ 是如何设计它的存储结构了。我首先想大家介绍 RocketMQ 消息存储架构中的三大角色——CommitLog 、ConsumeQueue 和 IndexFile 。</p><ul><li><p>CommitLog： 消息主体以及元数据的存储主体，存储 Producer 端写入的消息主体内容,消息内容不是定长的。单个文件大小默认1G ，文件名长度为20位，左边补零，剩余为起始偏移量，比如00000000000000000000代表了第一个文件，起始偏移量为0，文件大小为1G=1073741824；当第一个文件写满了，第二个文件为00000000001073741824，起始偏移量为1073741824，以此类推。消息主要是顺序写入日志文件，当文件满了，写入下一个文件。</p></li><li><p>ConsumeQueue： 消息消费队列，引入的目的主要是提高消息消费的性能(我们再前面也讲了)，由于RocketMQ 是基于主题 Topic 的订阅模式，消息消费是针对主题进行的，如果要遍历 commitlog 文件中根据 Topic 检索消息是非常低效的。Consumer 即可根据 ConsumeQueue 来查找待消费的消息。其中，ConsumeQueue（逻辑消费队列）作为消费消息的索引，保存了指定 Topic 下的队列消息在 CommitLog 中的起始物理偏移量 offset <strong>，消息大小 size 和消息 Tag 的 HashCode 值。consumequeue 文件可以看成是基于 topic 的 commitlog 索引文件</strong>，故 consumequeue 文件夹的组织方式如下：topic/queue/file三层组织结构，具体存储路径为：$HOME/store/consumequeue/{topic}/{queueId}/{fileName}。同样 consumequeue 文件采取定长设计，每一个条目共20个字节，分别为8字节的 commitlog 物理偏移量、4字节的消息长度、8字节tag hashcode，单个文件由30W个条目组成，可以像数组一样随机访问每一个条目，每个 ConsumeQueue文件大小约5.72M；</p></li><li><p>IndexFile： IndexFile（索引文件）提供了一种可以通过key或时间区间来查询消息的方法。这里只做科普不做详细介绍。</p></li></ul><p>总结来说，整个消息存储的结构，最主要的就是 CommitLoq 和 ConsumeQueue 。而 ConsumeQueue 你可以大概理解为 Topic 中的队列。</p><p><img src="/2020/05/18/2020-05-18-rocketmq/16ef3884c02acc72.jpg" alt></p><p>RocketMQ 采用的是 混合型的存储结构 ，即为 Broker 单个实例下所有的队列共用一个日志数据文件来存储消息。有意思的是在同样高并发的 Kafka 中会为每个 Topic 分配一个存储文件。这就有点类似于我们有一大堆书需要装上书架，RockeMQ 是不分书的种类直接成批的塞上去的，而 Kafka 是将书本放入指定的分类区域的。</p><p>而 RocketMQ 为什么要这么做呢？原因是 提高数据的写入效率 ，不分 Topic 意味着我们有更大的几率获取 成批 的消息进行数据写入，但也会带来一个麻烦就是读取消息的时候需要遍历整个大文件，这是非常耗时的。</p><p>所以，在 RocketMQ 中又使用了 ConsumeQueue 作为每个队列的索引文件来 提升读取消息的效率。我们可以直接根据队列的消息序号，计算出索引的全局位置（索引序号*索引固定⻓度20），然后直接读取这条索引，再根据索引中记录的消息的全局位置，找到消息。</p><p>讲到这里，你可能对 RockeMQ 的存储架构还有些模糊，没事，我们结合着图来理解一下。</p><p><img src="/2020/05/18/2020-05-18-rocketmq/16ef388763c25c62.jpg" alt></p><p>首先，在最上面的那一块就是我刚刚讲的你现在可以直接 把 ConsumerQueue 理解为 Queue。</p><p><strong>在图中最左边说明了 红色方块 代表被写入的消息，虚线方块代表等待被写入的。左边的生产者发送消息会指定 Topic 、QueueId 和具体消息内容，而在 Broker 中管你是哪门子消息，他直接 *全部顺序存储到了 CommitLog *。而根据生产者指定的 Topic 和 QueueId 将这条消息本身在 CommitLog 的偏移(offset)，消息本身大小，和tag的hash值存入对应的 ConsumeQueue 索引文件中。而在每个队列中都保存了 ConsumeOffset 即每个消费者组的消费位置(我在架构那里提到了，忘了的同学可以回去看一下)，而消费者拉取消息进行消费的时候只需要根据 ConsumeOffset 获取下一个未被消费的消息就行了。</strong></p><p><img src="/2020/05/18/2020-05-18-rocketmq/e314ee45gy1g05zgr67bbj20gp0b3aba.jpg" alt></p>]]></content>
      
      
      <categories>
          
          <category> 中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 消息队列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式锁</title>
      <link href="/2020/05/16/2020-05-16-fen-bu-shi-suo/"/>
      <url>/2020/05/16/2020-05-16-fen-bu-shi-suo/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>对于锁大家肯定不会陌生，在Java中synchronized关键字和ReentrantLock可重入锁在我们的代码中是经常见的，一般我们用其在多线程环境中控制对资源的并发访问，但是随着分布式的快速发展，本地的加锁往往不能满足我们的需要，在我们的分布式环境中上面加锁的方法就会失去作用。于是人们为了在分布式环境中也能实现本地锁的效果，也是纷纷各出其招，今天让我们来聊一聊一般分布式锁实现的套路。</p><h2 id="分布式锁"><a href="#分布式锁" class="headerlink" title="分布式锁"></a>分布式锁</h2><h3 id="为何需要分布式锁"><a href="#为何需要分布式锁" class="headerlink" title="为何需要分布式锁"></a>为何需要分布式锁</h3><ul><li>效率:使用分布式锁可以避免不同节点重复相同的工作，这些工作会浪费资源。比如用户付了钱之后有可能不同节点会发出多封短信。</li><li>正确性:加分布式锁同样可以避免破坏正确性的发生，如果两个节点在同一条数据上面操作，比如多个节点机器对同一个订单操作不同的流程有可能会导致该笔订单最后状态出现错误，造成损失。</li></ul><h3 id="分布式锁的一些特点"><a href="#分布式锁的一些特点" class="headerlink" title="分布式锁的一些特点"></a>分布式锁的一些特点</h3><p>当我们确定了在不同节点上需要分布式锁，那么我们需要了解分布式锁到底应该有哪些特点:</p><ul><li>互斥性:和我们本地锁一样互斥性是最基本，但是分布式锁需要保证在不同节点的不同线程的互斥。</li><li>可重入性:同一个节点上的同一个线程如果获取了锁之后那么也可以再次获取这个锁。</li><li>锁超时:和本地锁一样支持锁超时，防止死锁。</li><li>高效，高可用:加锁和解锁需要高效，同时也需要保证高可用防止分布式锁失效，可以增加降级。</li><li>支持阻塞和非阻塞:和ReentrantLock一样支持lock和trylock以及tryLock(long timeOut)。</li><li>支持公平锁和非公平锁(可选):公平锁的意思是按照请求加锁的顺序获得锁，非公平锁就相反是无序的。这个一般来说实现的比较少。</li></ul><h3 id="常见的分布式锁"><a href="#常见的分布式锁" class="headerlink" title="常见的分布式锁"></a>常见的分布式锁</h3><p>我们了解了一些特点之后，我们一般实现分布式锁有以下几个方式:</p><ul><li>MySql</li><li>Zk</li><li>Redis</li><li>自研分布式锁:如谷歌的Chubby。</li></ul><h2 id="Mysql分布式锁"><a href="#Mysql分布式锁" class="headerlink" title="Mysql分布式锁"></a>Mysql分布式锁</h2><p>首先来说一下Mysql分布式锁的实现原理，相对来说这个比较容易理解，毕竟数据库和我们开发人员在平时的开发中息息相关。对于分布式锁我们可以创建一个锁表:</p><p><img src="/2020/05/16/2020-05-16-fen-bu-shi-suo/1664ec7d7339a8ef.png" alt></p><p>前面我们所说的lock(),trylock(long timeout)，trylock()这几个方法可以用下面的伪代码实现。</p><h3 id="lock"><a href="#lock" class="headerlink" title="lock()"></a>lock()</h3><p>lock一般是阻塞式的获取锁，意思就是不获取到锁誓不罢休，那么我们可以写一个死循环来执行其操作: </p><p><img src="/2020/05/16/2020-05-16-fen-bu-shi-suo/1664ed1b12d0012f.png" alt></p><p>mysqlLock.lcok内部是一个sql,为了达到可重入锁的效果那么我们应该先进行查询，如果有值，那么需要比较node_info是否一致，这里的node_info可以用机器IP和线程名字来表示，如果一致那么就加可重入锁count的值，如果不一致那么就返回false。如果没有值那么直接插入一条数据。伪代码如下:</p><p><img src="/2020/05/16/2020-05-16-fen-bu-shi-suo/1664ed8f16d2277b.png" alt></p><p>需要注意的是这一段代码需要加事务，必须要保证这一系列操作的原子性。</p><h3 id="tryLock-和tryLock-long-timeout"><a href="#tryLock-和tryLock-long-timeout" class="headerlink" title="tryLock()和tryLock(long timeout)"></a>tryLock()和tryLock(long timeout)</h3><p>tryLock()是非阻塞获取锁，如果获取不到那么就会马上返回，代码可以如下: </p><p><img src="/2020/05/16/2020-05-16-fen-bu-shi-suo/1664edf0a83859bc.png" alt></p><p>tryLock(long timeout)实现如下: </p><p><img src="/2020/05/16/2020-05-16-fen-bu-shi-suo/1664ee2f0bf0a7ae.png" alt></p><p>mysqlLock.lock和上面一样，但是要注意的是select … for update这个是阻塞的获取行锁，如果同一个资源并发量较大还是有可能会退化成阻塞的获取锁。</p><h3 id="unlock"><a href="#unlock" class="headerlink" title="unlock()"></a>unlock()</h3><p>unlock的话如果这里的count为1那么可以删除，如果大于1那么需要减去1。</p><p><img src="/2020/05/16/2020-05-16-fen-bu-shi-suo/1664eea84d69a387.png" alt></p><h3 id="锁超时"><a href="#锁超时" class="headerlink" title="锁超时"></a>锁超时</h3><p>我们有可能会遇到我们的机器节点挂了，那么这个锁就不会得到释放，我们可以启动一个定时任务，通过计算一般我们处理任务的一般的时间，比如是5ms，那么我们可以稍微扩大一点，当这个锁超过20ms没有被释放我们就可以认定是节点挂了然后将其直接释放。</p><h3 id="Mysql小结"><a href="#Mysql小结" class="headerlink" title="Mysql小结"></a>Mysql小结</h3><ul><li>适用场景: Mysql分布式锁一般适用于资源不存在数据库，如果数据库存在比如订单，那么可以直接对这条数据加行锁，不需要我们上面多的繁琐的步骤，比如一个订单，那么我们可以用select * from order_table where id = ‘xxx’ for update进行加行锁，那么其他的事务就不能对其进行修改。</li><li>优点:理解起来简单，不需要维护额外的第三方中间件(比如Redis,Zk)。</li><li>缺点:虽然容易理解但是实现起来较为繁琐，需要自己考虑锁超时，加事务等等。性能局限于数据库，一般对比缓存来说性能较低。对于高并发的场景并不是很适合。</li></ul><h3 id="乐观锁"><a href="#乐观锁" class="headerlink" title="乐观锁"></a>乐观锁</h3><p>前面我们介绍的都是悲观锁，这里想额外提一下乐观锁，在我们实际项目中也是经常实现乐观锁，因为我们加行锁的性能消耗比较大，通常我们会对于一些竞争不是那么激烈，但是其又需要保证我们并发的顺序执行使用乐观锁进行处理，我们可以对我们的表加一个版本号字段，那么我们查询出来一个版本号之后，update或者delete的时候需要依赖我们查询出来的版本号，判断当前数据库和查询出来的版本号是否相等，如果相等那么就可以执行，如果不等那么就不能执行。这样的一个策略很像我们的CAS(Compare And Swap),比较并交换是一个原子操作。这样我们就能避免加select * for update行锁的开销。</p><h2 id="ZooKeeper"><a href="#ZooKeeper" class="headerlink" title="ZooKeeper"></a>ZooKeeper</h2><p>ZooKeeper也是我们常见的实现分布式锁方法，相比于数据库如果没了解过ZooKeeper可能上手比较难一些。ZooKeeper是以Paxos算法为基础分布式应用程序协调服务。Zk的数据节点和文件目录类似，所以我们可以用此特性实现分布式锁。我们以某个资源为目录，然后这个目录下面的节点就是我们需要获取锁的客户端，未获取到锁的客户端注册需要注册Watcher到上一个客户端，可以用下图表示。</p><p><img src="/2020/05/16/2020-05-16-fen-bu-shi-suo/166513ddf9f2bd87.png" alt></p><p>/lock是我们用于加锁的目录,/resource_name是我们锁定的资源，其下面的节点按照我们加锁的顺序排列。</p><h3 id="Curator"><a href="#Curator" class="headerlink" title="Curator"></a>Curator</h3><p>Curator封装了Zookeeper底层的Api，使我们更加容易方便的对Zookeeper进行操作，并且它封装了分布式锁的功能，这样我们就不需要再自己实现了。</p><p>Curator实现了可重入锁(InterProcessMutex),也实现了不可重入锁(InterProcessSemaphoreMutex)。在可重入锁中还实现了读写锁。</p><h3 id="InterProcessMutex"><a href="#InterProcessMutex" class="headerlink" title="InterProcessMutex"></a>InterProcessMutex</h3><p>InterProcessMutex是Curator实现的可重入锁，我们可以通过下面的一段代码实现我们的可重入锁:</p><p><img src="/2020/05/16/2020-05-16-fen-bu-shi-suo/166514a7db01ca4a.png" alt></p><p>我们利用acuire进行加锁，release进行解锁。</p><p>加锁的流程具体如下:</p><ol><li>首先进行可重入的判定:这里的可重入锁记录在ConcurrentMap&lt;Thread, LockData&gt; threadData这个Map里面，如果threadData.get(currentThread)是有值的那么就证明是可重入锁，然后记录就会加1。我们之前的Mysql其实也可以通过这种方法去优化，可以不需要count字段的值，将这个维护在本地可以提高性能。</li><li>然后在我们的资源目录下创建一个节点:比如这里创建一个/0000000002这个节点，这个节点需要设置为EPHEMERAL_SEQUENTIAL也就是临时节点并且有序。</li><li>获取当前目录下所有子节点，判断自己的节点是否位于子节点第一个。</li><li>如果是第一个，则获取到锁，那么可以返回。</li><li>如果不是第一个，则证明前面已经有人获取到锁了，那么需要获取自己节点的前一个节点。/0000000002的前一个节点是/0000000001，我们获取到这个节点之后，再上面注册Watcher(这里的watcher其实调用的是object.notifyAll(),用来解除阻塞)。</li><li>object.wait(timeout)或object.wait():进行阻塞等待这里和我们第5步的watcher相对应。</li></ol><p>解锁的具体流程:</p><ol><li>首先进行可重入锁的判定:如果有可重入锁只需要次数减1即可，减1之后加锁次数为0的话继续下面步骤，不为0直接返回。</li><li>删除当前节点。</li><li>删除threadDataMap里面的可重入锁的数据。</li></ol><h3 id="读写锁"><a href="#读写锁" class="headerlink" title="读写锁"></a>读写锁</h3><p>Curator提供了读写锁，其实现类是InterProcessReadWriteLock，这里的每个节点都会加上前缀：</p><pre><code>private static final String READ_LOCK_NAME  = &quot;__READ__&quot;;private static final String WRITE_LOCK_NAME = &quot;__WRIT__&quot;;</code></pre><p>根据不同的前缀区分是读锁还是写锁，对于读锁，如果发现前面有写锁，那么需要将watcher注册到和自己最近的写锁。写锁的逻辑和我们之前4.2分析的依然保持不变。</p><h3 id="锁超时-1"><a href="#锁超时-1" class="headerlink" title="锁超时"></a>锁超时</h3><p>Zookeeper不需要配置锁超时，由于我们设置节点是临时节点，我们的每个机器维护着一个ZK的session，通过这个session，ZK可以判断机器是否宕机。如果我们的机器挂掉的话，那么这个临时节点对应的就会被删除，所以我们不需要关心锁超时。</p><h3 id="ZK小结"><a href="#ZK小结" class="headerlink" title="ZK小结"></a>ZK小结</h3><ul><li>优点:ZK可以不需要关心锁超时时间，实现起来有现成的第三方包，比较方便，并且支持读写锁，ZK获取锁会按照加锁的顺序，所以其是公平锁。对于高可用利用ZK集群进行保证。</li><li>缺点:ZK需要额外维护，增加维护成本，性能和Mysql相差不大，依然比较差。并且需要开发人员了解ZK是什么。</li></ul><h2 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h2><p>大家在网上搜索分布式锁，恐怕最多的实现就是Redis了，Redis因为其性能好，实现起来简单所以让很多人都对其十分青睐。</p><h3 id="Redis分布式锁简单实现"><a href="#Redis分布式锁简单实现" class="headerlink" title="Redis分布式锁简单实现"></a>Redis分布式锁简单实现</h3><p>熟悉Redis的同学那么肯定对setNx(set if not exist)方法不陌生，如果不存在则更新，其可以很好的用来实现我们的分布式锁。对于某个资源加锁我们只需要</p><pre><code>setNx resourceName value</code></pre><p>这里有个问题，加锁了之后如果机器宕机那么这个锁就不会得到释放所以会加入过期时间，加入过期时间需要和setNx同一个原子操作，在Redis2.8之前我们需要使用Lua脚本达到我们的目的，但是redis2.8之后redis支持nx和ex操作是同一原子操作。</p><pre><code>set resourceName value ex 5 nx</code></pre><h3 id="Redission"><a href="#Redission" class="headerlink" title="Redission"></a>Redission</h3><p>Javaer都知道Jedis，Jedis是Redis的Java实现的客户端，其API提供了比较全面的Redis命令的支持。Redission也是Redis的客户端，相比于Jedis功能简单。Jedis简单使用阻塞的I/O和redis交互，Redission通过Netty支持非阻塞I/O。Jedis最新版本2.9.0是2016年的快3年了没有更新，而Redission最新版本是2018.10月更新。</p><p>Redission封装了锁的实现，其继承了java.util.concurrent.locks.Lock的接口，让我们像操作我们的本地Lock一样去操作Redission的Lock，下面介绍一下其如何实现分布式锁。</p><p><img src="/2020/05/16/2020-05-16-fen-bu-shi-suo/16652989862ce5af.png" alt></p><p>Redission不仅提供了Java自带的一些方法(lock,tryLock)，还提供了异步加锁，对于异步编程更加方便。 由于内部源码较多，就不贴源码了，这里用文字叙述来分析他是如何加锁的，这里分析一下tryLock方法:</p><ol><li>尝试加锁:首先会尝试进行加锁，由于保证操作是原子性，那么就只能使用lua脚本，相关的lua脚本如下： </li></ol><p><img src="/2020/05/16/2020-05-16-fen-bu-shi-suo/166529eb8139751a.png" alt></p><p>可以看见他并没有使用我们的sexNx来进行操作，而是使用的hash结构，我们的每一个需要锁定的资源都可以看做是一个HashMap，锁定资源的节点信息是Key,锁定次数是value。通过这种方式可以很好的实现可重入的效果，只需要对value进行加1操作，就能进行可重入锁。当然这里也可以用之前我们说的本地计数进行优化。</p><ol start="2"><li>如果尝试加锁失败，判断是否超时，如果超时则返回false。</li><li>如果加锁失败之后，没有超时，那么需要在名字为redisson_lock__channel+lockName的channel上进行订阅，用于订阅解锁消息，然后一直阻塞直到超时，或者有解锁消息。</li><li>重试步骤1，2，3，直到最后获取到锁，或者某一步获取锁超时。</li></ol><p>对于我们的unlock方法比较简单也是通过lua脚本进行解锁，如果是可重入锁，只是减1。如果是非加锁线程解锁，那么解锁失败。 </p><p><img src="/2020/05/16/2020-05-16-fen-bu-shi-suo/16652acd8c664482.png" alt></p><p>Redission还有公平锁的实现，对于公平锁其利用了list结构和hashset结构分别用来保存我们排队的节点，和我们节点的过期时间，用这两个数据结构帮助我们实现公平锁，这里就不展开介绍了，有兴趣可以参考源码。</p><h3 id="RedLock"><a href="#RedLock" class="headerlink" title="RedLock"></a>RedLock</h3><p>我们想象一个这样的场景当机器A申请到一把锁之后，如果Redis主宕机了，这个时候从机并没有同步到这一把锁，那么机器B再次申请的时候就会再次申请到这把锁，为了解决这个问题Redis作者提出了RedLock红锁的算法,在Redission中也对RedLock进行了实现。</p><p><img src="/2020/05/16/2020-05-16-fen-bu-shi-suo/16652bd95e11a8b3.png" alt></p><p>通过上面的代码，我们需要实现多个Redis集群，然后进行红锁的加锁，解锁。具体的步骤如下:</p><ol><li>首先生成多个Redis集群的Rlock，并将其构造成RedLock。</li><li>依次循环对三个集群进行加锁，加锁的过程和上面一致。</li><li>如果循环加锁的过程中加锁失败，那么需要判断加锁失败的次数是否超出了最大值，这里的最大值是根据集群的个数，比如三个那么只允许失败一个，五个的话只允许失败两个，要保证多数成功。</li><li>加锁的过程中需要判断是否加锁超时，有可能我们设置加锁只能用3ms，第一个集群加锁已经消耗了3ms了。那么也算加锁失败。</li><li>3，4步里面加锁失败的话，那么就会进行解锁操作，解锁会对所有的集群在请求一次解锁。</li></ol><p>可以看见RedLock基本原理是利用多个Redis集群，用多数的集群加锁成功，减少Redis某个集群出故障，造成分布式锁出现问题的概率。</p><h3 id="Redis小结"><a href="#Redis小结" class="headerlink" title="Redis小结"></a>Redis小结</h3><ul><li>优点:对于Redis实现简单，性能对比ZK和Mysql较好。如果不需要特别复杂的要求，那么自己就可以利用setNx进行实现，如果自己需要复杂的需求的话那么可以利用或者借鉴Redission。对于一些要求比较严格的场景来说的话可以使用RedLock。</li><li>缺点:需要维护Redis集群，如果要实现RedLock那么需要维护更多的集群。</li></ul><h2 id="分布式锁的安全问题"><a href="#分布式锁的安全问题" class="headerlink" title="分布式锁的安全问题"></a>分布式锁的安全问题</h2><p>上面我们介绍过红锁，但是Martin Kleppmann认为其依然不安全。有关于Martin反驳的几点，我认为其实不仅仅局限于RedLock,前面说的算法基本都有这个问题，下面我们来讨论一下这些问题:</p><ul><li>长时间的GC pause:熟悉Java的同学肯定对GC不陌生，在GC的时候会发生STW(stop-the-world),例如CMS垃圾回收器，他会有两个阶段进行STW防止引用继续进行变化。那么有可能会出现下面图(引用至Martin反驳Redlock的文章)中这个情况：</li></ul><p><img src="/2020/05/16/2020-05-16-fen-bu-shi-suo/16653450d45a2a96.png" alt></p><pre><code>client1获取了锁并且设置了锁的超时时间，但是client1之后出现了STW，这个STW时间比较长，导致分布式锁进行了释放，client2获取到了锁，这个时候client1恢复了锁，那么就会出现client1，2同时获取到锁，这个时候分布式锁不安全问题就出现了。这个其实不仅仅局限于RedLock,对于我们的ZK,Mysql一样的有同样的问题。</code></pre><ul><li><p>时钟发生跳跃:对于Redis服务器如果其时间发生了向跳跃，那么肯定会影响我们锁的过期时间，那么我们的锁过期时间就不是我们预期的了，也会出现client1和client2获取到同一把锁，那么也会出现不安全，这个对于Mysql也会出现。但是ZK由于没有设置过期时间，那么发生跳跃也不会受影响。</p></li><li><p>长时间的网络I/O:这个问题和我们的GC的STW很像，也就是我们这个获取了锁之后我们进行网络调用，其调用时间由可能比我们锁的过期时间都还长，那么也会出现不安全的问题，这个Mysql也会有，ZK也不会出现这个问题。</p></li></ul><h3 id="GC的STW"><a href="#GC的STW" class="headerlink" title="GC的STW"></a>GC的STW</h3><p>对于这个问题可以看见基本所有的都会出现问题，Martin给出了一个解法，对于ZK这种他会生成一个自增的序列，那么我们真正进行对资源操作的时候，需要判断当前序列是否是最新，有点类似于我们乐观锁。当然这个解法Redis作者进行了反驳，你既然都能生成一个自增的序列了那么你完全不需要加锁了，也就是可以按照类似于Mysql乐观锁的解法去做。</p><h3 id="时钟发生跳跃"><a href="#时钟发生跳跃" class="headerlink" title="时钟发生跳跃"></a>时钟发生跳跃</h3><p>Martin觉得RedLock不安全很大的原因也是因为时钟的跳跃，因为锁过期强依赖于时间，但是ZK不需要依赖时间，依赖每个节点的Session。Redis作者也给出了解答:对于时间跳跃分为人为调整和NTP自动调整。</p><ul><li>人为调整:人为调整影响的那么完全可以人为不调整，这个是处于可控的。</li><li>NTP自动调整:这个可以通过一定的优化，把跳跃时间控制的可控范围内，虽然会跳跃，但是是完全可以接受的。</li></ul><h3 id="长时间的网络I-O"><a href="#长时间的网络I-O" class="headerlink" title="长时间的网络I/O"></a>长时间的网络I/O</h3><p>这一块不是他们讨论的重点，我自己觉得，对于这个问题的优化可以控制网络调用的超时时间，把所有网络调用的超时时间相加，那么我们锁过期时间其实应该大于这个时间，当然也可以通过优化网络调用比如串行改成并行，异步化等。</p><h2 id="Chubby的一些优化"><a href="#Chubby的一些优化" class="headerlink" title="Chubby的一些优化"></a>Chubby的一些优化</h2><p>大家搜索ZK的时候，会发现他们都写了ZK是Chubby的开源实现，Chubby内部工作原理和ZK类似。但是Chubby的定位是分布式锁和ZK有点不同。Chubby也是使用上面自增序列的方案用来解决分布式不安全的问题，但是他提供了多种校验方法:</p><ul><li>CheckSequencer()：调用Chubby的API检查此时这个序列号是否有效。</li><li>访问资源服务器检查，判断当前资源服务器最新的序列号和我们的序列号的大小。</li><li>lock-delay:为了防止我们校验的逻辑入侵我们的资源服务器，其提供了一种方法当客户端失联的时候，并不会立即释放锁，而是在一定的时间内(默认1min)阻止其他客户端拿去这个锁，那么也就是给予了一定的buffer等待STW恢复，而我们的GC的STW时间如果比1min还长那么你应该检查你的程序，而不是怀疑你的分布式锁了。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式锁 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring</title>
      <link href="/2020/05/14/2020-05-14-spring/"/>
      <url>/2020/05/14/2020-05-14-spring/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是-Spring-框架"><a href="#什么是-Spring-框架" class="headerlink" title="什么是 Spring 框架"></a>什么是 Spring 框架</h2><p>Spring 是一种轻量级开发框架，旨在提高开发人员的开发效率以及系统的可维护性。<a href="https://spring.io/" target="_blank" rel="noopener">Spring 官网</a></p><p>我们一般说 Spring 框架指的都是 Spring Framework，它是很多模块的集合，使用这些模块可以很方便地协助我们进行开发。这些模块是：核心容器、数据访问/集成,、Web、AOP（面向切面编程）、工具、消息和测试模块。比如：Core Container 中的 Core 组件是Spring 所有组件的核心，Beans 组件和 Context 组件是实现IOC和依赖注入的基础，AOP组件用来实现面向切面编程。</p><p>Spring 官网列出的 Spring 的 6 个特征:</p><ul><li>核心技术 ：依赖注入(DI)，AOP，事件(events)，资源，i18n，验证，数据绑定，类型转换，SpEL。</li><li>测试 ：模拟对象，TestContext框架，Spring MVC 测试，WebTestClient。</li><li>数据访问 ：事务，DAO支持，JDBC，ORM，编组XML。</li><li>Web支持 : Spring MVC和Spring WebFlux Web框架。</li><li>集成 ：远程处理，JMS，JCA，JMX，电子邮件，任务，调度，缓存。</li><li>语言 ：Kotlin，Groovy，动态语言。</li></ul><h2 id="列举一些重要的Spring模块"><a href="#列举一些重要的Spring模块" class="headerlink" title="列举一些重要的Spring模块"></a>列举一些重要的Spring模块</h2><p>下图对应的是 Spring4.x 版本。目前最新的5.x版本中 Web 模块的 Portlet 组件已经被废弃掉，同时增加了用于异步响应式处理的 WebFlux 组件。</p><p><img src="/2020/05/14/2020-05-14-spring/2020051401.png" alt></p><ul><li>Spring Core： 基础,可以说 Spring 其他所有的功能都需要依赖于该类库。主要提供 IoC 依赖注入功能。</li><li>Spring Aspects ： 该模块为与AspectJ的集成提供支持。</li><li>Spring AOP ：提供了面向切面的编程实现。</li><li>Spring JDBC : Java数据库连接。</li><li>Spring JMS ：Java消息服务。</li><li>Spring ORM : 用于支持Hibernate等ORM工具。</li><li>Spring Web : 为创建Web应用程序提供支持。</li><li>Spring Test : 提供了对 JUnit 和 TestNG 测试的支持。</li></ul><h2 id="RestController-vs-Controller"><a href="#RestController-vs-Controller" class="headerlink" title="@RestController vs @Controller"></a>@RestController vs @Controller</h2><p>@Controller 返回一个页面</p><p>单独使用 @Controller 不加 @ResponseBody的话一般使用在要返回一个视图的情况，这种情况属于比较传统的Spring MVC 的应用，对应于前后端不分离的情况。</p><p><img src="/2020/05/14/2020-05-14-spring/2020051402.png" alt></p><p>@RestController 返回JSON 或 XML 形式数据</p><p>但@RestController只返回对象，对象数据直接以 JSON 或 XML 形式写入 HTTP 响应(Response)中，这种情况属于 RESTful Web服务，这也是目前日常开发所接触的最常用的情况（前后端分离）。</p><p><img src="/2020/05/14/2020-05-14-spring/2020051403.png" alt></p><p>@Controller +@ResponseBody 返回JSON 或 XML 形式数据</p><p>如果你需要在Spring4之前开发 RESTful Web服务的话，你需要使用@Controller 并结合@ResponseBody注解，也就是说@Controller +@ResponseBody= @RestController（Spring 4 之后新加的注解）。</p><blockquote><p>@ResponseBody 注解的作用是将 Controller 的方法返回的对象通过适当的转换器转换为指定的格式之后，写入到HTTP 响应(Response)对象的 body 中，通常用来返回 JSON 或者 XML 数据，返回 JSON 数据的情况比较多。</p></blockquote><p><img src="/2020/05/14/2020-05-14-spring/2020051404.png" alt></p><h2 id="Spring-IOC-amp-AOP"><a href="#Spring-IOC-amp-AOP" class="headerlink" title="Spring IOC &amp; AOP"></a>Spring IOC &amp; AOP</h2><h3 id="谈谈自己对于-Spring-IoC-和-AOP-的理解"><a href="#谈谈自己对于-Spring-IoC-和-AOP-的理解" class="headerlink" title="谈谈自己对于 Spring IoC 和 AOP 的理解"></a>谈谈自己对于 Spring IoC 和 AOP 的理解</h3><h4 id="IoC"><a href="#IoC" class="headerlink" title="IoC"></a>IoC</h4><p>IoC（Inverse of Control:控制反转）是一种设计思想，就是将原本在程序中手动创建对象的控制权，交由Spring框架来管理。IoC 容器是 Spring 用来实现 IoC 的载体， IoC 容器实际上就是个Map（key，value）,Map 中存放的是各种对象。</p><p>将对象之间的相互依赖关系交给 IoC 容器来管理，并由 IoC 容器完成对象的注入。这样可以很大程度上简化应用的开发，把应用从复杂的依赖关系中解放出来。 IoC 容器就像是一个工厂一样，当我们需要创建一个对象的时候，只需要配置好配置文件/注解即可，完全不用考虑对象是如何被创建出来的。 在实际项目中一个 Service 类可能有几百甚至上千个类作为它的底层，假如我们需要实例化这个 Service，你可能要每次都要搞清这个 Service 所有底层类的构造函数，这可能会把人逼疯。如果利用 IoC 的话，你只需要配置好，然后在需要的地方引用就行了，这大大增加了项目的可维护性且降低了开发难度。</p><p>Spring IoC的初始化过程：</p><p><img src="/2020/05/14/2020-05-14-spring/2020051405.png" alt></p><p>个人术语：</p><p><strong>IoC又叫控制反转，是一种设计思想。IoC的意思就是将原本在程序中手动创建对象的控制权，交给Spring框架来管理。IoC容器实际上就是个Map，以key，value的形式存在，map中存放的就是各种对象。IoC容器解决了各种对象之前复杂的依赖关系，只要加个注解引用就可以。</strong></p><h4 id="AOP"><a href="#AOP" class="headerlink" title="AOP"></a>AOP</h4><p>AOP(Aspect-Oriented Programming:面向切面编程)能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。</p><p>Spring AOP就是基于动态代理的，如果要代理的对象，<strong>实现了某个接口，那么Spring AOP会使用JDK Proxy</strong>，去创建代理对象，而对于<strong>没有实现接口的对象，就无法使用 JDK Proxy 去进行代理了</strong>，这时候Spring AOP会使用Cglib ，这时候Spring AOP会使用 Cglib 生成一个被代理对象的子类来作为代理，如下图所示：</p><p><img src="/2020/05/14/2020-05-14-spring/2020051406.png" alt></p><p>当然你也可以使用 AspectJ ,Spring AOP 已经集成了AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。</p><p>使用 AOP 之后我们可以把一些通用功能抽象出来，在需要用到的地方直接使用即可，这样大大简化了代码量。我们需要增加新功能时也方便，这样也提高了系统扩展性。日志功能、事务管理等等场景都用到了 AOP 。</p><p>个人话术：</p><p><strong>AOP又叫面向切面编程，将那些与业务无关的公共调用的逻辑（如事务处理，日志管理，权限管理等）封装起来，减少代码的冗余度，也降低模块间的耦合度。Spring AOP是基于动态代理的。当被代理的对象，实现了某个接口，那么就会使用JDK Proxy，否则将会使用Cglib，这个时候Cglib会生成一个被代理对象的子类来代理。</strong></p><h2 id="Spring-AOP-和-AspectJ-AOP-区别"><a href="#Spring-AOP-和-AspectJ-AOP-区别" class="headerlink" title="Spring AOP 和 AspectJ AOP 区别"></a>Spring AOP 和 AspectJ AOP 区别</h2><p><strong>Spring AOP 属于运行时增强，而 AspectJ 是编译时增强。 Spring AOP 基于代理(Proxying)，而 AspectJ 基于字节码操作(Bytecode Manipulation)。</strong></p><p>Spring AOP 已经集成了 AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。AspectJ 相比于 Spring AOP 功能更加强大，但是 Spring AOP 相对来说更简单，</p><p>如果我们的切面比较少，那么两者性能差异不大。但是，<strong>当切面太多的话，最好选择 AspectJ ，它比Spring AOP 快很多</strong>。</p><h2 id="Spring-bean"><a href="#Spring-bean" class="headerlink" title="Spring bean"></a>Spring bean</h2><h3 id="Spring-中的-bean-的作用域有哪些"><a href="#Spring-中的-bean-的作用域有哪些" class="headerlink" title="Spring 中的 bean 的作用域有哪些?"></a>Spring 中的 bean 的作用域有哪些?</h3><ul><li>singleton : 唯一 bean 实例，Spring 中的 bean 默认都是单例的。</li><li>prototype : 每次请求都会创建一个新的 bean 实例。</li><li>request : 每一次HTTP请求都会产生一个新的bean，该bean仅在当前HTTP request内有效。</li><li>session : 每一次HTTP请求都会产生一个新的 bean，该bean仅在当前 HTTP session 内有效。</li><li>global-session： 全局session作用域，仅仅在基于portlet的web应用中才有意义，Spring5已经没有了。Portlet是能够生成语义代码(例如：HTML)片段的小型Java Web插件。它们基于portlet容器，可以像servlet一样处理HTTP请求。但是，与 servlet 不同，每个 portlet 都有不同的会话</li></ul><h3 id="Spring-中的单例-bean-的线程安全问题了解吗？"><a href="#Spring-中的单例-bean-的线程安全问题了解吗？" class="headerlink" title="Spring 中的单例 bean 的线程安全问题了解吗？"></a>Spring 中的单例 bean 的线程安全问题了解吗？</h3><p>大部分时候我们并没有在系统中使用多线程，所以很少有人会关注这个问题。单例 bean 存在线程问题，主要是因为当多个线程操作同一个对象的时候，对这个对象的非静态成员变量的写操作会存在线程安全问题。</p><p>常见的有两种解决办法：</p><ol><li>在Bean对象中尽量避免定义可变的成员变量（不太现实）。</li><li>在类中定义一个ThreadLocal成员变量，将需要的可变成员变量保存在 ThreadLocal 中（推荐的一种方式）。</li></ol><h2 id="Component-和-Bean-的区别是什么？"><a href="#Component-和-Bean-的区别是什么？" class="headerlink" title="@Component 和 @Bean 的区别是什么？"></a>@Component 和 @Bean 的区别是什么？</h2><ol><li>作用对象不同: @Component 注解作用于类，而@Bean注解作用于方法。</li><li>@Component通常是通过类路径扫描来自动侦测以及自动装配到Spring容器中（我们可以使用 @ComponentScan 注解定义要扫描的路径从中找出标识了需要装配的类自动装配到 Spring 的 bean 容器中）。@Bean 注解通常是我们在标有该注解的方法中定义产生这个 bean,@Bean告诉了Spring这是某个类的示例，当我需要用它的时候还给我。</li><li>@Bean 注解比 Component 注解的自定义性更强，而且很多地方我们只能通过 @Bean 注解来注册bean。比如当我们引用第三方库中的类需要装配到 Spring容器时，则只能通过 @Bean来实现。</li></ol><pre><code>@Configurationpublic class AppConfig {    @Bean    public TransferService transferService() {        return new TransferServiceImpl();    }}</code></pre><p>上面的代码相当于下面的 xml 配置</p><pre><code>&lt;beans&gt;    &lt;bean id=&quot;transferService&quot; class=&quot;com.acme.TransferServiceImpl&quot;/&gt;&lt;/beans&gt;</code></pre><p>下面这个例子是通过 @Component 无法实现的。</p><pre><code>@Beanpublic OneService getService(status) {    case (status)  {        when 1:                return new serviceImpl1();        when 2:                return new serviceImpl2();        when 3:                return new serviceImpl3();    }}</code></pre><h2 id="将一个类声明为Spring的-bean-的注解有哪些"><a href="#将一个类声明为Spring的-bean-的注解有哪些" class="headerlink" title="将一个类声明为Spring的 bean 的注解有哪些?"></a>将一个类声明为Spring的 bean 的注解有哪些?</h2><p>我们一般使用 @Autowired 注解自动装配 bean，要想把类标识成可用于 @Autowired 注解自动装配的 bean 的类,采用以下注解可实现：</p><ul><li>@Component ：通用的注解，可标注任意类为 Spring 组件。如果一个Bean不知道属于哪个层，可以使用@Component 注解标注。</li><li>@Repository : 对应持久层即 Dao 层，主要用于数据库相关操作。</li><li>@Service : 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao层。</li><li>@Controller : 对应 Spring MVC 控制层，主要用户接受用户请求并调用 Service 层返回数据给前端页面。</li></ul><h2 id="Spring-中的-bean-生命周期"><a href="#Spring-中的-bean-生命周期" class="headerlink" title="Spring 中的 bean 生命周期?"></a>Spring 中的 bean 生命周期?</h2><ul><li>Bean 容器找到配置文件中 Spring Bean 的定义。</li><li>Bean 容器利用 Java Reflection API 创建一个Bean的实例。</li><li>如果涉及到一些属性值 利用 set()方法设置一些属性值。</li><li>如果 Bean 实现了 BeanNameAware 接口，调用 setBeanName()方法，传入Bean的名字。</li><li>如果 Bean 实现了 BeanClassLoaderAware 接口，调用 setBeanClassLoader()方法，传入 ClassLoader对象的实例。</li><li>与上面的类似，如果实现了其他 *.Aware接口，就调用相应的方法。</li><li>如果有和加载这个 Bean 的 Spring 容器相关的 BeanPostProcessor 对象，执行postProcessBeforeInitialization() 方法。</li><li>如果Bean实现了InitializingBean接口，执行afterPropertiesSet()方法。</li><li>如果 Bean 在配置文件中的定义包含 init-method 属性，执行指定的方法。</li><li>如果有和加载这个 Bean的 Spring 容器相关的 BeanPostProcessor 对象，执行postProcessAfterInitialization() 方法</li><li>当要销毁 Bean 的时候，如果 Bean 实现了 DisposableBean 接口，执行 destroy() 方法。</li><li>当要销毁 Bean 的时候，如果 Bean 在配置文件中的定义包含 destroy-method 属性，执行指定的方法。</li></ul><p>与之比较类似的中文版本:</p><p><img src="/2020/05/14/2020-05-14-spring/2020051408.jpg" alt></p><p><img src="/2020/05/14/2020-05-14-spring/2020051407.jpg" alt></p><h2 id="说说自己对于-Spring-MVC-了解"><a href="#说说自己对于-Spring-MVC-了解" class="headerlink" title="说说自己对于 Spring MVC 了解?"></a>说说自己对于 Spring MVC 了解?</h2><p>MVC 是一种设计模式,Spring MVC 是一款很优秀的 MVC 框架。Spring MVC 可以帮助我们进行更简洁的Web层的开发，并且它天生与 Spring 框架集成。Spring MVC 下我们一般把后端项目分为 Service层（处理业务）、Dao层（数据库操作）、Entity层（实体类）、Controller层(控制层，返回数据给前台页面)。</p><p>Spring MVC 的简单原理图如下：</p><p><img src="/2020/05/14/2020-05-14-spring/2020051409.jpg" alt></p><h2 id="SpringMVC-工作原理了解吗"><a href="#SpringMVC-工作原理了解吗" class="headerlink" title="SpringMVC 工作原理了解吗?"></a>SpringMVC 工作原理了解吗?</h2><p>原理如下图所示：</p><p><img src="/2020/05/14/2020-05-14-spring/2020051410.jpg" alt></p><p><strong>流程说明（重要）</strong>：</p><ol><li>客户端（浏览器）发送请求，直接请求到 DispatcherServlet。</li><li>DispatcherServlet 根据请求信息调用 HandlerMapping，解析请求对应的 Handler。</li><li>解析到对应的 Handler（也就是我们平常说的 Controller 控制器）后，开始由 HandlerAdapter 适配器处理。</li><li>HandlerAdapter 会根据 Handler来调用真正的处理器开处理请求，并处理相应的业务逻辑。</li><li>处理器处理完业务后，会返回一个 ModelAndView 对象，Model 是返回的数据对象，View 是个逻辑上的 View。</li><li>ViewResolver 会根据逻辑 View 查找实际的 View。</li><li>DispaterServlet 把返回的 Model 传给 View（视图渲染）。</li><li>把 View 返回给请求者（浏览器）。</li></ol><p>个人术语：</p><ol><li>客户端的所有请求都交给前端控制器DispatcherServlet来处理，DispatcherServlet会负责调用系统的其他模块来真正处理用户的请求；</li><li>DispatcherServlet收到请求后，将根据请求的信息（包括URL、请求参数等）以及映射处理器HandlerMapping的配置找到处理该请求的处理器；</li><li>在这个地方Spring会通过适配器处理器HandlerAdapter对该处理器进行封装；</li><li>处理器完成对用户请求的处理后，会返回一个ModelAndView对象给DispatcherServlet；</li><li>DispatcherServlet还要借助视图解析器ViewResolver完成从逻辑视图到真实视图对象的解析工作；</li><li>当得到真正的视图对象后，DispatcherServlet会利用视图对象对模型数据进行渲染。</li></ol><h2 id="Spring-启动过程"><a href="#Spring-启动过程" class="headerlink" title="Spring 启动过程"></a>Spring 启动过程</h2><ol><li>首先，web容器会创建一个上下文环境，这个上下文就是ServletContext，其为后面的spring IoC容器提供宿主环境；</li><li>其次，在web.xml中会提供有contextLoaderListener监听器。在web容器启动时，监听器会监听到容器初始化事件，其contextInitialized方法会被调用，在这个方法中，spring会初始化一个启动上下文，这个上下文被称为根上下文，即WebApplicationContext</li><li>再次，监听器初始化完毕后，开始初始化web.xml中配置的Servlet，这个servlet可以配置多个，以最常见的DispatcherServlet为例，这个servlet实际上是一个标准的前端控制器，用以转发、匹配、处理每个servlet请求。</li></ol><h2 id="Spring-框架中用到了哪些设计模式？"><a href="#Spring-框架中用到了哪些设计模式？" class="headerlink" title="Spring 框架中用到了哪些设计模式？"></a>Spring 框架中用到了哪些设计模式？</h2><ul><li>工厂设计模式 : Spring使用工厂模式通过 BeanFactory、ApplicationContext 创建 bean 对象。</li><li>代理设计模式 : Spring AOP 功能的实现。</li><li>单例设计模式 : Spring 中的 Bean 默认都是单例的。</li><li>模板方法模式 : Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。</li><li>包装器设计模式 : 我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。</li><li>观察者模式: Spring 事件驱动模型就是观察者模式很经典的一个应用。</li><li>适配器模式 :Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中也是用到了适配器模式适配Controller。</li><li>……</li></ul><h2 id="Spring-事务"><a href="#Spring-事务" class="headerlink" title="Spring 事务"></a>Spring 事务</h2><h3 id="Spring-管理事务的方式有几种？"><a href="#Spring-管理事务的方式有几种？" class="headerlink" title="Spring 管理事务的方式有几种？"></a>Spring 管理事务的方式有几种？</h3><ol><li>编程式事务，在代码中硬编码。(不推荐使用)</li><li>声明式事务，在配置文件中配置（推荐使用）</li></ol><p>声明式事务又分为两种：</p><ol><li>基于XML的声明式事务</li><li>基于注解的声明式事务</li></ol><h3 id="Spring-事务中的隔离级别有哪几种"><a href="#Spring-事务中的隔离级别有哪几种" class="headerlink" title="Spring 事务中的隔离级别有哪几种?"></a>Spring 事务中的隔离级别有哪几种?</h3><p>TransactionDefinition 接口中定义了五个表示隔离级别的常量：</p><ul><li>TransactionDefinition.ISOLATION_DEFAULT: 使用后端数据库默认的隔离级别，Mysql 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别.</li><li>TransactionDefinition.ISOLATION_READ_UNCOMMITTED: 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读</li><li>TransactionDefinition.ISOLATION_READ_COMMITTED: 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生</li><li>TransactionDefinition.ISOLATION_REPEATABLE_READ: 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。</li><li>TransactionDefinition.ISOLATION_SERIALIZABLE: 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。</li></ul><h3 id="Spring-事务中哪几种事务传播行为"><a href="#Spring-事务中哪几种事务传播行为" class="headerlink" title="Spring 事务中哪几种事务传播行为?"></a>Spring 事务中哪几种事务传播行为?</h3><p>支持当前事务的情况：</p><ul><li>TransactionDefinition.PROPAGATION_REQUIRED： 如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。</li><li>TransactionDefinition.PROPAGATION_SUPPORTS： 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。</li><li>TransactionDefinition.PROPAGATION_MANDATORY： 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性）</li></ul><p>不支持当前事务的情况：</p><ul><li>TransactionDefinition.PROPAGATION_REQUIRES_NEW： 如果当前存在事务，则把当前事务挂起,创建一个新的事务。</li><li>TransactionDefinition.PROPAGATION_NOT_SUPPORTED： 如果当前存在事务，则把当前事务挂起,以非事务方式运行。</li><li>TransactionDefinition.PROPAGATION_NEVER： 以非事务方式运行，如果当前存在事务，则抛出异常。</li></ul><p>其他情况：</p><ul><li>ransactionDefinition.PROPAGATION_NESTED： 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。</li></ul><h3 id="Transactional-rollbackFor-Exception-class-注解了解吗？"><a href="#Transactional-rollbackFor-Exception-class-注解了解吗？" class="headerlink" title="@Transactional(rollbackFor = Exception.class)注解了解吗？"></a>@Transactional(rollbackFor = Exception.class)注解了解吗？</h3><p>我们知道：Exception分为运行时异常RuntimeException和非运行时异常。事务管理对于企业应用来说是至关重要的，即使出现异常情况，它也可以保证数据的一致性。</p><p>当@Transactional注解作用于类上时，该类的所有 public 方法将都具有该类型的事务属性，同时，我们也可以在方法级别使用该标注来覆盖类级别的定义。如果类或者方法加了这个注解，那么这个类里面的方法抛出异常，就会回滚，数据库里面的数据也会回滚。</p><p>在@Transactional注解中如果不配置rollbackFor属性,那么事物只会在遇到RuntimeException的时候才会回滚,加上rollbackFor=Exception.class,可以让事物在遇到非运行时异常时也回滚。</p>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ZooKeeper（LOOK）</title>
      <link href="/2020/04/18/2020-04-18-zookeeper-zhi-shi/"/>
      <url>/2020/04/18/2020-04-18-zookeeper-zhi-shi/</url>
      
        <content type="html"><![CDATA[<h2 id="zookeeper是什么"><a href="#zookeeper是什么" class="headerlink" title="zookeeper是什么"></a>zookeeper是什么</h2><p>官方说辞：Zookeeper 分布式服务框架是Apache Hadoop 的一个子项目，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。</p><h2 id="zookeeper提供了什么"><a href="#zookeeper提供了什么" class="headerlink" title="zookeeper提供了什么"></a>zookeeper提供了什么</h2><p>简单的说，zookeeper=文件系统+通知机制。</p><h3 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h3><p>Zookeeper维护一个类似文件系统的数据结构：</p><p><img src="/2020/04/18/2020-04-18-zookeeper-zhi-shi/041801.png" alt></p><p>每个子目录项如 NameService 都被称作为 znode，和文件系统一样，我们能够自由的增加、删除znode，在一个znode下增加、删除子znode，唯一的不同在于znode是可以存储数据的。</p><p>有四种类型的znode：</p><ul><li>PERSISTENT-持久化目录节点<br>  客户端与zookeeper断开连接后，该节点依旧存在</li><li>PERSISTENT_SEQUENTIAL-持久化顺序编号目录节点<br>  客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号</li><li>EPHEMERAL-临时目录节点<br>  客户端与zookeeper断开连接后，该节点被删除</li><li>EPHEMERAL_SEQUENTIAL-临时顺序编号目录节点<br>  客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号</li></ul><h3 id="通知机制"><a href="#通知机制" class="headerlink" title="通知机制"></a>通知机制</h3><p>客户端注册监听它关心的目录节点，当目录节点发生变化（数据改变、被删除、子目录节点增加删除）时，zookeeper会通知客户端。 </p><h2 id="zookeeper能用做什么"><a href="#zookeeper能用做什么" class="headerlink" title="zookeeper能用做什么"></a>zookeeper能用做什么</h2><h3 id="命名服务"><a href="#命名服务" class="headerlink" title="命名服务"></a>命名服务</h3><p>这个似乎最简单，在zookeeper的文件系统里创建一个目录，即有唯一的path。在我们使用tborg无法确定上游程序的部署机器时即可与下游程序约定好path，通过path即能互相探索发现，不见不散了。</p><h3 id="配置管理"><a href="#配置管理" class="headerlink" title="配置管理"></a>配置管理</h3><p>程序总是需要配置的，如果程序分散部署在多台机器上，要逐个改变配置就变得困难。好吧，现在把这些配置全部放到zookeeper上去，保存在 Zookeeper 的某个目录节点中，然后所有相关应用程序对这个目录节点进行监听，一旦配置信息发生变化，每个应用程序就会收到 Zookeeper 的通知，然后从 Zookeeper 获取新的配置信息应用到系统中就好。</p><p><img src="/2020/04/18/2020-04-18-zookeeper-zhi-shi/041802.png" alt></p><h3 id="集群管理"><a href="#集群管理" class="headerlink" title="集群管理"></a>集群管理</h3><p>所谓集群管理无在乎两点：是否有机器退出和加入、选举master。</p><p>对于第一点，所有机器约定在父目录GroupMembers下创建临时目录节点，然后监听父目录节点的子节点变化消息。一旦有机器挂掉，该机器与 zookeeper的连接断开，其所创建的临时目录节点被删除，所有其他机器都收到通知：某个兄弟目录被删除，于是，所有人都知道：它上船了。新机器加入 也是类似，所有机器收到通知：新兄弟目录加入，highcount又有了。</p><p>对于第二点，我们稍微改变一下，所有机器创建临时顺序编号目录节点，每次选取编号最小的机器作为master就好。</p><p><img src="/2020/04/18/2020-04-18-zookeeper-zhi-shi/041803.png" alt></p><h3 id="分布式锁"><a href="#分布式锁" class="headerlink" title="分布式锁"></a>分布式锁</h3><p>有了zookeeper的一致性文件系统，锁的问题变得容易。锁服务可以分为两类，一个是保持独占，另一个是控制时序。</p><p>对于第一类，我们将zookeeper上的一个znode看作是一把锁，通过createznode的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。厕所有言：来也冲冲，去也冲冲，用完删除掉自己创建的distribute_lock 节点就释放出锁。</p><p>对于第二类， /distribute_lock 已经预先存在，所有客户端在它下面创建临时顺序编号目录节点，和选master一样，编号最小的获得锁，用完删除，依次方便。</p><p><img src="/2020/04/18/2020-04-18-zookeeper-zhi-shi/041804.png" alt></p><h3 id="队列管理"><a href="#队列管理" class="headerlink" title="队列管理"></a>队列管理</h3><p>两种类型的队列：</p><ul><li>同步队列，当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达。</li><li>队列按照 FIFO 方式进行入队和出队操作。</li></ul><p>第一类，在约定目录下创建临时目录节点，监听节点数目是否是我们要求的数目。</p><p>第二类，和分布式锁服务中的控制时序场景基本原理一致，入列有编号，出列按编号。  </p><h2 id="分布式与数据复制"><a href="#分布式与数据复制" class="headerlink" title="分布式与数据复制"></a>分布式与数据复制</h2><p>Zookeeper作为一个集群提供一致的数据服务，自然，它要在所有机器间做数据复制。数据复制的好处：</p><ul><li>容错<br>  一个节点出错，不致于让整个系统停止工作，别的节点可以接管它的工作；</li><li>提高系统的扩展能力<br>  把负载分布到多个节点上，或者增加节点来提高系统的负载能力；</li><li>提高性能<br>  让客户端本地访问就近的节点，提高用户访问速度。</li></ul><p>从客户端读写访问的透明度来看，数据复制集群系统分下面两种：</p><ul><li>写主(WriteMaster)<br>  对数据的修改提交给指定的节点。读无此限制，可以读取任何一个节点。这种情况下客户端需要对读与写进行区别，俗称读写分离；</li><li>写任意(Write Any)<br>  对数据的修改可提交给任意的节点，跟读一样。这种情况下，客户端对集群节点的角色与变化透明。</li></ul><p>对zookeeper来说，它采用的方式是写任意。通过增加机器，它的读吞吐能力和响应能力扩展性非常好，而写，随着机器的增多吞吐能力肯定下降（这 也是它建立observer的原因），而响应能力则取决于具体实现方式，是延迟复制保持最终一致性，还是立即复制快速响应。我们关注的重点还是在如何保证数据在集群所有机器的一致性，这就涉及到paxos算法。</p><h2 id="数据一致性与paxos算法"><a href="#数据一致性与paxos算法" class="headerlink" title="数据一致性与paxos算法"></a>数据一致性与paxos算法</h2><p>Paxos算法解决的什么问题呢，<strong>解决的就是保证每个节点执行相同的操作序列</strong>。好吧，这还不简单，master维护一个全局写队列，所有写操作都必须 放入这个队列编号，那么无论我们写多少个节点，只要写操作是按编号来的，就能保证一致性。没错，就是这样，可是如果master挂了呢。</p><p>Paxos算法通过投票来对写操作进行全局编号，同一时刻，只有一个写操作被批准，同时并发的写操作要去争取选票，只有获得过半数选票的写操作才会被 批准（所以永远只会有一个写操作得到批准），其他的写操作竞争失败只好再发起一轮投票，就这样，在日复一日年复一年的投票中，所有写操作都被严格编号排 序。编号严格递增，当一个节点接受了一个编号为100的写操作，之后又接受到编号为99的写操作（因为网络延迟等很多不可预见原因），它马上能意识到自己 数据不一致了，自动停止对外服务并重启同步过程。任何一个节点挂掉都不会影响整个集群的数据一致性（总2n+1台，除非挂掉大于n台）。</p><h2 id="Zookeeper的基本概念"><a href="#Zookeeper的基本概念" class="headerlink" title="Zookeeper的基本概念"></a>Zookeeper的基本概念</h2><h3 id="角色"><a href="#角色" class="headerlink" title="角色"></a>角色</h3><p>Zookeeper中的角色主要有以下三类，如下表所示：</p><p><img src="/2020/04/18/2020-04-18-zookeeper-zhi-shi/041805.png" alt></p><p>系统模型如图所示：</p><p><img src="/2020/04/18/2020-04-18-zookeeper-zhi-shi/041806.png" alt></p><h3 id="设计目的"><a href="#设计目的" class="headerlink" title="设计目的"></a>设计目的</h3><ul><li>最终一致性：client不论连接到哪个Server，展示给它都是同一个视图，这是zookeeper最重要的性能。</li><li>可靠性：具有简单、健壮、良好的性能，如果消息m被到一台服务器接受，那么它将被所有的服务器接受。</li><li>实时性：Zookeeper保证客户端将在一个时间间隔范围内获得服务器的更新信息，或者服务器失效的信息。但由于网络延时等原因，Zookeeper不能保证两个客户端能同时得到刚更新的数据，如果需要最新数据，应该在读数据之前调用sync()接口。</li><li>等待无关（wait-free）：慢的或者失效的client不得干预快速的client的请求，使得每个client都能有效的等待。</li><li>原子性：更新只能成功或者失败，没有中间状态。</li><li>顺序性：包括全局有序和偏序两种：全局有序是指如果在一台服务器上消息a在消息b前发布，则在所有Server上消息a都将在消息b前被发布；偏序是指如果一个消息b在消息a后被同一个发送者发布，a必将排在b前面。</li></ul><h2 id="ZooKeeper的工作原理"><a href="#ZooKeeper的工作原理" class="headerlink" title="ZooKeeper的工作原理"></a>ZooKeeper的工作原理</h2><p>Zookeeper的核心是原子广播，这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab协议。Zab协议有两种模式，它们分别是<strong>恢复模式（选主）和广播模式（同步）</strong>。当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数Server完成了和 leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和Server具有相同的系统状态。</p><p>为了保证事务的顺序一致性，zookeeper采用了递增的事务id号（zxid）来标识事务。所有的提议（proposal）都在被提出的时候加上 了zxid。实现中zxid是一个64位的数字，它高32位是epoch用来标识leader关系是否改变，每次一个leader被选出来，它都会有一个 新的epoch，标识当前属于那个leader的统治时期。低32位用于递增计数。</p><p>每个Server在工作过程中有三种状态：</p><ul><li>LOOKING：当前Server不知道leader是谁，正在搜寻</li><li>LEADING：当前Server即为选举出来的leader</li><li>FOLLOWING：leader已经选举出来，当前Server与之同步</li></ul><h3 id="选主流程"><a href="#选主流程" class="headerlink" title="选主流程"></a>选主流程</h3><p>当leader崩溃或者leader失去大多数的follower，这时候zk进入恢复模式，恢复模式需要重新选举出一个新的leader，让所有的 Server都恢复到一个正确的状态。Zk的选举算法有两种：一种是基于basic paxos实现的，另外一种是基于fast paxos算法实现的。系统默认的选举算法为fast paxos。</p><h4 id="basic-paxos流程："><a href="#basic-paxos流程：" class="headerlink" title="basic paxos流程："></a>basic paxos流程：</h4><ol><li>选举线程由当前Server发起选举的线程担任，其主要功能是对投票结果进行统计，并选出推荐的Server；</li><li>选举线程首先向所有Server发起一次询问(包括自己)；</li><li>选举线程收到回复后，验证是否是自己发起的询问(验证zxid是否一致)，然后获取对方的id(myid)，并存储到当前询问对象列表中，最后获取对方提议的leader相关信息(id,zxid)，并将这些信息存储到当次选举的投票记录表中；</li><li>收到所有Server回复以后，就计算出zxid最大的那个Server，并将这个Server相关信息设置成下一次要投票的Server；</li><li>线程将当前zxid最大的Server设置为当前Server要推荐的Leader，如果此时获胜的Server获得n/2 + 1的Server票数， 设置当前推荐的leader为获胜的Server，将根据获胜的Server相关信息设置自己的状态，否则，继续这个过程，直到leader被选举出来。</li></ol><p>通过流程分析我们可以得出：要使Leader获得多数Server的支持，则Server总数必须是奇数2n+1，且存活的Server的数目不得少于n+1.</p><p>每个Server启动后都会重复以上流程。在恢复模式下，如果是刚从崩溃状态恢复的或者刚启动的server还会从磁盘快照中恢复数据和会话信息，zk会记录事务日志并定期进行快照，方便在恢复时进行状态恢复。选主的具体流程图如下所示：</p><p><img src="/2020/04/18/2020-04-18-zookeeper-zhi-shi/f7l9u8vz.bmp" alt></p><h4 id="fast-paxos流程"><a href="#fast-paxos流程" class="headerlink" title="fast paxos流程"></a>fast paxos流程</h4><p>fast paxos流程是在选举过程中，某Server首先向所有Server提议自己要成为leader，当其它Server收到提议以后，解决epoch和 zxid的冲突，并接受对方的提议，然后向对方发送接受提议完成的消息，重复这个流程，最后一定能选举出Leader。其流程图如下所示：</p><p><img src="/2020/04/18/2020-04-18-zookeeper-zhi-shi/xtsv70ww.bmp" alt></p><h3 id="同步流程"><a href="#同步流程" class="headerlink" title="同步流程"></a>同步流程</h3><p>选完leader以后，zk就进入状态同步过程。</p><ol><li>leader等待server连接；</li><li>Follower连接leader，将最大的zxid发送给leader；</li><li>Leader根据follower的zxid确定同步点；</li><li>完成同步后通知follower 已经成为uptodate状态；</li><li>Follower收到uptodate消息后，又可以重新接受client的请求进行服务了。</li></ol><p><img src="/2020/04/18/2020-04-18-zookeeper-zhi-shi/6mzlqtpc.bmp" alt></p><h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><h4 id="Leader工作流程"><a href="#Leader工作流程" class="headerlink" title="Leader工作流程"></a>Leader工作流程</h4><p>Leader主要有三个功能：</p><ol><li>恢复数据；</li><li>维持与Learner的心跳，接收Learner请求并判断Learner的请求消息类型；</li><li>Learner的消息类型主要有PING消息、REQUEST消息、ACK消息、REVALIDATE消息，根据不同的消息类型，进行不同的处理。</li></ol><p>PING消息是指Learner的心跳信息；REQUEST消息是Follower发送的提议信息，包括写请求及同步请求；ACK消息是 Follower的对提议的回复，超过半数的Follower通过，则commit该提议；REVALIDATE消息是用来延长SESSION有效时间。</p><p>Leader的工作流程简图如下所示，在实际实现中，流程要比下图复杂得多，启动了三个线程来实现功能。</p><p><img src="/2020/04/18/2020-04-18-zookeeper-zhi-shi/obyft8om.bmp" alt></p><h4 id="Follower工作流程"><a href="#Follower工作流程" class="headerlink" title="Follower工作流程"></a>Follower工作流程</h4><p>Follower主要有四个功能：</p><ol><li>向Leader发送请求（PING消息、REQUEST消息、ACK消息、REVALIDATE消息）；</li><li>接收Leader消息并进行处理</li><li>接收Client的请求，如果为写请求，发送给Leader进行投票；</li><li>返回Client结果。</li></ol><p>Follower的消息循环处理如下几种来自Leader的消息：</p><ol><li>PING消息： 心跳消息；</li><li>PROPOSAL消息：Leader发起的提案，要求Follower投票；</li><li>COMMIT消息：服务器端最新一次提案的信息；</li><li>UPTODATE消息：表明同步完成；</li><li>REVALIDATE消息：根据Leader的REVALIDATE结果，关闭待revalidate的session还是允许其接受消息；</li><li>SYNC消息：返回SYNC结果到客户端，这个消息最初由客户端发起，用来强制得到最新的更新。</li></ol><p>Follower的工作流程简图如下所示，在实际实现中，Follower是通过5个线程来实现功能的。</p><p><img src="/2020/04/18/2020-04-18-zookeeper-zhi-shi/d9q0fnsk.bmp" alt></p><p>对于observer的流程不再叙述，observer流程和Follower的唯一不同的地方就是observer不会参加leader发起的投票。</p><h2 id="ZooKeeper典型使用场景一览"><a href="#ZooKeeper典型使用场景一览" class="headerlink" title="ZooKeeper典型使用场景一览"></a>ZooKeeper典型使用场景一览</h2><p><img src="/2020/04/18/2020-04-18-zookeeper-zhi-shi/20200627143134.jpg" alt></p><p><img src="/2020/04/18/2020-04-18-zookeeper-zhi-shi/20200627143408.jpg" alt></p><p><img src="/2020/04/18/2020-04-18-zookeeper-zhi-shi/20200627143602.jpg" alt></p><p><img src="/2020/04/18/2020-04-18-zookeeper-zhi-shi/20200627143747.jpg" alt></p>]]></content>
      
      
      <categories>
          
          <category> 中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ZooKeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ZooKeeper</title>
      <link href="/2020/04/16/2020-04-16-zookeeper/"/>
      <url>/2020/04/16/2020-04-16-zookeeper/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是-ZooKeeper"><a href="#什么是-ZooKeeper" class="headerlink" title="什么是 ZooKeeper"></a>什么是 ZooKeeper</h2><h3 id="ZooKeeper-概览"><a href="#ZooKeeper-概览" class="headerlink" title="ZooKeeper 概览"></a>ZooKeeper 概览</h3><p>ZooKeeper 是一个典型的分布式数据一致性解决方案，分布式应用程序可以基于 ZooKeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。</p><p>Zookeeper 一个最常用的使用场景就是用于担任服务生产者和服务消费者的注册中心(提供发布订阅服务)。 服务生产者将自己提供的服务注册到Zookeeper中心，服务的消费者在进行服务调用的时候先到Zookeeper中查找服务，获取到服务生产者的详细信息之后，再去调用服务生产者的内容与数据。如下图所示，在 Dubbo架构中 Zookeeper 就担任了注册中心这一角色。</p><p><img src="/2020/04/16/2020-04-16-zookeeper/041601.png" alt></p><h3 id="为什么最好使用奇数台服务器构成-ZooKeeper-集群"><a href="#为什么最好使用奇数台服务器构成-ZooKeeper-集群" class="headerlink" title="为什么最好使用奇数台服务器构成 ZooKeeper 集群"></a>为什么最好使用奇数台服务器构成 ZooKeeper 集群</h3><p>所谓的zookeeper容错是指，当宕掉几个zookeeper服务器之后，剩下的个数必须大于宕掉的个数的话整个zookeeper才依然可用。假如我们的集群中有n台zookeeper服务器，那么也就是剩下的服务数必须大于n/2。先说一下结论，2n和2n-1的容忍度是一样的，都是n-1，大家可以先自己仔细想一想，这应该是一个很简单的数学问题了。 比如假如我们有3台，那么最大允许宕掉1台zookeeper服务器，如果我们有4台的的时候也同样只允许宕掉1台。 假如我们有5台，那么最大允许宕掉2台zookeeper服务器，如果我们有6台的的时候也同样只允许宕掉2台。</p><h2 id="关于-ZooKeeper-的一些重要概念"><a href="#关于-ZooKeeper-的一些重要概念" class="headerlink" title="关于 ZooKeeper 的一些重要概念"></a>关于 ZooKeeper 的一些重要概念</h2><h3 id="重要概念总结"><a href="#重要概念总结" class="headerlink" title="重要概念总结"></a>重要概念总结</h3><ul><li>ZooKeeper 本身就是一个分布式程序（只要半数以上节点存活，ZooKeeper 就能正常服务）。</li><li>为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么 ZooKeeper 本身仍然是可用的。</li><li>ZooKeeper 将数据保存在内存中，这也就保证了 高吞吐量和低延迟（但是内存限制了能够存储的容量不太大，此限制也是保持znode中存储的数据量较小的进一步原因）。</li><li>ZooKeeper 是高性能的。 在“读”多于“写”的应用程序中尤其地高性能，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景。）</li><li>ZooKeeper有临时节点的概念。 当创建临时节点的客户端会话一直保持活动，瞬时节点就一直存在。而当会话终结时，瞬时节点被删除。持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上。</li><li>ZooKeeper 底层其实只提供了两个功能：①管理（存储、读取）用户程序提交的数据；②为用户程序提供数据节点监听服务。</li></ul><h3 id="会话（Session）"><a href="#会话（Session）" class="headerlink" title="会话（Session）"></a>会话（Session）</h3><p>Session 指的是 ZooKeeper 服务器与客户端会话。在 ZooKeeper 中，一个客户端连接是指客户端和服务器之间的一个 TCP 长连接。客户端启动的时候，首先会与服务器建立一个 TCP 连接，从第一次连接建立开始，客户端会话的生命周期也开始了。通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向Zookeeper服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的Watch事件通知。 Session的sessionTimeout值用来设置一个客户端会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在sessionTimeout规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。</p><p>在为客户端创建会话之前，服务端首先会为每个客户端都分配一个sessionID。由于 sessionID 是 Zookeeper 会话的一个重要标识，许多与会话相关的运行机制都是基于这个 sessionID 的，因此，无论是哪台服务器为客户端分配的 sessionID，都务必保证全局唯一。</p><h3 id="Znode"><a href="#Znode" class="headerlink" title="Znode"></a>Znode</h3><p>在谈到分布式的时候，我们通常说的“节点”是指组成集群的每一台机器。然而，在Zookeeper中，“节点”分为两类，第一类同样是指构成集群的机器，我们称之为机器节点；第二类则是指数据模型中的数据单元，我们称之为数据节点一一ZNode。</p><p>Zookeeper将所有数据存储在内存中，数据模型是一棵树（Znode Tree)，由斜杠（/）的进行分割的路径，就是一个Znode，例如/foo/path1。每个上都会保存自己的数据内容，同时还会保存一系列属性信息。</p><p>在Zookeeper中，node可以分为持久节点和临时节点两类。所谓持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上。而临时节点就不一样了，它的生命周期和客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。 另外，ZooKeeper还允许用户为每个节点添加一个特殊的属性：SEQUENTIAL.一旦节点被标记上这个属性，那么在这个节点被创建的时候，Zookeeper会自动在其节点名后面追加上一个整型数字，这个整型数字是一个由父节点维护的自增数字。</p><h3 id="版本"><a href="#版本" class="headerlink" title="版本"></a>版本</h3><p>在前面我们已经提到，Zookeeper 的每个 ZNode 上都会存储数据，对应于每个ZNode，Zookeeper 都会为其维护一个叫作 Stat 的数据结构，Stat 中记录了这个 ZNode 的三个数据版本，分别是version（当前ZNode的版本）、cversion（当前ZNode子节点的版本）和 aversion（当前ZNode的ACL版本）。</p><h3 id="Watcher"><a href="#Watcher" class="headerlink" title="Watcher"></a>Watcher</h3><p>Watcher（事件监听器），是Zookeeper中的一个很重要的特性。Zookeeper允许用户在指定节点上注册一些Watcher，并且在一些特定事件触发的时候，ZooKeeper服务端会将事件通知到感兴趣的客户端上去，该机制是Zookeeper实现分布式协调服务的重要特性。</p><h3 id="ACL"><a href="#ACL" class="headerlink" title="ACL"></a>ACL</h3><p>Zookeeper采用ACL（AccessControlLists）策略来进行权限控制，类似于 UNIX 文件系统的权限控制。</p><h2 id="ZooKeeper-特点"><a href="#ZooKeeper-特点" class="headerlink" title="ZooKeeper 特点"></a>ZooKeeper 特点</h2><ul><li>顺序一致性： 从同一客户端发起的事务请求，最终将会严格地按照顺序被应用到 ZooKeeper 中去。</li><li>原子性： 所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群中所有的机器都成功应用了某一个事务，要么都没有应用。</li><li>单一系统映像 ： 无论客户端连到哪一个 ZooKeeper 服务器上，其看到的服务端数据模型都是一致的。</li><li>可靠性： 一旦一次更改请求被应用，更改的结果就会被持久化，直到被下一次更改覆盖。</li></ul><h2 id="ZooKeeper-设计目标"><a href="#ZooKeeper-设计目标" class="headerlink" title="ZooKeeper 设计目标"></a>ZooKeeper 设计目标</h2><h3 id="简单的数据模型"><a href="#简单的数据模型" class="headerlink" title="简单的数据模型"></a>简单的数据模型</h3><p>ZooKeeper 允许分布式进程通过共享的层次结构命名空间进行相互协调，这与标准文件系统类似。 名称空间由 ZooKeeper 中的数据寄存器组成 - 称为znode，这些类似于文件和目录。 与为存储设计的典型文件系统不同，ZooKeeper数据保存在内存中，这意味着ZooKeeper可以实现高吞吐量和低延迟。</p><h3 id="可构建集群"><a href="#可构建集群" class="headerlink" title="可构建集群"></a>可构建集群</h3><p>为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么zookeeper本身仍然是可用的。 客户端在使用 ZooKeeper 时，需要知道集群机器列表，通过与集群中的某一台机器建立 TCP 连接来使用服务，客户端使用这个TCP链接来发送请求、获取结果、获取监听事件以及发送心跳包。如果这个连接异常断开了，客户端可以连接到另外的机器上。</p><h3 id="顺序访问"><a href="#顺序访问" class="headerlink" title="顺序访问"></a>顺序访问</h3><p>对于来自客户端的每个更新请求，ZooKeeper 都会分配一个全局唯一的递增编号，这个编号反应了所有事务操作的先后顺序，应用程序可以使用 ZooKeeper 这个特性来实现更高层次的同步原语。 这个编号也叫做时间戳——zxid（Zookeeper Transaction Id）</p><h3 id="高性能"><a href="#高性能" class="headerlink" title="高性能"></a>高性能</h3><p>ZooKeeper 是高性能的。 在“读”多于“写”的应用程序中尤其地高性能，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景。）</p><h2 id="ZooKeeper-集群角色介绍"><a href="#ZooKeeper-集群角色介绍" class="headerlink" title="ZooKeeper 集群角色介绍"></a>ZooKeeper 集群角色介绍</h2><p>最典型集群模式： Master/Slave 模式（主备模式）。在这种模式中，通常 Master服务器作为主服务器提供写服务，其他的 Slave 服务器从服务器通过异步复制的方式获取 Master 服务器最新的数据提供读服务。</p><p>但是，在 ZooKeeper 中没有选择传统的 Master/Slave 概念，而是引入了Leader、Follower 和 Observer 三种角色。</p><p>ZooKeeper 集群中的所有机器通过一个 Leader 选举过程来选定一台称为 “Leader” 的机器，Leader 既可以为客户端提供写服务又能提供读服务。除了 Leader 外，Follower 和 Observer 都只能提供读服务。Follower 和 Observer 唯一的区别在于 Observer 机器不参与 Leader 的选举过程，也不参与写操作的“过半写成功”策略，因此 Observer 机器可以在不影响写性能的情况下提升集群的读性能。</p><p>当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进人恢复模式并选举产生新的Leader服务器。这个过程大致是这样的：</p><ul><li>Leader election（选举阶段）：节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准 leader。</li><li>Discovery（发现阶段）：在这个阶段，followers 跟准 leader 进行通信，同步 followers 最近接收的事务提议。</li><li>Synchronization（同步阶段）:同步阶段主要是利用 leader 前一阶段获得的最新提议历史，同步集群中所有的副本。同步完成之后 准 leader 才会成为真正的 leader。</li><li>Broadcast（广播阶段） 到了这个阶段，Zookeeper 集群才能正式对外提供事务服务，并且 leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步。</li></ul><h2 id="ZooKeeper-amp-ZAB-协议-amp-Paxos算法"><a href="#ZooKeeper-amp-ZAB-协议-amp-Paxos算法" class="headerlink" title="ZooKeeper &amp;ZAB 协议&amp;Paxos算法"></a>ZooKeeper &amp;ZAB 协议&amp;Paxos算法</h2><h3 id="ZAB-协议-amp-Paxos算法"><a href="#ZAB-协议-amp-Paxos算法" class="headerlink" title="ZAB 协议&amp;Paxos算法"></a>ZAB 协议&amp;Paxos算法</h3><p>Paxos 算法应该可以说是 ZooKeeper 的灵魂了。但是，ZooKeeper 并没有完全采用 Paxos算法 ，而是使用 ZAB 协议作为其保证数据一致性的核心算法。另外，在ZooKeeper的官方文档中也指出，ZAB协议并不像 Paxos 算法那样，是一种通用的分布式一致性算法，它是一种特别为Zookeeper设计的崩溃可恢复的原子消息广播算法。</p><h3 id="ZAB-协议介绍"><a href="#ZAB-协议介绍" class="headerlink" title="ZAB 协议介绍"></a>ZAB 协议介绍</h3><p>ZAB（ZooKeeper Atomic Broadcast 原子广播） 协议是为分布式协调服务 ZooKeeper 专门设计的一种支持崩溃恢复的原子广播协议。 在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。</p><h3 id="ZAB-协议两种基本的模式：崩溃恢复和消息广播"><a href="#ZAB-协议两种基本的模式：崩溃恢复和消息广播" class="headerlink" title="ZAB 协议两种基本的模式：崩溃恢复和消息广播"></a>ZAB 协议两种基本的模式：崩溃恢复和消息广播</h3><p>ZAB协议包括两种基本的模式，分别是 崩溃恢复和消息广播。当整个服务框架在启动过程中，或是当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进人恢复模式并选举产生新的Leader服务器。当选举产生了新的 Leader 服务器，同时集群中已经有过半的机器与该Leader服务器完成了状态同步之后，ZAB协议就会退出恢复模式。其中，所谓的状态同步是指数据同步，用来保证集群中存在过半的机器能够和Leader服务器的数据状态保持一致。</p><p>当集群中已经有过半的Follower服务器完成了和Leader服务器的状态同步，那么整个服务框架就可以进人消息广播模式了。 当一台同样遵守ZAB协议的服务器启动后加人到集群中时，如果此时集群中已经存在一个Leader服务器在负责进行消息广播，那么新加人的服务器就会自觉地进人数据恢复模式：找到Leader所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。正如上文介绍中所说的，ZooKeeper设计成只允许唯一的一个Leader服务器来进行事务请求的处理。Leader服务器在接收到客户端的事务请求后，会生成对应的事务提案并发起一轮广播协议；而如果集群中的其他机器接收到客户端的事务请求，那么这些非Leader服务器会首先将这个事务请求转发给Leader服务器。</p>]]></content>
      
      
      <categories>
          
          <category> 中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ZooKeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>葵花宝典-Dubbo总结</title>
      <link href="/2020/04/14/2020-04-14-kui-hua-bao-dian-dubbo-zong-jie/"/>
      <url>/2020/04/14/2020-04-14-kui-hua-bao-dian-dubbo-zong-jie/</url>
      
        <content type="html"><![CDATA[<h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="Dubbo定义"><a href="#Dubbo定义" class="headerlink" title="Dubbo定义"></a>Dubbo定义</h3><p>Dubbo 是一款高性能、轻量级的开源 RPC 框架，提供服务自动注册、自动发现等高效服务治理方案， 可以和 Spring 框架无缝集成。</p><h3 id="Dubbo-的使用场景"><a href="#Dubbo-的使用场景" class="headerlink" title="Dubbo 的使用场景"></a>Dubbo 的使用场景</h3><ul><li>透明化的远程方法调用：就像调用本地方法一样调用远程方法，只需简单配置，没有任何API侵入。</li><li>软负载均衡及容错机制：可在内网替代 F5 等硬件负载均衡器，降低成本，减少单点。</li><li>服务自动注册与发现：不再需要写死服务提供方地址，注册中心基于接口名查询服务提供者的IP地址，并且能够平滑添加或删除服务提供者。</li></ul><h3 id="Dubbo-核心功能"><a href="#Dubbo-核心功能" class="headerlink" title="Dubbo 核心功能"></a>Dubbo 核心功能</h3><ul><li>Remoting：网络通信框架，提供对多种NIO框架抽象封装，包括“同步转异步”和“请求-响应”模式的信息交换方式。</li><li>Cluster：服务框架，提供基于接口方法的透明远程过程调用，包括多协议支持，以及软负载均衡，失败容错，地址路由，动态配置等集群支持。</li><li>Registry：服务注册，基于注册中心目录服务，使服务消费方能动态的查找服务提供方，使地址透明，使服务提供方可以平滑增加或减少机器。</li></ul><h3 id="Dubbo-核心组件"><a href="#Dubbo-核心组件" class="headerlink" title="Dubbo 核心组件"></a>Dubbo 核心组件</h3><p><img src="/2020/04/14/2020-04-14-kui-hua-bao-dian-dubbo-zong-jie/wp3md6h9.bmp" alt></p><ul><li>Provider：暴露服务的服务提供方</li><li>Consumer：调用远程服务消费方</li><li>Registry：服务注册与发现注册中心</li><li>Monitor：监控中心和访问调用统计</li><li>Container：服务运行容器</li></ul><h3 id="Dubbo-服务器注册与发现的流程"><a href="#Dubbo-服务器注册与发现的流程" class="headerlink" title="Dubbo 服务器注册与发现的流程"></a>Dubbo 服务器注册与发现的流程</h3><ul><li>服务容器Container负责启动，加载，运行服务提供者。</li><li>服务提供者Provider在启动时，向注册中心注册自己提供的服务。</li><li>服务消费者Consumer在启动时，向注册中心订阅自己所需的服务。</li><li>注册中心Registry返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。</li><li>服务消费者Consumer，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。</li><li>服务消费者Consumer和提供者Provider，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心Monitor。</li></ul><h2 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h2><h3 id="Dubbo-的整体架构设计分层"><a href="#Dubbo-的整体架构设计分层" class="headerlink" title="Dubbo 的整体架构设计分层"></a>Dubbo 的整体架构设计分层</h3><p><img src="/2020/04/14/2020-04-14-kui-hua-bao-dian-dubbo-zong-jie/3i2lahx5.bmp" alt></p><ul><li>接口服务层（Service）：该层与业务逻辑相关，根据 provider 和 consumer 的业务设计对应的接口和实现</li><li>配置层（Config）：对外配置接口，以 ServiceConfig 和 ReferenceConfig 为中心</li><li>服务代理层（Proxy）：服务接口透明代理，生成服务的客户端 Stub 和 服务端的 Skeleton，以 ServiceProxy 为中心，扩展接口为 ProxyFactory</li><li>服务注册层（Registry）：封装服务地址的注册和发现，以服务 URL 为中心，扩展接口为 RegistryFactory、Registry、RegistryService</li><li>路由层（Cluster）：封装多个提供者的路由和负载均衡，并桥接注册中心，以Invoker 为中心，扩展接口为 Cluster、Directory、Router 和 LoadBlancce</li><li>监控层（Monitor）：RPC 调用次数和调用时间监控，以 Statistics 为中心，扩展接口为 MonitorFactory、Monitor 和 MonitorService</li><li>远程调用层（Protocal）：封装 RPC 调用，以 Invocation 和 Result 为中心，扩展接口为 Protocal、Invoker 和 Exporter</li><li>信息交换层（Exchange）：封装请求响应模式，同步转异步。以 Request 和Response 为中心，扩展接口为 Exchanger、ExchangeChannel、ExchangeClient 和 ExchangeServer</li><li>网络传输层（Transport）：抽象 mina 和 netty 为统一接口，以 Message 为中心，扩展接口为 Channel、Transporter、Client、Server 和 Codec</li><li>数据序列化层（Serialize）：可复用的一些工具，扩展接口为 Serialization、ObjectInput、ObjectOutput 和 ThreadPool</li></ul><h3 id="Dubbo-Monitor-实现原理"><a href="#Dubbo-Monitor-实现原理" class="headerlink" title="Dubbo Monitor 实现原理"></a>Dubbo Monitor 实现原理</h3><p>Consumer 端在发起调用之前会先走 filter 链；provider 端在接收到请求时也是先走 filter 链，然后才进行真正的业务逻辑处理。默认情况下，在 consumer 和 provider 的 filter 链中都会有 Monitorfilter。</p><ol><li>MonitorFilter 向 DubboMonitor 发送数据</li><li>DubboMonitor 将数据进行聚合后（默认聚合 1min 中的统计数据）暂存到ConcurrentMap&lt;Statistics, AtomicReference&gt; statisticsMap，然后使用一个含有 3 个线程（线程名字：DubboMonitorSendTimer）的线程池每隔 1min 钟，调用 SimpleMonitorService 遍历发送 statisticsMap 中的统计数据，每发送完毕一个，就重置当前的 Statistics 的 AtomicReference</li><li>SimpleMonitorService 将这些聚合数据塞入 BlockingQueue queue 中（队列大写为 100000）</li><li>SimpleMonitorService 使用一个后台线程（线程名为：DubboMonitorAsyncWriteLogThread）将 queue 中的数据写入文件（该线程以死循环的形式来写）</li><li>SimpleMonitorService 还会使用一个含有 1 个线程（线程名字：DubboMonitorTimer）的线程池每隔 5min 钟，将文件中的统计数据画成图表</li></ol><h2 id="分布式框架"><a href="#分布式框架" class="headerlink" title="分布式框架"></a>分布式框架</h2><h3 id="Dubbo-和-Spring-Cloud-关系"><a href="#Dubbo-和-Spring-Cloud-关系" class="headerlink" title="Dubbo 和 Spring Cloud 关系"></a>Dubbo 和 Spring Cloud 关系</h3><p>Dubbo 是 SOA 时代的产物，它的关注点主要在于服务的调用，流量分发、流量监控和熔断。而 Spring Cloud 诞生于微服务架构时代，考虑的是微服务治理的方方面面，另外由于依托了 Spring、Spring Boot 的优势之上，两个框架在开始目标就不一致，Dubbo 定位服务治理、Spring Cloud 是打造一个生态。</p><h3 id="Dubbo-和-Spring-Cloud-区别"><a href="#Dubbo-和-Spring-Cloud-区别" class="headerlink" title="Dubbo 和 Spring Cloud 区别"></a>Dubbo 和 Spring Cloud 区别</h3><ul><li>Dubbo 底层是使用 Netty 这样的 NIO 框架，是基于 TCP 协议传输的，配合以 Hession 序列化完成 RPC 通信。</li><li>Spring Cloud <strong>是基于 Http 协议 Rest 接口调用远程过程的通信</strong>，相对来说 Http 请求会有更大的报文，占的带宽也会更多。但是 REST 相比 RPC 更为灵活，服务提供方和调用方的依赖只依靠一纸契约，不存在代码级别的强依赖，这在强调快速演化的微服务环境下，显得更为合适，至于注重通信速度还是方便灵活性，具体情况具体考虑。</li></ul><h3 id="Dubbo-和-Dubbox-之间的区别"><a href="#Dubbo-和-Dubbox-之间的区别" class="headerlink" title="Dubbo 和 Dubbox 之间的区别"></a>Dubbo 和 Dubbox 之间的区别</h3><p>Dubbox 是继 Dubbo 停止维护后，当当网基于 Dubbo 做的一个扩展项目，如加了服务可 Restful 调用，更新了开源组件等。</p><h2 id="注册中心"><a href="#注册中心" class="headerlink" title="注册中心"></a>注册中心</h2><h3 id="Dubbo-有哪些注册中心"><a href="#Dubbo-有哪些注册中心" class="headerlink" title="Dubbo 有哪些注册中心"></a>Dubbo 有哪些注册中心</h3><ul><li>Multicast 注册中心：Multicast 注册中心不需要任何中心节点，只要广播地址，就能进行服务注册和发现,基于网络中组播传输实现。</li><li>Zookeeper 注册中心：基于分布式协调系统 Zookeeper 实现，采用 Zookeeper 的 watch 机制实现数据变更。</li><li>Redis 注册中心：基于 Redis 实现，采用 key/map 存储，key 存储服务名和类型，map 中 key 存储服务 url，value 服务过期时间。基于 Redis 的发布/订阅模式通知数据变更。</li><li>Simple 注册中心。</li></ul><p>推荐使用 Zookeeper 作为注册中心</p><h3 id="Dubbo-的注册中心集群挂掉，发布者和订阅者之间还能通信"><a href="#Dubbo-的注册中心集群挂掉，发布者和订阅者之间还能通信" class="headerlink" title="Dubbo 的注册中心集群挂掉，发布者和订阅者之间还能通信"></a>Dubbo 的注册中心集群挂掉，发布者和订阅者之间还能通信</h3><p>启动 Dubbo 时，消费者会从 Zookeeper 拉取注册的生产者的地址接口等数据，缓存在本地。每次调用时，按照本地存储的地址进行调用。</p><h2 id="集群"><a href="#集群" class="headerlink" title="集群"></a>集群</h2><h3 id="Dubbo集群负载均衡策略"><a href="#Dubbo集群负载均衡策略" class="headerlink" title="Dubbo集群负载均衡策略"></a>Dubbo集群负载均衡策略</h3><ul><li>Random LoadBalance: 随机选取提供者策略，有利于动态调整提供者权重。截面碰撞率高，调用次数越多，分布越均匀。</li><li>RoundRobin LoadBalance: 轮循选取提供者策略，平均分布，但是存在请求累积的问题。</li><li>LeastActive LoadBalance: 最少活跃调用策略，解决慢提供者接收更少的请求。</li><li>ConstantHash LoadBalance: 一致性 Hash 策略，使相同参数请求总是发到同一提供者，一台机器宕机，可以基于虚拟节点，分摊至其他提供者，避免引起提供者的剧烈变动。</li></ul><p>默认为 Random 随机调用。</p><h3 id="Dubbo的集群容错方案"><a href="#Dubbo的集群容错方案" class="headerlink" title="Dubbo的集群容错方案"></a>Dubbo的集群容错方案</h3><ul><li>Failover Cluster：失败自动切换，当出现失败，重试其它服务器。通常用于读操作，但重试会带来更长延迟。</li><li>Failfast Cluster：快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。</li><li>Failsafe Cluster：失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作。</li><li>Failback Cluster：失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。</li><li>Forking Cluster：并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks=”2″ 来设置最大并行数。</li><li>Broadcast Cluster：广播调用所有提供者，逐个调用，任意一台报错则报错 。通常用于通知所有提供者更新缓存或日志等本地资源信息。</li></ul><p>默认的容错方案是 Failover Cluster。</p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="Dubbo-配置文件加载到-Spring-中"><a href="#Dubbo-配置文件加载到-Spring-中" class="headerlink" title="Dubbo 配置文件加载到 Spring 中"></a>Dubbo 配置文件加载到 Spring 中</h3><p>Spring 容器在启动的时候，会读取到 Spring 默认的一些 schema 以及 Dubbo 自定义的 schema，每个 schema 都会对应一个自己的 NamespaceHandler，NamespaceHandler 里面通过 BeanDefinitionParser 来解析配置信息并转化为需要加载的 bean 对象！</p><h3 id="核心配置"><a href="#核心配置" class="headerlink" title="核心配置"></a>核心配置</h3><p><img src="/2020/04/14/2020-04-14-kui-hua-bao-dian-dubbo-zong-jie/2020-05-30T15-19-20.560Z.png" alt></p><h3 id="Dubbo-超时设置方式"><a href="#Dubbo-超时设置方式" class="headerlink" title="Dubbo 超时设置方式"></a>Dubbo 超时设置方式</h3><p>Dubbo 超时设置有两种方式：</p><ul><li>服务提供者端设置超时时间，在Dubbo的用户文档中，推荐如果能在服务端多配置就尽量多配置，因为服务提供者比消费者更清楚自己提供的服务特性。</li><li>服务消费者端设置超时时间，如果在消费者端设置了超时时间，以消费者端为主，即优先级更高。因为服务调用方设置超时时间控制性更灵活。如果消费方超时，服务端线程不会定制，会产生警告。</li></ul><h3 id="服务调用超时影响"><a href="#服务调用超时影响" class="headerlink" title="服务调用超时影响"></a>服务调用超时影响</h3><p>dubbo 在调用服务不成功时，默认是会重试两次。</p><h2 id="通信协议"><a href="#通信协议" class="headerlink" title="通信协议"></a>通信协议</h2><h3 id="Dubbo-通信框架"><a href="#Dubbo-通信框架" class="headerlink" title="Dubbo 通信框架"></a>Dubbo 通信框架</h3><p>默认使用 Netty 作为通讯框架。</p><h3 id="Dubbo-支持协议以及优缺点"><a href="#Dubbo-支持协议以及优缺点" class="headerlink" title="Dubbo 支持协议以及优缺点"></a>Dubbo 支持协议以及优缺点</h3><ul><li>Dubbo： 单一长连接和 NIO 异步通讯，适合大并发小数据量的服务调用，以及消费者远大于提供者。传输协议 TCP，异步 Hessian 序列化。Dubbo推荐使用dubbo协议。</li><li>RMI： 采用 JDK 标准的 RMI 协议实现，传输参数和返回参数对象需要实现 Serializable 接口，使用 Java 标准序列化机制，使用阻塞式短连接，传输数据包大小混合，消费者和提供者个数差不多，可传文件，传输协议 TCP。 多个短连接 TCP 协议传输，同步传输，适用常规的远程服务调用和 RMI 互操作。在依赖低版本的 Common-Collections 包，Java 序列化存在安全漏洞。</li><li>WebService：基于 WebService 的远程调用协议，集成 CXF 实现，提供和原生 WebService 的互操作。多个短连接，基于 HTTP 传输，同步传输，适用系统集成和跨语言调用。</li><li>HTTP： 基于 Http 表单提交的远程调用协议，使用 Spring 的 HttpInvoke 实现。多个短连接，传输协议 HTTP，传入参数大小混合，提供者个数多于消费者，需要给应用程序和浏览器 JS 调用。</li><li>Hessian：集成 Hessian 服务，基于 HTTP 通讯，采用 Servlet 暴露服务，Dubbo 内嵌 Jetty 作为服务器时默认实现，提供与 Hession 服务互操作。多个短连接，同步 HTTP 传输，Hessian 序列化，传入参数较大，提供者大于消费者，提供者压力较大，可传文件。</li><li>Memcache：基于 Memcache实现的 RPC 协议。</li><li>Redis：基于 Redis 实现的RPC协议。</li></ul><h2 id="设计模式"><a href="#设计模式" class="headerlink" title="设计模式"></a>设计模式</h2><h3 id="Dubbo-用到的设计模式"><a href="#Dubbo-用到的设计模式" class="headerlink" title="Dubbo 用到的设计模式"></a>Dubbo 用到的设计模式</h3><p>Dubbo 框架在初始化和通信过程中使用了多种设计模式，可灵活控制类加载、权限控制等功能。</p><ul><li><p>工厂模式</p><p>  Provider 在 export 服务时，会调用 ServiceConfig 的 export 方法。ServiceConfig中有个字段：</p><pre><code>  private static final Protocol protocol =  ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension();</code></pre><p>  Dubbo 里有很多这种代码。这也是一种工厂模式，只是实现类的获取采用了 JDKSPI 的机制。这么实现的优点是可扩展性强，想要扩展实现，只需要在 classpath下增加个文件就可以了，代码零侵入。另外，像上面的 Adaptive 实现，可以做到调用时动态决定调用哪个实现，但是由于这种实现采用了动态代理，会造成代码调试比较麻烦，需要分析出实际调用的实现类。</p></li><li><p>装饰器模式</p><p>  Dubbo 在启动和调用阶段都大量使用了装饰器模式。以 Provider 提供的调用链为例，具体的调用链代码是在 ProtocolFilterWrapper 的 buildInvokerChain 完成的，具体是将注解中含有 group=provider 的 Filter 实现，按照 order 排序，最后的调用顺序是：</p><pre><code>  EchoFilter -&gt; ClassLoaderFilter -&gt; GenericFilter -&gt; ContextFilter -&gt;  ExecuteLimitFilter -&gt; TraceFilter -&gt; TimeoutFilter -&gt; MonitorFilter -&gt;  ExceptionFilter</code></pre><p>  更确切地说，这里是装饰器和责任链模式的混合使用。例如，EchoFilter 的作用是判断是否是回声测试请求，是的话直接返回内容，这是一种责任链的体现。而像ClassLoaderFilter 则只是在主功能上添加了功能，更改当前线程的 ClassLoader，这是典型的装饰器模式。</p></li><li><p>观察者模式</p><p>  Dubbo 的 Provider 启动时，需要与注册中心交互，先注册自己的服务，再订阅自己的服务，订阅时，采用了观察者模式，开启一个 listener。注册中心会每 5 秒定时检查是否有服务更新，如果有更新，向该服务的提供者发送一个 notify 消息，provider 接受到 notify 消息后，运行 NotifyListener 的 notify 方法，执行监听器方法。</p></li><li><p>动态代理模式</p><p>  Dubbo 扩展 JDK SPI 的类 ExtensionLoader 的 Adaptive 实现是典型的动态代理实现。Dubbo 需要灵活地控制实现类，即在调用阶段动态地根据参数决定调用哪个实现类，所以采用先生成代理类的方法，能够做到灵活的调用。生成代理类的代码是 ExtensionLoader 的 createAdaptiveExtensionClassCode 方法。代理类主要逻辑是，获取 URL 参数中指定参数的值作为获取实现类的 key。</p></li></ul><h2 id="运维管理"><a href="#运维管理" class="headerlink" title="运维管理"></a>运维管理</h2><h3 id="兼容旧版本"><a href="#兼容旧版本" class="headerlink" title="兼容旧版本"></a>兼容旧版本</h3><p>可以用版本号（version）过渡，多个不同版本的服务注册到注册中心，版本号不同的服务相互间不引用。这个和服务分组的概念有一点类似。</p><h3 id="Dubbo-telnet-命令"><a href="#Dubbo-telnet-命令" class="headerlink" title="Dubbo telnet 命令"></a>Dubbo telnet 命令</h3><p>dubbo 服务发布之后，我们可以利用 telnet 命令进行调试、管理。Dubbo2.0.5 以上版本服务提供端口支持 telnet 命令</p><h3 id="Dubbo-支持服务降级"><a href="#Dubbo-支持服务降级" class="headerlink" title="Dubbo 支持服务降级"></a>Dubbo 支持服务降级</h3><p>以通过 dubbo:reference 中设置 mock=“return null”。mock 的值也可以修改为 true，然后再跟接口同一个路径下实现一个 Mock 类，命名规则是 “接口名称+Mock” 后缀。然后在 Mock 类里实现自己的降级逻辑</p><h3 id="优雅停机"><a href="#优雅停机" class="headerlink" title="优雅停机"></a>优雅停机</h3><p>Dubbo 是通过 JDK 的 ShutdownHook 来完成优雅停机的，所以如果使用kill -9 PID 等强制关闭指令，是不会执行优雅停机的，只有通过 kill PID 时，才会执行。</p><h2 id="SPI"><a href="#SPI" class="headerlink" title="SPI"></a>SPI</h2><h3 id="Dubbo-SPI-和-Java-SPI-区别"><a href="#Dubbo-SPI-和-Java-SPI-区别" class="headerlink" title="Dubbo SPI 和 Java SPI 区别"></a>Dubbo SPI 和 Java SPI 区别</h3><ul><li><p>JDK SPI：</p><p>  JDK 标准的 SPI 会一次性加载所有的扩展实现，如果有的扩展很耗时，但也没用上，很浪费资源。所以只希望加载某个的实现，就不现实了</p></li><li><p>Dubbo SPI：</p><ol><li>对 Dubbo 进行扩展，不需要改动 Dubbo 的源码</li><li>延迟加载，可以一次只加载自己想要加载的扩展实现。</li><li>增加了对扩展点 IOC 和 AOP 的支持，一个扩展点可以直接 setter 注入其它扩展点。</li><li>Dubbo 的扩展机制能很好的支持第三方 IoC 容器，默认支持 Spring Bean。</li></ol></li></ul><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="Dubbo-不支持分布式事务"><a href="#Dubbo-不支持分布式事务" class="headerlink" title="Dubbo 不支持分布式事务"></a>Dubbo 不支持分布式事务</h3><ul><li>目前暂时不支持，可与通过 tcc-transaction 框架实现</li><li>介绍：tcc-transaction 是开源的 TCC 补偿性分布式事务框架</li><li>TCC-Transaction 通过 Dubbo 隐式传参的功能，避免自己对业务代码的入侵。</li></ul><h3 id="Dubbo-可以对结果进行缓存"><a href="#Dubbo-可以对结果进行缓存" class="headerlink" title="Dubbo 可以对结果进行缓存"></a>Dubbo 可以对结果进行缓存</h3><ul><li>为了提高数据访问的速度。Dubbo 提供了声明式缓存，以减少用户加缓存的工作量&lt;dubbo:reference cache=“true” /&gt;</li><li>其实比普通的配置文件就多了一个标签 cache=“true”</li></ul><h3 id="Dubbo-必须依赖的包"><a href="#Dubbo-必须依赖的包" class="headerlink" title="Dubbo 必须依赖的包"></a>Dubbo 必须依赖的包</h3><p>Dubbo 必须依赖 JDK，其他为可选。</p><h3 id="Dubbo-支持哪些序列化方式"><a href="#Dubbo-支持哪些序列化方式" class="headerlink" title="Dubbo 支持哪些序列化方式"></a>Dubbo 支持哪些序列化方式</h3><p>默认使用 Hessian 序列化，还有 Duddo、FastJson、Java 自带序列化。</p><h3 id="Dubbo-安全方面措施"><a href="#Dubbo-安全方面措施" class="headerlink" title="Dubbo 安全方面措施"></a>Dubbo 安全方面措施</h3><ul><li>Dubbo 通过 Token 令牌防止用户绕过注册中心直连，然后在注册中心上管理授权。</li><li>Dubbo 还提供服务黑白名单，来控制服务所允许的调用方。</li></ul><h3 id="服务调用是阻塞的吗"><a href="#服务调用是阻塞的吗" class="headerlink" title="服务调用是阻塞的吗"></a>服务调用是阻塞的吗</h3><p>默认是阻塞的，可以异步调用，没有返回值的可以这么做。Dubbo 是基于 NIO 的非阻塞实现并行调用，客户端不需要启动多线程即可完成并行调用多个远程服务，相对多线程开销较小，异步调用会返回一个 Future 对象。</p><h3 id="服务提供者能实现失效踢出原理"><a href="#服务提供者能实现失效踢出原理" class="headerlink" title="服务提供者能实现失效踢出原理"></a>服务提供者能实现失效踢出原理</h3><p>服务失效踢出基于 zookeeper 的临时节点原理。</p><h3 id="同一个服务多个注册的情况下可以直连某一个服务吗"><a href="#同一个服务多个注册的情况下可以直连某一个服务吗" class="headerlink" title="同一个服务多个注册的情况下可以直连某一个服务吗"></a>同一个服务多个注册的情况下可以直连某一个服务吗</h3><p>可以点对点直连，修改配置即可，也可以通过 telnet 直接某个服务。</p><h3 id="Dubbo-服务降级，失败重试怎么做"><a href="#Dubbo-服务降级，失败重试怎么做" class="headerlink" title="Dubbo 服务降级，失败重试怎么做"></a>Dubbo 服务降级，失败重试怎么做</h3><p>可以通过 dubbo:reference 中设置 mock=“return null”。mock 的值也可以修改为 true，然后再跟接口同一个路径下实现一个 Mock 类，命名规则是 “接口名称+Mock” 后缀。然后在 Mock 类里实现自己的降级逻辑</p><h2 id="RPC"><a href="#RPC" class="headerlink" title="RPC"></a>RPC</h2><h3 id="为什么要有RPC"><a href="#为什么要有RPC" class="headerlink" title="为什么要有RPC"></a>为什么要有RPC</h3><ul><li>http接口是在接口不多、系统与系统交互较少的情况下，解决信息孤岛初期常使用的一种通信手段；优点就是简单、直接、开发方便。利用现成的http协议进行传输。但是如果是一个大型的网站，内部子系统较多、接口非常多的情况下，RPC框架的好处就显示出来了，首先就是长链接，不必每次通信都要像http一样去3次握手什么的，减少了网络开销；其次就是RPC框架一般都有注册中心，有丰富的监控管理；发布、下线接口、动态扩展等，对调用方来说是无感知、统一化的操作。第三个来说就是安全性。最后就是最近流行的服务化架构、服务化治理，RPC框架是一个强力的支撑。</li><li>socket只是一个简单的网络通信方式，只是创建通信双方的通信通道，而要实现rpc的功能，还需要对其进行封装，以实现更多的功能。</li><li>RPC一般配合netty框架、spring自定义注解来编写轻量级框架，其实netty内部是封装了socket的，较新的jdk的IO一般是NIO，即非阻塞IO，在高并发网站中，RPC的优势会很明显</li></ul><h3 id="什么是RPC"><a href="#什么是RPC" class="headerlink" title="什么是RPC"></a>什么是RPC</h3><p>RPC（Remote Procedure Call Protocol）远程过程调用协议，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。简言之，RPC使得程序能够像访问本地系统资源一样，去访问远端系统资源。比较关键的一些方面包括：通讯协议、序列化、资源（接口）描述、服务框架、性能、语言支持等。</p><p><img src="/2020/04/14/2020-04-14-kui-hua-bao-dian-dubbo-zong-jie/0301.png" alt></p><p>简单的说，RPC就是从一台机器(客户端)上通过参数传递的方式调用另一台机器(服务器)上的一个函数或方法(可以统称为服务)并得到返回的结果。</p><h3 id="PRC架构组件"><a href="#PRC架构组件" class="headerlink" title="PRC架构组件"></a>PRC架构组件</h3><p>一个基本的RPC架构里面应该至少包含以下4个组件：</p><ul><li>客户端（Client）:服务调用方（服务消费者）</li><li>客户端存根（Client Stub）:存放服务端地址信息，将客户端的请求参数数据信息打包成网络消息，再通过网络传输发送给服务端</li><li>服务端存根（Server Stub）:接收客户端发送过来的请求消息并进行解包，然后再调用本地服务进行处理</li><li>服务端（Server）:服务的真正提供者</li></ul><p><img src="/2020/04/14/2020-04-14-kui-hua-bao-dian-dubbo-zong-jie/0302.png" alt></p><h3 id="RPC和SOA、SOAP、REST的区别"><a href="#RPC和SOA、SOAP、REST的区别" class="headerlink" title="RPC和SOA、SOAP、REST的区别"></a>RPC和SOA、SOAP、REST的区别</h3><ul><li><p>REST</p><p>  可以看着是HTTP协议的一种直接应用，默认基于JSON作为传输格式,使用简单,学习成本低效率高,但是安全性较低。</p></li><li><p>SOAP</p><p>  SOAP是一种数据交换协议规范,是一种轻量的、简单的、基于XML的协议的规范。而SOAP可以看着是一个重量级的协议，基于XML、SOAP在安全方面是通过使用XML-Security和XML-Signature两个规范组成了WS-Security来实现安全控制的,当前已经得到了各个厂商的支持 。易用、灵活、跨语言、跨平台。</p></li><li><p>SOA</p><p>  面向服务架构，它可以根据需求通过网络对松散耦合的粗粒度应用组件进行分布式部署、组合和使用。服务层是SOA的基础，可以直接被应用调用，从而有效控制系统中与软件代理交互的人为依赖性。</p><p>  SOA是一种粗粒度、松耦合服务架构，服务之间通过简单、精确定义接口进行通讯，不涉及底层编程接口和通讯模型。SOA可以看作是B/S模型、XML（标准通用标记语言的子集）/Web Service技术之后的自然延伸。</p></li></ul><h3 id="RPC框架需要解决的问题"><a href="#RPC框架需要解决的问题" class="headerlink" title="RPC框架需要解决的问题"></a>RPC框架需要解决的问题</h3><ul><li>如何确定客户端和服务端之间的通信协议</li><li>如何更高效地进行网络通信</li><li>服务端提供的服务如何暴露给客户端</li><li>客户端如何发现这些暴露的服务</li><li>如何更高效地对请求对象和响应结果进行序列化和反序列化操作</li></ul><h3 id="RPC的实现基础"><a href="#RPC的实现基础" class="headerlink" title="RPC的实现基础"></a>RPC的实现基础</h3><ul><li>需要有非常高效的网络通信，比如一般选择Netty作为网络通信框架</li><li>需要有比较高效的序列化框架，比如谷歌的Protobuf序列化框架</li><li>可靠的寻址方式（主要是提供服务的发现），比如可以使用Zookeeper来注册服务等等</li><li>如果是带会话（状态）的RPC调用，还需要有会话和状态保持的功能</li></ul><h3 id="RPC使用的关键技术"><a href="#RPC使用的关键技术" class="headerlink" title="RPC使用的关键技术"></a>RPC使用的关键技术</h3><ul><li><p>动态代理</p><p>  生成Client Stub（客户端存根）和Server Stub（服务端存根）的时候需要用到Java动态代理技术，可以使用JDK提供的原生的动态代理机制，也可以使用开源的：CGLib代理，Javassist字节码生成技术。</p></li><li><p>序列化和反序列化</p><p>  在网络中，所有的数据都将会被转化为字节进行传送，所以为了能够使参数对象在网络中进行传输，需要对这些参数进行序列化和反序列化操作。</p></li><li><p>NIO通信</p><p>  出于并发性能的考虑，传统的阻塞式 IO 显然不太合适，因此我们需要异步的 IO，即 NIO。Java 提供了 NIO 的解决方案，Java 7 也提供了更优秀的 NIO.2 支持。可以选择Netty或者MINA来解决NIO数据传输的问题。</p></li><li><p>服务注册中心</p><p>  可选：Redis、Zookeeper、Consul 、Etcd。一般使用ZooKeeper提供服务注册与发现功能，解决单点故障以及分布式部署的问题(注册中心)。</p></li></ul><h3 id="RPC的实现原理架构图"><a href="#RPC的实现原理架构图" class="headerlink" title="RPC的实现原理架构图"></a>RPC的实现原理架构图</h3><p><img src="/2020/04/14/2020-04-14-kui-hua-bao-dian-dubbo-zong-jie/0303.png" alt></p><p>也就是说两台服务器A，B，一个应用部署在A服务器上，想要调用B服务器上应用提供的函数/方法，由于不在一个内存空间，不能直接调用，需要通过网络来表达调用的语义和传达调用的数据。</p><p>比如说，A服务器想调用B服务器上的一个方法：</p><ol><li><p>建立通信</p><p> 首先要解决通讯的问题：即A机器想要调用B机器，首先得建立起通信连接。</p><p> 主要是通过在客户端和服务器之间建立TCP连接，远程过程调用的所有交换的数据都在这个连接里传输。连接可以是按需连接，调用结束后就断掉，也可以是长连接，多个远程过程调用共享同一个连接。</p><p> 通常这个连接可以是按需连接（需要调用的时候就先建立连接，调用结束后就立马断掉），也可以是长连接（客户端和服务器建立起连接之后保持长期持有，不管此时有无数据包的发送，可以配合心跳检测机制定期检测建立的连接是否存活有效），多个远程过程调用共享同一个连接。</p></li><li><p>服务寻址</p><p> 要解决寻址的问题，也就是说，A服务器上的应用怎么告诉底层的RPC框架，如何连接到B服务器（如主机或IP地址）以及特定的端口，方法的名称名称是什么。</p><p> 通常情况下我们需要提供B机器（主机名或IP地址）以及特定的端口，然后指定调用的方法或者函数的名称以及入参出参等信息，这样才能完成服务的一个调用。</p><p> 可靠的寻址方式（主要是提供服务的发现）是RPC的实现基石，比如可以采用Redis或者Zookeeper来注册服务等等。</p><ul><li><p>从服务提供者的角度看：<br>  当服务提供者启动的时候，需要将自己提供的服务注册到指定的注册中心，以便服务消费者能够通过服务注册中心进行查找；</p><p>  当服务提供者由于各种原因致使提供的服务停止时，需要向注册中心注销停止的服务；</p><p>  服务的提供者需要定期向服务注册中心发送心跳检测，服务注册中心如果一段时间未收到来自服务提供者的心跳后，认为该服务提供者已经停止服务，则将该服务从注册中心上去掉。</p></li><li><p>从调用者的角度看：</p><p>  服务的调用者启动的时候根据自己订阅的服务向服务注册中心查找服务提供者的地址等信息；</p><p>  当服务调用者消费的服务上线或者下线的时候，注册中心会告知该服务的调用者；</p><p>  服务调用者下线的时候，则取消订阅。</p></li></ul></li><li><p>网络传输</p><ul><li><p>序列化</p><p>  当A机器上的应用发起一个RPC调用时，调用方法和其入参等信息需要通过底层的网络协议如TCP传输到B机器，由于网络协议是基于二进制的，所有我们传输的参数数据都需要先进行序列化（Serialize）或者编组（marshal）成二进制的形式才能在网络中进行传输。然后通过寻址操作和网络传输将序列化或者编组之后的二进制数据发送给B机器。</p></li><li><p>反序列化</p><p>  当B机器接收到A机器的应用发来的请求之后，又需要对接收到的参数等信息进行反序列化操作（序列化的逆操作），即将二进制信息恢复为内存中的表达方式，然后再找到对应的方法（寻址的一部分）进行本地调用（一般是通过生成代理Proxy去调用,通常会有JDK动态代理、CGLIB动态代理、Javassist生成字节码技术等），之后得到调用的返回值。</p></li></ul></li><li><p>服务调用</p><p> B机器进行本地调用（通过代理Proxy和反射调用）之后得到了返回值，此时还需要再把返回值发送回A机器，同样也需要经过序列化操作，然后再经过网络传输将二进制数据发送回A机器，而当A机器接收到这些返回值之后，则再次进行反序列化操作，恢复为内存中的表达方式，最后再交给A机器上的应用进行相关处理（一般是业务逻辑处理操作）。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 葵花宝典 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Dubbo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>葵花宝典-Redis总结</title>
      <link href="/2020/04/12/2020-04-12-kui-hua-bao-dian-redis-zong-jie/"/>
      <url>/2020/04/12/2020-04-12-kui-hua-bao-dian-redis-zong-jie/</url>
      
        <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="Redis定义"><a href="#Redis定义" class="headerlink" title="Redis定义"></a>Redis定义</h3><p>Redis 是一个使用 C 语言写成的，开源的高性能key-value非关系缓存数据库。它支持存储的value类型相对更多，包括string(字符串)、list(链表)、set(集合)、zset(sorted set –有序集合)和hash（哈希类型）。Redis的数据都基于缓存的，所以很快，每秒可以处理超过 10万次读写操作，是已知性能最快的Key-Value DB。Redis也可以实现数据写入磁盘中，保证了数据的安全不丢失，而且Redis的操作是原子性的。</p><h3 id="Redis优缺点"><a href="#Redis优缺点" class="headerlink" title="Redis优缺点"></a>Redis优缺点</h3><ul><li>优点<ul><li>读写性能优异， Redis能读的速度是110000次/s，写的速度是81000次/s。</li><li>支持数据持久化，支持AOF和RDB两种持久化方式。</li><li>支持事务，Redis的所有操作都是原子性的，同时Redis还支持对几个操作合并后的原子性执行。</li><li>数据结构丰富，除了支持string类型的value外还支持hash、set、zset、list等数据结构。</li><li>支持主从复制，主机会自动将数据同步到从机，可以进行读写分离。</li></ul></li><li>缺点<ul><li>数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。</li><li>Redis 不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的IP才能恢复。</li><li>主机宕机，宕机前有部分数据未能及时同步到从机，切换IP后还会引入数据不一致的问题，降低了系统的可用性。</li><li>Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。</li></ul></li></ul><h3 id="使用redis有哪些好处？"><a href="#使用redis有哪些好处？" class="headerlink" title="使用redis有哪些好处？"></a>使用redis有哪些好处？</h3><ul><li>速度快，因为数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都很低</li><li>支持丰富数据类型，支持string，list，set，sorted set，hash</li><li>支持事务，操作都是原子性，所谓的原子性就是对数据的更改要么全部执行，要么全部不执行</li><li>丰富的特性：可用于缓存，消息，按key设置过期时间，过期后将会自动删除</li></ul><h3 id="为什么要用-Redis-为什么要用缓存"><a href="#为什么要用-Redis-为什么要用缓存" class="headerlink" title="为什么要用 Redis / 为什么要用缓存"></a>为什么要用 Redis / 为什么要用缓存</h3><p>主要从“高性能”和“高并发”这两点来看待这个问题。</p><ul><li><p>高性能：</p><p>  假如用户第一次访问数据库中的某些数据。这个过程会比较慢，因为是从硬盘上读取的。将该用户访问的数据存在数缓存中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可！</p></li><li><p>高并发：</p><p>  直接操作缓存能够承受的请求是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。</p></li></ul><h3 id="为什么要用-Redis-而不用-map-guava-做缓存"><a href="#为什么要用-Redis-而不用-map-guava-做缓存" class="headerlink" title="为什么要用 Redis 而不用 map/guava 做缓存?"></a>为什么要用 Redis 而不用 map/guava 做缓存?</h3><ul><li><p>缓存分为本地缓存和分布式缓存。以 Java 为例，使用自带的 map 或者 guava 实现的是本地缓存，最主要的特点是轻量以及快速，生命周期随着 jvm 的销毁而结束，并且在多实例的情况下，每个实例都需要各自保存一份缓存，缓存不具有一致性。</p></li><li><p>使用 redis 或 memcached 之类的称为分布式缓存，在多实例的情况下，各实例共用一份缓存数据，缓存具有一致性。缺点是需要保持 redis 或 memcached服务的高可用，整个程序架构上较为复杂。</p></li></ul><h3 id="Redis为什么这么快"><a href="#Redis为什么这么快" class="headerlink" title="Redis为什么这么快"></a>Redis为什么这么快</h3><ul><li>完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于 HashMap，HashMap 的优势就是查找和操作的时间复杂度都是O(1)；</li><li>数据结构简单，对数据操作也简单，Redis 中的数据结构是专门进行设计的；</li><li>采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；</li><li>使用 I/O 多路复用机制，非阻塞 IO；</li><li>使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis 直接自己构建了 VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求；</li></ul><h3 id="Redis有哪些数据类型"><a href="#Redis有哪些数据类型" class="headerlink" title="Redis有哪些数据类型"></a>Redis有哪些数据类型</h3><p><img src="/2020/04/12/2020-04-12-kui-hua-bao-dian-redis-zong-jie/0101.png" alt></p><h3 id="Redis的应用场景"><a href="#Redis的应用场景" class="headerlink" title="Redis的应用场景"></a>Redis的应用场景</h3><ul><li><p>计数器</p><p>  可以对 String 进行自增自减运算，从而实现计数器功能。Redis 这种内存型数据库的读写性能非常高，很适合存储频繁读写的计数量。</p></li><li><p>缓存</p><p>  将热点数据放到内存中，设置内存的最大使用量以及淘汰策略来保证缓存的命中率。</p></li><li><p>会话缓存</p><p>  可以使用 Redis 来统一存储多台应用服务器的会话信息。当应用服务器不再存储用户的会话信息，也就不再具有状态，一个用户可以请求任意一个应用服务器，从而更容易实现高可用性以及可伸缩性。</p></li><li><p>全页缓存（FPC）</p><p>  除基本的会话token之外，Redis还提供很简便的FPC平台。以Magento为例，Magento提供一个插件来使用Redis作为全页缓存后端。此外，对WordPress的用户来说，Pantheon有一个非常好的插件 wp-redis，这个插件能帮助你以最快速度加载你曾浏览过的页面。</p></li><li><p>查找表</p><p>  例如 DNS 记录就很适合使用 Redis 进行存储。查找表和缓存类似，也是利用了 Redis 快速的查找特性。但是查找表的内容不能失效，而缓存的内容可以失效，因为缓存不作为可靠的数据来源。</p></li><li><p>消息队列(发布/订阅功能)</p><p>  List 是一个双向链表，可以通过 lpush 和 rpop 写入和读取消息。不过最好使用 Kafka、RabbitMQ 等消息中间件。</p></li><li><p>分布式锁实现</p><p>  在分布式场景下，无法使用单机环境下的锁来对多个节点上的进程进行同步。可以使用 Redis 自带的 SETNX 命令实现分布式锁，除此之外，还可以使用官方提供的 RedLock 分布式锁实现。</p></li><li><p>其它</p><p>  Set 可以实现交集、并集等操作，从而实现共同好友等功能。ZSet 可以实现有序性操作，从而实现排行榜等功能。</p></li></ul><h3 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h3><p>持久化就是把内存的数据写到磁盘中去，防止服务宕机了内存数据丢失。</p><h3 id="Redis-的持久化机制"><a href="#Redis-的持久化机制" class="headerlink" title="Redis 的持久化机制"></a>Redis 的持久化机制</h3><p>Redis 提供两种持久化机制 RDB（默认） 和 AOF 机制</p><h4 id="RDB：是Redis-DataBase缩写快照"><a href="#RDB：是Redis-DataBase缩写快照" class="headerlink" title="RDB：是Redis DataBase缩写快照"></a>RDB：是Redis DataBase缩写快照</h4><ul><li><p>RDB是Redis默认的持久化方式。按照一定的时间将内存的数据以快照的形式保存到硬盘中，对应产生的数据文件为dump.rdb。通过配置文件中的save参数来定义快照的周期。</p><p>  <img src="/2020/04/12/2020-04-12-kui-hua-bao-dian-redis-zong-jie/041201.png" alt></p></li><li><p>优点：</p><ol><li>只有一个文件 dump.rdb，方便持久化。</li><li>容灾性好，一个文件可以保存到安全的磁盘。</li><li>性能最大化，fork 子进程来完成写操作，让主进程继续处理命令，所以是 IO 最大化。使用单独子进程来进行持久化，主进程不会进行任何 IO 操作，保证了 redis 的高性能。</li></ol></li><li><p>缺点：</p><ol><li>数据安全性低。RDB 是间隔一段时间进行持久化，如果持久化之间 redis 发生故障，会发生数据丢失。所以这种方式更适合数据要求不严谨的时候)</li></ol></li></ul><h4 id="AOF：持久化"><a href="#AOF：持久化" class="headerlink" title="AOF：持久化"></a>AOF：持久化</h4><ul><li><p>AOF持久化(即Append Only File持久化)，则是将Redis执行的每次写命令记录到单独的日志文件中，当重启Redis会重新将持久化的日志中文件恢复数据。</p></li><li><p>当两种方式同时开启时，数据恢复Redis会优先选择AOF恢复</p><p>  <img src="/2020/04/12/2020-04-12-kui-hua-bao-dian-redis-zong-jie/041202.png" alt></p></li><li><p>优点：</p><ol><li>数据安全，aof 持久化可以配置 appendfsync 属性，有 always，每进行一次 命令操作就记录到 aof 文件中一次。</li><li>通过 append 模式写文件，即使中途服务器宕机，可以通过 redis-check-aof 工具解决数据一致性问题。</li><li>AOF 机制的 rewrite 模式。AOF 文件没被 rewrite 之前（文件过大时会对命令 进行合并重写），可以删除其中的某些命令（比如误操作的 flushall）)</li></ol></li><li><p>缺点：</p><ol><li>AOF 文件比 RDB 文件大，且恢复速度慢。</li><li>数据集大的时候，比 RDB 启动效率低。</li></ol></li></ul><h3 id="俩种持久化的优缺点对比"><a href="#俩种持久化的优缺点对比" class="headerlink" title="俩种持久化的优缺点对比"></a>俩种持久化的优缺点对比</h3><ul><li>AOF文件比RDB更新频率高，优先使用AOF还原数据。</li><li>AOF比RDB更安全也更大</li><li>RDB性能比AOF好</li><li>如果两个都配了优先加载AOF</li></ul><h3 id="如何选择合适的持久化方式"><a href="#如何选择合适的持久化方式" class="headerlink" title="如何选择合适的持久化方式"></a>如何选择合适的持久化方式</h3><ul><li>一般来说， 如果想达到足以媲美PostgreSQL的数据安全性，你应该同时使用两种持久化功能。在这种情况下，当 Redis 重启的时候会优先载入AOF文件来恢复原始的数据，因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整。</li><li>如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失，那么你可以只使用RDB持久化。</li><li>有很多用户都只使用AOF持久化，但并不推荐这种方式，因为定时生成RDB快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比AOF恢复的速度要快，除此之外，使用RDB还可以避免AOF程序的bug。</li></ul><h3 id="Redis持久化数据和缓存怎么做扩容"><a href="#Redis持久化数据和缓存怎么做扩容" class="headerlink" title="Redis持久化数据和缓存怎么做扩容"></a>Redis持久化数据和缓存怎么做扩容</h3><ul><li>如果Redis被当做缓存使用，使用一致性哈希实现动态扩容缩容。</li><li>如果Redis被当做一个持久化存储使用，必须使用固定的keys-to-nodes映射关系，节点的数量一旦确定不能变化。否则的话(即Redis节点需要动态变化的情况），必须使用可以在运行时进行数据再平衡的一套系统，而当前只有Redis集群可以做到这样。</li></ul><h3 id="Redis的过期键的删除策略"><a href="#Redis的过期键的删除策略" class="headerlink" title="Redis的过期键的删除策略"></a>Redis的过期键的删除策略</h3><p>Redis中同时使用了定期过期和惰性过期两种过期策略。</p><ul><li>定期过期：每隔一定的时间，会扫描一定数量的数据库的expires字典中一定数量的key，并清除其中已过期的key。该策略是前两者的一个折中方案。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得CPU和内存资源达到最优的平衡效果。(expires字典会保存所有设置了过期时间的key的过期时间数据，其中，key是指向键空间中的某个键的指针，value是该键的毫秒精度的UNIX时间戳表示的过期时间。键空间是指该Redis集群中保存的所有键。)</li><li>惰性过期：只有当访问一个key时，才会判断该key是否已过期，过期则清除。该策略可以最大化地节省CPU资源，却对内存非常不友好。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，占用大量内存。</li></ul><h3 id="MySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据"><a href="#MySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据" class="headerlink" title="MySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据"></a>MySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据</h3><p>redis内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略。</p><h3 id="Redis的内存淘汰策略"><a href="#Redis的内存淘汰策略" class="headerlink" title="Redis的内存淘汰策略"></a>Redis的内存淘汰策略</h3><p>Redis的内存淘汰策略是指在Redis的用于缓存的内存不足时，怎么处理需要新写入且需要申请额外空间的数据。</p><ul><li><p>全局的键空间选择性移除</p><ul><li>noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。</li><li>allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。</li><li>allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。</li></ul></li><li><p>设置过期时间的键空间选择性移除</p><ul><li>volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。</li><li>volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。</li><li>volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。</li></ul></li><li><p>总结</p><p>  Redis的内存淘汰策略的选取并不会影响过期的key的处理。内存淘汰策略用于处理内存不足时的需要申请额外空间的数据；过期策略用于处理过期的缓存数据。</p></li></ul><h3 id="Redis主要消耗什么物理资源"><a href="#Redis主要消耗什么物理资源" class="headerlink" title="Redis主要消耗什么物理资源"></a>Redis主要消耗什么物理资源</h3><p>内存</p><h3 id="Redis的内存用完了会发生什么"><a href="#Redis的内存用完了会发生什么" class="headerlink" title="Redis的内存用完了会发生什么"></a>Redis的内存用完了会发生什么</h3><p>如果达到设置的上限，Redis的写命令会返回错误信息（但是读命令还可以正常返回。）或者你可以配置内存淘汰机制，当Redis达到内存上限时会冲刷掉旧的内容。</p><h2 id="线程模型"><a href="#线程模型" class="headerlink" title="线程模型"></a>线程模型</h2><h3 id="Redis线程模型"><a href="#Redis线程模型" class="headerlink" title="Redis线程模型"></a>Redis线程模型</h3><p>Redis基于Reactor模式开发了网络事件处理器，这个处理器被称为文件事件处理器（file event handler）。它的组成结构为4部分：多个套接字、IO多路复用程序、文件事件分派器、事件处理器。因为文件事件分派器队列的消费是单线程的，所以Redis才叫单线程模型。</p>]]></content>
      
      
      <categories>
          
          <category> 葵花宝典 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>葵花宝典-JAVA集合</title>
      <link href="/2020/04/10/2020-04-10-kui-hua-bao-dian-java-ji-he/"/>
      <url>/2020/04/10/2020-04-10-kui-hua-bao-dian-java-ji-he/</url>
      
        <content type="html"><![CDATA[<h2 id="集合容器概述"><a href="#集合容器概述" class="headerlink" title="集合容器概述"></a>集合容器概述</h2><h3 id="集合的特点"><a href="#集合的特点" class="headerlink" title="集合的特点"></a>集合的特点</h3><ul><li>集合的特点主要有如下两点：<ul><li>集合用于存储对象的容器，对象是用来封装数据，对象多了也需要存储集中式管理</li><li>和数组对比对象的大小不确定。因为集合是可变长度的。数组需要提前定义大小</li></ul></li></ul><h3 id="集合和数组的区别"><a href="#集合和数组的区别" class="headerlink" title="集合和数组的区别"></a>集合和数组的区别</h3><ul><li>数组是固定长度的；集合可变长度的。</li><li>数组可以存储基本数据类型，也可以存储引用数据类型；集合只能存储引用数据类型。</li><li>数组存储的元素必须是同一个数据类型；集合存储的对象可以是不同数据类型。</li></ul><h3 id="常用的集合类有哪些？"><a href="#常用的集合类有哪些？" class="headerlink" title="常用的集合类有哪些？"></a>常用的集合类有哪些？</h3><p>Map接口和Collection接口是所有集合框架的父接口：</p><ul><li>Collection接口的子接口包括：Set接口和List接口</li><li>Map接口的实现类主要有：HashMap、TreeMap、Hashtable、ConcurrentHashMap以及Properties等</li><li>Set接口的实现类主要有：HashSet、LinkedHashSet、TreeSet等</li><li>List接口的实现类主要有：ArrayList、LinkedList、Vector以及Stack等</li></ul><h3 id="List，Set，Map三者的区别？"><a href="#List，Set，Map三者的区别？" class="headerlink" title="List，Set，Map三者的区别？"></a>List，Set，Map三者的区别？</h3><p><img src="/2020/04/10/2020-04-10-kui-hua-bao-dian-java-ji-he/0001.png" alt></p><ul><li>Java 容器分为 Collection 和 Map 两大类，Collection集合的子接口有Set、List、Queue三种子接口。我们比较常用的是Set、List，Map接口不是collection的子接口。</li><li>Collection集合主要有List和Set两大接口<ul><li>List：一个有序（元素存入集合的顺序和取出的顺序一致）容器，元素可以重复，可以插入多个null元素，元素都有索引。常用的实现类有 ArrayList、LinkedList 和 Vector。</li><li>Set：一个无序（存入和取出顺序有可能不一致）容器，不可以存储重复元素，只允许存入一个null元素，必须保证元素唯一性。Set 接口常用实现类是 HashSet、LinkedHashSet 以及 TreeSet。</li></ul></li><li>Map是一个键值对集合，存储键、值和之间的映射。 Key无序，唯一；value 不要求有序，允许重复。Map没有继承于Collection接口，从Map集合中检索元素时，只要给出键对象，就会返回对应的值对象。<ul><li>Map 的常用实现类：HashMap、TreeMap、HashTable、LinkedHashMap、ConcurrentHashMap</li></ul></li></ul><h2 id="集合框架底层数据结构"><a href="#集合框架底层数据结构" class="headerlink" title="集合框架底层数据结构"></a>集合框架底层数据结构</h2><h3 id="Collection"><a href="#Collection" class="headerlink" title="Collection"></a>Collection</h3><ul><li>List<ul><li>Arraylist： Object数组</li><li>Vector： Object数组</li><li>LinkedList： 双向循环链表</li></ul></li><li>Set<ul><li>HashSet（无序，唯一）：基于 HashMap 实现的，底层采用 HashMap 来保存元素</li><li>LinkedHashSet： LinkedHashSet 继承与 HashSet，并且其内部是通过 LinkedHashMap 来实现的。有点类似于我们之前说的LinkedHashMap 其内部是基于 Hashmap 实现一样，不过还是有一点点区别的。</li><li>TreeSet（有序，唯一）： 红黑树(自平衡的排序二叉树。)</li></ul></li></ul><h3 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h3><ul><li>HashMap： JDK1.8之前HashMap由数组+链表组成的，数组是HashMap的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）.JDK1.8以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间.</li><li>LinkedHashMap：LinkedHashMap 继承自 HashMap，所以它的底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成。另外，LinkedHashMap 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。</li><li>HashTable： 数组+链表组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的</li><li>TreeMap： 红黑树（自平衡的排序二叉树）</li></ul><h3 id="哪些集合类是线程安全的？"><a href="#哪些集合类是线程安全的？" class="headerlink" title="哪些集合类是线程安全的？"></a>哪些集合类是线程安全的？</h3><ul><li>Vector：就比Arraylist多了个 synchronized （线程安全），因为效率较低，现在已经不太建议使用。</li><li>hashTable：就比hashMap多了个synchronized (线程安全)，不建议使用。</li><li>ConcurrentHashMap：是Java5中支持高并发、高吞吐量的线程安全HashMap实现。它由Segment数组结构和HashEntry数组结构组成。Segment数组在ConcurrentHashMap里扮演锁的角色，HashEntry则用于存储键-值对数据。一个ConcurrentHashMap里包含一个Segment数组，Segment的结构和HashMap类似，是一种数组和链表结构；一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元素；每个Segment守护着一个HashEntry数组里的元素，当对HashEntry数组的数据进行修改时，必须首先获得它对应的Segment锁。（推荐使用）</li></ul><h3 id="Java集合的快速失败机制-“fail-fast”？"><a href="#Java集合的快速失败机制-“fail-fast”？" class="headerlink" title="Java集合的快速失败机制 “fail-fast”？"></a>Java集合的快速失败机制 “fail-fast”？</h3><p>是java集合的一种错误检测机制，当多个线程对集合进行结构上的改变的操作时，有可能会产生 fail-fast 机制。</p><p>例如：假设存在两个线程（线程1、线程2），线程1通过Iterator在遍历集合A中的元素，在某个时候线程2修改了集合A的结构（是结构上面的修改，而不是简单的修改集合元素的内容），那么这个时候程序就会抛出 ConcurrentModificationException 异常，从而产生fail-fast机制。</p><p>原因：迭代器在遍历时直接访问集合中的内容，并且在遍历过程中使用一个 modCount 变量。集合在被遍历期间如果内容发生变化，就会改变modCount的值。每当迭代器使用hashNext()/next()遍历下一个元素之前，都会检测modCount变量是否为expectedmodCount值，是的话就返回遍历；否则抛出异常，终止遍历。</p><p>解决办法：</p><ol><li>在遍历过程中，所有涉及到改变modCount值得地方全部加上synchronized。</li><li>使用CopyOnWriteArrayList来替换ArrayList</li></ol><h3 id="怎么确保一个集合不能被修改？"><a href="#怎么确保一个集合不能被修改？" class="headerlink" title="怎么确保一个集合不能被修改？"></a>怎么确保一个集合不能被修改？</h3><p>可以使用 Collections. unmodifiableCollection(Collection c) 方法来创建一个只读集合，这样改变集合的任何操作都会抛出 Java. lang. UnsupportedOperationException 异常。</p><p>示例代码如下：</p><pre><code>List&lt;String&gt; list = new ArrayList&lt;&gt;();list. add(&quot;x&quot;);Collection&lt;String&gt; clist = Collections. unmodifiableCollection(list);clist. add(&quot;y&quot;); // 运行时此行报错System. out. println(list. size());</code></pre><h2 id="List接口"><a href="#List接口" class="headerlink" title="List接口"></a>List接口</h2><h3 id="迭代器-Iterator-是什么？"><a href="#迭代器-Iterator-是什么？" class="headerlink" title="迭代器 Iterator 是什么？"></a>迭代器 Iterator 是什么？</h3><p>Iterator 接口提供遍历任何 Collection 的接口。我们可以从一个 Collection 中使用迭代器方法来获取迭代器实例。迭代器取代了 Java 集合框架中的 Enumeration，迭代器允许调用者在迭代过程中移除元素。</p><p>因为所有Collection接继承了Iterator迭代器</p><p><img src="/2020/04/10/2020-04-10-kui-hua-bao-dian-java-ji-he/0002.png" alt></p><h3 id="Iterator-怎么使用？有什么特点？"><a href="#Iterator-怎么使用？有什么特点？" class="headerlink" title="Iterator 怎么使用？有什么特点？"></a>Iterator 怎么使用？有什么特点？</h3><p>Iterator 使用代码如下：</p><pre><code>List&lt;String&gt; list = new ArrayList&lt;&gt;();Iterator&lt;String&gt; it = list. iterator();while(it. hasNext()){  String obj = it. next();  System. out. println(obj);}</code></pre><p>Iterator 的特点是只能单向遍历，但是更加安全，因为它可以确保，在当前遍历的集合元素被更改的时候，就会抛出 ConcurrentModificationException 异常。</p><h3 id="如何边遍历边移除-Collection-中的元素？"><a href="#如何边遍历边移除-Collection-中的元素？" class="headerlink" title="如何边遍历边移除 Collection 中的元素？"></a>如何边遍历边移除 Collection 中的元素？</h3><p>边遍历边修改 Collection 的唯一正确方式是使用 Iterator.remove() 方法，如下：</p><pre><code>Iterator&lt;Integer&gt; it = list.iterator();while(it.hasNext()){   *// do something*   it.remove();}</code></pre><p>一种最常见的错误代码如下：</p><pre><code>for(Integer i : list){   list.remove(i)}</code></pre><p>运行以上错误代码会报 ConcurrentModificationException 异常。这是因为当使用 foreach(for(Integer i : list)) 语句时，会自动生成一个iterator 来遍历该 list，但同时该 list 正在被 Iterator.remove() 修改。Java 一般不允许一个线程在遍历 Collection 时另一个线程修改它。</p><h3 id="Iterator-和-ListIterator-有什么区别？"><a href="#Iterator-和-ListIterator-有什么区别？" class="headerlink" title="Iterator 和 ListIterator 有什么区别？"></a>Iterator 和 ListIterator 有什么区别？</h3><ul><li>Iterator 可以遍历 Set 和 List 集合，而 ListIterator 只能遍历 List。</li><li>Iterator 只能单向遍历，而 ListIterator 可以双向遍历（向前/后遍历）。</li><li>ListIterator 实现 Iterator 接口，然后添加了一些额外的功能，比如添加一个元素、替换一个元素、获取前面或后面元素的索引位置。</li></ul><h3 id="遍历一个-List-有哪些不同的方式？每种方法的实现原理是什么？Java-中-List-遍历的最佳实践是什么？"><a href="#遍历一个-List-有哪些不同的方式？每种方法的实现原理是什么？Java-中-List-遍历的最佳实践是什么？" class="headerlink" title="遍历一个 List 有哪些不同的方式？每种方法的实现原理是什么？Java 中 List 遍历的最佳实践是什么？"></a>遍历一个 List 有哪些不同的方式？每种方法的实现原理是什么？Java 中 List 遍历的最佳实践是什么？</h3><p>遍历方式有以下几种：</p><ul><li>for 循环遍历，基于计数器。在集合外部维护一个计数器，然后依次读取每一个位置的元素，当读取到最后一个元素后停止。</li><li>迭代器遍历，Iterator。Iterator 是面向对象的一个设计模式，目的是屏蔽不同数据集合的特点，统一遍历集合的接口。Java 在 Collections 中支持了 Iterator 模式。</li><li>foreach 循环遍历。foreach 内部也是采用了 Iterator 的方式实现，使用时不需要显式声明 Iterator 或计数器。优点是代码简洁，不易出错；缺点是只能做简单的遍历，不能在遍历过程中操作数据集合，例如删除、替换。</li></ul><p>最佳实践：Java Collections 框架中提供了一个 RandomAccess 接口，用来标记 List 实现是否支持 Random Access。</p><ul><li>如果一个数据集合实现了该接口，就意味着它支持 Random Access，按位置读取元素的平均时间复杂度为 O(1)，如ArrayList。</li><li>如果没有实现该接口，表示不支持 Random Access，如LinkedList。</li><li><strong>推荐的做法就是，支持 Random Access 的列表可用 for 循环遍历，否则建议用 Iterator 或 foreach 遍历。</strong></li></ul><h3 id="（重要）说一下-ArrayList-的优缺点"><a href="#（重要）说一下-ArrayList-的优缺点" class="headerlink" title="（重要）说一下 ArrayList 的优缺点"></a>（重要）说一下 ArrayList 的优缺点</h3><p>ArrayList的优点如下：</p><ul><li>ArrayList 底层以数组实现，是一种随机访问模式。ArrayList 实现了 RandomAccess 接口，因此查找的时候非常快。</li><li>ArrayList 在顺序添加一个元素的时候非常方便。</li></ul><p>ArrayList 的缺点如下：</p><ul><li>删除元素的时候，需要做一次元素复制操作。如果要复制的元素很多，那么就会比较耗费性能。</li><li>插入元素的时候，也需要做一次元素复制操作，缺点同上。</li></ul><p>ArrayList 比较适合顺序添加、随机访问的场景。</p><h3 id="如何实现数组和-List-之间的转换？"><a href="#如何实现数组和-List-之间的转换？" class="headerlink" title="如何实现数组和 List 之间的转换？"></a>如何实现数组和 List 之间的转换？</h3><p>数组转 List：使用 Arrays.asList(array) 进行转换。</p><p>List 转数组：使用 List 自带的 toArray() 方法。</p><pre><code>// list to arrayList&lt;String&gt; list = new ArrayList&lt;String&gt;();list.add(&quot;123&quot;);list.add(&quot;456&quot;);list.toArray();// array to listString[] array = new String[]{&quot;123&quot;,&quot;456&quot;};Arrays.asList(array);</code></pre><h3 id="（重要）ArrayList-和-LinkedList-的区别是什么？"><a href="#（重要）ArrayList-和-LinkedList-的区别是什么？" class="headerlink" title="（重要）ArrayList 和 LinkedList 的区别是什么？"></a>（重要）ArrayList 和 LinkedList 的区别是什么？</h3><ul><li>数据结构实现：ArrayList 是动态数组的数据结构实现，而 LinkedList 是双向链表的数据结构实现。</li><li>随机访问效率：ArrayList 比 LinkedList 在随机访问的时候效率要高，因为 LinkedList 是线性的数据存储方式，所以需要移动指针从前往后依次查找。</li><li>增加和删除效率：在非首尾的增加和删除操作，LinkedList 要比 ArrayList 效率要高，因为 ArrayList 增删操作要影响数组内的其他数据的下标。</li><li>内存空间占用：LinkedList 比 ArrayList 更占内存，因为 LinkedList 的节点除了存储数据，还存储了两个引用，一个指向前一个元素，一个指向后一个元素。</li><li>线程安全：ArrayList 和 LinkedList 都是不同步的，也就是不保证线程安全；</li><li>综合来说，在需要频繁读取集合中的元素时，更推荐使用 ArrayList，而在插入和删除操作较多时，更推荐使用 LinkedList。</li><li>LinkedList 的双向链表也叫双链表，是链表的一种，它的每个数据结点中都有两个指针，分别指向直接后继和直接前驱。所以，从双向链表中的任意一个结点开始，都可以很方便地访问它的前驱结点和后继结点。</li></ul><h3 id="（重要）ArrayList-和-Vector-的区别是什么？"><a href="#（重要）ArrayList-和-Vector-的区别是什么？" class="headerlink" title="（重要）ArrayList 和 Vector 的区别是什么？"></a>（重要）ArrayList 和 Vector 的区别是什么？</h3><ul><li>这两个类都实现了 List 接口（List 接口继承了 Collection 接口），他们都是有序集合<ul><li>线程安全：Vector 使用了 Synchronized 来实现线程同步，是线程安全的，而 ArrayList 是非线程安全的。</li><li>性能：ArrayList 在性能方面要优于 Vector。</li><li>扩容：ArrayList 和 Vector 都会根据实际的需要动态的调整容量，只不过在 Vector 扩容每次会增加 1 倍，而 ArrayList 只会增加 50%。</li></ul></li><li>Vector类的所有方法都是同步的。可以由两个线程安全地访问一个Vector对象、但是一个线程访问Vector的话代码要在同步操作上耗费大量的时间。</li><li>Arraylist不是同步的，所以在不需要保证线程安全时时建议使用Arraylist。</li></ul><h3 id="插入数据时，ArrayList、LinkedList、Vector谁速度较快？阐述-ArrayList、Vector、LinkedList-的存储性能和特性？"><a href="#插入数据时，ArrayList、LinkedList、Vector谁速度较快？阐述-ArrayList、Vector、LinkedList-的存储性能和特性？" class="headerlink" title="插入数据时，ArrayList、LinkedList、Vector谁速度较快？阐述 ArrayList、Vector、LinkedList 的存储性能和特性？"></a>插入数据时，ArrayList、LinkedList、Vector谁速度较快？阐述 ArrayList、Vector、LinkedList 的存储性能和特性？</h3><ul><li>ArrayList和Vector 底层的实现都是使用数组方式存储数据。数组元素数大于实际存储的数据以便增加和插入元素，它们都允许直接按序号索引元素，但是插入元素要涉及数组元素移动等内存操作，所以索引数据快而插入数据慢。</li><li>Vector 中的方法由于加了 synchronized 修饰，因此 Vector 是线程安全容器，但性能上较ArrayList差。</li><li>LinkedList 使用双向链表实现存储，按序号索引数据需要进行前向或后向遍历，但插入数据时只需要记录当前项的前后项即可，所以 LinkedList 插入速度较快。</li></ul><h3 id="多线程场景下如何使用-ArrayList？"><a href="#多线程场景下如何使用-ArrayList？" class="headerlink" title="多线程场景下如何使用 ArrayList？"></a>多线程场景下如何使用 ArrayList？</h3><p>ArrayList 不是线程安全的，如果遇到多线程场景，可以通过 Collections 的 synchronizedList 方法将其转换成线程安全的容器后再使用。例如像下面这样：</p><pre><code>List&lt;String&gt; synchronizedList = Collections.synchronizedList(list);synchronizedList.add(&quot;aaa&quot;);synchronizedList.add(&quot;bbb&quot;);for (int i = 0; i &lt; synchronizedList.size(); i++) {    System.out.println(synchronizedList.get(i));}</code></pre><h3 id="为什么-ArrayList-的-elementData-加上-transient-修饰？"><a href="#为什么-ArrayList-的-elementData-加上-transient-修饰？" class="headerlink" title="为什么 ArrayList 的 elementData 加上 transient 修饰？"></a>为什么 ArrayList 的 elementData 加上 transient 修饰？</h3><p>ArrayList 中的数组定义： </p><pre><code>private transient Object[] elementData;</code></pre><p>再看一下 ArrayList 的定义：</p><pre><code>public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt;     implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable</code></pre><p>可以看到 ArrayList 实现了 Serializable 接口，这意味着 ArrayList 支持序列化。transient 的作用是说不希望 elementData 数组被序列化，重写了 writeObject 实现：</p><pre><code>private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException{    *// Write out element count, and any hidden stuff*        int expectedModCount = modCount;    s.defaultWriteObject();    *// Write out array length*        s.writeInt(elementData.length);    *// Write out all elements in the proper order.*        for (int i=0; i&lt;size; i++)            s.writeObject(elementData[i]);    if (modCount != expectedModCount) {        throw new ConcurrentModificationException();}</code></pre><p>每次序列化时，先调用 defaultWriteObject() 方法序列化 ArrayList 中的非 transient 元素，然后遍历 elementData，只序列化已存入的元素，这样既加快了序列化的速度，又减小了序列化之后的文件大小。</p><h3 id="List-和-Set-的区别"><a href="#List-和-Set-的区别" class="headerlink" title="List 和 Set 的区别"></a>List 和 Set 的区别</h3><ul><li>List , Set 都是继承自Collection 接口</li><li>List 特点：一个有序（元素存入集合的顺序和取出的顺序一致）容器，元素可以重复，可以插入多个null元素，元素都有索引。常用的实现类有 ArrayList、LinkedList 和 Vector。</li><li>Set 特点：一个无序（存入和取出顺序有可能不一致）容器，不可以存储重复元素，只允许存入一个null元素，必须保证元素唯一性。Set 接口常用实现类是 HashSet、LinkedHashSet 以及 TreeSet。</li><li>另外 List 支持for循环，也就是通过下标来遍历，也可以用迭代器，但是set只能用迭代，因为他无序，无法用下标来取得想要的值。</li><li>Set和List对比<ul><li>Set：检索元素效率低下，删除和插入效率高，插入和删除不会引起元素位置改变。</li><li>List：和数组类似，List可以动态增长，查找元素效率高，插入删除元素效率低，因为会引起其他元素位置改变</li></ul></li></ul><h2 id="Set接口"><a href="#Set接口" class="headerlink" title="Set接口"></a>Set接口</h2><h3 id="HashSet的实现原理"><a href="#HashSet的实现原理" class="headerlink" title="HashSet的实现原理"></a>HashSet的实现原理</h3><p>HashSet 是基于 HashMap 实现的，HashSet的值存放于HashMap的key上，HashMap的value统一为present，因此 HashSet 的实现比较简单，相关 HashSet 的操作，基本上都是直接调用底层 HashMap 的相关方法来完成，HashSet 不允许重复的值。</p><h3 id="HashSet如何检查重复？HashSet是如何保证数据不可重复的？"><a href="#HashSet如何检查重复？HashSet是如何保证数据不可重复的？" class="headerlink" title="HashSet如何检查重复？HashSet是如何保证数据不可重复的？"></a>HashSet如何检查重复？HashSet是如何保证数据不可重复的？</h3><ul><li>向HashSet 中add ()元素时，判断元素是否存在的依据，不仅要比较hash值，同时还要结合equles 方法比较。</li><li>HashSet 中的add ()方法会使用HashMap 的put()方法。</li><li>HashMap 的 key 是唯一的，由源码可以看出 HashSet 添加进去的值就是作为HashMap 的key，并且在HashMap中如果K/V相同时，会用新的V覆盖掉旧的V，然后返回旧的V。所以不会重复（ HashMap 比较key是否相等是先比较hashcode 再比较equals ）。</li></ul><p>以下是HashSet 部分源码：</p><pre><code>private static final Object PRESENT = new Object();private transient HashMap&lt;E,Object&gt; map;public HashSet() {    map = new HashMap&lt;&gt;();}public boolean add(E e) {    // 调用HashMap的put方法,PRESENT是一个至始至终都相同的虚值    return map.put(e, PRESENT)==null;}</code></pre><h3 id="hashCode（）与equals（）的相关规定："><a href="#hashCode（）与equals（）的相关规定：" class="headerlink" title="hashCode（）与equals（）的相关规定："></a>hashCode（）与equals（）的相关规定：</h3><ul><li>如果两个对象相等，则hashcode一定也是相同的 （hashCode是jdk根据对象的地址或者字符串或者数字算出来的int类型的数值）</li><li>两个对象相等,对两个equals方法返回true</li><li>两个对象有相同的hashcode值，它们不一定是相等的</li><li>综上，equals方法被覆盖过，则hashCode方法也必须被覆盖</li><li>hashCode()的默认行为是对堆上的对象产生独特值。如果没有重写hashCode()，则该class的两个对象无论如何都不会相等（即使这两个对象指向相同的数据）。</li></ul><h3 id="与equals的区别"><a href="#与equals的区别" class="headerlink" title="==与equals的区别"></a>==与equals的区别</h3><ul><li>==是判断两个变量或实例是不是指向同一个内存空间 equals是判断两个变量或实例所指向的内存空间的值是不是相同</li><li>==是指对内存地址进行比较 equals()是对字符串的内容进行比较</li></ul><h3 id="HashSet与HashMap的区别"><a href="#HashSet与HashMap的区别" class="headerlink" title="HashSet与HashMap的区别"></a>HashSet与HashMap的区别</h3><p><img src="/2020/04/10/2020-04-10-kui-hua-bao-dian-java-ji-he/0003.png" alt></p><h2 id="Map接口"><a href="#Map接口" class="headerlink" title="Map接口"></a>Map接口</h2><h3 id="什么是Hash算法"><a href="#什么是Hash算法" class="headerlink" title="什么是Hash算法"></a>什么是Hash算法</h3><p>哈希算法是指把任意长度的二进制映射为固定长度的较小的二进制值，这个较小的二进制值叫做哈希值。</p><h3 id="什么是链表"><a href="#什么是链表" class="headerlink" title="什么是链表"></a>什么是链表</h3><p>链表是可以将物理地址上不连续的数据连接起来，通过指针来对物理地址进行操作，实现增删改查等功能。</p><p>链表大致分为单链表和双向链表</p><ul><li><p>单链表:每个节点包含两部分,一部分存放数据变量的data,另一部分是指向下一节点的next指针</p><p>  <img src="/2020/04/10/2020-04-10-kui-hua-bao-dian-java-ji-he/0004.png" alt></p></li><li><p>双向链表:除了包含单链表的部分,还增加的pre前一个节点的指针</p><p>  <img src="/2020/04/10/2020-04-10-kui-hua-bao-dian-java-ji-he/0005.png" alt></p></li></ul><p>链表的优点</p><ul><li>插入删除速度快（因为有next指针指向其下一个节点，通过改变指针的指向可以方便的增加删除元素）</li><li>内存利用率高，不会浪费内存（可以使用内存中细小的不连续空间（大于node节点的大小），并且在需要空间的时候才创建空间）</li><li>大小没有固定，拓展很灵活</li></ul><p>链表的缺点</p><ul><li>不能随机查找，必须从第一个开始遍历，查找效率低</li></ul><h3 id="说一下HashMap的实现原理？"><a href="#说一下HashMap的实现原理？" class="headerlink" title="说一下HashMap的实现原理？"></a>说一下HashMap的实现原理？</h3><p>HashMap概述： HashMap是基于哈希表的Map接口的非同步实现。此实现提供所有可选的映射操作，并允许使用null值和null键。此类不保证映射的顺序，特别是它不保证该顺序恒久不变。</p><p>HashMap的数据结构： 在Java编程语言中，最基本的结构就是两种，一个是数组，另外一个是模拟指针（引用），所有的数据结构都可以用这两个基本结构来构造的，HashMap也不例外。<strong>HashMap实际上是一个“链表散列”的数据结构，即数组和链表的结合体。</strong></p><p>HashMap 基于 Hash 算法实现的</p><ul><li><p>当我们往HashMap中put元素时，利用key的hashCode重新hash计算出当前对象的元素在数组中的下标</p></li><li><p>存储时，如果出现hash值相同的key，此时有两种情况。</p><ul><li>(1)如果key相同，则覆盖原始值；</li><li>​ (2)如果key不同（出现冲突），则将当前的key-value放入链表中</li></ul></li><li><p>获取时，直接找到hash值对应的下标，在进一步判断key是否相同，从而找到对应值。</p></li><li><p>理解了以上过程就不难明白HashMap是如何解决hash冲突的问题，核心就是使用了数组的存储方式，然后将冲突的key的对象放入链表中，一旦发现冲突就在链表中做进一步的对比。</p></li><li><p>需要注意Jdk 1.8中对HashMap的实现做了优化，当链表中的节点数据超过八个之后，该链表会转为红黑树来提高查询效率，从原来的O(n)到O(logn)</p></li></ul><h3 id="（重要）HashMap在JDK1-7和JDK1-8中有哪些不同？HashMap的底层实现"><a href="#（重要）HashMap在JDK1-7和JDK1-8中有哪些不同？HashMap的底层实现" class="headerlink" title="（重要）HashMap在JDK1.7和JDK1.8中有哪些不同？HashMap的底层实现"></a>（重要）HashMap在JDK1.7和JDK1.8中有哪些不同？HashMap的底层实现</h3><p>在Java中，保存数据有两种比较简单的数据结构：数组和链表。数组的特点是：寻址容易，插入和删除困难；链表的特点是：寻址困难，但插入和删除容易；所以我们将数组和链表结合在一起，发挥两者各自的优势，使用一种叫做拉链法的方式可以解决哈希冲突。</p><h4 id="HashMap-JDK1-8之前"><a href="#HashMap-JDK1-8之前" class="headerlink" title="HashMap JDK1.8之前"></a>HashMap JDK1.8之前</h4><p>JDK1.8之前采用的是拉链法。<strong>拉链法</strong>：将链表和数组相结合。也就是说创建一个链表数组，数组中每一格就是一个链表。若遇到哈希冲突，则将冲突的值加到链表中即可。</p><p><img src="/2020/04/10/2020-04-10-kui-hua-bao-dian-java-ji-he/0006.png" alt></p><h4 id="HashMap-JDK1-8之后"><a href="#HashMap-JDK1-8之后" class="headerlink" title="HashMap JDK1.8之后"></a>HashMap JDK1.8之后</h4><p>相比于之前的版本，jdk1.8在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间。</p><p><img src="/2020/04/10/2020-04-10-kui-hua-bao-dian-java-ji-he/0007.png" alt></p><h4 id="JDK1-7-VS-JDK1-8-比较"><a href="#JDK1-7-VS-JDK1-8-比较" class="headerlink" title="JDK1.7 VS JDK1.8 比较"></a>JDK1.7 VS JDK1.8 比较</h4><p>JDK1.8主要解决或优化了一下问题： </p><ul><li>resize 扩容优化</li><li>引入了红黑树，目的是避免单条链表过长而影响查询效率，红黑树算法请参考</li><li>解决了多线程死循环问题，但仍是非线程安全的，多线程时可能会造成数据丢失问题。</li></ul><p><img src="/2020/04/10/2020-04-10-kui-hua-bao-dian-java-ji-he/0008.png" alt></p><h3 id="红黑树"><a href="#红黑树" class="headerlink" title="红黑树"></a>红黑树</h3><h4 id="二叉树"><a href="#二叉树" class="headerlink" title="二叉树"></a>二叉树</h4><p>二叉树简单来说就是 每一个节上可以关联俩个子节点</p><pre><code>大概就是这样子：                       a                    /     \                  b          c                / \         /  \              d    e       f    g            /  \  / \     / \   / \           h   i  j  k   l   m n   o</code></pre><h4 id="红黑树-1"><a href="#红黑树-1" class="headerlink" title="红黑树"></a>红黑树</h4><p>红黑树是一种特殊的二叉查找树。红黑树的每个结点上都有存储位表示结点的颜色，可以是红(Red)或黑(Black)。</p><p><img src="/2020/04/10/2020-04-10-kui-hua-bao-dian-java-ji-he/0009.png" alt></p><ul><li>节点是红色或者黑色</li><li>根节点是黑色</li><li>每个叶子的节点都是黑色的空节点（NULL）</li><li>每个红色节点的两个子节点都是黑色的。</li><li>从任意节点到其每个叶子的所有路径都包含相同的黑色节点。</li></ul><h3 id="HashMap的put方法的具体流程"><a href="#HashMap的put方法的具体流程" class="headerlink" title="HashMap的put方法的具体流程"></a>HashMap的put方法的具体流程</h3><p><img src="/2020/04/10/2020-04-10-kui-hua-bao-dian-java-ji-he/0010.png" alt="putVal方法执行流程图"></p><pre><code>public V put(K key, V value) {    return putVal(hash(key), key, value, false, true);}static final int hash(Object key) {    int h;    return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);}//实现Map.put和相关方法final V putVal(int hash, K key, V value, boolean onlyIfAbsent,                   boolean evict) {    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i;    // 步骤①：tab为空则创建     // table未初始化或者长度为0，进行扩容    if ((tab = table) == null || (n = tab.length) == 0)        n = (tab = resize()).length;    // 步骤②：计算index，并对null做处理      // (n - 1) &amp; hash 确定元素存放在哪个桶中，桶为空，新生成结点放入桶中(此时，这个结点是放在数组中)    if ((p = tab[i = (n - 1) &amp; hash]) == null)        tab[i] = newNode(hash, key, value, null);    // 桶中已经存在元素    else {        Node&lt;K,V&gt; e; K k;        // 步骤③：节点key存在，直接覆盖value         // 比较桶中第一个元素(数组中的结点)的hash值相等，key相等        if (p.hash == hash &amp;&amp;            ((k = p.key) == key || (key != null &amp;&amp; key.equals(k))))                // 将第一个元素赋值给e，用e来记录                e = p;        // 步骤④：判断该链为红黑树         // hash值不相等，即key不相等；为红黑树结点        // 如果当前元素类型为TreeNode，表示为红黑树，putTreeVal返回待存放的node, e可能为null        else if (p instanceof TreeNode)            // 放入树中            e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value);        // 步骤⑤：该链为链表         // 为链表结点        else {            // 在链表最末插入结点            for (int binCount = 0; ; ++binCount) {                // 到达链表的尾部                //判断该链表尾部指针是不是空的                if ((e = p.next) == null) {                    // 在尾部插入新结点                    p.next = newNode(hash, key, value, null);                    //判断链表的长度是否达到转化红黑树的临界值，临界值为8                    if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st                        //链表结构转树形结构                        treeifyBin(tab, hash);                    // 跳出循环                    break;                }                // 判断链表中结点的key值与插入的元素的key值是否相等                if (e.hash == hash &amp;&amp;                    ((k = e.key) == key || (key != null &amp;&amp; key.equals(k))))                    // 相等，跳出循环                    break;                // 用于遍历桶中的链表，与前面的e = p.next组合，可以遍历链表                p = e;            }        }        //判断当前的key已经存在的情况下，再来一个相同的hash值、key值时，返回新来的value这个值        if (e != null) {             // 记录e的value            V oldValue = e.value;            // onlyIfAbsent为false或者旧值为null            if (!onlyIfAbsent || oldValue == null)                //用新值替换旧值                e.value = value;            // 访问后回调            afterNodeAccess(e);            // 返回旧值            return oldValue;        }    }    // 结构性修改    ++modCount;    // 步骤⑥：超过最大容量就扩容     // 实际大小大于阈值则扩容    if (++size &gt; threshold)        resize();    // 插入后回调    afterNodeInsertion(evict);    return null;}</code></pre><ol><li>判断键值对数组table[i]是否为空或为null，否则执行resize()进行扩容；</li><li>根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向⑥，如果table[i]不为空，转向③；</li><li>判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals；</li><li>判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向5；</li><li>遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可；</li><li>插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。</li></ol><h3 id="HashMap的扩容操作实现"><a href="#HashMap的扩容操作实现" class="headerlink" title="HashMap的扩容操作实现"></a>HashMap的扩容操作实现</h3><ul><li>在jdk1.8中，resize方法是在hashmap中的键值对大于阀值时或者初始化时，就调用resize方法进行扩容；</li><li>每次扩展的时候，都是扩展2倍；</li><li>扩展后Node对象的位置要么在原位置，要么移动到原偏移量两倍的位置。</li></ul><p>在putVal()中，我们看到在这个函数里面使用到了2次resize()方法，resize()方法表示的在进行第一次初始化时会对其进行扩容，或者当该数组的实际大小大于其临界值值(第一次为12),这个时候在扩容的同时也会伴随的桶上面的元素进行重新分发，这也是JDK1.8版本的一个优化的地方，在1.7中，扩容之后需要重新去计算其Hash值，根据Hash值对其进行分发，但在1.8版本中，则是根据在同一个桶的位置中进行判断(e.hash &amp; oldCap)是否为0，重新进行hash分配后，该元素的位置要么停留在原始位置，要么移动到原始位置+增加的数组大小这个位置上</p><pre><code>final Node&lt;K,V&gt;[] resize() {    Node&lt;K,V&gt;[] oldTab = table;//oldTab指向hash桶数组    int oldCap = (oldTab == null) ? 0 : oldTab.length;    int oldThr = threshold;    int newCap, newThr = 0;    if (oldCap &gt; 0) {//如果oldCap不为空的话，就是hash桶数组不为空        if (oldCap &gt;= MAXIMUM_CAPACITY) {//如果大于最大容量了，就赋值为整数最大的阀值            threshold = Integer.MAX_VALUE;            return oldTab;//返回        }//如果当前hash桶数组的长度在扩容后仍然小于最大容量 并且oldCap大于默认值16        else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp;                 oldCap &gt;= DEFAULT_INITIAL_CAPACITY)            newThr = oldThr &lt;&lt; 1; // double threshold 双倍扩容阀值threshold    }    // 旧的容量为0，但threshold大于零，代表有参构造有cap传入，threshold已经被初始化成最小2的n次幂    // 直接将该值赋给新的容量    else if (oldThr &gt; 0) // initial capacity was placed in threshold        newCap = oldThr;    // 无参构造创建的map，给出默认容量和threshold 16, 16*0.75    else {               // zero initial threshold signifies using defaults        newCap = DEFAULT_INITIAL_CAPACITY;        newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);    }    // 新的threshold = 新的cap * 0.75    if (newThr == 0) {        float ft = (float)newCap * loadFactor;        newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ?                  (int)ft : Integer.MAX_VALUE);    }    threshold = newThr;    // 计算出新的数组长度后赋给当前成员变量table    @SuppressWarnings({&quot;rawtypes&quot;,&quot;unchecked&quot;})        Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap];//新建hash桶数组    table = newTab;//将新数组的值复制给旧的hash桶数组    // 如果原先的数组没有初始化，那么resize的初始化工作到此结束，否则进入扩容元素重排逻辑，使其均匀的分散    if (oldTab != null) {        // 遍历新数组的所有桶下标        for (int j = 0; j &lt; oldCap; ++j) {            Node&lt;K,V&gt; e;            if ((e = oldTab[j]) != null) {                // 旧数组的桶下标赋给临时变量e，并且解除旧数组中的引用，否则就数组无法被GC回收                oldTab[j] = null;                // 如果e.next==null，代表桶中就一个元素，不存在链表或者红黑树                if (e.next == null)                    // 用同样的hash映射算法把该元素加入新的数组                    newTab[e.hash &amp; (newCap - 1)] = e;                // 如果e是TreeNode并且e.next!=null，那么处理树中元素的重排                else if (e instanceof TreeNode)                    ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap);                // e是链表的头并且e.next!=null，那么处理链表中元素重排                else { // preserve order                    // loHead,loTail 代表扩容后不用变换下标，见注1                    Node&lt;K,V&gt; loHead = null, loTail = null;                    // hiHead,hiTail 代表扩容后变换下标，见注1                    Node&lt;K,V&gt; hiHead = null, hiTail = null;                    Node&lt;K,V&gt; next;                    // 遍历链表                    do {                                     next = e.next;                        if ((e.hash &amp; oldCap) == 0) {                            if (loTail == null)                                // 初始化head指向链表当前元素e，e不一定是链表的第一个元素，初始化后loHead                                // 代表下标保持不变的链表的头元素                                loHead = e;                            else                                                                // loTail.next指向当前e                                loTail.next = e;                            // loTail指向当前的元素e                            // 初始化后，loTail和loHead指向相同的内存，所以当loTail.next指向下一个元素时，                            // 底层数组中的元素的next引用也相应发生变化，造成lowHead.next.next.....                            // 跟随loTail同步，使得lowHead可以链接到所有属于该链表的元素。                            loTail = e;                                                   }                        else {                            if (hiTail == null)                                // 初始化head指向链表当前元素e, 初始化后hiHead代表下标更改的链表头元素                                hiHead = e;                            else                                hiTail.next = e;                            hiTail = e;                        }                    } while ((e = next) != null);                    // 遍历结束, 将tail指向null，并把链表头放入新数组的相应下标，形成新的映射。                    if (loTail != null) {                        loTail.next = null;                        newTab[j] = loHead;                    }                    if (hiTail != null) {                        hiTail.next = null;                        newTab[j + oldCap] = hiHead;                    }                }            }        }    }    return newTab;}</code></pre><h3 id="HashMap是怎么解决哈希冲突的？"><a href="#HashMap是怎么解决哈希冲突的？" class="headerlink" title="HashMap是怎么解决哈希冲突的？"></a>HashMap是怎么解决哈希冲突的？</h3><h4 id="什么是哈希"><a href="#什么是哈希" class="headerlink" title="什么是哈希"></a>什么是哈希</h4><p>Hash，一般翻译为“散列”，也有直接音译为“哈希”的， Hash就是指使用哈希算法是指把任意长度的二进制映射为固定长度的较小的二进制值，这个较小的二进制值叫做哈希值。</p><h4 id="什么是哈希冲突"><a href="#什么是哈希冲突" class="headerlink" title="什么是哈希冲突"></a>什么是哈希冲突</h4><p>当两个不同的输入值，根据同一散列函数计算出相同的散列值的现象，我们就把它叫做碰撞（哈希碰撞）。</p><h4 id="HashMap的数据结构"><a href="#HashMap的数据结构" class="headerlink" title="HashMap的数据结构"></a>HashMap的数据结构</h4><ul><li>在Java中，保存数据有两种比较简单的数据结构：数组和链表。<ul><li>数组的特点是：寻址容易，插入和删除困难；</li><li>链表的特点是：寻址困难，但插入和删除容易；</li></ul></li><li>所以我们将数组和链表结合在一起，发挥两者各自的优势，就可以使用俩种方式：链地址法和开放地址法可以解决哈希冲突：</li></ul><p><img src="/2020/04/10/2020-04-10-kui-hua-bao-dian-java-ji-he/0011.png" alt></p><ul><li>链表法就是将相同hash值的对象组织成一个链表放在hash值对应的槽位；</li><li>开放地址法是通过一个探测算法，当某个槽位已经被占据的情况下继续查找下一个可以使用的槽位。</li><li>但相比于hashCode返回的int类型，我们HashMap初始的容量大小DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4（即2的四次方16）要远小于int类型的范围，所以我们如果只是单纯的用hashCode取余来获取对应的bucket这将会大大增加哈希碰撞的概率，并且最坏情况下还会将HashMap变成一个单链表，所以我们还需要对hashCode作一定的优化</li></ul><h4 id="hash-函数"><a href="#hash-函数" class="headerlink" title="hash()函数"></a>hash()函数</h4><ul><li>上面提到的问题，主要是因为如果使用hashCode取余，那么相当于参与运算的只有hashCode的低位，高位是没有起到任何作用的，所以我们的思路就是让hashCode取值出的高位也参与运算，进一步降低hash碰撞的概率，使得数据分布更平均，我们把这样的操作称为扰动，在JDK 1.8中的hash()函数如下：</li></ul><pre><code>static final int hash(Object key) {    int h;    return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);// 与自己右移16位进行异或运算（高低位异或）}</code></pre><ul><li>这比在JDK 1.7中，更为简洁，相比在1.7中的4次位运算，5次异或运算（9次扰动），在1.8中，只进行了1次位运算和1次异或运算（2次扰动）；</li></ul><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>简单总结一下HashMap是使用了哪些方法来有效解决哈希冲突的： </p><ul><li>链表法就是将相同hash值的对象组织成一个链表放在hash值对应的槽位；</li><li>开放地址法是通过一个探测算法，当某个槽位已经被占据的情况下继续查找下一个可以使用的槽位。</li></ul><h3 id="能否使用任何类作为-Map-的-key？"><a href="#能否使用任何类作为-Map-的-key？" class="headerlink" title="能否使用任何类作为 Map 的 key？"></a>能否使用任何类作为 Map 的 key？</h3><p>可以使用任何类作为 Map 的 key，然而在使用之前，需要考虑以下几点：</p><ul><li>如果类重写了 equals() 方法，也应该重写 hashCode() 方法。</li><li>类的所有实例需要遵循与 equals() 和 hashCode() 相关的规则。</li><li>如果一个类没有使用 equals()，不应该在 hashCode() 中使用它。</li><li>用户自定义 Key 类最佳实践是使之为不可变的，这样 hashCode() 值可以被缓存起来，拥有更好的性能。不可变的类也可以确保 hashCode() 和 equals() 在未来不会改变，这样就会解决与可变相关的问题了。</li></ul><h3 id="为什么HashMap中String、Integer这样的包装类适合作为K？"><a href="#为什么HashMap中String、Integer这样的包装类适合作为K？" class="headerlink" title="为什么HashMap中String、Integer这样的包装类适合作为K？"></a>为什么HashMap中String、Integer这样的包装类适合作为K？</h3><p>String、Integer等包装类的特性能够保证Hash值的不可更改性和计算准确性，能够有效的减少Hash碰撞的几率 </p><ul><li>都是final类型，即不可变性，保证key的不可更改性，不会存在获取hash值不同的情况</li><li>内部已重写了equals()、hashCode()等方法，遵守了HashMap内部的规范（不清楚可以去上面看看putValue的过程），不容易出现Hash值计算错误的情况；</li></ul><h3 id="如果使用Object作为HashMap的Key，应该怎么办呢？"><a href="#如果使用Object作为HashMap的Key，应该怎么办呢？" class="headerlink" title="如果使用Object作为HashMap的Key，应该怎么办呢？"></a>如果使用Object作为HashMap的Key，应该怎么办呢？</h3><p>重写hashCode()和equals()方法 </p><ul><li>重写hashCode()是因为需要计算存储数据的存储位置，需要注意不要试图从散列码计算中排除掉一个对象的关键部分来提高性能，这样虽然能更快但可能会导致更多的Hash碰撞；</li><li>重写equals()方法，需要遵守自反性、对称性、传递性、一致性以及对于任何非null的引用值x，x.equals(null)必须返回false的这几个特性，目的是为了保证key在哈希表中的唯一性；</li></ul><h3 id="HashMap为什么不直接使用hashCode-处理后的哈希值直接作为table的下标？"><a href="#HashMap为什么不直接使用hashCode-处理后的哈希值直接作为table的下标？" class="headerlink" title="HashMap为什么不直接使用hashCode()处理后的哈希值直接作为table的下标？"></a>HashMap为什么不直接使用hashCode()处理后的哈希值直接作为table的下标？</h3><p>hashCode()方法返回的是int整数类型，其范围为-(2 ^ 31)<del>(2 ^ 31 - 1)，约有40亿个映射空间，而HashMap的容量范围是在16（初始化默认值）</del>2 ^ 30，HashMap通常情况下是取不到最大值的，并且设备上也难以提供这么多的存储空间，从而导致通过hashCode()计算出的哈希值可能不在数组大小范围内，进而无法匹配存储位置；</p><ul><li>那怎么解决呢？<ul><li>HashMap自己实现了自己的hash()方法，通过两次扰动使得它自己的哈希值高低位自行进行异或运算，降低哈希碰撞概率也使得数据分布更平均；</li><li>在保证数组长度为2的幂次方的时候，使用hash()运算之后的值与运算（&amp;）（数组长度 - 1）来获取数组下标的方式进行存储，这样一来是比取余操作更加有效率，二来也是因为只有当数组长度为2的幂次方时，h&amp;(length-1)才等价于h%length，三来解决了“哈希值与数组大小范围不匹配”的问题；</li></ul></li></ul><h3 id="HashMap-的长度为什么是2的幂次方"><a href="#HashMap-的长度为什么是2的幂次方" class="headerlink" title="HashMap 的长度为什么是2的幂次方"></a>HashMap 的长度为什么是2的幂次方</h3><ul><li><p>为了能让 HashMap 存取高效，尽量较少碰撞，也就是要尽量把数据分配均匀，每个链表/红黑树长度大致相同。这个实现就是把数据存到哪个链表/红黑树中的算法。</p></li><li><p>这个算法应该如何设计呢？</p></li></ul><p>我们首先可能会想到采用%取余的操作来实现。但是，重点来了：“取余(%)操作中如果除数是2的幂次则等价于与其除数减一的与(&amp;)操作（也就是说 hash%length==hash&amp;(length-1)的前提是 length 是2的 n 次方；）。” 并且 采用二进制位操作 &amp;，相对于%能够提高运算效率，这就解释了 HashMap 的长度为什么是2的幂次方。</p><ul><li>那为什么是两次扰动呢？</li></ul><p>这样就是加大哈希值低位的随机性，使得分布更均匀，从而提高对应数组存储下标位置的随机性&amp;均匀性，最终减少Hash冲突，两次就够了，已经达到了高位低位同时参与运算的目的；</p><h3 id="HashMap-与-HashTable-的区别"><a href="#HashMap-与-HashTable-的区别" class="headerlink" title="HashMap 与 HashTable 的区别"></a>HashMap 与 HashTable 的区别</h3><ul><li>线程安全： HashMap 是非线程安全的，HashTable 是线程安全的；HashTable 内部的方法基本都经过 synchronized 修饰。（如果你要保证线程安全的话就使用 ConcurrentHashMap ）；</li><li>效率： 因为线程安全的问题，HashMap 要比 HashTable 效率高一点。另外，HashTable 基本被淘汰，不要在代码中使用它；（如果你要保证线程安全的话就使用 ConcurrentHashMap ）；</li><li>对Null key 和Null value的支持： HashMap 中，null 可以作为键，这样的键只有一个，可以有一个或多个键所对应的值为 null。但是在 HashTable 中 put 进的键值只要有一个 null，直接抛NullPointerException。</li><li>初始容量大小和每次扩充容量大小的不同 ：创建时如果不指定容量初始值，Hashtable 默认的初始大小为11，之后每次扩充，容量变为原来的2n+1。HashMap 默认的初始化大小为16。之后每次扩充，容量变为原来的2倍。创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为2的幂次方大小。也就是说 HashMap 总是使用2的幂作为哈希表的大小.</li><li>底层数据结构： JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间。Hashtable 没有这样的机制。</li><li>推荐使用：在 Hashtable 的类注释可以看到，Hashtable 是保留类不建议使用，推荐在单线程环境下使用 HashMap 替代，如果需要多线程使用则用 ConcurrentHashMap 替代。</li></ul><h3 id="TreeMap-简介"><a href="#TreeMap-简介" class="headerlink" title="TreeMap 简介"></a>TreeMap 简介</h3><ul><li>TreeMap 是一个有序的key-value集合，它是通过红黑树实现的。</li><li>TreeMap基于红黑树（Red-Black tree）实现。该映射根据其键的自然顺序进行排序，或者根据创建映射时提供的 Comparator 进行排序，具体取决于使用的构造方法。</li><li>TreeMap是线程非同步的。</li></ul><h3 id="如何决定使用-HashMap-还是-TreeMap？"><a href="#如何决定使用-HashMap-还是-TreeMap？" class="headerlink" title="如何决定使用 HashMap 还是 TreeMap？"></a>如何决定使用 HashMap 还是 TreeMap？</h3><ul><li>对于在Map中插入、删除和定位元素这类操作，HashMap是最好的选择。然而，假如你需要对一个有序的key集合进行遍历，TreeMap是更好的选择。基于你的collection的大小，也许向HashMap中添加元素会更快，将map换为TreeMap进行有序key的遍历。</li></ul><h3 id="HashMap-和-ConcurrentHashMap-的区别"><a href="#HashMap-和-ConcurrentHashMap-的区别" class="headerlink" title="HashMap 和 ConcurrentHashMap 的区别"></a>HashMap 和 ConcurrentHashMap 的区别</h3><ul><li>ConcurrentHashMap对整个桶数组进行了分割分段(Segment)，然后在每一个分段上都用lock锁进行保护，相对于HashTable的synchronized锁的粒度更精细了一些，并发性能更好，而HashMap没有锁机制，不是线程安全的。（JDK1.8之后ConcurrentHashMap启用了一种全新的方式实现,利用CAS算法。）</li><li>HashMap的键值对允许有null，但是ConCurrentHashMap都不允许。</li></ul><h3 id="ConcurrentHashMap-和-Hashtable-的区别？"><a href="#ConcurrentHashMap-和-Hashtable-的区别？" class="headerlink" title="ConcurrentHashMap 和 Hashtable 的区别？"></a>ConcurrentHashMap 和 Hashtable 的区别？</h3><p>ConcurrentHashMap 和 Hashtable 的区别主要体现在实现线程安全的方式上不同。</p><ul><li>底层数据结构： JDK1.7的 ConcurrentHashMap 底层采用 分段的数组+链表 实现，JDK1.8 采用的数据结构跟HashMap1.8的结构一样，数组+链表/红黑二叉树。Hashtable 和 JDK1.8 之前的 HashMap 的底层数据结构类似都是采用 数组+链表 的形式，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的；</li><li>实现线程安全的方式：<ul><li>在JDK1.7的时候，ConcurrentHashMap（分段锁） 对整个桶数组进行了分割分段(Segment)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。（默认分配16个Segment，比Hashtable效率提高16倍。） 到了 JDK1.8 的时候已经摒弃了Segment的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并发控制使用 synchronized 和 CAS 来操作。（JDK1.6以后 对 synchronized锁做了很多优化） 整个看起来就像是优化过且线程安全的 HashMap，虽然在JDK1.8中还能看到 Segment 的数据结构，但是已经简化了属性，只是为了兼容旧版本；</li><li>Hashtable(同一把锁) :使用 synchronized 来保证线程安全，效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用 put 添加元素，另一个线程不能使用 put 添加元素，也不能使用 get，竞争会越来越激烈效率越低。</li></ul></li></ul><p>两者的对比图：</p><ul><li>HashTable:</li></ul><p><img src="/2020/04/10/2020-04-10-kui-hua-bao-dian-java-ji-he/0012.png" alt></p><ul><li>JDK1.7的ConcurrentHashMap：</li></ul><p><img src="/2020/04/10/2020-04-10-kui-hua-bao-dian-java-ji-he/0013.png" alt></p><ul><li>JDK1.8的ConcurrentHashMap（TreeBin: 红黑二叉树节点 Node: 链表节点）：</li></ul><p><img src="/2020/04/10/2020-04-10-kui-hua-bao-dian-java-ji-he/0014.png" alt></p><p>ConcurrentHashMap 结合了 HashMap 和 HashTable 二者的优势。HashMap 没有考虑同步，HashTable 考虑了同步的问题使用了synchronized 关键字，所以 HashTable 在每次同步执行时都要锁住整个结构。 ConcurrentHashMap 锁的方式是稍微细粒度的。</p><h3 id="ConcurrentHashMap-底层具体实现知道吗？实现原理是什么？"><a href="#ConcurrentHashMap-底层具体实现知道吗？实现原理是什么？" class="headerlink" title="ConcurrentHashMap 底层具体实现知道吗？实现原理是什么？"></a>ConcurrentHashMap 底层具体实现知道吗？实现原理是什么？</h3><h4 id="JDK1-7"><a href="#JDK1-7" class="headerlink" title="JDK1.7"></a>JDK1.7</h4><ul><li>首先将数据分为一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据时，其他段的数据也能被其他线程访问。</li><li>在JDK1.7中，ConcurrentHashMap采用Segment + HashEntry的方式进行实现，结构如下：</li><li>一个 ConcurrentHashMap 里包含一个 Segment 数组。Segment 的结构和HashMap类似，是一种数组和链表结构，一个 Segment 包含一个 HashEntry 数组，每个 HashEntry 是一个链表结构的元素，每个 Segment 守护着一个HashEntry数组里的元素，当对 HashEntry 数组的数据进行修改时，必须首先获得对应的 Segment的锁。</li></ul><p><img src="/2020/04/10/2020-04-10-kui-hua-bao-dian-java-ji-he/0015.png" alt></p><p>该类包含两个静态内部类 HashEntry 和 Segment ；前者用来封装映射表的键值对，后者用来充当锁的角色；</p><p>Segment 是一种可重入的锁 ReentrantLock，每个 Segment 守护一个HashEntry 数组里得元素，当对 HashEntry 数组的数据进行修改时，必须首先获得对应的 Segment 锁。</p><h4 id="JDK1-8"><a href="#JDK1-8" class="headerlink" title="JDK1.8"></a>JDK1.8</h4><p>在JDK1.8中，放弃了Segment臃肿的设计，取而代之的是采用Node + CAS + Synchronized来保证并发安全进行实现，synchronized只锁定当前链表或红黑二叉树的首节点，这样只要hash不冲突，就不会产生并发，效率又提升N倍。</p><p><img src="/2020/04/10/2020-04-10-kui-hua-bao-dian-java-ji-he/0016.png" alt="结构图"></p><p>如果相应位置的Node还没有初始化，则调用CAS插入相应的数据；</p><pre><code>else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) {    if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null)))        break;                   // no lock when adding to empty bin}</code></pre><p>如果相应位置的Node不为空，且当前该节点不处于移动状态，则对该节点加synchronized锁，如果该节点的hash不小于0，则遍历链表更新节点或插入新节点；</p><pre><code>if (fh &gt;= 0) {    binCount = 1;    for (Node&lt;K,V&gt; e = f;; ++binCount) {        K ek;        if (e.hash == hash &amp;&amp;            ((ek = e.key) == key ||             (ek != null &amp;&amp; key.equals(ek)))) {            oldVal = e.val;            if (!onlyIfAbsent)                e.val = value;            break;        }        Node&lt;K,V&gt; pred = e;        if ((e = e.next) == null) {            pred.next = new Node&lt;K,V&gt;(hash, key, value, null);            break;        }    }}</code></pre><p>如果该节点是TreeBin类型的节点，说明是红黑树结构，则通过putTreeVal方法往红黑树中插入节点；如果binCount不为0，说明put操作对数据产生了影响，如果当前链表的个数达到8个，则通过treeifyBin方法转化为红黑树，如果oldVal不为空，说明是一次更新操作，没有对元素个数产生影响，则直接返回旧值；</p><p>如果插入的是一个新节点，则执行addCount()方法尝试更新元素个数baseCount；</p><h2 id="辅助工具类"><a href="#辅助工具类" class="headerlink" title="辅助工具类"></a>辅助工具类</h2><h3 id="Array-和-ArrayList-区别"><a href="#Array-和-ArrayList-区别" class="headerlink" title="Array 和 ArrayList 区别"></a>Array 和 ArrayList 区别</h3><ul><li>Array 可以存储基本数据类型和对象，ArrayList 只能存储对象。</li><li>Array 是指定固定大小的，而 ArrayList 大小是自动扩展的。</li><li>Array 内置方法没有 ArrayList 多，比如 addAll、removeAll、iteration 等方法只有 ArrayList 有。</li></ul><p>对于基本类型数据，集合使用自动装箱来减少编码工作量。但是，当处理固定大小的基本数据类型的时候，这种方式相对比较慢。</p><h3 id="如何实现-Array-和-List-之间的转换？"><a href="#如何实现-Array-和-List-之间的转换？" class="headerlink" title="如何实现 Array 和 List 之间的转换？"></a>如何实现 Array 和 List 之间的转换？</h3><ul><li>Array 转 List： Arrays.asList(array) ；</li><li>List 转 Array：List 的 toArray() 方法。</li></ul><h3 id="comparable-和-comparator的区别？"><a href="#comparable-和-comparator的区别？" class="headerlink" title="comparable 和 comparator的区别？"></a>comparable 和 comparator的区别？</h3><ul><li>comparable接口实际上是出自java.lang包，它有一个 compareTo(Object obj)方法用来排序</li><li>comparator接口实际上是出自 java.util 包，它有一个compare(Object obj1, Object obj2)方法用来排序</li><li>一般我们需要对一个集合使用自定义排序时，我们就要重写compareTo方法或compare方法，当我们需要对某一个集合实现两种排序方式，比如一个song对象中的歌名和歌手名分别采用一种排序方法的话，我们可以重写compareTo方法和使用自制的Comparator方法或者以两个Comparator来实现歌名排序和歌星名排序，第二种代表我们只能使用两个参数版的Collections.sort().</li></ul><h3 id="Collection-和-Collections-有什么区别？"><a href="#Collection-和-Collections-有什么区别？" class="headerlink" title="Collection 和 Collections 有什么区别？"></a>Collection 和 Collections 有什么区别？</h3><ul><li>java.util.Collection 是一个集合接口（集合类的一个顶级接口）。它提供了对集合对象进行基本操作的通用接口方法。Collection接口在Java 类库中有很多具体的实现。Collection接口的意义是为各种具体的集合提供了最大化的统一操作方式，其直接继承接口有List与Set。</li><li>Collections则是集合类的一个工具类/帮助类，其中提供了一系列静态方法，用于对集合中元素进行排序、搜索以及线程安全等各种操作。</li></ul><h3 id="TreeMap-和-TreeSet-在排序时如何比较元素？Collections-工具类中的-sort-方法如何比较元素？"><a href="#TreeMap-和-TreeSet-在排序时如何比较元素？Collections-工具类中的-sort-方法如何比较元素？" class="headerlink" title="TreeMap 和 TreeSet 在排序时如何比较元素？Collections 工具类中的 sort()方法如何比较元素？"></a>TreeMap 和 TreeSet 在排序时如何比较元素？Collections 工具类中的 sort()方法如何比较元素？</h3><ul><li>TreeSet 要求存放的对象所属的类必须实现 Comparable 接口，该接口提供了比较元素的 compareTo()方法，当插入元素时会回调该方法比较元素的大小。TreeMap 要求存放的键值对映射的键必须实现 Comparable 接口从而根据键对元素进 行排 序。</li><li>Collections 工具类的 sort 方法有两种重载的形式<ul><li>第一种要求传入的待排序容器中存放的对象比较实现 Comparable 接口以实现元素的比较；</li><li>第二种不强制性的要求容器中的元素必须可比较，但是要求传入第二个参数，参数是Comparator 接口的子类型（需要重写 compare 方法实现元素的比较），相当于一个临时定义的排序规则，其实就是通过接口注入比较元素大小的算法，也是对回调模式的应用（Java 中对函数式编程的支持）。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 葵花宝典 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 集合 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>发布博客</title>
      <link href="/2020/04/01/hello-world/"/>
      <url>/2020/04/01/hello-world/</url>
      
        <content type="html"><![CDATA[<h2 id="发布博客"><a href="#发布博客" class="headerlink" title="发布博客"></a>发布博客</h2><pre><code>title: JAVA基础date: 2020-01-13 23:45:46author: Adbosummary: 理论知识总结categories: JAVAtags:- JAVA基础</code></pre><pre><code>0. 配置公钥相关1. git clone 博客（BlogBackups） project2. import code（导入代码）3. npm install(执行这个命令，前提是安装nodejs，尽量是低版本，目测v12.16.3是可以的，https://nodejs.org/download/release/v12.16.3/)npm install -g cnpm --registry=https://registry.npm.taobao.org(如果执行上面的命令错误)安装低版本cnpm：npm install cnpm@7.1.0 -g --registry=https://registry.npm.taobao.org（如果上面的命令还是错误）4. cnpm install -g hexo-cli5. 执行 hexo -v (如果失败，那是因为package-lock.json和package.json有问题，上面的命令去除-g全局参数)</code></pre><pre><code>切换仓库修改 _config.yaml文件gitee仓库:deploy:  type: git  repo: https://gitee.com/adbo/adbo.git  branch: mastergithub仓库：deploy:  type: git  repo: git@github.com:adbbo/adbbo.github.io.git  branch: master</code></pre><pre><code>hexo new &quot;2020-01-16-JVM&quot;git commit -am &quot;udpate&quot;git push</code></pre><pre><code>hexo cleanhexo generatehexo deploy</code></pre>]]></content>
      
      
      <categories>
          
          <category> 备忘录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 发布博客 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入理解AQS</title>
      <link href="/2020/03/18/2020-03-18-bing-fa-aqs-shen-ru-li-jie/"/>
      <url>/2020/03/18/2020-03-18-bing-fa-aqs-shen-ru-li-jie/</url>
      
        <content type="html"><![CDATA[<p>谈到并发，我们不得不说AQS(AbstractQueuedSynchronizer)，所谓的AQS即是抽象的队列式的同步器，内部定义了很多锁相关的方法，我们熟知的ReentrantLock、ReentrantReadWriteLock、CountDownLatch、Semaphore等都是基于AQS来实现的。</p><p><img src="/2020/03/18/2020-03-18-bing-fa-aqs-shen-ru-li-jie/640.png" alt></p><h2 id="AQS实现原理"><a href="#AQS实现原理" class="headerlink" title="AQS实现原理"></a>AQS实现原理</h2><p>AQS中 维护了一个volatile int state（代表共享资源）和一个FIFO线程等待队列（多线程争用资源被阻塞时会进入此队列）。</p><p>这里volatile能够保证多线程下的可见性，当state=1则代表当前对象锁已经被占有，其他线程来加锁时则会失败，加锁失败的线程会被放入一个FIFO的等待队列中，比列会被UNSAFE.park()操作挂起，等待其他获取锁的线程释放锁才能够被唤醒。</p><p>另外state的操作都是通过CAS来保证其并发修改的安全性。</p><p>具体原理我们可以用一张图来简单概括：</p><p><img src="/2020/03/18/2020-03-18-bing-fa-aqs-shen-ru-li-jie/641.png" alt></p><p>AQS 中提供了很多关于锁的实现方法，</p><ul><li>getState()：获取锁的标志state值</li><li>setState()：设置锁的标志state值</li><li>tryAcquire(int)：独占方式获取锁。尝试获取资源，成功则返回true，失败则返回false。</li><li>tryRelease(int)：独占方式释放锁。尝试释放资源，成功则返回true，失败则返回false。</li></ul><p>这里还有一些方法并没有列出来，接下来我们以ReentrantLock作为突破点通过源码和画图的形式一步步了解AQS内部实现原理。</p><h2 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a>目录结构</h2><p>文章准备模拟多线程竞争锁、释放锁的场景来进行分析AQS源码：</p><p>三个线程(线程一、线程二、线程三)同时来加锁/释放锁</p><p>目录如下：</p><ul><li>线程一加锁成功时AQS内部实现</li><li>线程二/三加锁失败时AQS中等待队列的数据模型</li><li>线程一释放锁及线程二获取锁实现原理</li><li>通过线程场景来讲解公平锁具体实现原理</li><li>通过线程场景来讲解Condition中await()和signal()实现原理</li></ul><h2 id="场景分析"><a href="#场景分析" class="headerlink" title="场景分析"></a>场景分析</h2><h3 id="线程一加锁成功"><a href="#线程一加锁成功" class="headerlink" title="线程一加锁成功"></a>线程一加锁成功</h3><p>如果同时有三个线程并发抢占锁，此时线程一抢占锁成功，线程二和线程三抢占锁失败，具体执行流程如下：</p><p><img src="/2020/03/18/2020-03-18-bing-fa-aqs-shen-ru-li-jie/642.png" alt></p><p>此时AQS内部数据为：</p><p><img src="/2020/03/18/2020-03-18-bing-fa-aqs-shen-ru-li-jie/643.png" alt></p><h3 id="线程二、线程三加锁失败："><a href="#线程二、线程三加锁失败：" class="headerlink" title="线程二、线程三加锁失败："></a>线程二、线程三加锁失败：</h3><p><img src="/2020/03/18/2020-03-18-bing-fa-aqs-shen-ru-li-jie/644.png" alt></p><p>有图可以看出，等待队列中的节点Node是一个双向链表，这里SIGNAL是Node中waitStatus属性，Node中还有一个nextWaiter属性，这个并未在图中画出来，这个到后面Condition会具体讲解的。</p><h3 id="具体看下抢占锁代码实现："><a href="#具体看下抢占锁代码实现：" class="headerlink" title="具体看下抢占锁代码实现："></a>具体看下抢占锁代码实现：</h3><p>java.util.concurrent.locks.ReentrantLock .NonfairSync:</p><pre><code>static final class NonfairSync extends Sync {    final void lock() {        if (compareAndSetState(0, 1))            setExclusiveOwnerThread(Thread.currentThread());        else            acquire(1);    }    protected final boolean tryAcquire(int acquires) {        return nonfairTryAcquire(acquires);    }}</code></pre><p>这里使用的ReentrantLock非公平锁，线程进来直接利用CAS尝试抢占锁，如果抢占成功state值回被改为1，且设置对象独占锁线程为当前线程。如下所示：</p><pre><code>protected final boolean compareAndSetState(int expect, int update) {    return unsafe.compareAndSwapInt(this, stateOffset, expect, update);}protected final void setExclusiveOwnerThread(Thread thread) {    exclusiveOwnerThread = thread;}</code></pre><h3 id="线程二抢占锁失败"><a href="#线程二抢占锁失败" class="headerlink" title="线程二抢占锁失败"></a>线程二抢占锁失败</h3><p>我们按照真实场景来分析，线程一抢占锁成功后，state变为1，线程二通过CAS修改state变量必然会失败。此时AQS中FIFO(First In First Out 先进先出)队列中数据如图所示：</p><p><img src="/2020/03/18/2020-03-18-bing-fa-aqs-shen-ru-li-jie/645.png" alt></p><h3 id="我们将线程二执行的逻辑一步步拆解来看："><a href="#我们将线程二执行的逻辑一步步拆解来看：" class="headerlink" title="我们将线程二执行的逻辑一步步拆解来看："></a>我们将线程二执行的逻辑一步步拆解来看：</h3><p>java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire():</p><pre><code>public final void acquire(int arg) {    if (!tryAcquire(arg) &amp;&amp;        acquireQueued(addWaiter(Node.EXCLUSIVE), arg))        selfInterrupt();}</code></pre><p>先看看tryAcquire()的具体实现：java.util.concurrent.locks.ReentrantLock .nonfairTryAcquire():</p><pre><code>final boolean nonfairTryAcquire(int acquires) {    final Thread current = Thread.currentThread();    int c = getState();    if (c == 0) {        if (compareAndSetState(0, acquires)) {            setExclusiveOwnerThread(current);            return true;        }    }    else if (current == getExclusiveOwnerThread()) {        int nextc = c + acquires;        if (nextc &lt; 0)            throw new Error(&quot;Maximum lock count exceeded&quot;);        setState(nextc);        return true;    }    return false;}</code></pre><p>nonfairTryAcquire()方法中首先会获取state的值，如果不为0则说明当前对象的锁已经被其他线程所占有，接着判断占有锁的线程是否为当前线程，如果是则累加state值，这就是可重入锁的具体实现，累加state值，释放锁的时候也要依次递减state值。</p><p>如果state为0，则执行CAS操作，尝试更新state值为1，如果更新成功则代表当前线程加锁成功。</p><p>以线程二为例，因为线程一已经将state修改为1，所以线程二通过CAS修改state的值不会成功。加锁失败。</p><p>线程二执行tryAcquire()后会返回false，接着执行addWaiter(Node.EXCLUSIVE)逻辑，将自己加入到一个FIFO等待队列中，代码实现如下：</p><p>java.util.concurrent.locks.AbstractQueuedSynchronizer.addWaiter():</p><pre><code>private Node addWaiter(Node mode) {        Node node = new Node(Thread.currentThread(), mode);    Node pred = tail;    if (pred != null) {        node.prev = pred;        if (compareAndSetTail(pred, node)) {            pred.next = node;            return node;        }    }    enq(node);    return node;}</code></pre><p>这段代码首先会创建一个和当前线程绑定的Node节点，Node为双向链表。此时等待对内中的tail指针为空，直接调用enq(node)方法将当前线程加入等待队列尾部：</p><pre><code>private Node enq(final Node node) {    for (;;) {        Node t = tail;        if (t == null) {            if (compareAndSetHead(new Node()))                tail = head;        } else {            node.prev = t;            if (compareAndSetTail(t, node)) {                t.next = node;                return t;            }        }    }}</code></pre><p>第一遍循环时tail指针为空，进入if逻辑，使用CAS操作设置head指针，将head指向一个新创建的Node节点。此时AQS中数据：</p><p><img src="/2020/03/18/2020-03-18-bing-fa-aqs-shen-ru-li-jie/646.png" alt></p><p>执行完成之后，head、tail、t都指向第一个Node元素。</p><p>接着执行第二遍循环，进入else逻辑，此时已经有了head节点，这里要操作的就是将线程二对应的Node节点挂到head节点后面。此时队列中就有了两个Node节点：</p><p><img src="/2020/03/18/2020-03-18-bing-fa-aqs-shen-ru-li-jie/647.png" alt></p><p>addWaiter()方法执行完后，会返回当前线程创建的节点信息。继续往后执行acquireQueued(addWaiter(Node.EXCLUSIVE), arg)逻辑，此时传入的参数为线程二对应的Node节点信息：</p><p>java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued():</p><pre><code>final boolean acquireQueued(final Node node, int arg) {    boolean failed = true;    try {        boolean interrupted = false;        for (;;) {            final Node p = node.predecessor();            if (p == head &amp;&amp; tryAcquire(arg)) {                setHead(node);                p.next = null; // help GC                failed = false;                return interrupted;            }            if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;                parkAndChecknIterrupt())                interrupted = true;        }    } finally {        if (failed)            cancelAcquire(node);    }}private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) {    int ws = pred.waitStatus;    if (ws == Node.SIGNAL)        return true;    if (ws &gt; 0) {        do {            node.prev = pred = pred.prev;        } while (pred.waitStatus &gt; 0);        pred.next = node;    } else {        compareAndSetWaitStatus(pred, ws, Node.SIGNAL);    }    return false;}private final boolean parkAndCheckInterrupt() {    LockSupport.park(this);    return Thread.interrupted();}</code></pre><p>acquireQueued()这个方法会先判断当前传入的Node对应的前置节点是否为head，如果是则尝试加锁。加锁成功过则将当前节点设置为head节点，然后空置之前的head节点，方便后续被垃圾回收掉。</p><p>如果加锁失败或者Node的前置节点不是head节点，就会通过shouldParkAfterFailedAcquire方法 将head节点的waitStatus变为了SIGNAL=-1，最后执行parkAndChecknIterrupt方法，调用LockSupport.park()挂起当前线程。</p><p>此时AQS中的数据如下图：</p><p><img src="/2020/03/18/2020-03-18-bing-fa-aqs-shen-ru-li-jie/648.png" alt></p><p>此时线程二就静静的待在AQS的等待队列里面了，等着其他线程释放锁来唤醒它。</p><h3 id="线程三抢占锁失败"><a href="#线程三抢占锁失败" class="headerlink" title="线程三抢占锁失败"></a>线程三抢占锁失败</h3><p>看完了线程二抢占锁失败的分析，那么再来分析线程三抢占锁失败就很简单了，先看看addWaiter(Node mode)方法：</p><pre><code>private Node addWaiter(Node mode) {    Node node = new Node(Thread.currentThread(), mode);    Node pred = tail;    if (pred != null) {        node.prev = pred;        if (compareAndSetTail(pred, node)) {            pred.next = node;            return node;        }    }    enq(node);    return node;}</code></pre><p>此时等待队列的tail节点指向线程二，进入if逻辑后，通过CAS指令将tail节点重新指向线程三。</p><p>接着线程三调用enq()方法执行入队操作，和上面线程二执行方式是一致的，入队后会修改线程二对应的Node中的waitStatus=SIGNAL。最后线程三也会被挂起。此时等待队列的数据如图：</p><p><img src="/2020/03/18/2020-03-18-bing-fa-aqs-shen-ru-li-jie/649.png" alt></p><h3 id="线程一释放锁"><a href="#线程一释放锁" class="headerlink" title="线程一释放锁"></a>线程一释放锁</h3><p>现在来分析下释放锁的过程，首先是线程一释放锁，释放锁后会唤醒head节点的后置节点，也就是我们现在的线程二，具体操作流程如下：</p><p><img src="/2020/03/18/2020-03-18-bing-fa-aqs-shen-ru-li-jie/650.png" alt></p><p>执行完后等待队列数据如下：</p><p><img src="/2020/03/18/2020-03-18-bing-fa-aqs-shen-ru-li-jie/651.png" alt></p><p>此时线程二已经被唤醒，继续尝试获取锁，如果获取锁失败，则会继续被挂起。如果获取锁成功，则AQS中数据如图：</p><p><img src="/2020/03/18/2020-03-18-bing-fa-aqs-shen-ru-li-jie/652.png" alt></p><h3 id="接着还是一步步拆解来看，先看看线程一释放锁的代码："><a href="#接着还是一步步拆解来看，先看看线程一释放锁的代码：" class="headerlink" title="接着还是一步步拆解来看，先看看线程一释放锁的代码："></a>接着还是一步步拆解来看，先看看线程一释放锁的代码：</h3><p>java.util.concurrent.locks.AbstractQueuedSynchronizer.release()</p><pre><code>public final boolean release(int arg) {    if (tryRelease(arg)) {        Node h = head;        if (h != null &amp;&amp; h.waitStatus != 0)            unparkSuccessor(h);        return true;    }    return false;}</code></pre><p>这里首先会执行tryRelease()方法，这个方法具体实现在ReentrantLock中，如果tryRelease执行成功，则继续判断head节点的waitStatus是否为0</p><p>前面我们已经看到过，head的waitStatue为SIGNAL(-1)，这里就会执行unparkSuccessor()方法来唤醒head的后置节点，也就是我们上面图中线程二对应的Node节点。</p><p>此时看ReentrantLock.tryRelease()中的具体实现：</p><pre><code>protected final boolean tryRelease(int releases) {    int c = getState() - releases;    if (Thread.currentThread() != getExclusiveOwnerThread())        throw new IllegalMonitorStateException();    boolean free = false;    if (c == 0) {        free = true;        setExclusiveOwnerThread(null);    }    setState(c);    return free;}</code></pre><p>执行完ReentrantLock.tryRelease()后，state被设置成0，Lock对象的独占锁被设置为null。此时看下AQS中的数据：</p><p><img src="/2020/03/18/2020-03-18-bing-fa-aqs-shen-ru-li-jie/653.png" alt></p><p>接着执行java.util.concurrent.locks.AbstractQueuedSynchronizer.unparkSuccessor()方法，唤醒head的后置节点：</p><pre><code>private void unparkSuccessor(Node node) {    int ws = node.waitStatus;    if (ws &lt; 0)        compareAndSetWaitStatus(node, ws, 0);    Node s = node.next;    if (s == null || s.waitStatus &gt; 0) {        s = null;        for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev)            if (t.waitStatus &lt;= 0)                s = t;    }    if (s != null)        LockSupport.unpark(s.thread);}</code></pre><p>这里主要是将head节点的waitStatus设置为0，然后解除head节点next的指向，使head节点空置，等待着被垃圾回收。</p><p>此时重新将head指针指向线程二对应的Node节点，且使用LockSupport.unpark方法来唤醒线程二。</p><p>被唤醒的线程二会接着尝试获取锁，用CAS指令修改state数据。执行完成后可以查看AQS中数据：</p><p><img src="/2020/03/18/2020-03-18-bing-fa-aqs-shen-ru-li-jie/654.png" alt></p><p>此时线程二被唤醒，线程二接着之前被park的地方继续执行，继续执行acquireQueued()方法。</p><h3 id="线程二唤醒继续加锁"><a href="#线程二唤醒继续加锁" class="headerlink" title="线程二唤醒继续加锁"></a>线程二唤醒继续加锁</h3><pre><code>final boolean acquireQueued(final Node node, int arg) {    boolean failed = true;    try {        boolean interrupted = false;        for (;;) {            final Node p = node.predecessor();            if (p == head &amp;&amp; tryAcquire(arg)) {                setHead(node);                p.next = null; // help GC                failed = false;                return interrupted;            }            if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;                parkAndCheckInterrupt())                interrupted = true;        }    } finally {        if (failed)            cancelAcquire(node);    }}</code></pre><p>此时线程二被唤醒，继续执行for循环，判断线程二的前置节点是否为head，如果是则继续使用tryAcquire()方法来尝试获取锁，其实就是使用CAS操作来修改state值，如果修改成功则代表获取锁成功。接着将线程二设置为head节点，然后空置之前的head节点数据，被空置的节点数据等着被垃圾回收。</p><p>此时线程三获取锁成功，AQS中队列数据如下：</p><p><img src="/2020/03/18/2020-03-18-bing-fa-aqs-shen-ru-li-jie/655.png" alt></p><p>等待队列中的数据都等待着被垃圾回收。</p><h3 id="线程二释放锁-线程三加锁"><a href="#线程二释放锁-线程三加锁" class="headerlink" title="线程二释放锁/线程三加锁"></a>线程二释放锁/线程三加锁</h3><p>当线程二释放锁时，会唤醒被挂起的线程三，流程和上面大致相同，被唤醒的线程三会再次尝试加锁，具体代码可以参考上面内容。具体流程图如下：</p><p><img src="/2020/03/18/2020-03-18-bing-fa-aqs-shen-ru-li-jie/656.png" alt></p><p>此时AQS中队列数据如图：</p><p><img src="/2020/03/18/2020-03-18-bing-fa-aqs-shen-ru-li-jie/657.png" alt></p><h2 id="划重点，总结辣："><a href="#划重点，总结辣：" class="headerlink" title="划重点，总结辣："></a>划重点，总结辣：</h2><h3 id="AQS实现原理："><a href="#AQS实现原理：" class="headerlink" title="AQS实现原理："></a>AQS实现原理：</h3><ul><li>AQS中 维护了一个volatile int state（代表共享资源）和一个FIFO双向等待CLH队列（多线程争用资源被阻塞时会进入此队列）。</li><li>另外state的操作都是通过CAS来保证其并发修改的安全性。</li><li>这里volatile能够保证多线程下的可见性，当state=1则代表当前对象锁已经被占有，其他线程来加锁时则会失败，加锁失败的线程会被放入一个FIFO的等待队列中，比列会被UNSAFE.park()操作挂起，等待其他获取锁的线程释放锁才能够被唤醒。</li><li>当线程执行完逻辑后，会释放同步状态，释放后会唤醒其后继节点，使其再次尝试获取同步状态。</li></ul><h3 id="加锁和挂起线程流程："><a href="#加锁和挂起线程流程：" class="headerlink" title="加锁和挂起线程流程："></a>加锁和挂起线程流程：</h3><ol><li><p>首先调用nonfairTryAcquire()获取<code>state</code>的值。如果为0，利用<code>CAS</code>尝试抢占锁compareAndSetState(0, 1)，且设置对象独占锁线程为当前线程setExclusiveOwnerThread(Thread.currentThread())；如果不为0则说明当前对象的锁已经被其他线程所占有，接着判断占有锁的线程是否为当前线程，如果是则累加<code>state</code>值。获取锁成功。</p></li><li><p>如果获取锁失败：首先执行<code>addWaiter(Node.EXCLUSIVE)</code>，加入到一个<code>FIFO</code>   CLH等待队列中。<code>addWaiter()</code>方法执行完后，会返回当前线程创建的节点信息。继续往后执行<code>acquireQueued(addWaiter(Node.EXCLUSIVE), arg)</code>逻辑。<code>acquireQueued()</code>这个方法会先判断当前传入的<code>Node</code>对应的前置节点是否为<code>head</code>，如果是则尝试加锁。加锁成功过则将当前节点设置为<code>head</code>节点，然后空置之前的<code>head</code>节点，方便后续被垃圾回收掉。如果加锁失败或者<code>Node</code>的前置节点不是<code>head</code>节点，就会通过<code>shouldParkAfterFailedAcquire</code>方法 将前一个节点的<code>waitStatus</code>变为了<code>SIGNAL=-1</code>，最后执行<code>parkAndChecknIterrupt</code>方法，调用<code>LockSupport.park()</code>挂起当前线程，等着其他线程释放锁来唤醒它。</p></li></ol><h3 id="释放锁和唤醒线程流程："><a href="#释放锁和唤醒线程流程：" class="headerlink" title="释放锁和唤醒线程流程："></a>释放锁和唤醒线程流程：</h3><ol><li>首先是拥有锁的线程释放锁，释放锁后会唤醒<code>head</code>节点的后置节点。</li><li>首先会执行<code>tryRelease()</code>方法，执行完<code>ReentrantLock.tryRelease()</code>后，<code>state</code>被设置成0，Lock对象的独占锁被设置为null。如果<code>tryRelease</code>执行成功，则继续判断<code>head</code>节点的<code>waitStatus</code>是否为0，就会执行<code>unparkSuccessor()</code>方法来唤醒<code>head</code>的后置节点。<code>unparkSuccessor()</code>主要是将<code>head</code>节点的<code>waitStatus</code>设置为0，然后解除<code>head</code>节点<code>next</code>的指向，使<code>head</code>节点空置，等待着被垃圾回收。然后唤醒的节点就可以执行加锁流程了。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA并发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入分析CAS</title>
      <link href="/2020/03/17/2020-03-17-bing-fa-cas/"/>
      <url>/2020/03/17/2020-03-17-bing-fa-cas/</url>
      
        <content type="html"><![CDATA[<p>CAS，Compare And Swap，即比较并交换。整个AQS同步组件、Atomic原子类操作等等都是以CAS实现的，甚至ConcurrentHashMap在1.8的版本中也调整为了CAS+Synchronized。可以说CAS是整个JUC的基石。</p><p><img src="/2020/03/17/2020-03-17-bing-fa-cas/sbwtrygz.bmp" alt></p><h2 id="CAS分析"><a href="#CAS分析" class="headerlink" title="CAS分析"></a>CAS分析</h2><p>在CAS中有三个参数：内存值V、旧的预期值A、要更新的值B，当且仅当内存值V的值等于旧的预期值A时才会将内存值V的值修改为B，否则什么都不干。其伪代码如下：</p><pre><code>if(this.value == A){    this.value = B    return true;}else{    return false;}</code></pre><p>JUC下的atomic类都是通过CAS来实现的，下面就以AtomicInteger为例来阐述CAS的实现。如下：</p><pre><code>private static final Unsafe unsafe = Unsafe.getUnsafe();    private static final long valueOffset;    static {        try {            valueOffset = unsafe.objectFieldOffset(AtomicInteger.class.getDeclaredField(&quot;value&quot;));        } catch (Exception ex) {             throw new Error(ex);         }    }    private volatile int value;</code></pre><p>Unsafe是CAS的核心类，Java无法直接访问底层操作系统，而是通过本地（native）方法来访问。不过尽管如此，JVM还是开了一个后门：Unsafe，它提供了硬件级别的原子操作。</p><p>valueOffset为变量值在内存中的偏移地址，unsafe就是通过偏移地址来得到数据的原值的。</p><p>value当前值，使用volatile修饰，保证多线程环境下看见的是同一个。</p><p>我们就以AtomicInteger的addAndGet()方法来做说明，先看源代码：</p><pre><code>public final int addAndGet(int delta) {    return unsafe.getAndAddInt(this, valueOffset, delta) + delta;}public final int getAndAddInt(Object var1, long var2, int var4) {    int var5;    do {        var5 = this.getIntVolatile(var1, var2);    } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4));    return var5;}</code></pre><p>内部调用unsafe的getAndAddInt方法，在getAndAddInt方法中主要是看compareAndSwapInt方法：</p><pre><code>public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5);</code></pre><p>该方法为本地方法，有四个参数，分别代表：对象、对象的地址、预期值、修改值（有位伙伴告诉我他面试的时候就问到这四个变量是啥意思…+_+）。该方法的实现这里就不做详细介绍了，有兴趣的伙伴可以看看openjdk的源码。</p><p>CAS可以保证一次的读-改-写操作是原子操作，在单处理器上该操作容易实现，但是在多处理器上实现就有点儿复杂了。</p><p><strong>CPU提供了两种方法来实现多处理器的原子操作：总线加锁或者缓存加锁</strong></p><p>总线加锁：总线加锁就是使用处理器提供的一个LOCK#信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住,那么该处理器可以独占使用共享内存。但是这种处理方式显得有点儿霸道，不厚道，他把CPU和内存之间的通信锁住了，在锁定期间，其他处理器都不能其他内存地址的数据，其开销有点儿大。所以就有了缓存加锁。</p><p>缓存加锁：其实针对于上面那种情况我们只需要保证在同一时刻对某个内存地址的操作是原子性的即可。缓存加锁就是缓存在内存区域的数据如果在加锁期间，当它执行锁操作写回内存时，处理器不在输出LOCK#信号，而是修改内部的内存地址，利用缓存一致性协议来保证原子性。缓存一致性机制可以保证同一个内存区域的数据仅能被一个处理器修改，也就是说当CPU1修改缓存行中的i时使用缓存锁定，那么CPU2就不能同时缓存了i的缓存行。</p><h2 id="CAS缺陷"><a href="#CAS缺陷" class="headerlink" title="CAS缺陷"></a>CAS缺陷</h2><p><strong>CAS虽然高效地解决了原子操作，但是还是存在一些缺陷的，主要表现在三个方法：循环时间太长、只能保证一个共享变量原子操作、ABA问题。</strong></p><h3 id="循环时间太长"><a href="#循环时间太长" class="headerlink" title="循环时间太长"></a>循环时间太长</h3><p>如果CAS一直不成功呢？这种情况绝对有可能发生，如果自旋CAS长时间地不成功，则会给CPU带来非常大的开销。在JUC中有些地方就限制了CAS自旋的次数，例如BlockingQueue的SynchronousQueue。</p><h3 id="只能保证一个共享变量原子操作"><a href="#只能保证一个共享变量原子操作" class="headerlink" title="只能保证一个共享变量原子操作"></a>只能保证一个共享变量原子操作</h3><p>看了CAS的实现就知道这只能针对一个共享变量，如果是多个共享变量就只能使用锁了，当然如果你有办法把多个变量整成一个变量，利用CAS也不错。例如读写锁中state的高地位</p><h3 id="ABA问题"><a href="#ABA问题" class="headerlink" title="ABA问题"></a>ABA问题</h3><p>CAS需要检查操作值有没有发生改变，如果没有发生改变则更新。但是存在这样一种情况：如果一个值原来是A，变成了B，然后又变成了A，那么在CAS检查的时候会发现没有改变，但是实质上它已经发生了改变，这就是所谓的ABA问题。对于ABA问题其解决方案是加上版本号，即在每个变量都加上一个版本号，每次改变时加1，即A —&gt; B —&gt; A，变成1A —&gt; 2B —&gt; 3A。</p><p>用一个例子来阐述ABA问题所带来的影响。有如下链表</p><p><img src="/2020/03/17/2020-03-17-bing-fa-cas/t4nx8xbp.bmp" alt></p><p>假如我们想要把B替换为A，也就是compareAndSet(this,A,B)。线程1执行B替换A操作，线程2主要执行如下动作，A 、B出栈，然后C、A入栈，最终该链表如下：</p><p><img src="/2020/03/17/2020-03-17-bing-fa-cas/8537ax53.bmp" alt></p><p>完成后线程1发现仍然是A，那么compareAndSet(this,A,B)成功，但是这时会存在一个问题就是B.next = null,compareAndSet(this,A,B)后，会导致C丢失，改栈仅有一个B元素，平白无故把C给丢失了。</p><p><strong>CAS的ABA隐患问题，解决方案则是版本号，Java提供了AtomicStampedReference来解决。</strong>AtomicStampedReference通过包装[E,Integer]的元组来对对象标记版本戳stamp，从而避免ABA问题。对于上面的案例应该线程1会失败。</p><p>AtomicStampedReference的compareAndSet()方法定义如下：</p><pre><code>public boolean compareAndSet(V expectedReference, V newReference, int expectedStamp, int newStamp) {    Pair&lt;V&gt; current = pair;    return        expectedReference == current.reference &amp;&amp;        expectedStamp == current.stamp &amp;&amp;        ((newReference == current.reference &amp;&amp;            newStamp == current.stamp) ||            casPair(current, Pair.of(newReference, newStamp)));}</code></pre><p>compareAndSet有四个参数，分别表示：预期引用、更新后的引用、预期标志、更新后的标志。源码很好理解，预期的引用 == 当前引用，预期的标识 == 当前标识，如果更新后的引用和标志和当前的引用和标志相等则直接返回true，否则通过Pair生成一个新的pair对象与当前pair CAS替换。Pair为AtomicStampedReference的内部类，主要用于记录引用和版本戳信息（标识），定义如下：</p><pre><code>private static class Pair&lt;T&gt; {    final T reference;    final int stamp;    private Pair(T reference, int stamp) {        this.reference = reference;        this.stamp = stamp;    }    static &lt;T&gt; Pair&lt;T&gt; of(T reference, int stamp) {        return new Pair&lt;T&gt;(reference, stamp);    }}private volatile Pair&lt;V&gt; pair;</code></pre><p>Pair记录着对象的引用和版本戳，版本戳为int型，保持自增。同时Pair是一个不可变对象，其所有属性全部定义为final，对外提供一个of方法，该方法返回一个新建的Pari对象。pair对象定义为volatile，保证多线程环境下的可见性。在AtomicStampedReference中，大多方法都是通过调用Pair的of方法来产生一个新的Pair对象，然后赋值给变量pair。如set方法：</p><pre><code>public void set(V newReference, int newStamp) {    Pair&lt;V&gt; current = pair;    if (newReference != current.reference || newStamp != current.stamp)        this.pair = Pair.of(newReference, newStamp);}</code></pre><p>下面我们将通过一个例子可以可以看到AtomicStampedReference和AtomicInteger的区别。我们定义两个线程，线程1负责将100 —&gt; 110 —&gt; 100，线程2执行 100 —&gt;120，看两者之间的区别。</p><pre><code>public class Test {    private static AtomicInteger atomicInteger = new AtomicInteger(100);    private static AtomicStampedReference atomicStampedReference = new AtomicStampedReference(100,1);    public static void main(String[] args) throws InterruptedException {        //AtomicInteger        Thread at1 = new Thread(new Runnable() {            @Override            public void run() {                atomicInteger.compareAndSet(100,110);                atomicInteger.compareAndSet(110,100);            }        });        Thread at2 = new Thread(new Runnable() {            @Override            public void run() {                try {                    TimeUnit.SECONDS.sleep(2);      // at1,执行完                } catch (InterruptedException e) {                    e.printStackTrace();                }                System.out.println(&quot;AtomicInteger:&quot; + atomicInteger.compareAndSet(100,120));            }        });        at1.start();        at2.start();        at1.join();        at2.join();        //AtomicStampedReference        Thread tsf1 = new Thread(new Runnable() {            @Override            public void run() {                try {                    //让 tsf2先获取stamp，导致预期时间戳不一致                    TimeUnit.SECONDS.sleep(2);                } catch (InterruptedException e) {                    e.printStackTrace();                }                // 预期引用：100，更新后的引用：110，预期标识getStamp() 更新后的标识getStamp() + 1                atomicStampedReference.compareAndSet(100,110,atomicStampedReference.getStamp(),atomicStampedReference.getStamp() + 1);                atomicStampedReference.compareAndSet(110,100,atomicStampedReference.getStamp(),atomicStampedReference.getStamp() + 1);            }        });        Thread tsf2 = new Thread(new Runnable() {            @Override            public void run() {                int stamp = atomicStampedReference.getStamp();                try {                    TimeUnit.SECONDS.sleep(2);      //线程tsf1执行完                } catch (InterruptedException e) {                    e.printStackTrace();                }                System.out.println(&quot;AtomicStampedReference:&quot; +atomicStampedReference.compareAndSet(100,120,stamp,stamp + 1));            }        });        tsf1.start();        tsf2.start();    }}</code></pre><p>运行结果：</p><p><img src="/2020/03/17/2020-03-17-bing-fa-cas/felgivl0.bmp" alt></p><p>运行结果充分展示了AtomicInteger的ABA问题和AtomicStampedReference解决ABA问题。</p><p>其他用法：</p><pre><code>private static AtomicReference&lt;String&gt; ar = new AtomicReference&lt;String&gt;();private static AtomicStampedReference&lt;Integer&gt; atomicStampedRef = new AtomicStampedReference&lt;&gt;(1, 0);</code></pre><h2 id="划重点，总结辣："><a href="#划重点，总结辣：" class="headerlink" title="划重点，总结辣："></a>划重点，总结辣：</h2><ul><li>CAS，Compare And Swap，即比较和交换。</li><li>在CAS中有三个参数：内存值V、旧的预期值A、要更新的值B，当且仅当内存值V的值等于旧的预期值A时才会将内存值V的值修改为B，否则什么都不干。</li><li>CAS虽然高效地解决了原子操作，但是还是存在一些缺陷的，主要表现在三个方法：循环时间太长、只能保证一个共享变量原子操作、ABA问题。<ul><li>循环时间太长：如果自旋CAS长时间地不成功，则会给CPU带来非常大的开销。在JUC中有些地方就限制了CAS自旋的次数，例如BlockingQueue的SynchronousQueue。</li><li>只能保证一个共享变量原子操作：看了CAS的实现就知道这只能针对一个共享变量，如果是多个共享变量就只能使用锁了，当然如果你有办法把多个变量整成一个变量，利用CAS也不错。</li><li>ABA问题：对于ABA问题其解决方案是加上版本号，即在每个变量都加上一个版本号，每次改变时加1，即A —&gt; B —&gt; A，变成1A —&gt; 2B —&gt; 3A。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA并发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>J.U.C之AQS：阻塞和唤醒线程</title>
      <link href="/2020/03/16/2020-03-16-bing-fa-aqs-zu-sai-he-huan-xing-xian-cheng/"/>
      <url>/2020/03/16/2020-03-16-bing-fa-aqs-zu-sai-he-huan-xing-xian-cheng/</url>
      
        <content type="html"><![CDATA[<p><strong>在AQS中维护着一个FIFO的同步队列，当线程获取同步状态失败后，则会加入到这个CLH同步队列的队尾并一直保持着自旋。在CLH同步队列中的线程在自旋时会判断其前驱节点是否为首节点，如果为首节点则不断尝试获取同步状态，获取成功则退出CLH同步队列。当线程执行完逻辑后，会释放同步状态，释放后会唤醒其后继节点。</strong></p><pre><code>final boolean acquireQueued(final Node node, int arg) {    boolean failed = true;    try {        //中断标志        boolean interrupted = false;        /*         * 自旋过程，其实就是一个死循环而已         */        for (;;) {            //当前线程的前驱节点            final Node p = node.predecessor();            //当前线程的前驱节点是头结点，且同步状态成功            if (p == head &amp;&amp; tryAcquire(arg)) {                setHead(node);                p.next = null; // help GC                failed = false;                return interrupted;            }            //获取失败，线程等待--具体后面介绍            if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;                    parkAndCheckInterrupt())                interrupted = true;        }    } finally {        if (failed)            cancelAcquire(node);    }}</code></pre><h2 id="阻塞"><a href="#阻塞" class="headerlink" title="阻塞"></a>阻塞</h2><p>在线程获取同步状态时如果获取失败，则加入CLH同步队列，通过自旋的方式不断获取同步状态，但是在自旋的过程中则需要判断当前线程是否需要阻塞，其主要方法在acquireQueued()：</p><pre><code>if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;    parkAndCheckInterrupt())    interrupted = true;</code></pre><p>通过这段代码我们可以看到，在获取同步状态失败后，线程并不是立马进行阻塞，需要检查该线程的状态，检查状态的方法为 shouldParkAfterFailedAcquire(Node pred, Node node) 方法，该方法主要靠前驱节点判断当前线程是否应该被阻塞，代码如下：</p><pre><code>private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) {    //前驱节点    int ws = pred.waitStatus;    //状态为signal，表示当前线程处于等待状态，直接放回true    if (ws == Node.SIGNAL)        return true;    //前驱节点状态 &gt; 0 ，则为Cancelled,表明该节点已经超时或者被中断了，需要从同步队列中取消    if (ws &gt; 0) {        do {            node.prev = pred = pred.prev;        } while (pred.waitStatus &gt; 0);        pred.next = node;    }     //前驱节点状态为Condition、propagate    else {        compareAndSetWaitStatus(pred, ws, Node.SIGNAL);    }    return false;}</code></pre><p>这段代码主要检查当前线程是否需要被阻塞，具体规则如下：</p><ol><li>如果当前线程的前驱节点状态为SINNAL，则表明当前线程需要被阻塞，调用unpark()方法唤醒，直接返回true，当前线程阻塞</li><li>如果当前线程的前驱节点状态为CANCELLED（ws &gt; 0），则表明该线程的前驱节点已经等待超时或者被中断了，则需要从CLH队列中将该前驱节点删除掉，直到回溯到前驱节点状态 &lt;= 0 ，返回false</li><li>如果前驱节点非SINNAL，非CANCELLED，则通过CAS的方式将其前驱节点设置为SINNAL，返回false</li></ol><p>如果 shouldParkAfterFailedAcquire(Node pred, Node node) 方法返回true，则调用parkAndCheckInterrupt()方法阻塞当前线程：</p><pre><code>private final boolean parkAndCheckInterrupt() {    LockSupport.park(this);    return Thread.interrupted();}</code></pre><p>parkAndCheckInterrupt() 方法主要是把当前线程挂起，从而阻塞住线程的调用栈，同时返回当前线程的中断状态。其内部则是调用LockSupport工具类的park()方法来阻塞该方法。</p><h2 id="唤醒"><a href="#唤醒" class="headerlink" title="唤醒"></a>唤醒</h2><p>当线程释放同步状态后，则需要唤醒该线程的后继节点：</p><pre><code>public final boolean release(int arg) {    if (tryRelease(arg)) {        Node h = head;        if (h != null &amp;&amp; h.waitStatus != 0)            //唤醒后继节点            unparkSuccessor(h);        return true;    }    return false;}</code></pre><p>调用unparkSuccessor(Node node)唤醒后继节点：</p><pre><code>private void unparkSuccessor(Node node) {    //当前节点状态    int ws = node.waitStatus;    //当前状态 &lt; 0 则设置为 0    if (ws &lt; 0)        compareAndSetWaitStatus(node, ws, 0);    //当前节点的后继节点    Node s = node.next;    //后继节点为null或者其状态 &gt; 0 (超时或者被中断了)    if (s == null || s.waitStatus &gt; 0) {        s = null;        //从tail节点来找可用节点        for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev)            if (t.waitStatus &lt;= 0)                s = t;    }    //唤醒后继节点    if (s != null)        LockSupport.unpark(s.thread);}</code></pre><p>可能会存在当前线程的后继节点为null，超时、被中断的情况，如果遇到这种情况了，则需要跳过该节点，但是为何是从tail尾节点开始，而不是从node.next开始呢？原因在于node.next仍然可能会存在null或者取消了，所以采用tail回溯办法找第一个可用的线程。最后调用LockSupport的unpark(Thread thread)方法唤醒该线程。</p><h2 id="LockSupport"><a href="#LockSupport" class="headerlink" title="LockSupport"></a>LockSupport</h2><p>从上面我可以看到，当需要阻塞或者唤醒一个线程的时候，AQS都是使用LockSupport这个工具类来完成的。</p><p>LockSupport是用来创建锁和其他同步类的基本线程阻塞原语</p><p>每个使用LockSupport的线程都会与一个许可关联，如果该许可可用，并且可在进程中使用，则调用park()将会立即返回，否则可能阻塞。如果许可尚不可用，则可以调用 unpark 使其可用。但是注意许可不可重入，也就是说只能调用一次park()方法，否则会一直阻塞。</p><p>LockSupport定义了一系列以park开头的方法来阻塞当前线程，unpark(Thread thread)方法来唤醒一个被阻塞的线程。如下：</p><p><img src="/2020/03/16/2020-03-16-bing-fa-aqs-zu-sai-he-huan-xing-xian-cheng/cghrhlp8.bmp" alt></p><p>park(Object blocker)方法的blocker参数，主要是用来标识当前线程在等待的对象，该对象主要用于问题排查和系统监控。</p><p>park方法和unpark(Thread thread)都是成对出现的，同时unpark必须要在park执行之后执行，当然并不是说没有不调用unpark线程就会一直阻塞，park有一个方法，它带了时间戳（parkNanos(long nanos)：为了线程调度禁用当前线程，最多等待指定的等待时间，除非许可可用）。</p><p>park()方法的源码如下：</p><pre><code>public static void park() {    UNSAFE.park(false, 0L);}</code></pre><p>unpark(Thread thread)方法源码如下：</p><pre><code>public static void unpark(Thread thread) {    if (thread != null)        UNSAFE.unpark(thread);}</code></pre><p>从上面可以看出，其内部的实现都是通过UNSAFE（sun.misc.Unsafe UNSAFE）来实现的，其定义如下：</p><pre><code>public native void park(boolean var1, long var2);public native void unpark(Object var1);</code></pre><p>两个都是native本地方法。Unsafe 是一个比较危险的类，主要是用于执行低级别、不安全的方法集合。尽管这个类和所有的方法都是公开的（public），但是这个类的使用仍然受限，你无法在自己的java程序中直接使用该类，因为只有授信的代码才能获得该类的实例。</p><h2 id="划重点，总结辣："><a href="#划重点，总结辣：" class="headerlink" title="划重点，总结辣："></a>划重点，总结辣：</h2><ul><li>在AQS中维护着一个FIFO的同步队列，当线程获取同步状态失败后，则会加入到这个CLH同步队列的队尾并一直保持着自旋。在CLH同步队列中的线程在自旋时会判断其前驱节点是否为首节点，如果为首节点则不断尝试获取同步状态，获取成功则退出CLH同步队列。当线程执行完逻辑后，会释放同步状态，释放后会唤醒其后继节点。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA并发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>J.U.C之AQS：同步状态的获取与释放</title>
      <link href="/2020/03/15/2020-03-15-bing-fa-aqs-tong-bu-zhuang-tai-de-huo-qu-yu-shi-fang/"/>
      <url>/2020/03/15/2020-03-15-bing-fa-aqs-tong-bu-zhuang-tai-de-huo-qu-yu-shi-fang/</url>
      
        <content type="html"><![CDATA[<p>AQS是构建Java同步组件的基础，我们期待它能够成为实现大部分同步需求的基础。AQS的设计模式采用的模板方法模式，子类通过继承的方式，实现它的抽象方法来管理同步状态，对于子类而言它并没有太多的活要做，AQS提供了大量的模板方法来实现同步，主要是分为三类：独占式获取和释放同步状态、共享式获取和释放同步状态、查询同步队列中的等待线程情况。自定义子类使用AQS提供的模板方法就可以实现自己的同步语义。</p><h2 id="独占式"><a href="#独占式" class="headerlink" title="独占式"></a>独占式</h2><p>独占式，同一时刻仅有一个线程持有同步状态。</p><h3 id="独占式同步状态获取"><a href="#独占式同步状态获取" class="headerlink" title="独占式同步状态获取"></a>独占式同步状态获取</h3><p>acquire(int arg)方法为AQS提供的模板方法，该方法为独占式获取同步状态，但是该方法对中断不敏感，也就是说由于线程获取同步状态失败加入到CLH同步队列中，后续对线程进行中断操作时，线程不会从同步队列中移除。代码如下：</p><pre><code>public final void acquire(int arg) {    if (!tryAcquire(arg) &amp;&amp;        acquireQueued(addWaiter(Node.EXCLUSIVE), arg))        selfInterrupt();}</code></pre><p>各个方法定义如下：</p><ol><li>tryAcquire：去尝试获取锁，获取成功则设置锁状态并返回true，否则返回false。该方法自定义同步组件自己实现，该方法必须要保证线程安全的获取同步状态。</li><li>addWaiter：如果tryAcquire返回FALSE（获取同步状态失败），则调用该方法将当前线程加入到CLH同步队列尾部。</li><li>acquireQueued：当前线程会根据公平性原则来进行阻塞等待（自旋）,直到获取锁为止；并且返回当前线程在等待过程中有没有中断过。</li><li>selfInterrupt：产生一个中断。</li></ol><p>acquireQueued方法为一个自旋的过程，也就是说当前线程（Node）进入同步队列后，就会进入一个自旋的过程，每个节点都会自省地观察，当条件满足，获取到同步状态后，就可以从这个自旋过程中退出，否则会一直执行下去。如下：</p><pre><code>final boolean acquireQueued(final Node node, int arg) {    boolean failed = true;    try {        //中断标志        boolean interrupted = false;        /*         * 自旋过程，其实就是一个死循环而已         */        for (;;) {            //当前线程的前驱节点            final Node p = node.predecessor();            //当前线程的前驱节点是头结点，且同步状态成功            if (p == head &amp;&amp; tryAcquire(arg)) {                setHead(node);                p.next = null; // help GC                failed = false;                return interrupted;            }            //获取失败，线程等待--具体后面介绍            if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;                    parkAndCheckInterrupt())                interrupted = true;        }    } finally {        if (failed)            cancelAcquire(node);    }}</code></pre><p>从上面代码中可以看到，当前线程会一直尝试获取同步状态，当然前提是只有其前驱节点为头结点才能够尝试获取同步状态，理由：</p><ol><li>保持FIFO同步队列原则。</li><li>头节点释放同步状态后，将会唤醒其后继节点，后继节点被唤醒后需要检查自己是否为头节点。</li></ol><p>acquire(int arg)方法流程图如下：</p><p><img src="/2020/03/15/2020-03-15-bing-fa-aqs-tong-bu-zhuang-tai-de-huo-qu-yu-shi-fang/34hrfcfc.bmp" alt></p><h3 id="独占式获取响应中断"><a href="#独占式获取响应中断" class="headerlink" title="独占式获取响应中断"></a>独占式获取响应中断</h3><p>AQS提供了acquire(int arg)方法以供独占式获取同步状态，但是该方法对中断不响应，对线程进行中断操作后，该线程会依然位于CLH同步队列中等待着获取同步状态。为了响应中断，AQS提供了acquireInterruptibly(int arg)方法，该方法在等待获取同步状态时，如果当前线程被中断了，会立刻响应中断抛出异常InterruptedException。</p><pre><code>public final void acquireInterruptibly(int arg)        throws InterruptedException {    if (Thread.interrupted())        throw new InterruptedException();    if (!tryAcquire(arg))        doAcquireInterruptibly(arg);}</code></pre><p>首先校验该线程是否已经中断了，如果是则抛出InterruptedException，否则执行tryAcquire(int arg)方法获取同步状态，如果获取成功，则直接返回，否则执行doAcquireInterruptibly(int arg)。doAcquireInterruptibly(int arg)定义如下：</p><pre><code>private void doAcquireInterruptibly(int arg)    throws InterruptedException {    final Node node = addWaiter(Node.EXCLUSIVE);    boolean failed = true;    try {        for (;;) {            final Node p = node.predecessor();            if (p == head &amp;&amp; tryAcquire(arg)) {                setHead(node);                p.next = null; // help GC                failed = false;                return;            }            if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;                parkAndCheckInterrupt())                throw new InterruptedException();        }    } finally {        if (failed)            cancelAcquire(node);    }}</code></pre><p>doAcquireInterruptibly(int arg)方法与acquire(int arg)方法仅有两个差别。1.方法声明抛出InterruptedException异常，2.在中断方法处不再是使用interrupted标志，而是直接抛出InterruptedException异常。</p><h3 id="独占式超时获取"><a href="#独占式超时获取" class="headerlink" title="独占式超时获取"></a>独占式超时获取</h3><p>AQS除了提供上面两个方法外，还提供了一个增强版的方法：tryAcquireNanos(int arg,long nanos)。该方法为acquireInterruptibly方法的进一步增强，它除了响应中断外，还有超时控制。即如果当前线程没有在指定时间内获取同步状态，则会返回false，否则返回true。如下：</p><pre><code>public final boolean tryAcquireNanos(int arg, long nanosTimeout)        throws InterruptedException {    if (Thread.interrupted())        throw new InterruptedException();    return tryAcquire(arg) ||        doAcquireNanos(arg, nanosTimeout);}</code></pre><p>tryAcquireNanos(int arg, long nanosTimeout)方法超时获取最终是在doAcquireNanos(int arg, long nanosTimeout)中实现的，如下：</p><pre><code>private boolean doAcquireNanos(int arg, long nanosTimeout)        throws InterruptedException {    //nanosTimeout &lt;= 0    if (nanosTimeout &lt;= 0L)        return false;    //超时时间    final long deadline = System.nanoTime() + nanosTimeout;    //新增Node节点    final Node node = addWaiter(Node.EXCLUSIVE);    boolean failed = true;    try {        //自旋        for (;;) {            final Node p = node.predecessor();            //获取同步状态成功            if (p == head &amp;&amp; tryAcquire(arg)) {                setHead(node);                p.next = null; // help GC                failed = false;                return true;            }            /*             * 获取失败，做超时、中断判断             */            //重新计算需要休眠的时间            nanosTimeout = deadline - System.nanoTime();            //已经超时，返回false            if (nanosTimeout &lt;= 0L)                return false;            //如果没有超时，则等待nanosTimeout纳秒            //注：该线程会直接从LockSupport.parkNanos中返回，            //LockSupport为JUC提供的一个阻塞和唤醒的工具类，后面做详细介绍            if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;                    nanosTimeout &gt; spinForTimeoutThreshold)                LockSupport.parkNanos(this, nanosTimeout);            //线程是否已经中断了            if (Thread.interrupted())                throw new InterruptedException();        }    } finally {        if (failed)            cancelAcquire(node);    }}</code></pre><p>针对超时控制，程序首先记录唤醒时间deadline ，deadline = System.nanoTime() + nanosTimeout（时间间隔）。如果获取同步状态失败，则需要计算出需要休眠的时间间隔nanosTimeout（= deadline – System.nanoTime()），如果nanosTimeout &lt;= 0 表示已经超时了，返回false，如果大于spinForTimeoutThreshold（1000L）则需要休眠nanosTimeout ，如果nanosTimeout &lt;= spinForTimeoutThreshold ，就不需要休眠了，直接进入快速自旋的过程。原因在于 spinForTimeoutThreshold 已经非常小了，非常短的时间等待无法做到十分精确，如果这时再次进行超时等待，相反会让nanosTimeout 的超时从整体上面表现得不是那么精确，所以在超时非常短的场景中，AQS会进行无条件的快速自旋。</p><p>整个流程如下：</p><p><img src="/2020/03/15/2020-03-15-bing-fa-aqs-tong-bu-zhuang-tai-de-huo-qu-yu-shi-fang/91bf3uo8.bmp" alt></p><h3 id="独占式同步状态释放"><a href="#独占式同步状态释放" class="headerlink" title="独占式同步状态释放"></a>独占式同步状态释放</h3><p>当线程获取同步状态后，执行完相应逻辑后就需要释放同步状态。AQS提供了release(int arg)方法释放同步状态：</p><pre><code>public final boolean release(int arg) {    if (tryRelease(arg)) {        Node h = head;        if (h != null &amp;&amp; h.waitStatus != 0)            unparkSuccessor(h);        return true;    }    return false;}</code></pre><p>该方法同样是先调用自定义同步器自定义的tryRelease(int arg)方法来释放同步状态，释放成功后，会调用unparkSuccessor(Node node)方法唤醒后继节点。</p><p><strong>在AQS中维护着一个FIFO的同步队列，当线程获取同步状态失败后，则会加入到这个CLH同步队列的队尾并一直保持着自旋。在CLH同步队列中的线程在自旋时会判断其前驱节点是否为首节点，如果为首节点则不断尝试获取同步状态，获取成功则退出CLH同步队列。当线程执行完逻辑后，会释放同步状态，释放后会唤醒其后继节点。</strong></p><h2 id="共享式"><a href="#共享式" class="headerlink" title="共享式"></a>共享式</h2><p>共享式与独占式的最主要区别在于同一时刻独占式只能有一个线程获取同步状态，而共享式在同一时刻可以有多个线程获取同步状态。例如读操作可以有多个线程同时进行，而写操作同一时刻只能有一个线程进行写操作，其他操作都会被阻塞。</p><h3 id="共享式同步状态获取"><a href="#共享式同步状态获取" class="headerlink" title="共享式同步状态获取"></a>共享式同步状态获取</h3><p>AQS提供acquireShared(int arg)方法共享式获取同步状态：</p><pre><code>public final void acquireShared(int arg) {    if (tryAcquireShared(arg) &lt; 0)    // 获取失败，自旋获取同步状态        doAcquireShared(arg);}</code></pre><p>从上面程序可以看出，方法首先是调用tryAcquireShared(int arg)方法尝试获取同步状态，如果获取失败则调用doAcquireShared(int arg)自旋方式获取同步状态，共享式获取同步状态的标志是返回 &gt;= 0 的值表示获取成功。自旋式获取同步状态如下：</p><pre><code>private void doAcquireShared(int arg) {    /共享式节点    final Node node = addWaiter(Node.SHARED);    boolean failed = true;    try {        boolean interrupted = false;        for (;;) {            //前驱节点            final Node p = node.predecessor();            //如果其前驱节点，获取同步状态            if (p == head) {                //尝试获取同步                int r = tryAcquireShared(arg);                if (r &gt;= 0) {                    setHeadAndPropagate(node, r);                    p.next = null; // help GC                    if (interrupted)                        selfInterrupt();                    failed = false;                    return;                }            }            if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;                    parkAndCheckInterrupt())                interrupted = true;        }    } finally {        if (failed)            cancelAcquire(node);    }}</code></pre><p>tryAcquireShared(int arg)方法尝试获取同步状态，返回值为int，当其 &gt;= 0 时，表示能够获取到同步状态，这个时候就可以从自旋过程中退出。</p><p>acquireShared(int arg)方法不响应中断，与独占式相似，AQS也提供了响应中断、超时的方法，分别是：acquireSharedInterruptibly(int arg)、tryAcquireSharedNanos(int arg,long nanos)，这里就不做解释了。</p><h3 id="共享式同步状态释放"><a href="#共享式同步状态释放" class="headerlink" title="共享式同步状态释放"></a>共享式同步状态释放</h3><p>获取同步状态后，需要调用release(int arg)方法释放同步状态，方法如下：</p><pre><code>public final boolean releaseShared(int arg) {    if (tryReleaseShared(arg)) {        doReleaseShared();        return true;    }    return false;}</code></pre><p>因为可能会存在多个线程同时进行释放同步状态资源，所以需要确保同步状态安全地成功释放，一般都是通过CAS和循环来完成的。</p><h2 id="划重点，总结辣："><a href="#划重点，总结辣：" class="headerlink" title="划重点，总结辣："></a>划重点，总结辣：</h2><ul><li><p>acquire(int arg)方法流程图如下：</p><p>  <img src="/2020/03/15/2020-03-15-bing-fa-aqs-tong-bu-zhuang-tai-de-huo-qu-yu-shi-fang/34hrfcfc.bmp" alt></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA并发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>J.U.C之AQS：CLH同步队列</title>
      <link href="/2020/03/14/2020-03-14-bing-fa-aqs-clh/"/>
      <url>/2020/03/14/2020-03-14-bing-fa-aqs-clh/</url>
      
        <content type="html"><![CDATA[<p><strong>AQS内部维护着一个FIFO队列，该队列就是CLH同步队列。</strong></p><p><strong>CLH同步队列是一个FIFO双向队列，AQS依赖它来完成同步状态的管理，当前线程如果获取同步状态失败时，AQS则会将当前线程已经等待状态等信息构造成一个节点（Node）并将其加入到CLH同步队列，同时会阻塞当前线程，当同步状态释放时，会把首节点唤醒（公平锁），使其再次尝试获取同步状态。</strong></p><p>在CLH同步队列中，一个节点表示一个线程，它保存着线程的引用（thread）、状态（waitStatus）、前驱节点（prev）、后继节点（next）.</p><pre><code>static final class Node {    /** 共享 */    static final Node SHARED = new Node();    /** 独占 */    static final Node EXCLUSIVE = null;    /**     * 因为超时或者中断，节点会被设置为取消状态，被取消的节点时不会参与到竞争中的，他会一直保持取消状态不会转变为其他状态；     */    static final int CANCELLED =  1;    /**     * 后继节点的线程处于等待状态，而当前节点的线程如果释放了同步状态或者被取消，将会通知后继节点，使后继节点的线程得以运行     */    static final int SIGNAL    = -1;    /**     * 节点在等待队列中，节点线程等待在Condition上，当其他线程对Condition调用了signal()后，改节点将会从等待队列中转移到同步队列中，加入到同步状态的获取中     */    static final int CONDITION = -2;    /**     * 表示下一次共享式同步状态获取将会无条件地传播下去     */    static final int PROPAGATE = -3;    /** 等待状态 */    volatile int waitStatus;    /** 前驱节点 */    volatile Node prev;    /** 后继节点 */    volatile Node next;    /** 获取同步状态的线程 */    volatile Thread thread;    Node nextWaiter;    final boolean isShared() {        return nextWaiter == SHARED;    }    final Node predecessor() throws NullPointerException {        Node p = prev;        if (p == null)            throw new NullPointerException();        else            return p;    }    Node() {    }    Node(Thread thread, Node mode) {        this.nextWaiter = mode;        this.thread = thread;    }    Node(Thread thread, int waitStatus) {        this.waitStatus = waitStatus;        this.thread = thread;    }}</code></pre><p>CLH同步队列结构图如下</p><p><img src="/2020/03/14/2020-03-14-bing-fa-aqs-clh/xft5m3zj.bmp" alt></p><h2 id="入列"><a href="#入列" class="headerlink" title="入列"></a>入列</h2><p>CLH队列入列是再简单不过了，无非就是tail指向新节点、新节点的prev指向当前最后的节点，当前最后一个节点的next指向当前节点。代码我们可以看看addWaiter(Node node)方法：</p><pre><code>private Node addWaiter(Node mode) {    //新建Node    Node node = new Node(Thread.currentThread(), mode);    //快速尝试添加尾节点    Node pred = tail;    if (pred != null) {        node.prev = pred;        //CAS设置尾节点        if (compareAndSetTail(pred, node)) {            pred.next = node;            return node;        }    }    //多次尝试    enq(node);    return node;}</code></pre><p>addWaiter(Node node)先通过快速尝试设置尾节点，如果失败，则调用enq(Node node)方法设置尾节点</p><pre><code>private Node enq(final Node node) {    //多次尝试，直到成功为止    for (;;) {        Node t = tail;        //tail不存在，设置为首节点        if (t == null) {            if (compareAndSetHead(new Node()))                tail = head;        } else {            //设置为尾节点            node.prev = t;            if (compareAndSetTail(t, node)) {                t.next = node;                return t;            }        }    }}</code></pre><p>在上面代码中，两个方法都是通过一个CAS方法compareAndSetTail(Node expect, Node update)来设置尾节点，该方法可以确保节点是线程安全添加的。在enq(Node node)方法中，AQS通过“死循环”的方式来保证节点可以正确添加，只有成功添加后，当前线程才会从该方法返回，否则会一直执行下去。</p><h2 id="出列"><a href="#出列" class="headerlink" title="出列"></a>出列</h2><p>CLH同步队列遵循FIFO，首节点的线程释放同步状态后，将会唤醒它的后继节点（next），而后继节点将会在获取同步状态成功时将自己设置为首节点，这个过程非常简单，head执行该节点并断开原首节点的next和当前节点的prev即可，注意在这个过程是不需要使用CAS来保证的，因为只有一个线程能够成功获取到同步状态。过程图如下：</p><p><img src="/2020/03/14/2020-03-14-bing-fa-aqs-clh/uadl1ixp.bmp" alt></p><h2 id="划重点，总结辣："><a href="#划重点，总结辣：" class="headerlink" title="划重点，总结辣："></a>划重点，总结辣：</h2><ul><li>AQS内部维护着一个FIFO队列，该队列就是CLH同步队列。</li><li>CLH同步队列是一个FIFO双向队列，AQS依赖它来完成同步状态的管理，当前线程如果获取同步状态失败时，AQS则会将当前线程已经等待状态等信息构造成一个节点（Node）并将其加入到CLH同步队列，同时会阻塞当前线程，当同步状态释放时，会把首节点唤醒（公平锁），使其再次尝试获取同步状态。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA并发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>J.U.C之AQS：AQS简介</title>
      <link href="/2020/03/13/2020-03-13-bing-fa-aqs/"/>
      <url>/2020/03/13/2020-03-13-bing-fa-aqs/</url>
      
        <content type="html"><![CDATA[<h2 id="AQS介绍"><a href="#AQS介绍" class="headerlink" title="AQS介绍"></a>AQS介绍</h2><p>AQS，AbstractQueuedSynchronizer，即队列同步器。它是构建锁或者其他同步组件的基础框架（如ReentrantLock、ReentrantReadWriteLock、Semaphore等），JUC并发包的作者（Doug Lea）期望它能够成为实现大部分同步需求的基础。它是JUC并发包中的核心基础组件。</p><p>AQS解决了实现同步器时涉及当的大量细节问题，例如获取同步状态、FIFO同步队列。基于AQS来构建同步器可以带来很多好处。它不仅能够极大地减少实现工作，而且也不必处理在多个位置上发生的竞争问题。</p><p>在基于AQS构建的同步器中，只能在一个时刻发生阻塞，从而降低上下文切换的开销，提高了吞吐量。同时在设计AQS时充分考虑了可伸缩行，因此J.U.C中所有基于AQS构建的同步器均可以获得这个优势。</p><p>AQS的主要使用方式是继承，子类通过继承同步器并实现它的抽象方法来管理同步状态。</p><p><strong>AQS使用一个int类型的成员变量state来表示同步状态，当state&gt;0时表示已经获取了锁，当state = 0时表示释放了锁。它提供了三个方法（getState()、setState(int newState)、compareAndSetState(int expect,int update)）来对同步状态state进行操作，当然AQS可以确保对state的操作是安全的。</strong></p><p><strong>AQS通过内置的FIFO同步队列来完成资源获取线程的排队工作，如果当前线程获取同步状态（锁）失败时，AQS则会将当前线程以及等待状态等信息构造成一个节点（Node）并将其加入同步队列，同时会阻塞当前线程，当同步状态释放时，则会把节点中的线程唤醒，使其再次尝试获取同步状态。</strong></p><p>AQS主要提供了如下一些方法：</p><ul><li>getState()：返回同步状态的当前值；</li><li>setState(int newState)：设置当前同步状态；</li><li>compareAndSetState(int expect, int update)：使用CAS设置当前状态，该方法能够保证状态设置的原子性；</li><li>tryAcquire(int arg)：独占式获取同步状态，获取同步状态成功后，其他线程需要等待该线程释放同步状态才能获取同步状态；</li><li>tryRelease(int arg)：独占式释放同步状态；</li><li>tryAcquireShared(int arg)：共享式获取同步状态，返回值大于等于0则表示获取成功，否则获取失败；</li><li>tryReleaseShared(int arg)：共享式释放同步状态；</li><li>isHeldExclusively()：当前同步器是否在独占式模式下被线程占用，一般该方法表示是否被当前线程所独占；</li><li>acquire(int arg)：独占式获取同步状态，如果当前线程获取同步状态成功，则由该方法返回，否则，将会进入同步队列等待，该方法将会调用可重写的tryAcquire(int arg)方法；</li><li>acquireInterruptibly(int arg)：与acquire(int arg)相同，但是该方法响应中断，当前线程为获取到同步状态而进入到同步队列中，如果当前线程被中断，则该方法会抛出InterruptedException异常并返回；</li><li>tryAcquireNanos(int arg,long nanos)：超时获取同步状态，如果当前线程在nanos时间内没有获取到同步状态，那么将会返回false，已经获取则返回true；</li><li>acquireShared(int arg)：共享式获取同步状态，如果当前线程未获取到同步状态，将会进入同步队列等待，与独占式的主要区别是在同一时刻可以有多个线程获取到同步状态；</li><li>acquireSharedInterruptibly(int arg)：共享式获取同步状态，响应中断；</li><li>tryAcquireSharedNanos(int arg, long nanosTimeout)：共享式获取同步状态，增加超时限制；</li><li>release(int arg)：独占式释放同步状态，该方法会在释放同步状态之后，将同步队列中第一个节点包含的线程唤醒；</li><li>releaseShared(int arg)：共享式释放同步状态；</li></ul><h2 id="划重点，总结辣："><a href="#划重点，总结辣：" class="headerlink" title="划重点，总结辣："></a>划重点，总结辣：</h2><ul><li>AQS使用一个int类型的成员变量state来表示同步状态，当state&gt;0时表示已经获取了锁，当state = 0时表示释放了锁。它提供了三个方法（getState()、setState(int newState)、compareAndSetState(int expect,int update)）来对同步状态state进行操作，当然AQS可以确保对state的操作是安全的。</li><li>AQS通过内置的FIFO双向同步队列来完成资源获取线程的排队工作，如果当前线程获取同步状态（锁）失败时，AQS则会将当前线程以及等待状态等信息构造成一个节点（Node）并将其加入同步队列，同时会阻塞当前线程，当同步状态释放时，则会把节点中的线程唤醒，使其再次尝试获取同步状态。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA并发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java内存模型之总结</title>
      <link href="/2020/02/19/2020-02-19-bing-fa-nei-cun-mo-xing-zong-jie/"/>
      <url>/2020/02/19/2020-02-19-bing-fa-nei-cun-mo-xing-zong-jie/</url>
      
        <content type="html"><![CDATA[<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Java采用内存共享的模式来实现线程之间的通信。编译器和处理器可以对程序进行重排序优化处理，但是需要遵守一些规则，不能随意重排序。</p><ul><li><p>原子性：</p><p>  一个操作或者多个操作要么全部执行要么全部不执行；</p></li><li><p>可见性：</p><p>  当多个线程同时访问一个共享变量时，如果其中某个线程更改了该共享变量，其他线程应该可以立刻看到这个改变；</p></li><li><p>有序性：</p><p>  程序的执行要按照代码的先后顺序执行；</p></li></ul><p>在并发编程模式中，势必会遇到上面三个概念，JMM对原子性并没有提供确切的解决方案，但是JMM解决了可见性和有序性，至于原子性则需要通过锁或者Synchronized来解决了。</p><p>如果一个操作A的操作结果需要对操作B可见，那么我们就认为操作A和操作B之间存在happens-before关系，即A happens-before B。</p><p>happens-before原则是JMM中非常重要的一个原则，它是判断数据是否存在竞争、线程是否安全的主要依据，依靠这个原则，我们可以解决在并发环境下两个操作之间是否存在冲突的所有问题。<strong>JMM规定，两个操作存在happens-before关系并不一定要A操作先于B操作执行，只要A操作的结果对B操作可见即可。</strong></p><p>在程序运行过程中，为了执行的效率，编译器和处理器是可以对程序进行一定的重排序，但是他们必须要满足两个条件：<strong>1 执行的结果保持不变，2 存在数据依赖的不能重排序。</strong>重排序是引起多线程不安全的一个重要因素。</p><p>同时顺序一致性是一个比较理想化的参考模型，它为我们提供了强大而又有力的内存可见性保证，他主要有两个特征：1 一个线程中的所有操作必须按照程序的顺序来执行；2 所有线程都只能看到一个单一的操作执行顺序，在顺序一致性模型中，每个操作都必须原则执行且立刻对所有线程可见。</p><h2 id="划重点，总结辣："><a href="#划重点，总结辣：" class="headerlink" title="划重点，总结辣："></a>划重点，总结辣：</h2><ul><li>Java采用内存共享的模式来实现线程之间的通信。</li><li>并发编程模式中，势必会遇到三个概念：<ul><li>原子性：一个操作或者多个操作要么全部执行要么全部不执行；</li><li>可见性：当多个线程同时访问一个共享变量时，如果其中某个线程更改了该共享变量，其他线程应该可以立刻看到这个改变；</li><li>有序性：程序的执行要按照代码的先后顺序执行；</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA并发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java内存模型之从JMM角度分析DCL</title>
      <link href="/2020/02/18/2020-02-18-bing-fa-dcl/"/>
      <url>/2020/02/18/2020-02-18-bing-fa-dcl/</url>
      
        <content type="html"><![CDATA[<p>DCL，即Double Check Lock，中卫双重检查锁定。</p><h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><p>我们先看单例模式里面的懒汉式：</p><pre><code>public class Singleton {    private static Singleton singleton;    private Singleton(){}    public static Singleton getInstance(){        if(singleton == null){            singleton = new Singleton();        }        return singleton;    }}</code></pre><p>我们都知道这种写法是错误的，因为它无法保证线程的安全性。优化如下：</p><pre><code>public class Singleton {    private static Singleton singleton;    private Singleton(){}    public static synchronized Singleton getInstance(){        if(singleton == null){            singleton = new Singleton();        }        return singleton;    }}</code></pre><p>优化非常简单，就是在getInstance方法上面做了同步，但是synchronized就会导致这个方法比较低效，导致程序性能下降，那么怎么解决呢？聪明的人们想到了双重检查 DCL：</p><pre><code>public class Singleton {    private static Singleton singleton;    private Singleton(){}    public static Singleton getInstance(){        if(singleton == null){                              // 1            synchronized (Singleton.class){                 // 2                if(singleton == null){                      // 3                    singleton = new Singleton();            // 4                }            }        }        return singleton;    }}</code></pre><p>就如上面所示，这个代码看起来很完美，理由如下：</p><ul><li>如果检查第一个singleton不为null,则不需要执行下面的加锁动作，极大提高了程序的性能；</li><li>如果第一个singleton为null,即使有多个线程同一时间判断，但是由于synchronized的存在，只会有一个线程能够创建对象；</li><li>当第一个获取锁的线程创建完成后singleton对象后，其他的在第二次判断singleton一定不会为null，则直接返回已经创建好的singleton对象；</li></ul><p>通过上面的分析，DCL看起确实是非常完美，但是可以明确地告诉你，这个错误的。上面的逻辑确实是没有问题，分析也对，但是就是有问题，那么问题出在哪里呢？在回答这个问题之前，我们先来复习一下创建对象过程，实例化一个对象要分为三个步骤：</p><ol><li>分配内存空间</li><li>初始化对象</li><li>将内存空间的地址赋值给对应的引用</li></ol><p>但是由于重排序的缘故，步骤2、3可能会发生重排序，其过程如下：</p><ol><li>分配内存空间</li><li>将内存空间的地址赋值给对应的引用</li><li>初始化对象</li></ol><p>如果2、3发生了重排序就会导致第二个判断会出错，singleton != null，但是它其实仅仅只是一个地址而已，此时对象还没有被初始化，所以return的singleton对象是一个没有被初始化的对象，如下：</p><p><img src="/2020/02/18/2020-02-18-bing-fa-dcl/w2jgisvv.bmp" alt></p><p>按照上面图例所示，线程B访问的是一个没有被初始化的singleton对象。</p><p>通过上面的阐述，我们可以判断DCL的错误根源在于步骤4：</p><pre><code>singleton = new Singleton();</code></pre><p>知道问题根源所在，那么怎么解决呢？有两个解决办法：</p><ol><li>不允许初始化阶段步骤2 、3发生重排序。</li><li>允许初始化阶段步骤2 、3发生重排序，但是不允许其他线程“看到”这个重排序。</li></ol><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><h3 id="基于volatile解决方案"><a href="#基于volatile解决方案" class="headerlink" title="基于volatile解决方案"></a>基于volatile解决方案</h3><p>对于上面的DCL其实只需要做一点点修改即可：<strong>将变量singleton生命为volatile即可</strong>：</p><pre><code>public class Singleton {    //通过volatile关键字来确保安全    private volatile static Singleton singleton;    private Singleton(){}    public static Singleton getInstance(){        if(singleton == null){            synchronized (Singleton.class){                if(singleton == null){                    singleton = new Singleton();                }            }        }        return singleton;    }}</code></pre><p>当singleton声明为volatile后，步骤2、步骤3就不会被重排序了，也就可以解决上面那问题了。</p><h3 id="基于类初始化的解决方案"><a href="#基于类初始化的解决方案" class="headerlink" title="基于类初始化的解决方案"></a>基于类初始化的解决方案</h3><p>该解决方案的根本就在于：利用classloder的机制来保证初始化instance时只有一个线程。JVM在类初始化阶段会获取一个锁，这个锁可以同步多个线程对同一个类的初始化。</p><pre><code>public class Singleton {    private static class SingletonHolder{        public static Singleton singleton = new Singleton();    }    public static Singleton getInstance(){        return SingletonHolder.singleton;    }}</code></pre><p>这种解决方案的实质是：运行步骤2和步骤3重排序，但是不允许其他线程看见。</p><p><strong>Java语言规定，对于每一个类或者接口C,都有一个唯一的初始化锁LC与之相对应。从C到LC的映射，由JVM的具体实现去自由实现。JVM在类初始化阶段期间会获取这个初始化锁，并且每一个线程至少获取一次锁来确保这个类已经被初始化过了。</strong></p><p><img src="/2020/02/18/2020-02-18-bing-fa-dcl/4vkvcssd.bmp" alt></p>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA并发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java内存模型之happens-before</title>
      <link href="/2020/02/17/2020-02-17-bing-fa-happens-before/"/>
      <url>/2020/02/17/2020-02-17-bing-fa-happens-before/</url>
      
        <content type="html"><![CDATA[<p><strong>在JMM中，如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须存在happens-before关系。</strong></p><p>appens-before原则非常重要，它是判断数据是否存在竞争、线程是否安全的主要依据，依靠这个原则，我们解决在并发环境下两操作之间是否可能存在冲突的所有问题。</p><h3 id="栗子1："><a href="#栗子1：" class="headerlink" title="栗子1："></a>栗子1：</h3><pre><code>i = 1;       //线程A执行j = i ;      //线程B执行</code></pre><p>j 是否等于1呢？假定线程A的操作（i = 1）happens-before线程B的操作（j = i）,那么可以确定线程B执行后j = 1 一定成立，如果他们不存在happens-before原则，那么j = 1 不一定成立。这就是happens-before原则的威力。</p><h3 id="happens-before原则定义如下："><a href="#happens-before原则定义如下：" class="headerlink" title="happens-before原则定义如下："></a>happens-before原则定义如下：</h3><ol><li><strong>如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。</strong> </li><li><strong>两个操作之间存在happens-before关系，并不意味着一定要按照happens-before原则制定的顺序来执行。如果重排序之后的执行结果与按照happens-before关系来执行的结果一致，那么这种重排序并不非法。</strong></li></ol><h3 id="happens-before原则规则："><a href="#happens-before原则规则：" class="headerlink" title="happens-before原则规则："></a>happens-before原则规则：</h3><ol><li>程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作；</li><li>锁定规则：一个unLock操作先行发生于后面对同一个锁的lock操作；</li><li>volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作；</li><li>传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C；</li><li>线程启动规则：Thread对象的start()方法先行发生于此线程的每个一个动作；</li><li>线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生；</li><li>线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行；</li><li>对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始；</li></ol><ul><li>程序次序规则：一段代码在单线程中执行的结果是有序的。注意是执行结果，因为虚拟机、处理器会对指令进行重排序。虽然重排序了，但是并不会影响程序的执行结果，所以程序最终执行的结果与顺序执行的结果是一致的。故而这个规则只对单线程有效，在多线程环境下无法保证正确性。</li><li>锁定规则：这个规则比较好理解，无论是在单线程环境还是多线程环境，一个锁处于被锁定状态，那么必须先执行unlock操作后面才能进行lock操作。</li><li>volatile变量规则：这是一条比较重要的规则，它标志着volatile保证了线程可见性。通俗点讲就是如果一个线程先去写一个volatile变量，然后一个线程去读这个变量，那么这个写操作一定是happens-before读操作的。</li><li>传递规则：提现了happens-before原则具有传递性，即A happens-before B , B happens-before C，那么A happens-before C</li><li>线程启动规则：假定线程A在执行过程中，通过执行ThreadB.start()来启动线程B，那么线程A对共享变量的修改在接下来线程B开始执行后确保对线程B可见。</li><li>线程终结规则：假定线程A在执行的过程中，通过制定ThreadB.join()等待线程B终止，那么线程B在终止之前对共享变量的修改在线程A等待返回后可见。</li></ul><p>上面八条是原生Java满足Happens-before关系的规则，但是我们可以对他们进行推导出其他满足happens-before的规则：</p><ol><li>将一个元素放入一个线程安全的队列的操作Happens-Before从队列中取出这个元素的操作</li><li>将一个元素放入一个线程安全容器的操作Happens-Before从容器中取出这个元素的操作</li><li>在CountDownLatch上的倒数操作Happens-Before CountDownLatch#await()操作</li><li>释放Semaphore许可的操作Happens-Before获得许可操作</li><li>Future表示的任务的所有操作Happens-Before Future#get()操作</li><li>向Executor提交一个Runnable或Callable的操作Happens-Before任务开始执行操作</li></ol><p><strong>如果两个操作不存在上述（前面8条 + 后面6条）任一一个happens-before规则，那么这两个操作就没有顺序的保障，JVM可以对这两个操作进行重排序。如果操作A happens-before操作B，那么操作A在内存上所做的操作对操作B都是可见的。</strong></p><h3 id="栗子2："><a href="#栗子2：" class="headerlink" title="栗子2："></a>栗子2：</h3><pre><code>private int i = 0;public void write(int j){    i = j;}public int read(){    return i;}</code></pre><p>我们约定线程A执行write()，线程B执行read()，且线程A优先于线程B执行，那么线程B获得结果是什么？；我们就这段简单的代码一次分析happens-before的规则（规则5、6、7、8 + 推导的6条可以忽略，因为他们和这段代码毫无关系）：</p><p>1.由于两个方法是由不同的线程调用，所以肯定不满足程序次序规则；</p><p>2.两个方法都没有使用锁，所以不满足锁定规则；</p><p>3.变量i不是用volatile修饰的，所以volatile变量规则不满足；</p><p>4.传递规则肯定不满足；</p><p>所以我们无法通过happens-before原则推导出线程A happens-before线程B，虽然可以确认在时间上线程A优先于线程B指定，但是就是无法确认线程B获得的结果是什么，所以这段代码不是线程安全的。那么怎么修复这段代码呢？满足规则2、3任一即可。</p>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA并发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java内存模型之重排序</title>
      <link href="/2020/02/16/2020-02-16-bing-fa-chong-pai-xu/"/>
      <url>/2020/02/16/2020-02-16-bing-fa-chong-pai-xu/</url>
      
        <content type="html"><![CDATA[<p>在执行程序时，为了提供性能，处理器和编译器常常会对指令进行重排序，但是不能随意重排序，不是你想怎么排序就怎么排序，它需要满足以下两个条件：</p><ol><li>在单线程环境下不能改变程序运行的结果；</li><li>存在数据依赖关系的不允许重排序</li></ol><h2 id="as-if-serial语义"><a href="#as-if-serial语义" class="headerlink" title="as-if-serial语义"></a>as-if-serial语义</h2><p>as-if-serial语义的意思是，所有的操作均可以为了优化而被重排序，但是你必须要保证重排序后执行的结果不能被改变，编译器、runtime、处理器都必须遵守as-if-serial语义。注意as-if-serial只保证单线程环境，多线程环境下无效。</p><h3 id="栗子1："><a href="#栗子1：" class="headerlink" title="栗子1："></a>栗子1：</h3><pre><code>int a = 1 ;      //Aint b = 2 ;      //Bint c = a + b;   //C</code></pre><p>A、B、C三个操作存在如下关系：A、B不存在数据依赖关系，A和C、B和C存在数据依赖关系，因此在进行重排序的时候，A、B可以随意排序，但是必须位于C的前面，执行顺序可以是A –&gt; B –&gt; C或者B –&gt; A –&gt; C。但是无论是何种执行顺序最终的结果C总是等于3。</p><p>as-if-serail语义把单线程程序保护起来了，它可以保证在重排序的前提下程序的最终结果始终都是一致的。</p><p>as-if-serail语义把单线程程序保护起来了，它可以保证在重排序的前提下程序的最终结果始终都是一致的。</p><p>其实对于上段代码，他们存在这样的happen-before关系：</p><p>1.A happens-before B</p><p>2.B happens-before C</p><p>3.A happens-before C</p><p>1、2是程序顺序次序规则，3是传递性。但是，不是说通过重排序，B可能会排在A之前执行么，为何还会存在存在A happens-beforeB呢？这里再次申明<strong>A happens-before B不是A一定会在B之前执行，而是A的对B可见</strong>，但是相对于这个程序A的执行结果不需要对B可见，且他们重排序后不会影响结果，所以JMM不会认为这种重排序非法。</p><h3 id="栗子2"><a href="#栗子2" class="headerlink" title="栗子2"></a>栗子2</h3><pre><code>public class RecordExample1 {    public static void main(String[] args){        int a = 1;        int b = 2;        try {            a = 3;           //A            b = 1 / 0;       //B        } catch (Exception e) {        } finally {            System.out.println(&quot;a = &quot; + a);        }    }}</code></pre><p>按照重排序的规则，操作A与操作B有可能会进行重排序，如果重排序了，B会抛出异常（ / by zero），此时A语句一定会执行不到，那么a还会等于3么？如果按照as-if-serial原则它就改变了程序的结果。其实JVM对异常做了一种特殊的处理，为了保证as-if-serial语义，Java异常处理机制对重排序做了一种特殊的处理：JIT在重排序时会在catch语句中插入错误代偿代码（a = 3）,这样做虽然会导致cathc里面的逻辑变得复杂，但是JIT优化原则是：尽可能地优化程序正常运行下的逻辑，哪怕以catch块逻辑变得复杂为代价。</p><h2 id="重排序对多线程的影响"><a href="#重排序对多线程的影响" class="headerlink" title="重排序对多线程的影响"></a>重排序对多线程的影响</h2><p>在单线程环境下由于as-if-serial语义，重排序无法影响最终的结果，但是对于多线程环境呢？</p><pre><code>public class RecordExample2 {    int a = 0;    boolean flag = false;    /**     * A线程执行     */    public void writer(){        a = 1;                  // 1        flag = true;            // 2    }    /**     * B线程执行     */    public void read(){        if(flag){                  // 3           int i = a + a;          // 4        }    }}</code></pre><p>A线程执行writer()，线程B执行read()，线程B在执行时能否读到 a = 1 呢？答案是不一定（注：X86CPU不支持写写重排序，如果是在x86上面操作，这个一定会是a=1）</p><p>由于操作1 和操作2 之间没有数据依赖性，所以可以进行重排序处理，操作3 和操作4 之间也没有数据依赖性，他们亦可以进行重排序，但是操作3 和操作4 之间存在控制依赖性。假如操作1 和操作2 之间重排序：</p><p><img src="/2020/02/16/2020-02-16-bing-fa-chong-pai-xu/gfjk4vdt.bmp" alt></p><p>按照这种执行顺序线程B肯定读不到线程A设置的a值，在这里多线程的语义就已经被重排序破坏了。</p><p>通过上面的分析，<strong>重排序不会影响单线程环境的执行结果，但是会破坏多线程的执行语义</strong>。</p>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA并发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java内存模型之分析volatile</title>
      <link href="/2020/02/15/2020-02-15-bing-fa-volatile/"/>
      <url>/2020/02/15/2020-02-15-bing-fa-volatile/</url>
      
        <content type="html"><![CDATA[<h2 id="volatile的特性"><a href="#volatile的特性" class="headerlink" title="volatile的特性"></a>volatile的特性</h2><ol><li>volatile可见性；对一个volatile的读，总可以看到对这个变量最终的写；</li><li>volatile原子性；volatile对单个读/写具有原子性（32位Long、Double），但是复合操作除外，例如i++;</li><li>JVM底层采用“内存屏障”来实现volatile语义。</li></ol><h2 id="volatile的内存语义及其实现"><a href="#volatile的内存语义及其实现" class="headerlink" title="volatile的内存语义及其实现"></a>volatile的内存语义及其实现</h2><p>在JMM中，线程之间的通信采用共享内存来实现的。</p><p>volatile的内存语义是：当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量值立即刷新到主内存中。<br>当读一个volatile变量时，JMM会把该线程对应的本地内存设置为无效，直接从主内存中读取共享变量。</p><p><strong>所以volatile的写内存语义是直接刷新到主内存中，读的内存语义是直接从主内存中读取。</strong></p><p>实现volatile的内存语义JMM会限制重排序，其重排序规则如下：</p><ul><li>如果第一个操作为volatile读，则不管第二个操作是啥，都不能重排序。这个操作确保volatile读之后的操作不会被编译器重排序到volatile读之前；</li><li>当第二个操作为volatile写是，则不管第一个操作是啥，都不能重排序。这个操作确保volatile写之前的操作不会被编译器重排序到volatile写之后；</li><li>当第一个操作volatile写，第二操作为volatile读时，不能重排序。</li></ul><p>volatile的底层实现是通过插入内存屏障，但是对于编译器来说，发现一个最优布置来最小化插入内存屏障的总数几乎是不可能的，所以，JMM采用了保守策略：</p><ul><li>在每一个volatile写操作前面插入一个StoreStore屏障</li><li>在每一个volatile写操作后面插入一个StoreLoad屏障</li><li>在每一个volatile读操作后面插入一个LoadLoad屏障</li><li>在每一个volatile读操作后面插入一个LoadStore屏障</li></ul><ol><li>StoreStore屏障可以保证在volatile写之前，其前面的所有普通写操作都已经刷新到主内存中。</li><li><strong>StoreLoad</strong>屏障的作用是避免volatile写与后面可能有的volatile读/写操作重排序。</li><li>LoadLoad屏障用来禁止处理器把上面的volatile读与下面的普通读重排序。</li><li>LoadStore屏障用来禁止处理器把上面的volatile读与下面的普通写重排序。</li></ol><h2 id="栗子1"><a href="#栗子1" class="headerlink" title="栗子1"></a>栗子1</h2><pre><code>public class VolatileTest {int i = 0;volatile boolean flag = false;public void write(){    i = 2;    flag = true;}public void read(){    if(flag){        System.out.println(&quot;---i = &quot; + i);     }}}</code></pre><p><img src="/2020/02/15/2020-02-15-bing-fa-volatile/sg10pliy.bmp" alt></p><h2 id="栗子2"><a href="#栗子2" class="headerlink" title="栗子2"></a>栗子2</h2><p>volatile的内存屏障插入策略非常保守，其实在实际中，只要不改变volatile写-读得内存语义，编译器可以根据具体情况优化，省略不必要的屏障。</p><pre><code>public class VolatileBarrierExample {    int a = 0;    volatile int v1 = 1;    volatile int v2 = 2;    void readAndWrite(){        int i = v1;     //volatile读        int j = v2;     //volatile读        a = i + j;      //普通读        v1 = i + 1;     //volatile写        v2 = j * 2;     //volatile写    }}</code></pre><p>没有优化的示例图如下：</p><p><img src="/2020/02/15/2020-02-15-bing-fa-volatile/70qjeyj8.bmp" alt><br>我们来分析上图有哪些内存屏障指令是多余的</p><ol><li>这个肯定要保留了</li><li>禁止下面所有的普通写与上面的volatile读重排序，但是由于存在第二个volatile读，那个普通的读根本无法越过第二个volatile读。所以可以省略。</li><li>下面已经不存在普通读了，可以省略。</li><li>保留</li><li>保留</li><li>下面跟着一个volatile写，所以可以省略</li><li>保留</li><li>保留</li></ol><p>所以2、3、6可以省略，其示意图如下：</p><p><img src="/2020/02/15/2020-02-15-bing-fa-volatile/ki0pmlqx.bmp" alt></p><h2 id="划重点，总结辣："><a href="#划重点，总结辣：" class="headerlink" title="划重点，总结辣："></a>划重点，总结辣：</h2><ul><li><p>在JMM（内存模型）中，线程之间的通信采用共享内存来实现的。</p></li><li><p>volatile的写内存语义是直接刷新到主内存中，读的内存语义是直接从主内存中读取。</p></li><li><p>实现volatile的内存语义JMM会限制重排序，其重排序规则如下：</p><ul><li>如果当前为volatile读时，后面的操作不能重排序。（这个操作确保volatile读之后的操作不会被编译器重排序到volatile读之前）；</li><li>如果当前为volatile写时，前面的操作不能重排序。（这个操作确保volatile写之前的操作不会被编译器重排序到volatile写之后）；</li><li>如果当前为volatile写时，紧接操作为volatile读时，不能重排序。</li></ul></li><li><p>volatile的底层是通过插入内存屏障实现的。</p><ul><li>在每一个volatile写操作前面插入一个StoreStore屏障</li><li>在每一个volatile写操作后面插入一个StoreLoad屏障</li><li>在每一个volatile读操作后面插入一个LoadLoad屏障</li><li>在每一个volatile读操作后面插入一个LoadStore屏障</li></ul><ol><li>StoreStore屏障可以保证在volatile写之前，其前面的所有普通写操作都已经刷新到主内存中。</li><li>StoreLoad屏障的作用是避免volatile写与后面可能有的volatile读/写操作重排序。</li><li>LoadLoad屏障用来禁止处理器把上面的volatile读与下面的普通读重排序。</li><li>LoadStore屏障用来禁止处理器把上面的volatile读与下面的普通写重排序。</li></ol></li><li><p>volatile可以保证可见性，对一个volatile的读，总可以看到对这个变量最终的写；</p></li><li><p>volatile不可以保证原子性，volatile对单个读/写具有原子性（32位Long、Double），但是复合操作除外，例如i++;</p></li><li><p>volatile可以保证有序性，JVM底层采用“内存屏障”来实现。</p></li><li><p>volatile 主要解决的是一个线程修改变量值之后，其他线程立马可以读到最新的值，是解决这个问题的，也就是可见性！但是如果是多个线程同时修改一个变量的值，那还是可能出现多线程并发的安全问题，导致数据值修改错乱，volatile 是不负责解决这个问题的，也就是不负责解决原子性问题！原子性问题，得依赖 synchronized、ReentrantLock 等加锁机制来解决。</p></li><li><p>对一个变量加了 volatile 关键字修饰之后，只要一个线程修改了这个变量的值，立马强制刷回主内存。接着强制过期其他线程的本地工作内存中的缓存，最后其他线程读取变量值的时候，强制重新从主内存来加载最新的值！</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA并发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入分析synchronized的实现原理</title>
      <link href="/2020/02/14/2020-02-14-bing-fa-synchronized/"/>
      <url>/2020/02/14/2020-02-14-bing-fa-synchronized/</url>
      
        <content type="html"><![CDATA[<h2 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h2><p><strong>synchronized可以保证方法或者代码块在运行时，同一时刻只有一个方法可以进入到临界区，同时它还可以保证共享变量的内存可见性。</strong></p><p>Java中每一个对象都可以作为锁，这是synchronized实现同步的基础：</p><ul><li>普通同步方法，锁是当前实例对象</li><li>静态同步方法，锁是当前类的class对象</li><li>同步方法块，锁是括号里面的对象</li></ul><p>当一个线程访问同步代码块时，它首先是需要得到锁才能执行同步代码，当退出或者抛出异常时必须要释放锁。</p><pre><code>public class SynchronizedTest {    public synchronized void test1(){    }    public void test2(){        synchronized (this){        }    }}</code></pre><p>利用javap工具查看生成的class文件信息来分析Synchronize的实现</p><p><img src="/2020/02/14/2020-02-14-bing-fa-synchronized/x77sm08k.bmp" alt></p><p>同步代码块是使用monitorenter和monitorexit指令实现的，同步方法（在这看不出来需要看JVM底层实现）依靠的是方法修饰符上的ACCSYNCHRONIZED实现。</p><p><strong>同步代码块</strong>：monitorenter指令插入到同步代码块的开始位置，monitorexit指令插入到同步代码块的结束位置，JVM需要保证每一个monitorenter都有一个monitorexit与之相对应。任何对象都有一个monitor与之相关联，当且一个monitor被持有之后，他将处于锁定状态。线程执行到monitorenter指令时，将会尝试获取对象所对应的monitor所有权，即尝试获取对象的锁；</p><p><strong>同步方法</strong>：synchronized方法则会被翻译成普通的方法调用和返回指令如:invokevirtual、areturn指令，在VM字节码层面并没有任何特别的指令来实现被synchronized修饰的方法，而是在Class文件的方法表中将该方法的accessflags字段中的synchronized标志位置1，表示该方法是同步方法并使用调用该方法的对象或该方法所属的Class在JVM的内部对象表示Klass做为锁对象。</p><h2 id="Java对象头、monitor"><a href="#Java对象头、monitor" class="headerlink" title="Java对象头、monitor"></a>Java对象头、monitor</h2><p>Java对象头和monitor是实现synchronized的基础！</p><h3 id="Java对象头"><a href="#Java对象头" class="headerlink" title="Java对象头"></a>Java对象头</h3><p>synchronized用的锁是存在Java对象头里的，那么什么是Java对象头呢？Hotspot虚拟机的对象头主要包括两部分数据：Mark Word（标记字段）、Klass Pointer（类型指针）。其中Klass Point是是对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例，Mark Word用于存储对象自身的运行时数据，它是实现轻量级锁和偏向锁的关键。</p><h4 id="Mark-Word："><a href="#Mark-Word：" class="headerlink" title="Mark Word："></a>Mark Word：</h4><p><strong>Mark Word用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程 ID、偏向时间戳等等</strong>。Java对象头一般占有两个机器码（在32位虚拟机中，1个机器码等于4字节，也就是32bit），但是如果对象是数组类型，则需要三个机器码，因为JVM虚拟机可以通过Java对象的元数据信息确定Java对象的大小，但是无法从数组的元数据来确认数组的大小，所以用一块来记录数组长度。</p><p><img src="/2020/02/14/2020-02-14-bing-fa-synchronized/bxfdckv9.bmp" alt></p><p>对象头信息是与对象自身定义的数据无关的额外存储成本，但是考虑到虚拟机的空间效率，Mark Word被设计成一个非固定的数据结构以便在极小的空间内存存储尽量多的数据，它会根据对象的状态复用自己的存储空间，也就是说，Mark Word会随着程序的运行发生变化。</p><p><img src="/2020/02/14/2020-02-14-bing-fa-synchronized/a6e378ec.bmp" alt></p><h4 id="Monitor"><a href="#Monitor" class="headerlink" title="Monitor"></a>Monitor</h4><p>与一切皆对象一样，所有的Java对象是天生的Monitor，每一个Java对象都有成为Monitor的潜质。</p><p>Monitor 是线程私有的数据结构，每一个线程都有一个可用monitor record列表，同时还有一个全局的可用列表。每一个被锁住的对象都会和一个monitor关联（对象头的MarkWord中的LockWord指向monitor的起始地址），同时monitor中有一个Owner字段存放拥有该锁的线程的唯一标识，表示该锁被这个线程占用。其结构如下：</p><p><img src="/2020/02/14/2020-02-14-bing-fa-synchronized/e30cqbz4.bmp" alt></p><p>Owner：初始时为NULL表示当前没有任何线程拥有该monitor record，当线程成功拥有该锁后保存线程唯一标识，当锁被释放时又设置为NULL；</p><p>EntryQ:关联一个系统互斥锁（semaphore），阻塞所有试图锁住monitor record失败的线程。</p><p>RcThis:表示blocked或waiting在该monitor record上的所有线程的个数。</p><p>Nest:用来实现重入锁的计数。</p><p>HashCode:保存从对象头拷贝过来的HashCode值（可能还包含GC age）。</p><p>Candidate:用来避免不必要的阻塞或等待线程唤醒，因为每一次只有一个线程能够成功拥有锁，如果每次前一个释放锁的线程唤醒所有正在阻塞或等待的线程，会引起不必要的上下文切换（从阻塞到就绪然后因为竞争锁失败又被阻塞）从而导致性能严重下降。Candidate只有两种可能的值0表示没有需要唤醒的线程1表示要唤醒一个继任线程来竞争锁。</p><h2 id="锁优化"><a href="#锁优化" class="headerlink" title="锁优化"></a>锁优化</h2><p>jdk1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。</p><p><strong>锁主要存在四中状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈而逐渐升级。注意锁可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率。</strong></p><h2 id="自旋锁"><a href="#自旋锁" class="headerlink" title="自旋锁"></a>自旋锁</h2><p><strong>线程的阻塞和唤醒需要CPU从用户态转为核心态，频繁的阻塞和唤醒对CPU来说是一件负担很重的工作</strong>，势必会给系统的并发性能带来很大的压力。同时我们发现在许多应用上面，对象锁的锁状态只会持续很短一段时间，为了这一段很短的时间频繁地阻塞和唤醒线程是非常不值得的。所以引入自旋锁。</p><p>何谓自旋锁？</p><p>所谓自旋锁，就是让该线程等待一段时间，不会被立即挂起，看持有锁的线程是否会很快释放锁。怎么等待呢？执行一段无意义的循环即可（自旋）。</p><p>自旋等待不能替代阻塞，先不说对处理器数量的要求（多核，貌似现在没有单核的处理器了），虽然它<strong>可以避免线程切换带来的开销</strong>，但是它占用了处理器的时间。如果持有锁的线程很快就释放了锁，那么自旋的效率就非常好，反之，自旋的线程就会白白消耗掉处理的资源，它不会做任何有意义的工作，典型的占着茅坑不拉屎，这样反而会带来性能上的浪费。所以说，自旋等待的时间（自旋的次数）必须要有一个限度，如果自旋超过了定义的时间仍然没有获取到锁，则应该被挂起。</p><p>自旋锁在JDK 1.4.2中引入，默认关闭，但是可以使用-XX:+UseSpinning开开启，在JDK1.6中默认开启。同时自旋的默认次数为10次，可以通过参数-XX:PreBlockSpin来调整；<br>如果通过参数-XX:preBlockSpin来调整自旋锁的自旋次数，会带来诸多不便。假如我将参数调整为10，但是系统很多线程都是等你刚刚退出的时候就释放了锁（假如你多自旋一两次就可以获取锁），你是不是很尴尬。于是JDK1.6引入自适应的自旋锁，让虚拟机会变得越来越聪明。</p><h2 id="适应自旋锁"><a href="#适应自旋锁" class="headerlink" title="适应自旋锁"></a>适应自旋锁</h2><p>JDK 1.6引入了更加聪明的自旋锁，即自适应自旋锁。所谓自适应就意味着自旋的次数不再是固定的，它是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。它怎么做呢？线程如果自旋成功了，那么下次自旋的次数会更加多，因为虚拟机认为既然上次成功了，那么此次自旋也很有可能会再次成功，那么它就会允许自旋等待持续的次数更多。反之，如果对于某个锁，很少有自旋能够成功的，那么在以后要或者这个锁的时候自旋的次数会减少甚至省略掉自旋过程，以免浪费处理器资源。</p><p>有了自适应自旋锁，随着程序运行和性能监控信息的不断完善，虚拟机对程序锁的状况预测会越来越准确，虚拟机会变得越来越聪明。</p><h2 id="锁消除"><a href="#锁消除" class="headerlink" title="锁消除"></a>锁消除</h2><p>为了保证数据的完整性，我们在进行操作时需要对这部分操作进行同步控制，但是在有些情况下，JVM检测到不可能存在共享数据竞争，这是JVM会对这些同步锁进行锁消除。锁消除的依据是逃逸分析的数据支持。</p><p>如果不存在竞争，为什么还需要加锁呢？所以锁消除可以节省毫无意义的请求锁的时间。变量是否逃逸，对于虚拟机来说需要使用数据流分析来确定，但是对于我们程序员来说这还不清楚么？我们会在明明知道不存在数据竞争的代码块前加上同步吗？但是有时候程序并不是我们所想的那样？我们虽然没有显示使用锁，但是我们在使用一些JDK的内置API时，如StringBuffer、Vector、HashTable等，这个时候会存在隐形的加锁操作。比如StringBuffer的append()方法，Vector的add()方法：</p><pre><code>public void vectorTest(){    Vector&lt;String&gt; vector = new Vector&lt;String&gt;();    for(int i = 0 ; i &lt; 10 ; i++){        vector.add(i + &quot;&quot;);    }    System.out.println(vector);}</code></pre><p>在运行这段代码时，JVM可以明显检测到变量vector没有逃逸出方法vectorTest()之外，所以JVM可以大胆地将vector内部的加锁操作消除。</p><h2 id="锁粗化"><a href="#锁粗化" class="headerlink" title="锁粗化"></a>锁粗化</h2><p>我们知道在使用同步锁的时候，需要让同步块的作用范围尽可能小—仅在共享数据的实际作用域中才进行同步，这样做的目的是为了使需要同步的操作数量尽可能缩小，如果存在锁竞争，那么等待锁的线程也能尽快拿到锁。</p><p>在大多数的情况下，上述观点是正确的。但是如果一系列的连续加锁解锁操作，可能会导致不必要的性能损耗，所以引入锁粗化的概念。</p><p>锁粗化概念比较好理解，就是将多个连续的加锁、解锁操作连接在一起，扩展成一个范围更大的锁。如上面实例：vector每次add的时候都需要加锁操作，JVM检测到对同一个对象（vector）连续加锁、解锁操作，会合并一个更大范围的加锁、解锁操作，即加锁解锁操作会移到for循环之外。</p><h2 id="轻量级锁"><a href="#轻量级锁" class="headerlink" title="轻量级锁"></a>轻量级锁</h2><p>引入轻量级锁的主要目的是在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗。当关闭偏向锁功能或者多个线程竞争偏向锁导致偏向锁升级为轻量级锁，则会尝试获取轻量级锁，其步骤如下：</p><h3 id="获取锁"><a href="#获取锁" class="headerlink" title="获取锁"></a>获取锁</h3><ol><li>判断当前对象是否处于无锁状态（hashcode、0、01），若是，则JVM首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝（官方把这份拷贝加了一个Displaced前缀，即Displaced Mark Word）；否则执行步骤（3）；</li><li>JVM利用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指针，如果成功表示竞争到锁，则将锁标志位变成00（表示此对象处于轻量级锁状态），执行同步操作；如果失败则执行步骤（3）；</li><li>判断当前对象的Mark Word是否指向当前线程的栈帧，如果是则表示当前线程已经持有当前对象的锁，则直接执行同步代码块；否则只能说明该锁对象已经被其他线程抢占了，这时轻量级锁需要膨胀为重量级锁，锁标志位变成10，后面等待的线程将会进入阻塞状态；</li></ol><h3 id="释放锁轻量级锁的释放也是通过CAS操作来进行的，主要步骤如下："><a href="#释放锁轻量级锁的释放也是通过CAS操作来进行的，主要步骤如下：" class="headerlink" title="释放锁轻量级锁的释放也是通过CAS操作来进行的，主要步骤如下："></a>释放锁轻量级锁的释放也是通过CAS操作来进行的，主要步骤如下：</h3><ol><li>取出在获取轻量级锁保存在Displaced Mark Word中的数据；</li><li>用CAS操作将取出的数据替换当前对象的Mark Word中，如果成功，则说明释放锁成功，否则执行（3）；</li><li>如果CAS操作替换失败，说明有其他线程尝试获取该锁，则需要在释放锁的同时需要唤醒被挂起的线程。</li></ol><p>对于轻量级锁，其性能提升的依据是“对于绝大部分的锁，在整个生命周期内都是不会存在竞争的”，如果打破这个依据则除了互斥的开销外，还有额外的CAS操作，因此在有多线程竞争的情况下，轻量级锁比重量级锁更慢；</p><p>下图是轻量级锁的获取和释放过程</p><p><img src="/2020/02/14/2020-02-14-bing-fa-synchronized/uta8xosj.bmp" alt></p><h2 id="偏向锁"><a href="#偏向锁" class="headerlink" title="偏向锁"></a>偏向锁</h2><p>引入偏向锁主要目的是：为了在无多线程竞争的情况下尽量减少不必要的轻量级锁执行路径。</p><p>上面提到了轻量级锁的加锁解锁操作是需要依赖多次CAS原子指令的。那么偏向锁是如何来减少不必要的CAS操作呢？我们可以查看Mark word的结构就明白了。只需要检查是否为偏向锁、锁标识为以及ThreadID即可，处理流程如下：</p><h3 id="获取锁-1"><a href="#获取锁-1" class="headerlink" title="获取锁"></a>获取锁</h3><ol><li>检测Mark Word是否为可偏向状态，即是否为偏向锁1，锁标识位为01；</li><li>若为可偏向状态，则测试线程ID是否为当前线程ID，如果是，则执行步骤（5），否则执行步骤（3）；</li><li>如果线程ID不为当前线程ID，则通过CAS操作竞争锁，竞争成功，则将Mark Word的线程ID替换为当前线程ID，否则执行线程（4）；</li><li>通过CAS竞争锁失败，证明当前存在多线程竞争情况，当到达全局安全点，获得偏向锁的线程被挂起，偏向锁升级为轻量级锁，然后被阻塞在安全点的线程继续往下执行同步代码块；</li><li>执行同步代码块</li></ol><h3 id="释放锁"><a href="#释放锁" class="headerlink" title="释放锁"></a>释放锁</h3><p>偏向锁的释放采用了一种只有竞争才会释放锁的机制，线程是不会主动去释放偏向锁，需要等待其他线程来竞争。偏向锁的撤销需要等待全局安全点（这个时间点是上没有正在执行的代码）。其步骤如下：</p><ol><li>暂停拥有偏向锁的线程，判断锁对象是否还处于被锁定状态；</li><li>撤销偏向锁，恢复到无锁状态（01）或者轻量级锁的状态；</li></ol><p>下图是偏向锁的获取和释放流程</p><p><img src="/2020/02/14/2020-02-14-bing-fa-synchronized/yaiopcog.bmp" alt></p><h2 id="重量级锁"><a href="#重量级锁" class="headerlink" title="重量级锁"></a>重量级锁</h2><p>重量级锁通过对象内部的监视器（monitor）实现，其中monitor的本质是依赖于底层操作系统的Mutex Lock实现，操作系统实现线程之间的切换需要从用户态到内核态的切换，切换成本非常高。</p><h2 id="划重点，总结辣："><a href="#划重点，总结辣：" class="headerlink" title="划重点，总结辣："></a>划重点，总结辣：</h2><ul><li><p>普通同步方法，锁是当前实例对象；静态同步方法，锁是当前类的class对象；同步代码块，锁是括号里面的对象。</p></li><li><p>同步代码块：同步代码块是使用monitorenter和monitorexit指令实现的。monitorenter指令插入到同步代码块的开始位置，monitorexit指令插入到同步代码块的结束位置，JVM需要保证每一个monitorenter都有一个monitorexit与之相对应。任何对象都有一个monitor与之相关联，当且一个monitor被持有之后，他将处于锁定状态。线程执行到monitorenter指令时，将会尝试获取对象所对应的monitor所有权，即尝试获取对象的锁。</p></li><li><p>同步方法：在Class文件的方法表中将该方法的accessflags字段中的synchronized标志位置1，表示该方法是同步方法，并使用调用该方法的对象或该方法所属的Class在JVM的内部对象做为锁对象。</p></li><li><p>Java对象头和monitor是实现synchronized的基础，synchronized用的锁是存在Java对象头里的。对象头包含哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程 ID等等。</p></li><li><p>jdk1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。</p></li><li><p>锁消除：JVM检测到不可能存在共享数据竞争，这是JVM会对这些同步锁进行锁消除。锁消除的依据是逃逸分析的数据支持。例如方法中定义的局部变量没有逃逸出方法之外，可以加锁操作消除。</p></li><li><p>锁粗化：如果一系列的连续加锁解锁操作，可能会导致不必要的性能损耗，所以引入锁粗化的概念。锁粗化概念比较好理解，就是将多个连续的加锁、解锁操作连接在一起，扩展成一个范围更大的锁。例如一个for不断枷锁解锁，可以把锁放到for循环外。</p></li><li><p>锁主要存在四种状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈而逐渐升级。锁只可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率。</p></li><li><p>自旋锁：就是让该线程等待一段时间，不断循环去尝试获取锁。</p></li><li><p>自适应自旋锁：自旋的次数不再是固定的，它是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。线程如果自旋成功了，那么下次自旋的次数会更加多，反之，如果对于某个锁，很少有自旋能够成功的，那么在以后要或者这个锁的时候自旋的次数会减少。</p></li><li><p>对象头的Mark Word结构图<br>  <img src="/2020/02/14/2020-02-14-bing-fa-synchronized/a6e378ec.bmp" alt></p></li><li><p>偏向锁<br>  <img src="/2020/02/14/2020-02-14-bing-fa-synchronized/yaiopcog.bmp" alt="偏向锁的获取和释放流程"></p><ul><li><p>获取锁</p><ol><li>检测Mark Word是否为可偏向状态，即是否为偏向锁1，锁标识位为01；</li><li>若为可偏向状态，则测试线程ID是否为当前线程ID，如果是，执行同步代码块；</li><li>如果线程ID不为当前线程ID，则通过CAS操作竞争锁，竞争成功，则将Mark Word的线程ID替换为当前线程ID，执行同步代码块；</li><li>通过CAS竞争锁失败，证明当前存在多线程竞争情况，获得偏向锁的线程被挂起，偏向锁升级为轻量级锁，然后被阻塞的线程继续往下执行同步代码块；</li></ol></li><li><p>释放锁：偏向锁的释放采用了一种只有竞争才会释放锁的机制，线程是不会主动去释放偏向锁，需要等待其他线程来竞争。</p><ol><li>暂停拥有偏向锁的线程，判断锁对象是否还处于被锁定状态；</li><li>撤销偏向锁，恢复到无锁状态（01）或者轻量级锁的状态。</li></ol></li></ul></li><li><p>轻量级锁</p><p>  <img src="/2020/02/14/2020-02-14-bing-fa-synchronized/uta8xosj.bmp" alt="轻量级锁的获取和释放过程"></p><ul><li><p>获取锁</p><ol><li>判断当前对象是否处于无锁状态（hashcode、0、01），若是，则JVM首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝；</li><li>JVM利用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指针，如果成功表示竞争到锁，则将锁标志位变成00（表示此对象处于轻量级锁状态），执行同步代码块；</li><li>否则只能说明该锁对象已经被其他线程抢占了，这时轻量级锁需要膨胀为重量级锁，锁标志位变成10，后面等待的线程将会进入阻塞状态。</li></ol></li><li><p>释放锁</p><ol><li>取出在获取轻量级锁保存在栈中的数据；</li><li>用CAS操作将取出的数据替换当前对象的Mark Word中，如果成功，则说明释放锁成功；</li><li>如果CAS操作替换失败，说明有其他线程尝试获取该锁，则需要在释放锁的同时需要唤醒被挂起的线程。</li></ol></li></ul></li><li><p>重量级锁：重量级锁通过对象内部的监视器（monitor）实现，操作系统实现线程之间的切换需要从用户态到内核态的切换，切换成本非常高。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA并发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入分析ThreadLocal</title>
      <link href="/2020/02/13/2020-02-13-bing-fa-threadlocal/"/>
      <url>/2020/02/13/2020-02-13-bing-fa-threadlocal/</url>
      
        <content type="html"><![CDATA[<h2 id="ThreadLoacal是什么？"><a href="#ThreadLoacal是什么？" class="headerlink" title="ThreadLoacal是什么？"></a>ThreadLoacal是什么？</h2><p>ThreadLocal虽然提供了一种解决多线程环境下成员变量的问题，但是它并不是解决多线程共享变量的问题。</p><p>该类提供了线程局部 (thread-local) 变量。这些变量不同于它们的普通对应物，因为访问某个变量（通过其 get 或 set 方法）的每个线程都有自己的局部变量，它独立于变量的初始化副本。 ThreadLocal实例通常是类中的 private static 字段，它们希望将状态与某一个线程（例如，用户 ID 或事务 ID）相关联。</p><p><strong>所以ThreadLocal与线程同步机制不同，线程同步机制是多个线程共享同一个变量，而ThreadLocal是为每一个线程创建一个单独的变量副本，故而每个线程都可以独立地改变自己所拥有的变量副本，而不会影响其他线程所对应的副本。</strong></p><p>ThreadLocal定义了四个方法：</p><ul><li>get()：返回此线程局部变量的当前线程副本中的值。</li><li>initialValue()：返回此线程局部变量的当前线程的“初始值”。</li><li>remove()：移除此线程局部变量当前线程的值。</li><li>set(T value)：将此线程局部变量的当前线程副本中的值设置为指定值。</li></ul><p>除了这四个方法，<strong>ThreadLocal内部还有一个静态内部类ThreadLocalMap，该内部类才是实现线程隔离机制的关键</strong>，get()、set()、remove()都是基于该内部类操作。ThreadLocalMap提供了一种用键值对方式存储每一个线程的变量副本的方法，key为当前ThreadLocal对象，value则是对应线程的变量副本。</p><p>对于ThreadLocal需要注意的有两点：</p><ol><li>ThreadLocal实例本身是不存储值，它只是提供了一个在当前线程中找到副本值得key。</li><li>是ThreadLocal包含在Thread中，而不是Thread包含在ThreadLocal中。</li></ol><p><img src="/2020/02/13/2020-02-13-bing-fa-threadlocal/0qstmvei.bmp" alt></p><h2 id="ThreadLocal使用示例"><a href="#ThreadLocal使用示例" class="headerlink" title="ThreadLocal使用示例"></a>ThreadLocal使用示例</h2><pre><code>public class SeqCount {    private static ThreadLocal&lt;Integer&gt; seqCount = new ThreadLocal&lt;Integer&gt;(){        // 实现initialValue()        public Integer initialValue() {            return 0;        }    };    public int nextSeq(){        seqCount.set(seqCount.get() + 1);        return seqCount.get();    }    public static void main(String[] args){        SeqCount seqCount = new SeqCount();        SeqThread thread1 = new SeqThread(seqCount);        SeqThread thread2 = new SeqThread(seqCount);        SeqThread thread3 = new SeqThread(seqCount);        SeqThread thread4 = new SeqThread(seqCount);        thread1.start();        thread2.start();        thread3.start();        thread4.start();    }    private static class SeqThread extends Thread{        private SeqCount seqCount;        SeqThread(SeqCount seqCount){            this.seqCount = seqCount;        }        public void run() {            for(int i = 0 ; i &lt; 3 ; i++){                System.out.println(Thread.currentThread().getName() + &quot; seqCount :&quot; + seqCount.nextSeq());            }        }    }}</code></pre><p>运行结果：</p><p><img src="/2020/02/13/2020-02-13-bing-fa-threadlocal/z8yx9c27.bmp" alt></p><p>从运行结果可以看出，ThreadLocal确实是可以达到线程隔离机制，确保变量的安全性。</p><h2 id="ThreadLocalMap"><a href="#ThreadLocalMap" class="headerlink" title="ThreadLocalMap"></a>ThreadLocalMap</h2><p><strong>ThreadLocalMap其内部利用Entry来实现key-value的存储</strong>，如下：</p><pre><code>static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; {    /** The value associated with this ThreadLocal. */    Object value;    Entry(ThreadLocal&lt;?&gt; k, Object v) {        super(k);        value = v;    }}</code></pre><p><strong>从上面代码中可以看出Entry的key就是ThreadLocal，而value就是值。</strong>同时，Entry也继承WeakReference，所以说<strong>Entry所对应key（ThreadLocal实例）的引用为一个弱引用</strong></p><p>ThreadLocalMap最核心的方法getEntry()、set(ThreadLocal key, Object value)方法。</p><h3 id="get"><a href="#get" class="headerlink" title="get()"></a>get()</h3><p>返回当前线程所对应的线程变量</p><pre><code>public T get() {    // 获取当前线程    Thread t = Thread.currentThread();    // 获取当前线程的成员变量 threadLocal    ThreadLocalMap map = getMap(t);    if (map != null) {        // 从当前线程的ThreadLocalMap获取相对应的Entry        ThreadLocalMap.Entry e = map.getEntry(this);        if (e != null) {            @SuppressWarnings(&quot;unchecked&quot;)            // 获取目标值                    T result = (T)e.value;            return result;        }    }    return setInitialValue();}</code></pre><p>首先通过当前线程获取所对应的成员变量ThreadLocalMap，然后通过ThreadLocalMap获取当前ThreadLocal的Entry，最后通过所获取的Entry获取目标值result。</p><p>getMap()方法可以获取当前线程所对应的ThreadLocalMap，如下：</p><pre><code>ThreadLocalMap getMap(Thread t) {    return t.threadLocals;}</code></pre><h3 id="set-T-value"><a href="#set-T-value" class="headerlink" title="set(T value)"></a>set(T value)</h3><p>设置当前线程的线程局部变量的值。</p><pre><code>public void set(T value) {    Thread t = Thread.currentThread();    ThreadLocalMap map = getMap(t);    if (map != null)        map.set(this, value);    else        createMap(t, value);}</code></pre><p>获取当前线程所对应的ThreadLocalMap，如果不为空，则调用ThreadLocalMap的set()方法，key就是当前ThreadLocal，如果不存在，则调用createMap()方法新建一个，如下：</p><pre><code>void createMap(Thread t, T firstValue) {    t.threadLocals = new ThreadLocalMap(this, firstValue);}</code></pre><p>set(ThreadLocal key, Object value)</p><pre><code>private void set(ThreadLocal&lt;?&gt; key, Object value) {    ThreadLocal.ThreadLocalMap.Entry[] tab = table;    int len = tab.length;    // 根据 ThreadLocal 的散列值，查找对应元素在数组中的位置    int i = key.threadLocalHashCode &amp; (len-1);    // 采用“线性探测法”，寻找合适位置    for (ThreadLocal.ThreadLocalMap.Entry e = tab[i];        e != null;        e = tab[i = nextIndex(i, len)]) {        ThreadLocal&lt;?&gt; k = e.get();        // key 存在，直接覆盖        if (k == key) {            e.value = value;            return;        }        // key == null，但是存在值（因为此处的e != null），说明之前的ThreadLocal对象已经被回收了        if (k == null) {            // 用新元素替换陈旧的元素            replaceStaleEntry(key, value, i);            return;        }    }    // ThreadLocal对应的key实例不存在也没有陈旧元素，new 一个    tab[i] = new ThreadLocal.ThreadLocalMap.Entry(key, value);    int sz = ++size;    // cleanSomeSlots 清楚陈旧的Entry（key == null）    // 如果没有清理陈旧的 Entry 并且数组中的元素大于了阈值，则进行 rehash    if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold)        rehash();}</code></pre><p>这个set()操作和我们在集合了解的put()方式有点儿不一样，虽然他们都是key-value结构，不同在于他们解决散列冲突的方式不同。集合Map的put()采用的是拉链法，而ThreadLocalMap的set()则是采用开放定址法。</p><p>set()操作除了存储元素外，还有一个很重要的作用，就是replaceStaleEntry()和cleanSomeSlots()，这两个方法可以清除掉key == null 的实例，防止内存泄漏。</p><h3 id="initialValue"><a href="#initialValue" class="headerlink" title="initialValue()"></a>initialValue()</h3><p>返回该线程局部变量的初始值。</p><pre><code>protected T initialValue() {    return null;}</code></pre><p>该方法定义为protected级别且返回为null，很明显是要子类实现它的，所以我们在使用ThreadLocal的时候一般都应该覆盖该方法。该方法不能显示调用，只有在第一次调用get()或者set()方法时才会被执行，并且仅执行1次。</p><h3 id="remove"><a href="#remove" class="headerlink" title="remove()"></a>remove()</h3><p>将当前线程局部变量的值删除。</p><pre><code>public void remove() {    ThreadLocalMap m = getMap(Thread.currentThread());    if (m != null)        m.remove(this);}</code></pre><p>该方法的目的是减少内存的占用。当然，我们不需要显示调用该方法，因为一个线程结束后，它所对应的局部变量就会被垃圾回收。</p><h2 id="ThreadLocal为什么会内存泄漏"><a href="#ThreadLocal为什么会内存泄漏" class="headerlink" title="ThreadLocal为什么会内存泄漏"></a>ThreadLocal为什么会内存泄漏</h2><p>前面提到每个Thread都有一个ThreadLocal.ThreadLocalMap的map，该map的key为ThreadLocal实例，它为一个弱引用，我们知道弱引用有利于GC回收。当ThreadLocal的key == null时，GC就会回收这部分空间，但是value却不一定能够被回收，因为他还与Current Thread存在一个强引用关系。</p><p><img src="/2020/02/13/2020-02-13-bing-fa-threadlocal/y8gischv.bmp" alt><br>由于存在这个强引用关系，会导致value无法回收。如果这个线程对象不会销毁那么这个强引用关系则会一直存在，就会出现内存泄漏情况。所以说只要这个线程对象能够及时被GC回收，就不会出现内存泄漏。如果碰到线程池，那就更坑了。</p><p>那么要怎么避免这个问题呢？</p><p>在前面提过，在ThreadLocalMap中的setEntry()、getEntry()，如果遇到key == null的情况，会对value设置为null。当然我们也可以显示调用ThreadLocal的remove()方法进行处理。</p><h2 id="划重点，总结辣："><a href="#划重点，总结辣：" class="headerlink" title="划重点，总结辣："></a>划重点，总结辣：</h2><ul><li>ThreadLocal与线程同步机制不同，线程同步机制是多个线程共享同一个变量，而ThreadLocal是为每一个线程创建一个单独的变量副本，故而每个线程都可以独立地改变自己所拥有的变量副本，而不会影响其他线程所对应的副本。（ThreadLocal在多线程下，每个线程对共享变量都会创建一份变量副本去操作，不会影响其他线程的变量副本）</li><li>ThreadLocal内部还有一个静态内部类ThreadLocalMap，该内部类才是实现线程隔离机制的关键。ThreadLocalMap提供了一种用键值对方式存储每一个线程的变量副本的方法，key为当前ThreadLocal对象，value则是对应线程的变量副本。</li><li>ThreadLocal为什么会内存泄漏？每个Thread都有一个ThreadLocal.ThreadLocalMap的map，该map的key为ThreadLocal实例，它为一个弱引用，我们知道弱引用有利于GC回收。当ThreadLocal的key == null时，GC就会回收这部分空间，但是value却不一定能够被回收，因为他还与Current Thread存在一个强引用关系。由于存在这个强引用关系，会导致value无法回收。如果这个线程对象不会销毁那么这个强引用关系则会一直存在，就会出现内存泄漏情况。那么要怎么避免这个问题呢？在ThreadLocalMap中的setEntry()、getEntry()，如果遇到key == null的情况，会对value设置为null。当然我们也可以显示调用ThreadLocal的remove()方法进行处理。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 复习PLUS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA并发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis</title>
      <link href="/2020/01/18/2020-01-18-redis/"/>
      <url>/2020/01/18/2020-01-18-redis/</url>
      
        <content type="html"><![CDATA[<h1 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h1><h2 id="1-redis-简介"><a href="#1-redis-简介" class="headerlink" title="1 redis 简介"></a>1 redis 简介</h2><p>简单来说 redis 就是一个数据库，不过与传统数据库不同的是 redis 的数据是存在内存中的，所以读写速度非常快，因此 redis 被广泛应用于缓存方向。另外，redis 也经常用来做分布式锁。redis 提供了多种数据类型来支持不同的业务场景。除此之外，redis 支持事务 、持久化、LUA脚本、LRU驱动事件、多种集群方案。 </p><ul><li><p>为什么要用 redis/为什么要用缓存</p><p>  主要从“高性能”和“高并发”这两点来看待这个问题。</p><ul><li><p>高性能：</p><p>  假如用户第一次访问数据库中的某些数据。这个过程会比较慢，因为是从硬盘上读取的。将该用户访问的数据存在缓存中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可！</p><p>  <img src="/2020/01/18/2020-01-18-redis/22uuyxt7.bmp" alt></p></li><li><p>高并发：</p><p>  直接操作缓存能够承受的请求是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。</p><p>  <img src="/2020/01/18/2020-01-18-redis/flz218ms.bmp" alt></p></li></ul></li><li><p>为什么要用 redis 而不用 map/guava 做缓存?</p><p>  缓存分为本地缓存和分布式缓存。以 Java 为例，使用自带的 map 或者 guava 实现的是本地缓存，最主要的特点是轻量以及快速，生命周期随着 jvm 的销毁而结束，并且在多实例的情况下，每个实例都需要各自保存一份缓存，缓存不具有一致性。</p><p>  使用 redis 或 memcached 之类的称为分布式缓存，在多实例的情况下，各实例共用一份缓存数据，缓存具有一致性。缺点是需要保持 redis 或 memcached服务的高可用，整个程序架构上较为复杂。</p></li></ul><h2 id="2-redis-的线程模型"><a href="#2-redis-的线程模型" class="headerlink" title="2 redis 的线程模型"></a>2 redis 的线程模型</h2><p>redis 内部使用文件事件处理器 file event handler，这个文件事件处理器是单线程的，所以 redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 socket，根据 socket 上的事件来选择对应的事件处理器进行处理。</p><p>文件事件处理器的结构包含 4 个部分：</p><ul><li>多个 socket</li><li>IO 多路复用程序</li><li>文件事件分派器</li><li>事件处理器（连接应答处理器、命令请求处理器、命令回复处理器）</li></ul><p><strong>多个 socket 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 socket，会将 socket 产生的事件放入队列中排队，事件分派器每次从队列中取出一个事件，把该事件交给对应的事件处理器进行处理。</strong><br> <img src="/2020/01/18/2020-01-18-redis/790f6f223d68bdf725d5e08320cac2e.png" alt></p><p> 首先，redis 服务端进程初始化的时候，会将 server socket 的 AE_READABLE 事件与连接应答处理器关联。</p><p> 客户端 socket01 向 redis 进程的 server socket 请求建立连接，此时 server socket 会产生一个 AE_READABLE 事件，IO 多路复用程序监听到 server socket 产生的事件后，将该 socket 压入队列中。文件事件分派器从队列中获取 socket，交给连接应答处理器。连接应答处理器会创建一个能与客户端通信的 socket01，并将该 socket01 的 AE_READABLE 事件与命令请求处理器关联。</p><p> 假设此时客户端发送了一个 set key value 请求，此时 redis 中的 socket01 会产生 AE_READABLE 事件，IO 多路复用程序将 socket01 压入队列，此时事件分派器从队列中获取到 socket01 产生的 AE_READABLE 事件，由于前面 socket01 的 AE_READABLE 事件已经与命令请求处理器关联，因此事件分派器将事件交给命令请求处理器来处理。命令请求处理器读取 socket01 的 key value 并在自己内存中完成 key value 的设置。操作完成后，它会将 socket01 的 AE_WRITABLE 事件与命令回复处理器关联。</p><p> 如果此时客户端准备好接收返回结果了，那么 redis 中的 socket01 会产生一个 AE_WRITABLE 事件，同样压入队列中，事件分派器找到相关联的命令回复处理器，由命令回复处理器对 socket01 输入本次操作的一个结果，比如 ok，之后解除 socket01 的 AE_WRITABLE 事件与命令回复处理器的关联。</p><h2 id="3-redis-和-memcached-的区别"><a href="#3-redis-和-memcached-的区别" class="headerlink" title="3 redis 和 memcached 的区别"></a>3 redis 和 memcached 的区别</h2><ol><li>redis支持更丰富的数据类型（支持更复杂的应用场景）：Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。memcache支持简单的数据类型，String。</li><li>Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而Memecache把数据全部存在内存之中。</li><li>集群模式：memcached没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；但是 redis 目前是原生支持 cluster 模式的.</li><li>Memcached是多线程，非阻塞IO复用的网络模型；Redis使用单线程的 IO 多路复用模型。</li></ol><p><img src="/2020/01/18/2020-01-18-redis/7jdj1vt8.bmp" alt></p><h2 id="4-redis-常见数据结构以及使用场景分析"><a href="#4-redis-常见数据结构以及使用场景分析" class="headerlink" title="4 redis 常见数据结构以及使用场景分析"></a>4 redis 常见数据结构以及使用场景分析</h2><ul><li><p>String</p><p>  常用命令: set,get,decr,incr,mget 等。</p><p>  String数据结构是简单的key-value类型，value其实不仅可以是String，也可以是数字。 常规key-value缓存应用； 常规计数：微博数，粉丝数等。</p></li><li><p>Hash</p><p>  常用命令： hget,hset,hgetall 等。</p><p>  hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象，后续操作的时候，你可以直接仅仅修改这个对象中的某个字段的值。</p></li><li><p>List</p><p>  常用命令: lpush,rpush,lpop,rpop,lrange等</p><p>  list 就是链表，Redis list 的应用场景非常多，也是Redis最重要的数据结构之一，比如微博的关注列表，粉丝列表，消息列表等功能都可以用Redis的 list 结构来实现。</p><p>  Redis list 的实现为一个双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销。</p><p>  另外可以通过 lrange 命令，就是从某个元素开始读取多少个元素，可以基于 list 实现分页查询，这个很棒的一个功能，基于 redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西（一页一页的往下走），性能高。</p></li><li><p>Set</p><p>  常用命令： sadd,spop,smembers,sunion 等</p><p>  set 对外提供的功能与list类似是一个列表的功能，特殊之处在于 set 是可以自动排重的。</p><p>  当你需要存储一个列表数据，又不希望出现重复数据时，set是一个很好的选择，并且set提供了判断某个成员是否在一个set集合内的重要接口，这个也是list所不能提供的。可以基于 set 轻易实现交集、并集、差集的操作。</p></li><li><p>Sorted Set</p><p>  常用命令： zadd,zrange,zrem,zcard等</p><p>  和set相比，sorted set增加了一个权重参数score，使得集合中的元素能够按score进行有序排列。</p></li></ul><h2 id="5-redis-设置过期时间"><a href="#5-redis-设置过期时间" class="headerlink" title="5 redis 设置过期时间"></a>5 redis 设置过期时间</h2><ul><li><p>定期删除+惰性删除。</p><ul><li><p>定期删除：redis默认是每隔 100ms 就随机抽取一些设置了过期时间的key，检查其是否过期，如果过期就删除。注意这里是随机抽取的。为什么要随机呢？你想一想假如 redis 存了几十万个 key ，每隔100ms就遍历所有的设置过期时间的 key 的话，就会给 CPU 带来很大的负载！</p></li><li><p>惰性删除 ：定期删除可能会导致很多过期 key 到了时间并没有被删除掉。所以就有了惰性删除。假如你的过期 key，靠定期删除没有被删除掉，还停留在内存里，除非你的系统去查一下那个 key，才会被redis给删除掉。</p><p>但是仅仅通过设置过期时间还是有问题的。我们想一下：如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期key堆积在内存里，导致redis内存块耗尽了。怎么解决这个问题呢？ redis 内存淘汰机制。</p></li></ul></li></ul><h2 id="6-redis-内存淘汰机制-MySQL里有2000w数据，Redis中只存20w的数据，如何保证Redis中的数据都是热点数据"><a href="#6-redis-内存淘汰机制-MySQL里有2000w数据，Redis中只存20w的数据，如何保证Redis中的数据都是热点数据" class="headerlink" title="6 redis 内存淘汰机制(MySQL里有2000w数据，Redis中只存20w的数据，如何保证Redis中的数据都是热点数据?)"></a>6 redis 内存淘汰机制(MySQL里有2000w数据，Redis中只存20w的数据，如何保证Redis中的数据都是热点数据?)</h2><ul><li>redis 提供 6种数据淘汰策略：<ul><li>volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰</li><li>volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰</li><li>volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰</li><li>allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key（这个是最常用的）</li><li>allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰</li><li>no-eviction：禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。这个应该没人使用吧！</li></ul></li><li>4.0版本后增加以下两种：<ul><li>volatile-lfu：从已设置过期时间的数据集(server.db[i].expires)中挑选最不经常使用的数据淘汰</li><li>allkeys-lfu：当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的key</li></ul></li></ul><h2 id="7-redis-持久化机制-怎么保证-redis-挂掉之后再重启数据可以进行恢复"><a href="#7-redis-持久化机制-怎么保证-redis-挂掉之后再重启数据可以进行恢复" class="headerlink" title="7 redis 持久化机制(怎么保证 redis 挂掉之后再重启数据可以进行恢复)"></a>7 redis 持久化机制(怎么保证 redis 挂掉之后再重启数据可以进行恢复)</h2><p>Redis不同于Memcached的很重一点就是，Redis支持持久化，而且支持两种不同的持久化操作。Redis的一种持久化方式叫快照（snapshotting，RDB），另一种方式是只追加文件（append-only file,AOF）。</p><ul><li><p>快照（snapshotting）持久化（RDB）</p><p>  Redis可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。Redis创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis主从结构，主要用来提高Redis性能），还可以将快照留在原地以便重启服务器的时候使用。</p><p>  快照持久化是Redis默认采用的持久化方式，在redis.conf配置文件中默认有此下配置：</p><pre><code>  save 900 1           #在900秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发BGSAVE命令创建快照。  save 300 10          #在300秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发BGSAVE命令创建快照。  save 60 10000        #在60秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发BGSAVE命令创建快照。</code></pre></li><li><p>AOF（append-only file）持久化</p><p>  与快照持久化相比，AOF持久化 的实时性更好，因此已成为主流的持久化方案。默认情况下Redis没有开启AOF（append only file）方式的持久化，可以通过appendonly参数开启：</p><pre><code>  appendonly yes</code></pre><p>  开启AOF持久化后每执行一条会更改Redis中的数据的命令，Redis就会将该命令写入硬盘中的AOF文件。AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的，默认的文件名是appendonly.aof。</p><p>  在Redis的配置文件中存在三种不同的 AOF 持久化方式，它们分别是：</p></li></ul><pre><code>```appendfsync always    #每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度appendfsync everysec  #每秒钟同步一次，显示地将多个写命令同步到硬盘appendfsync no        #让操作系统决定何时进行同步```为了兼顾数据和写入性能，用户可以考虑 appendfsync everysec选项 ，让Redis每秒同步一次AOF文件，Redis性能几乎没受到任何影响。而且这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操作的时候，Redis还会优雅的放慢自己的速度以便适应硬盘的最大写入速度。</code></pre><ul><li><p>Redis 4.0 对于持久化机制的优化</p><p>  Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。</p><p>  如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分是压缩格式不再是 AOF 格式，可读性较差。</p></li><li><p>补充内容：AOF 重写</p><p>  AOF重写可以产生一个新的AOF文件，这个新的AOF文件和原有的AOF文件所保存的数据库状态一样，但体积更小。</p><p>  AOF重写是一个有歧义的名字，该功能是通过读取数据库中的键值对来实现的，程序无须对现有AOF文件进行任何读入、分析或者写入操作。</p><p>  在执行 BGREWRITEAOF 命令时，Redis 服务器会维护一个 AOF 重写缓冲区，该缓冲区会在子进程创建新AOF文件期间，记录服务器执行的所有写命令。当子进程完成创建新AOF文件的工作之后，服务器会将重写缓冲区中的所有内容追加到新AOF文件的末尾，使得新旧两个AOF文件所保存的数据库状态一致。最后，服务器用新的AOF文件替换旧的AOF文件，以此来完成AOF文件重写操作</p></li><li><p><strong>持久化的RDB和AOF的优缺点（没有表达，需补充）</strong></p></li></ul><h2 id="8-redis-事务"><a href="#8-redis-事务" class="headerlink" title="8 redis 事务"></a>8 redis 事务</h2><p>Redis 通过 MULTI、EXEC、WATCH 等命令来实现事务(transaction)功能。事务提供了一种将多个命令请求打包，然后一次性、按顺序地执行多个命令的机制，并且在事务执行期间，服务器不会中断事务而改去执行其他客户端的命令请求，它会将事务中的所有命令都执行完毕，然后才去处理其他客户端的命令请求。</p><p>在传统的关系式数据库中，常常用 ACID 性质来检验事务功能的可靠性和安全性。在 Redis 中，事务总是具有原子性（Atomicity）、一致性（Consistency）和隔离性（Isolation），并且当 Redis 运行在某种特定的持久化模式下时，事务也具有持久性（Durability）。</p><p><strong>redis同一个事务中如果有一条命令执行失败，其后的命令仍然会被执行，没有回滚。</strong></p><h2 id="9-缓存雪崩和缓存穿透问题解决方案"><a href="#9-缓存雪崩和缓存穿透问题解决方案" class="headerlink" title="9 缓存雪崩和缓存穿透问题解决方案"></a>9 缓存雪崩和缓存穿透问题解决方案</h2><ul><li><p>缓存雪崩</p><p>  简介：缓存同一时间大面积的失效，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉。</p></li><li><p>有哪些解决办法？</p><ul><li><p>事前：尽量保证整个 redis 集群的高可用性，发现机器宕机尽快补上。选择合适的内存淘汰策略。</p></li><li><p>事中：本地ehcache缓存 + hystrix限流&amp;降级，避免MySQL崩掉</p></li><li><p>事后：利用 redis 持久化机制保存的数据尽快恢复缓存</p><p><img src="/2020/01/18/2020-01-18-redis/bxthl0km.bmp" alt></p></li></ul></li><li><p>缓存穿透</p><p>  缓存穿透说简单点就是大量请求的 key 根本不存在于缓存中，导致请求直接到了数据库上，根本没有经过缓存这一层。举个例子：某个黑客故意制造我们缓存中不存在的 key 发起大量请求，导致大量请求落到数据库。</p><p>  正常缓存处理流程：</p><p>  <img src="/2020/01/18/2020-01-18-redis/fiadrywf.bmp" alt></p><p>  缓存穿透情况处理流程：</p><p>  <img src="/2020/01/18/2020-01-18-redis/p00dahdi.bmp" alt></p><p>  一般MySQL 默认的最大连接数在 150 左右，这个可以通过 show variables like ‘%max_connections%’;命令来查看。最大连接数一个还只是一个指标，cpu，内存，磁盘，网络等无力条件都是其运行指标，这些指标都会限制其并发能力！所以，一般 3000 个并发请求就能打死大部分数据库了。</p></li><li><p>有哪些解决办法？</p><p>  最基本的就是首先做好参数校验，一些不合法的参数请求直接抛出异常信息返回给客户端。比如查询的数据库 id 不能小于 0、传入的邮箱格式不对的时候直接返回错误消息给客户端等等。</p><ul><li><p>1）缓存无效 key : 如果缓存和数据库都查不到某个 key 的数据就写一个到 redis 中去并设置过期时间，具体命令如下：SET key value EX 10086。这种方式可以解决请求的 key 变化不频繁的情况，如果黑客恶意攻击，每次构建不同的请求key，会导致 redis 中缓存大量无效的 key 。很明显，这种方案并不能从根本上解决此问题。如果非要用这种方式来解决穿透问题的话，尽量将无效的 key 的过期时间设置短一点比如 1 分钟。</p></li><li><p>2）布隆过滤器：布隆过滤器是一个非常神奇的数据结构，通过它我们可以非常方便地判断一个给定数据是否存在与海量数据中。具体是这样做的：把所有可能存在的请求的值都存放在布隆过滤器中，当用户请求过来，我会先判断用户发来的请求的值是否存在于布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端，存在的话才会走下面的流程</p><p><img src="/2020/01/18/2020-01-18-redis/k0ml0thj.bmp" alt></p><p>更多关于布隆过滤器的内容：<a href="https://github.com/Snailclimb/JavaGuide/blob/master/docs/dataStructures-algorithms/data-structure/bloom-filter.md" target="_blank" rel="noopener">https://github.com/Snailclimb/JavaGuide/blob/master/docs/dataStructures-algorithms/data-structure/bloom-filter.md</a></p></li></ul></li></ul><h2 id="10-如何解决-Redis-的并发竞争-Key-问题"><a href="#10-如何解决-Redis-的并发竞争-Key-问题" class="headerlink" title="10 如何解决 Redis 的并发竞争 Key 问题"></a>10 如何解决 Redis 的并发竞争 Key 问题</h2><p>所谓 Redis 的并发竞争 Key 的问题也就是多个系统同时对一个 key 进行操作，但是最后执行的顺序和我们期望的顺序不同，这样也就导致了结果的不同！</p><p>推荐一种方案：分布式锁（zookeeper 和 redis 都可以实现分布式锁）。（如果不存在 Redis 的并发竞争 Key 问题，不要使用分布式锁，这样会影响性能）</p><p>基于zookeeper临时有序节点可以实现的分布式锁。大致思想为：每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。完成业务流程后，删除对应的子节点释放锁。</p><p>在实践中，当然是从以可靠性为主。所以首推Zookeeper。</p><h2 id="11-如何保证缓存与数据库双写时的数据一致性"><a href="#11-如何保证缓存与数据库双写时的数据一致性" class="headerlink" title="11 如何保证缓存与数据库双写时的数据一致性?"></a>11 如何保证缓存与数据库双写时的数据一致性?</h2><p>一般来说，就是如果你的系统不是严格要求缓存+数据库必须一致性的话，缓存可以稍微的跟数据库偶尔有不一致的情况，最好不要做这个方案，读请求和写请求串行化，串到一个内存队列里去，这样就可以保证一定不会出现不一致的情况</p><p>串行化之后，就会导致系统的吞吐量会大幅度的降低，用比正常情况下多几倍的机器去支撑线上的一个请求。</p><p>更多内容可以查看：<a href="https://github.com/doocs/advanced-java/blob/master/docs/high-concurrency/redis-consistence.md" target="_blank" rel="noopener">https://github.com/doocs/advanced-java/blob/master/docs/high-concurrency/redis-consistence.md</a></p><h2 id="12-待补充-如何保证缓存与数据库双写时的数据一致性"><a href="#12-待补充-如何保证缓存与数据库双写时的数据一致性" class="headerlink" title="12 (待补充)如何保证缓存与数据库双写时的数据一致性?"></a>12 (待补充)如何保证缓存与数据库双写时的数据一致性?</h2>]]></content>
      
      
      <categories>
          
          <category> 中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL</title>
      <link href="/2020/01/17/2020-01-17-mysql/"/>
      <url>/2020/01/17/2020-01-17-mysql/</url>
      
        <content type="html"><![CDATA[<h1 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h1><h2 id="1-存储引擎"><a href="#1-存储引擎" class="headerlink" title="1 存储引擎"></a>1 存储引擎</h2><ul><li><p>MyISAM和InnoDB区别</p><ul><li>是否支持行级锁 : MyISAM 只有表级锁(table-level locking)，而InnoDB 支持行级锁(row-level locking)和表级锁,默认为行级锁。</li><li>是否支持事务和崩溃后的安全恢复： MyISAM 强调的是性能，每次查询具有原子性,其执行速度比InnoDB类型更快，但是不提供事务支持。但是InnoDB 提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。</li><li>是否支持外键： MyISAM不支持，而InnoDB支持。</li><li>是否支持MVCC ：仅 InnoDB 支持。应对高并发事务, MVCC比单纯的加锁更高效;MVCC只在 READ COMMITTED 和 REPEATABLE READ 两个隔离级别下工作;MVCC可以使用 乐观(optimistic)锁 和 悲观(pessimistic)锁来实现;各数据库中MVCC实现并不统一。推荐阅读：MySQL-InnoDB-MVCC多版本并发控制</li></ul></li></ul><h2 id="2-事物的四大特性-ACID"><a href="#2-事物的四大特性-ACID" class="headerlink" title="2 事物的四大特性(ACID)"></a>2 事物的四大特性(ACID)</h2><ul><li>原子性（Atomicity）： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；</li><li>一致性（Consistency）： 执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的；</li><li>隔离性（Isolation）： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的；</li><li>持久性（Durability）： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。</li></ul><h2 id="3-并发事务带来哪些问题"><a href="#3-并发事务带来哪些问题" class="headerlink" title="3 并发事务带来哪些问题?"></a>3 并发事务带来哪些问题?</h2><ul><li>脏读（Dirty read）: 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。</li><li>丢失修改（Lost to modify）: 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。</li><li>不可重复读（Unrepeatableread）: 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。</li><li>幻读（Phantom read）: 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。</li></ul><p>不可重复读的重点是修改比如多次读取一条记录发现其中某些列的值被修改，幻读的重点在于新增或者删除比如多次读取一条记录发现记录增多或减少了。</p><h2 id="4-事务隔离级别有哪些-MySQL的默认隔离级别是"><a href="#4-事务隔离级别有哪些-MySQL的默认隔离级别是" class="headerlink" title="4 事务隔离级别有哪些?MySQL的默认隔离级别是?"></a>4 事务隔离级别有哪些?MySQL的默认隔离级别是?</h2><ul><li>READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。</li><li>READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。</li><li>REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。</li><li>SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。</li></ul><p><img src="/2020/01/17/2020-01-17-mysql/1sqrxyoa.bmp" alt><br>MySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读）。可以通过SELECT @@tx_isolation;命令来查看。</p><h2 id="5-锁机制与InnoDB锁算法"><a href="#5-锁机制与InnoDB锁算法" class="headerlink" title="5 锁机制与InnoDB锁算法"></a>5 锁机制与InnoDB锁算法</h2><ul><li>MyISAM和InnoDB存储引擎使用的锁：<ul><li>MyISAM采用表级锁(table-level locking)。</li><li>InnoDB支持行级锁(row-level locking)和表级锁,默认为行级锁</li></ul></li><li>表级锁和行级锁对比：<ul><li>表级锁： MySQL中锁定 粒度最大 的一种锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。其锁定粒度最大，触发锁冲突的概率最高，并发度最低，MyISAM和 InnoDB引擎都支持表级锁。</li><li>行级锁： MySQL中锁定 粒度最小 的一种锁，只针对当前操作的行进行加锁。 行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。 </li></ul></li><li>InnoDB存储引擎的锁的算法有三种：<ul><li>Record lock：单个行记录上的锁</li><li>Gap lock：间隙锁，锁定一个范围，不包括记录本身</li><li>Next-key lock：record+gap 锁定一个范围，包含记录本身</li></ul></li><li>相关知识点：<ul><li>innodb对于行的查询使用next-key lock</li><li>Next-locking keying为了解决Phantom Problem幻读问题</li><li>当查询的索引含有唯一属性时，将next-key lock降级为record key</li><li>Gap锁设计的目的是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生</li><li>有两种方式显式关闭gap锁：（除了外键约束和唯一性检查外，其余情况仅使用record lock） A. 将事务隔离级别设置为RC B. 将参数innodb_locks_unsafe_for_binlog设置为1</li></ul></li></ul><h2 id="6-大表优化"><a href="#6-大表优化" class="headerlink" title="6 大表优化"></a>6 大表优化</h2><ul><li><p>读/写分离</p><p>经典的数据库拆分方案，主库负责写，从库负责读；</p><ul><li><p>垂直分区</p><p> 根据数据库里面数据表的相关性进行拆分。 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。</p><p> <img src="/2020/01/17/2020-01-17-mysql/w1pft649.bmp" alt></p><ul><li>垂直拆分的优点： 可以使得列数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。</li><li>垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂；</li></ul></li><li><p>水平分区</p><p> 保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。</p><p> <img src="/2020/01/17/2020-01-17-mysql/gv5r1ghp.bmp" alt></p><p> 水平拆分能够 支持非常大的数据量存储，应用端改造也少，但 分片事务难以解决 ，跨节点Join性能较差，逻辑复杂。</p></li><li><p>数据库分片的两种常见方案：</p><ul><li>客户端代理： 分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。 当当网的 Sharding-JDBC 、阿里的TDDL是两种比较常用的实现。</li><li>中间件代理： 在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。 我们现在谈的 <strong>Mycat</strong> 、360的Atlas、网易的DDB等等都是这种架构的实现。</li></ul></li></ul></li></ul><h2 id="7-解释一下什么是池化设计思想。什么是数据库连接池-为什么需要数据库连接池"><a href="#7-解释一下什么是池化设计思想。什么是数据库连接池-为什么需要数据库连接池" class="headerlink" title="7 解释一下什么是池化设计思想。什么是数据库连接池?为什么需要数据库连接池?"></a>7 解释一下什么是池化设计思想。什么是数据库连接池?为什么需要数据库连接池?</h2><p>设计会初始预设资源，解决的问题就是抵消每次获取资源的消耗，如创建线程的开销，获取远程连接的开销等。</p><p>数据库连接本质就是一个 socket 的连接。数据库服务端还要维护一些缓存和用户权限信息之类的 所以占用了一些内存。我们可以把数据库连接池是看做是维护的数据库连接的缓存，以便将来需要对数据库的请求时可以重用这些连接。为每个用户打开和维护数据库连接，尤其是对动态数据库驱动的网站应用程序的请求，既昂贵又浪费资源。在连接池中，创建连接后，将其放置在池中，并再次使用它，因此不必建立新的连接。如果使用了所有连接，则会建立一个新连接并将其添加到池中。 连接池还减少了用户必须等待建立与数据库的连接的时间。</p><h2 id="建议继续看其他MySQL知识，这里不够完整"><a href="#建议继续看其他MySQL知识，这里不够完整" class="headerlink" title="建议继续看其他MySQL知识，这里不够完整"></a>建议继续看其他MySQL知识，这里不够完整</h2>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM</title>
      <link href="/2020/01/16/2020-01-16-jvm/"/>
      <url>/2020/01/16/2020-01-16-jvm/</url>
      
        <content type="html"><![CDATA[<h2 id="1-介绍下-Java-内存区域-运行时数据区"><a href="#1-介绍下-Java-内存区域-运行时数据区" class="headerlink" title="1 介绍下 Java 内存区域(运行时数据区)"></a>1 介绍下 Java 内存区域(运行时数据区)</h2><ul><li><p>JDK 1.8之前：</p><p>  <img src="/2020/01/16/2020-01-16-jvm/hlwi08xk.bmp" alt></p></li><li><p>JDK 1.8 ：</p><p>  <img src="/2020/01/16/2020-01-16-jvm/j141wm5o.bmp" alt></p></li><li><p>程序计数器：</p><p>  注意：程序计数器是唯一一个不会出现 OutOfMemoryError 的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。</p><ol><li><p>字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。</p></li><li><p>在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。</p></li></ol></li><li><p>Java 虚拟机栈</p><p>  Java 内存可以粗糙的区分为堆内存（Heap）和栈内存(Stack),其中栈就是现在说的虚拟机栈，或者说是虚拟机栈中局部变量表部分。 </p><p>  局部变量表主要存放了编译器可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。</p><p>  Java 虚拟机栈会出现两种异常：StackOverFlowError 和 OutOfMemoryError。</p><ul><li><p>StackOverFlowError： 若Java虚拟机栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前Java虚拟机栈的最大深度的时候，就抛出StackOverFlowError异常。</p></li><li><p>OutOfMemoryError： 若 Java 虚拟机栈的内存大小允许动态扩展，且当线程请求栈时内存用完了，无法再动态扩展了，此时抛出OutOfMemoryError异常。</p><p>Java 虚拟机栈也是线程私有的，每个线程都有各自的Java虚拟机栈，而且随着线程的创建而创建，随着线程的死亡而死亡。Java方法有两种返回方式：</p><p>Java 栈可用类比数据结构中栈，Java 栈中保存的主要内容是栈帧，每一次函数调用都会有一个对应的栈帧被压入Java栈，每一个函数调用结束后，都会有一个栈帧被弹出。Java方法有两种返回方式：</p><ul><li>return 语句。</li><li>抛出异常。</li></ul><p>不管哪种返回方式都会导致栈帧被弹出。</p></li></ul></li><li><p>本地方法栈</p><p>  和虚拟机栈所发挥的作用非常相似，区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。</p><p>  本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。</p><p>  方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现 StackOverFlowError 和 OutOfMemoryError 两种异常。</p></li><li><p>堆</p><p>  <strong>Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。</strong></p><p>  <strong>Java 堆是垃圾收集器管理的主要区域</strong>，因此也被称作GC堆（Garbage Collected Heap）.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以Java堆还可以细分为：新生代和老年代：再细致一点有：Eden空间、From Survivor、To Survivor空间等。<strong>进一步划分的目的是更好地回收内存，或者更快地分配内存。</strong></p><p>  <img src="/2020/01/16/2020-01-16-jvm/0md51ltn.bmp" alt></p><p>  上图所示的 eden区、s0区、s1区都属于新生代，tentired 区属于老年代。大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden区-&gt;Survivor 区后对象的初始年龄变为1)，当它的年龄增加到一定程度（默认为15岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。</p></li><li><p>方法区</p><p>  方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。</p><p>  <strong>方法区也被称为永久代。</strong> 相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。</p><p>  JDK 1.8 的时候，方法区（HotSpot的永久代）被彻底移除了（JDK1.7就已经开始了），取而代之是元空间，元空间使用的是直接内存。</p><ol><li><p>为什么要将永久代(PermGen)替换为元空间(MetaSpace)呢?</p><p>  整个永久代有一个 JVM 本身设置固定大小上线，无法进行调整，而元空间使用的是直接内存，受本机可用内存的限制，并且永远不会得到java.lang.OutOfMemoryError。你可以使用 -XX：MaxMetaspaceSize 标志设置最大元空间大小，默认值为 unlimited，这意味着它只受系统内存的限制。-XX：MetaspaceSize 调整标志定义元空间的初始大小如果未指定此标志，则 Metaspace 将根据运行时的应用程序需求动态地重新调整大小。</p></li></ol></li><li><p>运行时常量池</p><p>  运行时常量池是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池信息（用于存放编译期生成的各种字面量和符号引用）</p><p>  既然运行时常量池时方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 异常。</p><p>  <strong>DK1.7及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。</strong></p><p>  <img src="/2020/01/16/2020-01-16-jvm/7p80rev4.bmp" alt></p></li><li><p>直接内存</p><p>  直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用。而且也可能导致 OutOfMemoryError 异常出现。</p></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>线程私有的是JAVA 虚拟机栈、本地方法栈、程序计数器；线程共享的是堆和方法区。</li><li>Java 虚拟机栈：虚拟机栈中主要保存了局部变量，包含各种数据类型（boolean、byte、char、short、int、float、long、double）和对象引用（reference类型）。Java 虚拟机栈会出现两种异常：StackOverFlowError 和 OutOfMemoryError。</li><li>本地方法栈：本地方法栈和和虚拟机栈所发挥的作用非常相似。两者的区别在于：虚拟机栈执行的是JAVA方法，而本地方法栈调用的是nvtice方法。也会出现 StackOverFlowError 和 OutOfMemoryError 两种异常。</li><li>程序计数器：程序计数器实现代码的流程控制，如：顺序执行、选择、循环、异常处理。在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。程序计数器是唯一一个不会出现 OutOfMemoryError 的内存区域.</li><li>堆：堆存放的是对象实例，几乎所有的对象实例以及数组都在这里分配内存。堆是垃圾收集器管理的主要区域，因此也被称作GC堆.</li><li>方法区：方法区主要存储加载的类信息、常量、静态变量等。方法区也被称为永久代。在JDK 1.8 的时候，方法区（背取代成元空间，元空间使用的是直接内存。因为方法区受jvm的大小配置限制，而元空间直接受机器的内存管理，永远不会得到java.lang.OutOfMemoryError。</li><li>运行时常量池：常量池是方法区的一部分。主要存放符号引用。会抛出 OutOfMemoryError 异常。DK1.7及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。</li></ul><h2 id="2-说一下Java对象的创建过程"><a href="#2-说一下Java对象的创建过程" class="headerlink" title="2 说一下Java对象的创建过程"></a><strong>2 说一下Java对象的创建过程</strong></h2><p><strong>下图便是 Java 对象的创建过程，建议最好是能默写出来，并且要掌握每一步在做什么</strong><br><img src="/2020/01/16/2020-01-16-jvm/1rfud9wf.bmp" alt></p><ol><li><p>①类加载检查： 虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。</p></li><li><p>②分配内存： 在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。分配方式有 “指针碰撞” 和 “空闲列表” 两种，选择那种分配方式由 Java 堆是否规整决定，而Java堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。</p><p> 内存分配的两种方式：（补充内容，需要掌握）</p><p>  选择以上两种方式中的哪一种，取决于 Java 堆内存是否规整。而 Java 堆内存是否规整，取决于 GC 收集器的算法是”标记-清除”，还是”标记-整理”（也称作”标记-压缩”），值得注意的是，复制算法内存也是规整的</p><p>  <img src="/2020/01/16/2020-01-16-jvm/y2du1php.bmp" alt></p><p> 内存分配并发问题（补充内容，需要掌握）</p><p> 在创建对象的时候有一个很重要的问题，就是线程安全，因为在实际开发过程中，创建对象是很频繁的事情，作为虚拟机来说，必须要保证线程是安全的，通常来讲，虚拟机采用两种方式来保证线程安全：</p><ul><li>CAS+失败重试： CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性。</li><li>TLAB： 为每一个线程预先在Eden区分配一块儿内存，JVM在给线程中的对象分配内存时，首先在TLAB分配，当对象大于TLAB中的剩余内存或TLAB的内存已用尽时，再采用上述的CAS进行内存分配</li></ul></li><li><p>③初始化零值： 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。</p></li><li><p>④设置对象头： 初始化零值完成之后，虚拟机要对对象进行必要的设置，例如这个对象是那个类的实例、如何才能找到类的元数据信息、对象的哈希吗、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。</p></li><li><p>⑤执行 init 方法： 在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，<init> 方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 <init> 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。</init></init></p></li></ol><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><ol><li>类加载检查：虚拟机遇到一条new指令时，会先在常量池中定位是否有类的符号引用，并且检查这个类是否已经被加载过，如果没有被加载过，就先执行类的加载过程。</li><li>分配内存：当类加载检查完成后，对象所需的内存大小也就确定了。分配内存主要有两种方式：“指针碰撞” 和 “空闲列表” 。用哪种方式主要看内存的规整来决定。如果内存规整，也就是用指针碰撞的形式，如serial，parlnew垃圾收集器，回收算法是标志-整理和复制算法。如果内存不规整，用空闲列表的形式，如CMS垃圾收集器，回收算法是标志-清除。</li><li>初始化零值：这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用。</li><li>设置对象头：主要把类的信息，hash码以及分代年龄等信息存放在对象头中。</li><li>执行init方法：把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。</li></ol><h2 id="3-对象的访问定位有哪两种方式"><a href="#3-对象的访问定位有哪两种方式" class="headerlink" title="3 对象的访问定位有哪两种方式?"></a>3 对象的访问定位有哪两种方式?</h2><ul><li>建立对象就是为了使用对象，我们的Java程序通过栈上的 reference 数据来操作堆上的具体对象。对象的访问方式有虚拟机实现而定，目前主流的访问方式有①使用句柄和②直接指针两种：</li></ul><ol><li>句柄： 如果使用句柄的话，那么Java堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息；</li></ol><p><img src="/2020/01/16/2020-01-16-jvm/dllw5s27.bmp" alt></p><ol start="2"><li>直接指针： 如果使用直接指针访问，那么 Java 堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而reference 中存储的直接就是对象的地址。</li></ol><p><img src="/2020/01/16/2020-01-16-jvm/aqodd113.bmp" alt></p><p>这两种对象访问方式各有优势。使用句柄来访问的最大好处是 reference 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而 reference 本身不需要修改。使用直接指针访问方式最大的好处就是速度快，它节省了一次指针定位的时间开销。</p><h2 id="4-说一下堆内存中对象的分配的基本策略"><a href="#4-说一下堆内存中对象的分配的基本策略" class="headerlink" title="4 说一下堆内存中对象的分配的基本策略"></a>4 说一下堆内存中对象的分配的基本策略</h2><p>堆空间的基本结构：</p><p><img src="/2020/01/16/2020-01-16-jvm/al16mkpf.bmp" alt></p><p>上图所示的 eden区、s0区、s1区都属于新生代，tentired 区属于老年代。大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden区-&gt;Survivor 区后对象的初始年龄变为1)，当它的年龄增加到一定程度（默认为15岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。</p><p>另外，大对象和长期存活的对象会直接进入老年代。</p><p><img src="/2020/01/16/2020-01-16-jvm/40cpg52o.bmp" alt></p><h2 id="5-Minor-Gc和Full-GC-有什么不同呢？"><a href="#5-Minor-Gc和Full-GC-有什么不同呢？" class="headerlink" title="5 Minor Gc和Full GC 有什么不同呢？"></a>5 Minor Gc和Full GC 有什么不同呢？</h2><p>大多数情况下，对象在新生代中 eden 区分配。当 eden 区没有足够空间进行分配时，虚拟机将发起一次Minor GC。</p><ul><li>新生代GC（Minor GC）:指发生新生代的的垃圾收集动作，Minor GC非常频繁，回收速度一般也比较快。</li><li>老年代GC（Major GC/Full GC）:指发生在老年代的GC，出现了Major GC经常会伴随至少一次的Minor GC（并非绝对），Major GC的速度一般会比Minor GC的慢10倍以上。</li></ul><h2 id="6-如何判断对象是否死亡-两种方法"><a href="#6-如何判断对象是否死亡-两种方法" class="headerlink" title="6 如何判断对象是否死亡?(两种方法)"></a>6 如何判断对象是否死亡?(两种方法)</h2><p>堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断哪些对象已经死亡（即不能再被任何途径使用的对象）。</p><ul><li><p>引用计数法</p><p>  给对象中添加一个引用计数器，每当有一个地方引用它，计数器就加1；当引用失效，计数器就减1；任何时候计数器为0的对象就是不可能再被使用的。</p></li><li><p>可达性分析算法</p><p>  这个算法的基本思想就是通过一系列的称为 “GC Roots” 的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的。</p><p>  <img src="/2020/01/16/2020-01-16-jvm/2h0me53s.bmp" alt></p></li></ul><h2 id="7-简单的介绍一下强引用-软引用-弱引用-虚引用"><a href="#7-简单的介绍一下强引用-软引用-弱引用-虚引用" class="headerlink" title="7 简单的介绍一下强引用,软引用,弱引用,虚引用"></a>7 简单的介绍一下强引用,软引用,弱引用,虚引用</h2><p>无论是通过引用计数法判断对象引用数量，还是通过可达性分析法判断对象的引用链是否可达，判定对象的存活都与“引用”有关。</p><ul><li><p>强引用(StrongReference)</p><p>  以前我们使用的大部分引用实际上都是强引用，这是使用最普遍的引用。如果一个对象具有强引用，那就类似于必不可少的生活用品，垃圾回收器绝不会回收它。当内存空 间不足，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。</p></li><li><p>软引用(SoftReference)</p><p>  如果一个对象只具有软引用，那就类似于可有可无的生活用品。如果内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。</p><p>  软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收，JAVA虚拟机就会把这个软引用加入到与之关联的引用队列中。</p></li><li><p>弱引用(WeakReference)</p><p>  如果一个对象只具有弱引用，那就类似于可有可无的生活用品。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它 所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。 </p><p>  弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。</p></li><li><p>4．虚引用（PhantomReference）</p><p>  “虚引用”顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。</p><p>  虚引用主要用来跟踪对象被垃圾回收的活动。</p><p>  虚引用与软引用和弱引用的一个区别在于： 虚引用必须和引用队列（ReferenceQueue）联合使用。当垃 圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是 否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 </p><p>  特别注意，在程序设计中一般很少使用弱引用与虚引用，使用软引用的情况较多，这是因为软引用可以加速JVM对垃圾内存的回收速度，可以维护系统的运行安全，防止内存溢出（OutOfMemory）等问题的产生。</p></li></ul><h2 id="8-如何判断一个常量是废弃常量"><a href="#8-如何判断一个常量是废弃常量" class="headerlink" title="8 如何判断一个常量是废弃常量?"></a>8 如何判断一个常量是废弃常量?</h2><p>运行时常量池主要回收的是废弃的常量。那么，我们如何判断一个常量是废弃常量呢？</p><p>假如在常量池中存在字符串 “abc”，如果当前没有任何String对象引用该字符串常量的话，就说明常量 “abc” 就是废弃常量，如果这时发生内存回收的话而且有必要的话，”abc” 就会被系统清理出常量池。</p><h2 id="9-如何判断一个类是无用的类"><a href="#9-如何判断一个类是无用的类" class="headerlink" title="9 如何判断一个类是无用的类?"></a>9 如何判断一个类是无用的类?</h2><ul><li>该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。</li><li>加载该类的 ClassLoader 已经被回收。</li><li>该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。</li></ul><h2 id="10-垃圾收集有哪些算法，各自的特点？"><a href="#10-垃圾收集有哪些算法，各自的特点？" class="headerlink" title="10 垃圾收集有哪些算法，各自的特点？"></a>10 垃圾收集有哪些算法，各自的特点？</h2><p><img src="/2020/01/16/2020-01-16-jvm/3mdcl3lc.bmp" alt></p><ul><li><p>标记-清除算法</p><p>  算法分为“标记”和“清除”阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。它是最基础的收集算法，后续的算法都是对其不足进行改进得到。这种垃圾收集算法会带来两个明显的问题：</p><pre><code>  - 效率问题  - 空间问题（标记清除后会产生大量不连续的碎片）</code></pre><p>  <img src="/2020/01/16/2020-01-16-jvm/tpi6cpml.bmp" alt></p></li><li><p>复制算法</p><p>  为了解决效率问题，“复制”收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。</p><p>  <img src="/2020/01/16/2020-01-16-jvm/dknyft70.bmp" alt></p></li><li><p>标记-整理算法</p><p>  根据老年代的特点特出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。<br>  <img src="/2020/01/16/2020-01-16-jvm/qish34yq.bmp" alt></p></li><li><p>分代收集算法</p><p>  当前虚拟机的垃圾收集都采用分代收集算法，这种算法没有什么新的思想，只是根据对象存活周期的不同将内存分为几块。一般将java堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。</p><p>  比如在新生代中，每次收集都会有大量对象死去，所以可以选择复制算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。</p></li></ul><h2 id="11-HotSpot为什么要分为新生代和老年代？"><a href="#11-HotSpot为什么要分为新生代和老年代？" class="headerlink" title="11 HotSpot为什么要分为新生代和老年代？"></a>11 HotSpot为什么要分为新生代和老年代？</h2><p><strong>主要是为了提升GC效率。</strong></p><h2 id="12-常见的垃圾回收器有那些"><a href="#12-常见的垃圾回收器有那些" class="headerlink" title="12 常见的垃圾回收器有那些?"></a>12 常见的垃圾回收器有那些?</h2><p><img src="/2020/01/16/2020-01-16-jvm/u1otfict.bmp" alt><br>如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。<br>根据具体应用场景选择适合的垃圾收集器。</p><ul><li><p>Serial收集器</p><p>  Serial（串行）收集器收集器是最基本、历史最悠久的垃圾收集器了。大家看名字就知道这个收集器是一个单线程收集器了。它的 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ “Stop The World” ），直到它收集结束。</p><p>  <img src="/2020/01/16/2020-01-16-jvm/w2jtzc95.bmp" alt></p><p>  <img src="/2020/01/16/2020-01-16-jvm/image7.png" alt></p><p>  <strong>新生代采用复制算法，老年代采用标记-整理算法</strong></p><p>  虚拟机的设计者们当然知道Stop The World带来的不良用户体验，所以在后续的垃圾收集器设计中停顿时间在不断缩短（仍然还有停顿，寻找最优秀的垃圾收集器的过程仍然在继续）。</p><p>  但是Serial收集器有没有优于其他垃圾收集器的地方呢？当然有，它简单而高效（与其他收集器的单线程相比）。Serial收集器由于没有线程交互的开销，自然可以获得很高的单线程收集效率。Serial收集器对于运行在Client模式下的虚拟机来说是个不错的选择。</p></li><li><p>ParNew收集器</p><p>  ParNew收集器其实就是Serial收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和Serial收集器完全一样。</p><p>  <img src="/2020/01/16/2020-01-16-jvm/ngopf1fj.bmp" alt></p><p>  <img src="/2020/01/16/2020-01-16-jvm/image10.png" alt></p><p>  <strong>新生代采用复制算法，老年代采用标记-整理算法。</strong></p><p>  它是许多运行在Server模式下的虚拟机的首要选择，除了Serial收集器外，只有它能与CMS收集器（真正意义上的并发收集器）配合工作。</p><p>  并行和并发概念补充：</p><ul><li>并行（Parallel） ：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。</li><li>并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行，可能会交替执行），用户程序在继续运行，而垃圾收集器运行在另一个CPU上。</li></ul></li><li><p>Parallel Scavenge收集器</p><p>  Parallel Scavenge 收集器类似于ParNew 收集器。</p><pre><code>  -XX:+UseParallelGC       使用Parallel收集器+ 老年代串行  -XX:+UseParallelOldGC      使用Parallel收集器+ 老年代并行</code></pre><p>  Parallel Scavenge收集器关注点是吞吐量（高效率的利用CPU）。CMS等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是CPU中用于运行用户代码的时间与CPU总消耗时间的比值。 Parallel Scavenge收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解的话，手工优化存在的话可以选择把内存管理优化交给虚拟机去完成也是一个不错的选择。</p><p>  <img src="/2020/01/16/2020-01-16-jvm/2tlrkyi5.bmp" alt></p><p>  <strong>新生代采用复制算法，老年代采用标记-整理算法。</strong></p></li><li><p>Serial Old收集器</p><p>  Serial收集器的老年代版本，它同样是一个单线程收集器。它主要有两大用途：一种用途是在JDK1.5以及以前的版本中与Parallel Scavenge收集器搭配使用，另一种用途是作为CMS收集器的后备方案。</p></li><li><p>Parallel Old收集器</p><p>  Parallel Scavenge收集器的老年代版本。使用多线程和“标记-整理”算法。在注重吞吐量以及CPU资源的场合，都可以优先考虑 Parallel Scavenge收集器和Parallel Old收集器。</p></li><li><p>CMS收集器</p><p>  CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它而非常符合在注重用户体验的应用上使用。</p><p>  CMS（Concurrent Mark Sweep）收集器是HotSpot虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。</p><p>  从名字中的Mark Sweep这两个词可以看出，CMS收集器是一种 “标记-清除”算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤：</p><ul><li><p>初始标记： 暂停所有的其他线程，并记录下直接与root相连的对象，速度很快 ；</p></li><li><p>并发标记： 同时开启GC和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以GC线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。</p></li><li><p>重新标记： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短</p></li><li><p>并发清除： 开启用户线程，同时GC线程开始对为标记的区域做清扫。</p><p><img src="/2020/01/16/2020-01-16-jvm/yjdb4a3c.bmp" alt></p><p>从它的名字就可以看出它是一款优秀的垃圾收集器，主要优点：并发收集、低停顿。但是它有下面三个明显的缺点：</p></li><li><p>对CPU资源敏感；</p></li><li><p>无法处理浮动垃圾；</p></li><li><p>它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。</p><p>由于 CMS 的并发标记和并发清除阶段，用户线程还在进行运行，所以也会不断产生垃圾，而 CMS 无法在当次收集中处理它们，只能等到下一次垃圾收集时再清理掉，这部分垃圾就称为浮动垃圾。</p></li></ul></li><li><p>G1收集器</p><p>  G1 (Garbage-First)是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足GC停顿时间要求的同时,还具备高吞吐量性能特征.</p><p>  被视为JDK1.7中HotSpot虚拟机的一个重要进化特征。它具备一下特点：</p><ul><li><p>并行与并发：G1能充分利用CPU、多核环境下的硬件优势，使用多个CPU（CPU或者CPU核心）来缩短Stop-The-World停顿时间。部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让java程序继续执行。</p></li><li><p>分代收集：虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但是还是保留了分代的概念。</p></li><li><p>空间整合：与CMS的“标记–清理”算法不同，G1从整体来看是基于“标记整理”算法实现的收集器；从局部上来看是基于“复制”算法实现的。</p></li><li><p>可预测的停顿：这是G1相对于CMS的另一个大优势，降低停顿时间是G1 和 CMS 共同的关注点，但G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内。</p><p>G1收集器的运作大致分为以下几个步骤：</p></li><li><p>初始标记</p></li><li><p>并发标记</p></li><li><p>最终标记</p></li><li><p>筛选回收</p><p>G1收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的Region(这也就是它的名字Garbage-First的由来)。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了GF收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。</p><p><img src="/2020/01/16/2020-01-16-jvm/r0psxic5.bmp" alt></p></li></ul></li></ul><h2 id="13-类文件结构"><a href="#13-类文件结构" class="headerlink" title="13 类文件结构"></a>13 类文件结构</h2><p>根据 Java 虚拟机规范，类文件由单个 ClassFile 结构组成：</p><pre><code>ClassFile {    u4             magic; //Class 文件的标志    u2             minor_version;//Class 的小版本号    u2             major_version;//Class 的大版本号    u2             constant_pool_count;//常量池的数量    cp_info        constant_pool[constant_pool_count-1];//常量池    u2             access_flags;//Class 的访问标记    u2             this_class;//当前类    u2             super_class;//父类    u2             interfaces_count;//接口    u2             interfaces[interfaces_count];//一个类可以实现多个接口    u2             fields_count;//Class 文件的字段属性    field_info     fields[fields_count];//一个类会可以有个字段    u2             methods_count;//Class 文件的方法数量    method_info    methods[methods_count];//一个类可以有个多个方法    u2             attributes_count;//此类的属性表中的属性数    attribute_info attributes[attributes_count];//属性表集合}</code></pre><p>Class文件字节码结构组织示意图</p><p><img src="/2020/01/16/2020-01-16-jvm/n30hvpi9.bmp" alt></p><h2 id="14-类加载过程"><a href="#14-类加载过程" class="headerlink" title="14 类加载过程"></a>14 类加载过程</h2><p>类加载过程：加载-&gt;连接-&gt;初始化。连接过程又可分为三步:验证-&gt;准备-&gt;解析。</p><p><img src="/2020/01/16/2020-01-16-jvm/dpt93lb1.bmp" alt></p><p>类加载过程的第一步，主要完成下面3件事情：</p><ol><li>通过全类名获取定义此类的二进制字节流</li><li>将字节流所代表的静态存储结构转换为方法区的运行时数据结构</li><li>在内存中生成一个代表该类的 Class 对象,作为方法区这些数据的访问入口</li></ol><p><strong>一个非数组类的加载阶段（加载阶段获取类的二进制字节流的动作）是可控性最强的阶段，这一步我们可以去完成还可以自定义类加载器去控制字节流的获取方式（重写一个类加载器的 loadClass() 方法）。数组类型不通过类加载器创建，它由 Java 虚拟机直接创建。</strong></p><ul><li><p>知道哪些类加载器?</p><p>  JVM 中内置了三个重要的 ClassLoader，除了 BootstrapClassLoader 其他类加载器均由 Java 实现且全部继承自java.lang.ClassLoader：</p><ol><li>BootstrapClassLoader(启动类加载器) ：最顶层的加载类，由C++实现，负责加载 %JAVA_HOME%/lib目录下的jar包和类或者或被 -Xbootclasspath参数指定的路径中的所有类。</li><li>ExtensionClassLoader(扩展类加载器) ：主要负责加载目录 %JRE_HOME%/lib/ext 目录下的jar包和类，或被 java.ext.dirs 系统变量所指定的路径下的jar包。</li><li>AppClassLoader(应用程序类加载器) :面向我们用户的加载器，负责加载当前应用classpath下的所有jar包和类。</li></ol></li><li><p>双亲委派模型知道吗？能介绍一下吗?</p><p>  <strong>每一个类都有一个对应它的类加载器。系统中的 ClassLoder 在协同工作的时候会默认使用 双亲委派模型 。即在类加载的时候，系统会首先判断当前类是否被加载过。已经被加载的类会直接返回，否则才会尝试加载。加载的时候，首先会把该请求委派该父类加载器的 loadClass() 处理，因此所有的请求最终都应该传送到顶层的启动类加载器 BootstrapClassLoader 中。当父类加载器无法处理时，才由自己来处理。当父类加载器为null时，会使用启动类加载器 BootstrapClassLoader 作为父类加载器。</strong></p><p>  <img src="/2020/01/16/2020-01-16-jvm/rxtzc1ki.bmp" alt></p><p>  每个类加载都有一个父类加载器，我们通过下面的程序来验证。</p><pre><code>  public class ClassLoaderDemo {      public static void main(String[] args) {          System.out.println(&quot;ClassLodarDemo&#39;s ClassLoader is &quot; + ClassLoaderDemo.class.getClassLoader());          System.out.println(&quot;The Parent of ClassLodarDemo&#39;s ClassLoader is &quot; + ClassLoaderDemo.class.getClassLoader().getParent());          System.out.println(&quot;The GrandParent of ClassLodarDemo&#39;s ClassLoader is &quot; + ClassLoaderDemo.class.getClassLoader().getParent().getParent());      }  }</code></pre><pre><code>  ClassLodarDemo&#39;s ClassLoader is sun.misc.Launcher$AppClassLoader@18b4aac2  The Parent of ClassLodarDemo&#39;s ClassLoader is sun.misc.Launcher$ExtClassLoader@1b6d3586  The GrandParent of ClassLodarDemo&#39;s ClassLoader is null</code></pre><p>  AppClassLoader的父类加载器为ExtClassLoader ExtClassLoader的父类加载器为null，null并不代表ExtClassLoader没有父类加载器，而是 Bootstrap ClassLoader 。</p><ul><li><p>双亲委派模型实现源码分析</p><p>双亲委派模型的实现代码非常简单，逻辑非常清晰，都集中在 java.lang.ClassLoader 的 loadClass() 中，相关代码如下所示。</p><pre><code>private final ClassLoader parent; protected Class&lt;?&gt; loadClass(String name, boolean resolve)      throws ClassNotFoundException  {      synchronized (getClassLoadingLock(name)) {          // 首先，检查请求的类是否已经被加载过          Class&lt;?&gt; c = findLoadedClass(name);          if (c == null) {              long t0 = System.nanoTime();              try {                  if (parent != null) {//父加载器不为空，调用父加载器loadClass()方法处理                      c = parent.loadClass(name, false);                  } else {//父加载器为空，使用启动类加载器 BootstrapClassLoader 加载                      c = findBootstrapClassOrNull(name);                  }              } catch (ClassNotFoundException e) {                 //抛出异常说明父类加载器无法完成加载请求              }              if (c == null) {                  long t1 = System.nanoTime();                  //自己尝试加载                  c = findClass(name);                  // this is the defining class loader; record the stats                  sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0);                  sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1);                  sun.misc.PerfCounter.getFindClasses().increment();              }          }          if (resolve) {              resolveClass(c);          }          return c;      }  }</code></pre></li><li><p><strong>双亲委派模型带来了什么好处呢？</strong><br>  <strong>双亲委派模型保证了Java程序的稳定运行，可以避免类的重复加载（JVM 区分不同类的方式不仅仅根据类名，相同的类文件被不同的类加载器加载产生的是两个不同的类），也保证了 Java 的核心 API 不被篡改。如果不用没有使用双亲委派模型，而是每个类加载器加载自己的话就会出现一些问题，比如我们编写一个称为 java.lang.Object 类的话，那么程序运行的时候，系统就会出现多个不同的 Object 类。</strong></p></li><li><p>如果我们不想用双亲委派模型怎么办？</p><p>为了避免双亲委托机制，我们可以自己定义一个类加载器，然后重载 loadClass() 即可。</p></li><li><p>如何自定义类加载器?</p><p>除了 BootstrapClassLoader 其他类加载器均由 Java 实现且全部继承自java.lang.ClassLoader。如果我们要自定义自己的类加载器，很明显需要继承 ClassLoader。</p></li></ul></li></ul><h2 id="15-类的加载过程"><a href="#15-类的加载过程" class="headerlink" title="15 类的加载过程"></a>15 类的加载过程</h2><p>类的加载过程为：加载、链接和初始化，其中链接可以细分为：验证、准备和解析。</p><p><img src="/2020/01/16/2020-01-16-jvm/image-20200705082601441.png" alt></p><h3 id="加载阶段"><a href="#加载阶段" class="headerlink" title="加载阶段"></a>加载阶段</h3><ul><li>通过类的全限定名读取此类二进制字节流。</li><li>将字节流转化为方法区的运行时数据结构。</li><li>在堆内存中生成该类的 Class 对象，作为该类的访问入口。</li></ul><h3 id="链接阶段"><a href="#链接阶段" class="headerlink" title="链接阶段"></a>链接阶段</h3><ul><li>验证：确保被加载类的正确性。</li><li>准备：为类变量分配内存并设置默认初始值。</li><li>解析：将常量池内的符号引用转换为直接引用。(符号引用就是描述目标，直接引用就是指向目标的地址。举个例子来说，现在调用方法 hello，这个方法的地址是 1234567，那么 hello 就是符号引用，1234567 就是直接引用)</li></ul><h3 id="初始化阶段"><a href="#初始化阶段" class="headerlink" title="初始化阶段"></a>初始化阶段</h3><p>执行类的初始化方法 clinit，执行内容包括静态变量初始化和静态块的执行。</p><h2 id="16-双亲委派机制"><a href="#16-双亲委派机制" class="headerlink" title="16 双亲委派机制"></a>16 双亲委派机制</h2><h3 id="类加载器"><a href="#类加载器" class="headerlink" title="类加载器"></a>类加载器</h3><ul><li><p>启动类加载器属于虚拟机的一部分，是用 C++ 写的，看不到源码。</p><p>  启动类加载器：加载的是 jre/lib 目录下的核心库。</p></li><li><p>其他类加载器是用 Java 写的。</p><ul><li><p>扩展类加载器：加载的是 jre/lib/ext 目录下的扩展包。</p></li><li><p>应用类加载器：加载的是 我们自己的 Java 代码编译成的 Class 文件的目录（ClassPath）。</p></li><li><p>自定义类加载器。</p></li></ul></li></ul><h3 id="双亲委派机制的工作原理"><a href="#双亲委派机制的工作原理" class="headerlink" title="双亲委派机制的工作原理"></a>双亲委派机制的工作原理</h3><ol><li>当一个类加载器接收到类加载任务时，先查缓存里有没有，如果没有，将任务委托给它的父加载器去执行。</li><li>父加载器也做同样的事情，一层一层往上委托，直到最顶层的启动类加载器为止。</li><li>如果启动类加载器没有找到所需加载的类，便将此加载任务退回给下一级类加载器去执行，而下一级的类加载器也做同样的事情。</li><li>如果最底层类加载器仍然没有找到所需要的 class 文件，则抛出异常。</li></ol><p><img src="/2020/01/16/2020-01-16-jvm/image-20200705105151258.png" alt></p><h3 id="双亲委派机制的优势"><a href="#双亲委派机制的优势" class="headerlink" title="双亲委派机制的优势"></a>双亲委派机制的优势</h3><p>避免类被重复加载 + 避免核心 API 被篡改。</p><p>如果没有双亲委托机制来确保类的全局唯一性，谁都可以编写一个 java.lang.String 类放在 classpath 下，那应用程序就乱套了。所以自定义一个与核心类库重名的类，会发现这个类可以被正常编译，但永远无法被加载运行。因为这个类不会被应用类加载器加载，而是被委托到顶层，被启动类加载器在核心类库中找到了。</p><h3 id="如何破坏双亲委派模型"><a href="#如何破坏双亲委派模型" class="headerlink" title="如何破坏双亲委派模型"></a>如何破坏双亲委派模型</h3><p>自定义类加载器，重写 loadClass 方法。</p><h2 id="17-堆"><a href="#17-堆" class="headerlink" title="17 堆"></a>17 堆</h2><p><img src="/2020/01/16/2020-01-16-jvm/20210401230845268.png" alt></p><h3 id="默认新生代和老年代的占比"><a href="#默认新生代和老年代的占比" class="headerlink" title="默认新生代和老年代的占比"></a>默认新生代和老年代的占比</h3><p>默认 -XX:NewRatio=2，代表新生代占 1，老年代占 2，即新生代占整个堆的 1/3。</p><h3 id="默认-Eden-和-Survivor-的占比"><a href="#默认-Eden-和-Survivor-的占比" class="headerlink" title="默认 Eden 和 Survivor 的占比"></a>默认 Eden 和 Survivor 的占比</h3><p>默认 -xx:SurvivorRatio=8，代表 Eden 和 From Survivor，To Survivor 占比是 8：1：1。</p><h3 id="堆对象流转过程"><a href="#堆对象流转过程" class="headerlink" title="堆对象流转过程"></a>堆对象流转过程</h3><p><img src="/2020/01/16/2020-01-16-jvm/20210401230851499.png" alt></p><ol><li>创建的对象优先在 Eden 区分配（较长的字符串，数组，大对象直接进老年代）。</li><li>如果发现 Eden 满了，会执行 Minor GC 回收新生代，把 Eden 和 From Survivor 存活的对象复制到 To Survivor 上（对象年龄+1），然后 To Survivor 和 From Survivor 交换身份。</li><li>默认情况下如果对象经过 15 次回收还存活，则放到老年代上。</li><li>当老年代内存不足时，会触发 Major GC 对老年代进行回收。</li></ol><h2 id="18-遇到过哪些-OOM-问题"><a href="#18-遇到过哪些-OOM-问题" class="headerlink" title="18 遇到过哪些 OOM 问题"></a>18 遇到过哪些 OOM 问题</h2><ul><li>内存泄漏memory leak :是指程序在申请内存后，无法释放已申请的内存空间，一次内存泄漏似乎不会有大的影响，但内存泄漏堆积后的后果就是内存溢出。 </li><li>内存溢出out of memory :指程序申请内存时，没有足够的内存供申请者使用，或者说，给了你一块存储int类型数据的存储空间，但是你却存储long类型的数据，那么结果就是内存不够用，此时就会报错OOM,即所谓的内存溢出。</li></ul><h3 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h3><ol><li><p>一般首先考虑内存泄露/内存溢出的情况，需要配置 JVM 参数输出 dump 文件。</p><p> -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/heapdump.hprof。</p><p> -XX:+HeapDumpOnOutOfMemoryError 设置当首次遭遇内存溢出时导出此时堆中相关信息。</p><p> -XX:HeapDumpPath=/tmp/heapdump.hprof 指定导出堆信息时的路径或文件名。</p></li><li><p>接着通过内存映像分析工具（如 JProfiler，MAT）对 dump 出来的堆转储快照进行分析，确认内存中的对象是否是必要的，也就是要先分清楚到底是出现了内存泄漏还是内存溢出。</p></li><li><p>如果是内存泄漏，可进一步通过工具查看泄漏对象到 GC Roots 的引用链。于是就能找到泄漏对象是通过怎样的路径与 GCRoots 相关联并导致垃圾收集器无法自动回收它们的。掌握了泄漏对象的类型信息，以及 GCRoots 引用链的信息，就可以比较准确地定位出泄漏代码的位置。</p></li><li><p>如果不存在内存泄漏，换句话说就是内存中的对象确实都还必须存活着，那就应当检查虚拟机的堆参数（-Xmx与-Xms），与机器物理内存对比看是否还可以调大，从代码上检查是否存在某些对象生命周期过长、持有状态时间过长的情况，尝试减少程序运行期的内存消耗。</p></li></ol><h3 id="溢出场景"><a href="#溢出场景" class="headerlink" title="溢出场景"></a>溢出场景</h3><h4 id="内存溢出"><a href="#内存溢出" class="headerlink" title="内存溢出"></a>内存溢出</h4><p>如突发高峰期时，内存存活对象使用空间的量超出最大堆，并且无法回收。</p><ul><li>小公司没这个可能，可以说是某个时间段导出特大报表导致。</li><li>想下项目有哪些垃圾代码使用了大内存耗时操作，且长时间不能被回收，什么全表查数据计算，循环做 String 拼接操作等等。</li></ul><h4 id="内存泄漏"><a href="#内存泄漏" class="headerlink" title="内存泄漏"></a>内存泄漏</h4><p>重新不在使用的对象仍然存在引用导致无法回收，随着时间推移，泄漏内存对象占用了所有可用堆。</p><ul><li>比如 dubbo 泛化调用没缓存 ReferenceConfig 实例导致了内存泄漏。</li></ul><h3 id="OOM-原因"><a href="#OOM-原因" class="headerlink" title="OOM 原因"></a>OOM 原因</h3><h4 id="Java-heap-space（应用尝试从堆申请一个区域时，堆没有可分配的空间）"><a href="#Java-heap-space（应用尝试从堆申请一个区域时，堆没有可分配的空间）" class="headerlink" title="Java heap space（应用尝试从堆申请一个区域时，堆没有可分配的空间）"></a>Java heap space（应用尝试从堆申请一个区域时，堆没有可分配的空间）</h4><p>原因：内存泄漏、内存溢出</p><h4 id="GC-overhead-limit-exceeded（超过-98-的时间在-GC，且回收了不到-2-的堆内存）"><a href="#GC-overhead-limit-exceeded（超过-98-的时间在-GC，且回收了不到-2-的堆内存）" class="headerlink" title="GC overhead limit exceeded（超过 98% 的时间在 GC，且回收了不到 2% 的堆内存）"></a>GC overhead limit exceeded（超过 98% 的时间在 GC，且回收了不到 2% 的堆内存）</h4><p>原因：内存泄漏、内存溢出</p><h4 id="unable-to-create-new-native-thread（无法创建新的本地线程）"><a href="#unable-to-create-new-native-thread（无法创建新的本地线程）" class="headerlink" title="unable to create new native thread（无法创建新的本地线程）"></a>unable to create new native thread（无法创建新的本地线程）</h4><ul><li><p>原因</p><ul><li>系统内存耗尽，无法为新线程分配内存</li><li>创建线程数超过了操作系统的限制</li></ul></li><li><p>解决</p><ul><li>排查应用线程数：通过 jstack 排查应用哪个地方创建了过多的线程。</li><li>调整操作系统线程数阈值：通过 ulimit -u 查看操作系统的最大进程数是否需要修改。</li><li>减小堆内存：JVM 是否占满了系统总内存，导致系统线程没有足够的内存分配，如果是这种，就考虑降低堆内存，否则就增加系统内存。</li></ul></li></ul><h4 id="Metaspace（元空间不足，加载到内存中的-class-数量太多或者体积太大）"><a href="#Metaspace（元空间不足，加载到内存中的-class-数量太多或者体积太大）" class="headerlink" title="Metaspace（元空间不足，加载到内存中的 class 数量太多或者体积太大）"></a>Metaspace（元空间不足，加载到内存中的 class 数量太多或者体积太大）</h4><p>解决：修改 -XX:MetaspaceSize 和 -XX:MaxMetaspaceSize。</p><h4 id="Direct-buffer-memory（直接内存不足，通常是-NIO-引起的）"><a href="#Direct-buffer-memory（直接内存不足，通常是-NIO-引起的）" class="headerlink" title="Direct buffer memory（直接内存不足，通常是 NIO 引起的）"></a>Direct buffer memory（直接内存不足，通常是 NIO 引起的）</h4><ul><li><p>原因</p><p>  ByteBuffer.allocteDirect(capability) 是直接分配 OS 本地内存，如何不断分配本地内存，堆内存很少使用，那么 JVM 就不需要执行 GC，DirectByteBuffer 就不会被回收，这时候堆内存充足，但本地内存很快就给耗光了。</p></li><li><p>解决</p><p>  加大 -XX:MaxDirectMemorySize。</p></li></ul><h4 id="Requested-array-size-exceeds-VM-limit（数组太大，超过了平台限制值）"><a href="#Requested-array-size-exceeds-VM-limit（数组太大，超过了平台限制值）" class="headerlink" title="Requested array size exceeds VM limit（数组太大，超过了平台限制值）"></a>Requested array size exceeds VM limit（数组太大，超过了平台限制值）</h4><p>解决：减小数组长度，拆分大数组等。</p><h4 id="Out-of-swap-space（交换空间空间不足，表明请求分配内存的操作失败了）"><a href="#Out-of-swap-space（交换空间空间不足，表明请求分配内存的操作失败了）" class="headerlink" title="Out of swap space（交换空间空间不足，表明请求分配内存的操作失败了）"></a>Out of swap space（交换空间空间不足，表明请求分配内存的操作失败了）</h4><p>解决：java 进程使用了虚拟内存才会发生，这时得增加虚拟内存的大小。</p><h3 id="堆是分配对象的唯一选择"><a href="#堆是分配对象的唯一选择" class="headerlink" title="堆是分配对象的唯一选择"></a>堆是分配对象的唯一选择</h3><p>不一定，在 JDK 1.7 版本之后，HotSpot 中默认就已经开启了逃逸分析。</p><p>如果经过逃逸分析后发现，一个对象没有逃逸出方法的话，就会被优化成栈上分配，以此达到降低 GC 的回收频率和提升 GC 的回收效率的目的。</p><p>逃逸分析的基本行为就是分析对象动态作用域：</p><ul><li>当一个对象在方法中被定义后，对象只在方法内部使用，则认为没有发生逃逸。</li><li>当一个对象在方法中被定义后，它被外部方法所引用，则认为发生逃逸。例如作为调用参数传递到其他地方中。</li></ul><h2 id="19-方法区与其他内存结构关系"><a href="#19-方法区与其他内存结构关系" class="headerlink" title="19 方法区与其他内存结构关系"></a>19 方法区与其他内存结构关系</h2><ul><li>Person：存放在元空间，也可以说方法区。</li><li>person：存放在Java栈的局部变量表中。</li><li>new Person()：存放在Java堆中。</li></ul><p><img src="/2020/01/16/2020-01-16-jvm/image-20200708094747667.png" alt></p><h2 id="20-垃圾回收相关"><a href="#20-垃圾回收相关" class="headerlink" title="20 垃圾回收相关"></a>20 垃圾回收相关</h2><h3 id="标记垃圾的算法"><a href="#标记垃圾的算法" class="headerlink" title="标记垃圾的算法"></a>标记垃圾的算法</h3><p>堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断哪些对象已经死亡（垃圾标记阶段）。</p><p>判断对象存活一般有两种方式：引用计数算法和可达性分析算法。</p><h4 id="引用计数法"><a href="#引用计数法" class="headerlink" title="引用计数法"></a>引用计数法</h4><p>给对象中添加一个引用计数器，每当有一个地方引用它，计数器就加 1；当引用失效，计数器就减 1；任何时候计数器为 0 的对象就是不可能再被使用的。</p><ul><li>优点：实现简单，垃圾对象便于辨识；判定效率高，回收没有延迟性。</li><li>缺点：无法解决循环引用问题</li></ul><p><img src="/2020/01/16/2020-01-16-jvm/image.png" alt></p><h4 id="可达性分析法"><a href="#可达性分析法" class="headerlink" title="可达性分析法"></a>可达性分析法</h4><p>这个算法的基本思想就是通过一系列的称为 “GC Roots” 的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的。</p><p><img src="/2020/01/16/2020-01-16-jvm/image1.png" alt></p><h2 id="21-并发的可达性分析（三色标记）"><a href="#21-并发的可达性分析（三色标记）" class="headerlink" title="21 并发的可达性分析（三色标记）"></a>21 并发的可达性分析（三色标记）</h2><p>首先需要引入三种颜色：</p><ul><li>白色：对象没被 GC 访问过。若分析结束还是白色的对象，则代表不可达。</li><li>黑色：对象已经被 GC 访问过，且这个对象的所有引用都扫描过了，他是安全存活的。</li><li>灰色：对象已经被 GC 访问过，但这个对象上还存在没有被扫描过的引用。</li></ul><p><img src="/2020/01/16/2020-01-16-jvm/image2.png" alt></p><p>并发标记阶段，会出现两种问题：</p><ul><li>把白色对象错标成了黑色对象（问题不大，只是产生了浮动垃圾，下次再清理）。</li><li>把黑色对象错标成白色对象（裂了啊，我的对象怎么不见了）。并发时，灰色对象的一个引用被切断，然后和已扫描过的黑色对象建立了引用关系。</li></ul><p>解决这个问题的方案有两个：增量更新和原始快照。</p><ul><li>增量更新（CMS）：当黑色对象和白色对象建立引用关系时，会记录下来黑色对象的引用，等到最终标记时，再将这些黑色对象作为根，再重新扫描。</li><li>原始快照（G1）：当灰色对象删除对白色对象的引用关系时，会记录下来白色对象的引用，再以这些白色对象作为根，再重新扫描。</li></ul><h2 id="22-垃圾收集算法"><a href="#22-垃圾收集算法" class="headerlink" title="22 垃圾收集算法"></a>22 垃圾收集算法</h2><p>注意，这些只是基本的算法思路，实际 GC 实现过程要复杂的多，目前还在发展中的前沿 GC 都是复合算法，并且并行和并发兼备。</p><h3 id="标记-复制算法（Copying）"><a href="#标记-复制算法（Copying）" class="headerlink" title="标记-复制算法（Copying）"></a>标记-复制算法（Copying）</h3><p>标记-复制算法会从根节点开始遍历，标记所有存活对象，同时将这些对象复制到一块新内存中，之后将原来那块内存全部回收掉。</p><p><img src="/2020/01/16/2020-01-16-jvm/v2-1ef719211b88b62dbbc09036ec7adf09_720w.jpg" alt></p><p>适用场合：</p><ul><li>存活对象较少的情况下比较高效（新生代）。</li><li>只扫描了整个空间一次（标记存活对象并复制移动）。</li></ul><p>优点：</p><ul><li>效率高</li><li>内存规整</li></ul><p>缺点：</p><ul><li>需要两倍的内存空间。</li><li>像老年代那种回收完还有大量的对象存活的区域，需要复制的对象将会有很多，效率会很低。</li><li>对于 G1 这种分拆成为大量 Region 的 GC，复制而不是移动，意味着 GC 需要维护 Region 之间对象引用关系，不管是内存占用或者时间开销也不小。</li></ul><h3 id="标记-清除算法（Mark-Sweep）"><a href="#标记-清除算法（Mark-Sweep）" class="headerlink" title="标记-清除算法（Mark-Sweep）"></a>标记-清除算法（Mark-Sweep）</h3><p>标记-清除算法将垃圾回收分为两个阶段：</p><ul><li>标记：从根节点开始遍历，标记所有被引用的对象。一般是在对象头中记录为可达对象。</li><li>清除：对堆内存从头到尾进行线性的遍历，如果发现某个对象在其对象头中没有标记为可达对象，则将其回收。</li></ul><p><img src="/2020/01/16/2020-01-16-jvm/v2-09e2d6d8a1bacee67fffc2f98151c184_720w.png" alt></p><p>适用场景：存活对象较多的情况下比较高效（老年代）。</p><p>优点：</p><ul><li>使用内存少</li></ul><p>缺点：</p><ul><li>效率低。扫描了整个空间两次。</li><li>会产生内存碎片，导致大对象无法在空闲列表找到可分配的空间。</li></ul><h3 id="标记-整理算法（Mark-Compact）"><a href="#标记-整理算法（Mark-Compact）" class="headerlink" title="标记-整理算法（Mark-Compact）"></a>标记-整理算法（Mark-Compact）</h3><p>标记-压缩算法是一种老年代的回收算法，它在标记-清除算法的基础上做了一些优化。</p><p>首先也需要从根节点开始对所有可达对象做一次标记，但之后，它并不简单地清理未标记的对象，而是将所有的存活对象压缩到内存的一端。之后，清理边界外所有的空间。这种方法既避免了碎片的产生，又不需要两块相同的内存空间，因此，其性价比比较高。</p><p><img src="/2020/01/16/2020-01-16-jvm/v2-59f55bce5abb3fc375f8e8eb9bc1a02e_720w.jpg" alt></p><p>优点：</p><ul><li>解决了标记-复制算法当中内存减半的代价。</li><li>解决了标记-清除算法当中的内存碎片问题。</li></ul><p>缺点：</p><ul><li>回收效率是最低的。</li></ul><h3 id="分代收集算法（Generational-Collection）"><a href="#分代收集算法（Generational-Collection）" class="headerlink" title="分代收集算法（Generational Collection）"></a>分代收集算法（Generational Collection）</h3><p>分代收集算法，是基于这样一个事实：不同的对象的生命周期是不一样的。因此，不同生命周期的对象可以采取不同的收集方式，以便提高回收效率。一般是把堆分为新生代和老年代，这样就可以根据各个年代的特点使用不同的回收算法，以提高垃圾回收的效率。</p><p>目前几乎所有的 GC 都采用分代收集算法执行垃圾回收的。</p><p>在 HotSpot 中，基于分代的概念，GC 所使用的内存回收算法必须结合年轻代和老年代各自的特点。</p><p><img src="/2020/01/16/2020-01-16-jvm/v2-0b59dcdadf331d2e2bb6e1d392fa23ae_720w.jpg" alt></p><ul><li><p>年轻代特点：区域相对老年代较小，对象生命周期短、存活率低，回收频繁。</p><p>  这种情况复制算法的回收整理，速度是最快的。复制算法的效率只和当前存活对象大小有关，因此很适用于年轻代的回收。而复制算法内存利用率不高的问题，通过 Hotspot 中的两个 Survivor 的设计得到缓解。</p></li><li><p>老年代特点：区域较大，对象生命周期长、存活率高，回收不及年轻代频繁。</p><p>  这种情况存在大量存活率高的对象，复制算法明显变得不合适。一般是由标记-清除或者是标记-清除与标记-整理的混合实现。</p></li></ul><h2 id="23-垃圾收集器"><a href="#23-垃圾收集器" class="headerlink" title="23 垃圾收集器"></a>23 垃圾收集器</h2><h3 id="有哪些垃圾收集器"><a href="#有哪些垃圾收集器" class="headerlink" title="有哪些垃圾收集器"></a>有哪些垃圾收集器</h3><ul><li>串行回收器：Serial、Serial Old。</li><li>并行回收器：ParNew、Parallel Scavenge、Parallel Old。</li><li>并发回收器：CMS、G1。</li></ul><p>垃圾收集器与分代之间的关系：</p><ul><li>新生代收集器：Serial、ParNew、Parallel Scavenge。</li><li>老年代收集器：Serial Old、Parallel Old、CMS。</li><li>整堆收集器：G1。</li></ul><p><img src="/2020/01/16/2020-01-16-jvm/image-20200713093757644.png" alt></p><p>垃圾收集器的组合关系：</p><p><img src="/2020/01/16/2020-01-16-jvm/image-20200713094745366.png" alt></p><ul><li><p>Serial + Serial Old，Serial + CMS</p></li><li><p>ParNew + Serial Old，ParNew + CMS</p></li><li><p>Parallel Scavenge + Serial Old，Parallel Scavenge + Parallel Old。</p></li><li><p>其中 Serial Old 还会作为 CMS 出现 “Concurrent Mode Failure” 失败的后备预案。</p></li><li><p>红色虚线：在 JDK 8 时将 Serial + CMS，ParNew + Serial Old 声明为废弃，并在 JDK9 中彻底取消了对这些组合的支持。</p></li><li><p>绿色虚线：在 JDK14 中，弃用 Parallel Scavenge + Serial Old 的组合。</p></li><li><p>青色虚线：在 JDK14 中，删除 CMS 垃圾收集器。</p></li></ul><h3 id="查看默认收集器"><a href="#查看默认收集器" class="headerlink" title="查看默认收集器"></a>查看默认收集器</h3><p>Java -XX:+PrintCommandLineFlags -version</p><p><img src="/2020/01/16/2020-01-16-jvm/image5.png" alt></p><h2 id="24-分析-GC-日志"><a href="#24-分析-GC-日志" class="headerlink" title="24 分析 GC 日志"></a>24 分析 GC 日志</h2><h3 id="Young-GC-参数图"><a href="#Young-GC-参数图" class="headerlink" title="Young GC 参数图"></a>Young GC 参数图</h3><p><img src="/2020/01/16/2020-01-16-jvm/image-20200714082555688.png" alt></p><h3 id="Full-GC-参数图"><a href="#Full-GC-参数图" class="headerlink" title="Full GC 参数图"></a>Full GC 参数图</h3><p><img src="/2020/01/16/2020-01-16-jvm/image-20200714082714690.png" alt></p><h2 id="25-JVM参数"><a href="#25-JVM参数" class="headerlink" title="25 JVM参数"></a>25 JVM参数</h2><h3 id="标准参数"><a href="#标准参数" class="headerlink" title="标准参数"></a>标准参数</h3><p>标准参数以-开头，比较稳定，后续版本基本不会变化。</p><p>在控制台输入java -help，可以看到所有的标准参数</p><h3 id="X参数"><a href="#X参数" class="headerlink" title="-X参数"></a>-X参数</h3><p>非标准化参数，功能比较稳定，后续版本可能会变更，以-X开头</p><p>在控制台输入java -X，可以看到相关参数</p><ul><li>-Xinit：禁用JIT，所有的字节码都被解释执行，速度最慢</li><li>-Xcomp，所有的字节码第一次就被编译，然后再执行</li><li>-Xmixed，混合模式，默认，让JIT根据程序运行情况，动态编译</li><li>-Xms，初始化java堆的大小，相当于-XX:InitialHeapSize，如-Xms2g</li><li>-Xmx，设置最大java堆大小，相当于-XX:MaxHeapSize</li><li>-Xss，设置java线程栈的大小，相当于-XX:ThreadStackSize</li></ul><h3 id="XX参数"><a href="#XX参数" class="headerlink" title="-XX参数"></a>-XX参数</h3><p>使用最多的参数类型，比较不稳定，以-XX开头</p><p>-XX参数分为Boolean类型和非Boolean类型</p><h4 id="Boolean类型"><a href="#Boolean类型" class="headerlink" title="Boolean类型"></a>Boolean类型</h4><p>-XX:+，表示开启option</p><p>-XX:-，表示关闭option，有些参数是默认开启的，可以通过-进行关闭</p><p>如，-XX:+UseParallelGC，-XX:+UseAdaptiveSizePolicy</p><h4 id="非Boolean"><a href="#非Boolean" class="headerlink" title="非Boolean"></a>非Boolean</h4><p>非Boolean，分为数值类型和非数值类型，如果是数值类型，就进行赋值</p><p>-XX:=<option>=<string></string></option></p><p>如，-XX:NewRatio=2，-XX:NewSize=2M</p><p>如果是非数值类型，则填写对应的值</p><p>-XX:=<option>=<string></string></option></p><p>如，-XX:HeapDumpPath=/xxx/xxx/duml.hprof</p><h4 id="PrintFlagsFinal"><a href="#PrintFlagsFinal" class="headerlink" title="PrintFlagsFinal"></a>PrintFlagsFinal</h4><p>-XX:+PrintFlagsFinal，输出所有参数的名称和默认值，默认不包括诊断和实验的参数</p><h2 id="大厂面试题"><a href="#大厂面试题" class="headerlink" title="大厂面试题"></a>大厂面试题</h2><h3 id="阿里"><a href="#阿里" class="headerlink" title="阿里"></a>阿里</h3><ul><li>你知道哪几种垃圾回收器，各自的优缺点，重点讲一下 CMS 和 G1？</li><li>JVM GC 算法有哪些，目前的 JDK 版本采用什么回收算法？可以说用的 JDK8，Parallel Scavenge（复制） + Parallel Old（标记-整理）。</li><li>G1 回收器讲下回收过程？GC 是什么？为什么要有 GC？</li><li>CMS 收集器与 G1 收集器的特点？</li><li>CMS 特点，垃圾回收算法有哪些？各自的优缺点，他们共同的缺点是什么？</li><li>JDK8 的内存分代改进？JVM 内存分哪几个区，每个区的作用是什么？</li><li>JVM 内存分布/内存结构？</li><li>栈和堆的区别？</li><li>堆的结构？</li><li>为什么两个 Survivor 区？ </li><li>Eden 和 Survior 的比例分配？</li><li>讲一讲垃圾回收算法。</li><li>什么情况下触发垃圾回收？</li><li>如何选择合适的垃圾收集算法？</li><li>JVM有哪三种垃圾回收器？</li></ul><h3 id="字节"><a href="#字节" class="headerlink" title="字节"></a>字节</h3><ul><li>常见的垃圾回收器算法有哪些，各有什么优劣？</li><li>System.gc() 和 Runtime.gc() 会做什么事情？</li><li>Java GC 机制？GC Roots 有哪些？</li><li>Java对象的回收方式，回收算法。</li><li>CMS 和 G1 了解么，CMS 解决什么问题，说一下回收的过程。</li><li>CMS 回收停顿了几次，为什么要停顿两次？</li><li>什么时候对象会进入老年代？</li></ul><h3 id="百度"><a href="#百度" class="headerlink" title="百度"></a>百度</h3><ul><li>说一下 JVM 内存模型吧，有哪些区？分别干什么的？</li><li>说一下 GC 算法，分代回收说下。</li></ul><h3 id="京东"><a href="#京东" class="headerlink" title="京东"></a>京东</h3><ul><li>你知道哪几种垃圾收集器，各自的优缺点，重点讲下 CMS 和 G1。 </li></ul><h3 id="小米"><a href="#小米" class="headerlink" title="小米"></a>小米</h3><ul><li>JVM 内存分区，为什么要有新生代和老年代？</li></ul><h3 id="美团"><a href="#美团" class="headerlink" title="美团"></a>美团</h3><ul><li>JVM 的永久代中会发生垃圾回收吗？</li></ul><h3 id="滴滴"><a href="#滴滴" class="headerlink" title="滴滴"></a>滴滴</h3><ul><li>Java 的垃圾回收器都有哪些，说下 G1 的应用场景，平时你是如何搭配使用垃圾回收器的？</li></ul>]]></content>
      
      
      <categories>
          
          <category> JAVA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java多线程</title>
      <link href="/2020/01/15/2020-01-15-java-duo-xian-cheng/"/>
      <url>/2020/01/15/2020-01-15-java-duo-xian-cheng/</url>
      
        <content type="html"><![CDATA[<h1 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h1><h2 id="1-什么是线程和进程"><a href="#1-什么是线程和进程" class="headerlink" title="1 什么是线程和进程?"></a>1 什么是线程和进程?</h2><ol><li><p>何为进程?</p><p> 进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。</p><p> 在 Java 中，当我们启动 main 函数时其实就是启动了一个 JVM 的进程，而 main 函数所在的线程就是这个进程中的一个线程，也称主线程。</p></li><li><p>何为线程?</p><p> 线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享进程的堆和方法区资源，但每个线程有自己的程序计数器、虚拟机栈和本地方法栈，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。</p></li><li><p>个人话术：</p><p> 进程是系统运行程序的基本单位，在JAVA中，每当我们执行一个main方法时，也就是启动了一个jvm进程。一个进程在执行的过程中可以产生多个线程，线程共享jvm的堆和方法区资源，但每个线程都有自己的虚拟机栈、本地方法栈和程序计数器。</p></li></ol><h2 id="2-请简要描述线程与进程的关系-区别及优缺点？"><a href="#2-请简要描述线程与进程的关系-区别及优缺点？" class="headerlink" title="2 请简要描述线程与进程的关系,区别及优缺点？"></a>2 请简要描述线程与进程的关系,区别及优缺点？</h2><ul><li><p>图解进程和线程的关系</p><p>  <img src="/2020/01/15/2020-01-15-java-duo-xian-cheng/9r0sp8po.bmp" alt></p><p>  <strong>一个进程中可以有多个线程，多个线程共享进程的堆和方法区 (JDK1.8 之后的元空间)资源，但是每个线程有自己的程序计数器、虚拟机栈 和 本地方法栈。</strong></p><p>  总结： 线程 是 进程 划分成的更小的运行单位。线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。线程执行开销小，但不利于资源的管理和保护；而进程正相反</p></li><li><p>程序计数器为什么是私有的?</p><p>  程序计数器私有主要是为了线程切换后能恢复到正确的执行位置。</p></li><li><p>虚拟机栈和本地方法栈为什么是私有的?</p><p>  为了保证线程中的局部变量不被别的线程访问到，虚拟机栈和本地方法栈是线程私有的。</p></li></ul><h2 id="3-说说并发与并行的区别"><a href="#3-说说并发与并行的区别" class="headerlink" title="3 说说并发与并行的区别?"></a>3 说说并发与并行的区别?</h2><ul><li>并发： 同一时间段，多个任务都在执行 (单位时间内不一定同时执行)；</li><li>并行： 单位时间内，多个任务同时执行。</li></ul><h2 id="4-为什么要使用多线程呢"><a href="#4-为什么要使用多线程呢" class="headerlink" title="4 为什么要使用多线程呢?"></a>4 为什么要使用多线程呢?</h2><ul><li>减少了线程上下文切换的开销。</li><li>可以大大提高系统整体的并发能力以及性能。</li><li>提高 CPU 利用率。</li></ul><h2 id="5-使用多线程可能带来什么问题"><a href="#5-使用多线程可能带来什么问题" class="headerlink" title="5 使用多线程可能带来什么问题?"></a>5 使用多线程可能带来什么问题?</h2><ul><li>并发编程的目的就是为了能提高程序的执行效率提高程序运行速度，但是并发编程并不总是能提高程序运行速度的，而且并发编程可能会遇到很多问题，比如：内存泄漏、上下文切换、死锁还有受限于硬件和软件的资源闲置问题。</li></ul><h2 id="6-说说线程的生命周期和状态"><a href="#6-说说线程的生命周期和状态" class="headerlink" title="6 说说线程的生命周期和状态?"></a>6 说说线程的生命周期和状态?</h2><p><img src="/2020/01/15/2020-01-15-java-duo-xian-cheng/cw5izr3c.bmp" alt></p><p><img src="/2020/01/15/2020-01-15-java-duo-xian-cheng/degboi6r.bmp" alt></p><ul><li>线程创建之后它将处于 NEW（新建） 状态，调用 start() 方法后开始运行，线程这时候处于 READY（可运行） 状态。可运行状态的线程获得了 CPU 时间片（timeslice）后就处于 RUNNING（运行） 状态。</li><li>当线程执行 wait()方法之后，线程进入 WAITING（等待） 状态。进入等待状态的线程需要依靠其他线程的通知才能够返回到运行状态，而 TIME_WAITING(超时等待) 状态相当于在等待状态的基础上增加了超时限制，比如通过 sleep（long millis）方法或 wait（long millis）方法可以将 Java 线程置于 TIMED WAITING 状态。当超时时间到达后 Java 线程将会返回到 RUNNABLE 状态。当线程调用同步方法时，在没有获取到锁的情况下，线程将会进入到 BLOCKED（阻塞） 状态。线程在执行 Runnable 的run()方法之后将会进入到 TERMINATED（终止） 状态。</li></ul><h2 id="7-什么是上下文切换"><a href="#7-什么是上下文切换" class="headerlink" title="7 什么是上下文切换?"></a>7 什么是上下文切换?</h2><ul><li>当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。<strong>任务从保存到再加载的过程就是一次上下文切换。</strong></li></ul><h2 id="8-什么是线程死锁-如何避免死锁"><a href="#8-什么是线程死锁-如何避免死锁" class="headerlink" title="8 什么是线程死锁?如何避免死锁?"></a>8 什么是线程死锁?如何避免死锁?</h2><h2 id="9-说说-sleep-方法和-wait-方法区别和共同点"><a href="#9-说说-sleep-方法和-wait-方法区别和共同点" class="headerlink" title="9 说说 sleep() 方法和 wait() 方法区别和共同点?"></a>9 说说 sleep() 方法和 wait() 方法区别和共同点?</h2><ul><li><strong>两者最主要的区别在于：sleep 方法没有释放锁，而 wait 方法释放了锁 。</strong></li><li>两者都可以暂停线程的执行。</li><li>Wait 通常被用于线程间交互/通信，sleep 通常被用于暂停执行。</li><li><strong>wait() 方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的 notify() 或者 notifyAll() 方法。sleep() 方法执行完成后，线程会自动苏醒。或者可以使用 wait(long timeout)超时后线程会自动苏醒。</strong></li></ul><h2 id="10-为什么我们调用-start-方法时会执行-run-方法，为什么我们不能直接调用-run-方法？"><a href="#10-为什么我们调用-start-方法时会执行-run-方法，为什么我们不能直接调用-run-方法？" class="headerlink" title="10 为什么我们调用 start() 方法时会执行 run() 方法，为什么我们不能直接调用 run() 方法？"></a>10 为什么我们调用 start() 方法时会执行 run() 方法，为什么我们不能直接调用 run() 方法？</h2><ul><li>new 一个 Thread，线程进入了新建状态;调用 start() 方法，会启动一个线程并使线程进入了就绪状态，当分配到时间片后就可以开始运行了。 start() 会执行线程的相应准备工作，然后自动执行 run() 方法的内容，这是真正的多线程工作。 而直接执行 run() 方法，会把 run 方法当成一个 main 线程下的普通方法去执行，并不会在某个线程中执行它，所以这并不是多线程工作。</li></ul><h2 id="11-synchronized-关键字"><a href="#11-synchronized-关键字" class="headerlink" title="11 synchronized 关键字"></a>11 synchronized 关键字</h2><ol><li><p>说一说自己对于 synchronized 关键字的了解</p><p> synchronized关键字解决的是多个线程之间访问资源的同步性，synchronized关键字可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。（可以谈下synchronize首先获取volatile，最后写volatile这些）</p></li><li><p>说说自己是怎么使用 synchronized 关键字，在项目中用到了吗</p></li><li><p>synchronized关键字最主要的三种使用方式</p></li></ol><ul><li>修饰实例方法: 作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁</li><li>修饰静态方法: 也就是给当前类加锁，会作用于类的所有对象实例，因为静态成员不属于任何一个实例对象，是类成员（ static 表明这是该类的一个静态资源，不管new了多少个对象，只有一份）。所以如果一个线程A调用一个实例对象的非静态 synchronized 方法，而线程B需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。</li><li>修饰代码块: 指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。</li></ul><ol start="4"><li>讲一下 synchronized 关键字的底层原理</li></ol><ul><li><p>① synchronized 同步语句块的情况</p><p>  synchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指明同步代码块的结束位置。 当执行 monitorenter 指令时，线程试图获取锁也就是获取 monitor(monitor对象存在于每个Java对象的对象头中，synchronized 锁便是通过这种方式获取锁的，也是为什么Java中任意对象可以作为锁的原因) 的持有权。当计数器为0则可以成功获取，获取后将锁计数器设为1也就是加1。相应的在执行 monitorexit 指令后，将锁计数器设为0，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。</p></li><li><p>② synchronized 修饰方法的的情况</p><p>  synchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法，JVM 通过该 ACC_SYNCHRONIZED 访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。</p></li></ul><ol start="5"><li>说说 JDK1.6 之后的synchronized 关键字底层做了哪些优化，可以详细介绍一下这些优化吗</li></ol><ul><li>锁主要存在四种状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈而逐渐升级。注意锁可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率。</li></ul><ol start="6"><li>谈谈 synchronized和ReentrantLock 的区别</li></ol><ul><li><p>① 两者都是可重入锁</p><p>  两者都是可重入锁。“可重入锁”概念是：自己可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果不可锁重入的话，就会造成死锁。同一个线程每次获取锁，锁的计数器都自增1，所以要等到锁的计数器下降为0时才能释放锁。</p></li><li><p>② synchronized 依赖于 JVM 而 ReentrantLock 依赖于 API</p></li><li><p>③ ReentrantLock 比 synchronized 增加了一些高级功能</p><p>  相比synchronized，ReentrantLock增加了一些高级功能。主要来说主要有三点：①等待可中断；②可实现公平锁；③可实现选择性通知（锁可以绑定多个条件）</p></li></ul><h2 id="12-volatile关键字"><a href="#12-volatile关键字" class="headerlink" title="12 volatile关键字"></a>12 volatile关键字</h2><ul><li><p>1 讲一下Java内存模型</p><p>  在 JDK1.2 之前，Java的内存模型实现总是从主存（即共享内存）读取变量，是不需要进行特别的注意的。而在当前的 Java 内存模型下，线程可以把变量保存本地内存（比如机器的寄存器）中，而不是直接在主存中进行读写。这就可能造成一个线程在主存中修改了一个变量的值，而另外一个线程还继续使用它在寄存器中的变量值的拷贝，造成数据的不一致。</p></li><li><p>2 并发编程的三个重要特性</p><ol><li>原子性 : 一个的操作或者多次操作，要么所有的操作全部都得到执行并且不会收到任何因素的干扰而中断，要么所有的操作都执行，要么都不执行。<strong>synchronized 可以保证代码片段的原子性。</strong></li><li>可见性 ：当一个变量对共享变量进行了修改，那么另外的线程都是立即可以看到修改后的最新值。<strong>volatile 关键字可以保证共享变量的可见性。</strong></li><li>有序性 ：代码在执行的过程中的先后顺序，Java 在编译器以及运行期间的优化，代码的执行顺序未必就是编写代码时候的顺序。<strong>volatile 关键字可以禁止指令进行重排序优化（内存屏障）。</strong></li></ol></li><li><p>3 说说 synchronized 关键字和 volatile 关键字的区别</p><ul><li>volatile关键字是线程同步的轻量级实现，所以volatile性能肯定比synchronized关键字要好。</li><li>多线程访问volatile关键字不会发生阻塞，而synchronized关键字可能会发生阻塞</li><li>volatile关键字能保证数据的可见性，但不能保证数据的原子性。synchronized关键字两者都能保证。</li><li>volatile关键字主要用于解决变量在多个线程之间的可见性，而 synchronized关键字解决的是多个线程之间访问资源的同步性。</li></ul></li></ul><h2 id="13-ThreadLocal"><a href="#13-ThreadLocal" class="headerlink" title="13 ThreadLocal"></a>13 ThreadLocal</h2><ol><li><p>ThreadLocal简介</p><ul><li>ThreadLocal类主要解决的就是让每个线程绑定自己的值，可以将ThreadLocal类形象的比喻成存放数据的盒子，盒子中可以存储每个线程的私有数据。<ul><li>如果你创建了一个ThreadLocal变量，那么访问这个变量的每个线程都会有这个变量的本地副本，这也是ThreadLocal变量名的由来。他们可以使用 get（） 和 set（） 方法来获取默认值或将其值更改为当前线程所存的副本的值，从而避免了线程安全问题。</li></ul></li></ul></li><li><p>ThreadLocal原理</p><p> 最终的变量是放在了当前线程的 ThreadLocalMap 中，并不是存在 ThreadLocal 上，ThreadLocal 可以理解为只是ThreadLocalMap的封装，传递了变量值。 ThrealLocal 类中可以通过Thread.currentThread()获取到当前线程对象后，直接通过getMap(Thread t)可以访问到该线程的ThreadLocalMap对象。</p></li><li><p>ThreadLocal 内存泄露问题</p><p> ThreadLocalMap 中使用的 key 为 ThreadLocal 的弱引用,而 value 是强引用。所以，如果 ThreadLocal 没有被外部强引用的情况下，在垃圾回收的时候，key 会被清理掉，而 value 不会被清理掉。这样一来，ThreadLocalMap 中就会出现key为null的Entry。假如我们不做任何措施的话，value 永远无法被GC 回收，这个时候就可能会产生内存泄露。ThreadLocalMap实现中已经考虑了这种情况，在调用 set()、get()、remove() 方法的时候，会清理掉 key 为 null 的记录。使用完 ThreadLocal方法后 最好手动调用remove()方法</p></li></ol><h2 id="14-线程池"><a href="#14-线程池" class="headerlink" title="14 线程池"></a>14 线程池</h2><ol><li><p>为什么要用线程池？</p><ul><li>降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。</li><li>提高响应速度。当任务到达时，任务可以不需要的等到线程创建就能立即执行。</li><li>提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。</li></ul></li><li><p>实现Runnable接口和Callable接口的区别</p><p> Runnable 接口不会返回结果或抛出检查异常，但是Callable 接口可以。</p></li><li><p>执行execute()方法和submit()方法的区别是什么呢？</p><ul><li>execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功与否；</li><li>submit()方法用于提交需要返回值的任务。线程池会返回一个 Future 类型的对象，通过这个 Future 对象可以判断任务是否执行成功，并且可以通过 Future 的 get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用 get（long timeout，TimeUnit unit）方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。</li></ul></li><li><p>如何创建线程池</p><ul><li><p>方式一：通过构造方法实现</p><p><img src="/2020/01/15/2020-01-15-java-duo-xian-cheng/9lnmoa38.bmp" alt></p></li></ul></li></ol><ul><li><p>二：通过Executor 框架的工具类Executors来实现 我们可以创建三种类型的ThreadPoolExecutor：</p><ul><li><p>FixedThreadPool ： 该方法返回一个固定线程数量的线程池。该线程池中的线程数量始终不变。当有一个新的任务提交时，线程池中若有空闲线程，则立即执行。若没有，则新的任务会被暂存在一个任务队列中，待有线程空闲时，便处理在任务队列中的任务。</p></li><li><p>SingleThreadExecutor： 方法返回一个只有一个线程的线程池。若多余一个任务被提交到该线程池，任务会被保存在一个任务队列中，待线程空闲，按先入先出的顺序执行队列中的任务。</p></li><li><p>CachedThreadPool： 该方法返回一个可根据实际情况调整线程数量的线程池。线程池的线程数量不确定，但若有空闲线程可以复用，则会优先使用可复用的线程。若所有线程均在工作，又有新的任务提交，则会创建新的线程处理任务。所有线程在当前任务执行完毕后，将返回线程池进行复用。</p><p><img src="/2020/01/15/2020-01-15-java-duo-xian-cheng/nub2xrat.bmp" alt></p></li></ul></li></ul><ol start="5"><li>ThreadPoolExecutor 类分析</li></ol><pre><code>    /**     * 用给定的初始参数创建一个新的ThreadPoolExecutor。     */    public ThreadPoolExecutor(int corePoolSize,                              int maximumPoolSize,                              long keepAliveTime,                              TimeUnit unit,                              BlockingQueue&lt;Runnable&gt; workQueue,                              ThreadFactory threadFactory,                              RejectedExecutionHandler handler) {        if (corePoolSize &lt; 0 ||            maximumPoolSize &lt;= 0 ||            maximumPoolSize &lt; corePoolSize ||            keepAliveTime &lt; 0)            throw new IllegalArgumentException();        if (workQueue == null || threadFactory == null || handler == null)            throw new NullPointerException();        this.corePoolSize = corePoolSize;        this.maximumPoolSize = maximumPoolSize;        this.workQueue = workQueue;        this.keepAliveTime = unit.toNanos(keepAliveTime);        this.threadFactory = threadFactory;        this.handler = handler;    }</code></pre><ul><li><p>ThreadPoolExecutor 3 个最重要的参数：</p><ul><li><p>corePoolSize : 核心线程数线程数定义了最小可以同时运行的线程数量。</p></li><li><p>maximumPoolSize : 当队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。</p></li><li><p>workQueue: 当新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中。</p><ul><li>ThreadPoolExecutor其他常见参数:</li></ul></li><li><p>keepAliveTime:当线程池中的线程数量大于 corePoolSize 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 keepAliveTime才会被回收销毁；</p></li><li><p>unit : keepAliveTime 参数的时间单位。</p></li><li><p>threadFactory :executor 创建新线程的时候会用到。</p></li><li><p>handler :饱和策略。</p><ul><li>ThreadPoolExecutor 饱和策略</li></ul><p>如果当前同时运行的线程数量达到最大线程数量并且队列也已经被放满了任时，ThreadPoolTaskExecutor 定义一些策略:</p></li><li><p>ThreadPoolExecutor.AbortPolicy：抛出 RejectedExecutionException来拒绝新任务的处理。<strong>(默认使用)</strong></p></li><li><p>ThreadPoolExecutor.CallerRunsPolicy：调用执行自己的线程运行任务。您不会任务请求。但是这种策略会降低对于新任务提交速度，影响程序的整体性能。另外，这个策略喜欢增加队列容量。如果您的应用程序可以承受此延迟并且你不能任务丢弃任何一个任务请求的话，你可以选择这个策略。</p></li><li><p>ThreadPoolExecutor.DiscardPolicy： 不处理新任务，直接丢弃掉。</p></li><li><p>ThreadPoolExecutor.DiscardOldestPolicy： 此策略将丢弃最早的未处理的任务请求。</p><ol start="7"><li>线程池原理分析</li></ol><p><img src="/2020/01/15/2020-01-15-java-duo-xian-cheng/60xumphz.bmp" alt></p></li></ul></li></ul><h2 id="15-Atomic-原子类"><a href="#15-Atomic-原子类" class="headerlink" title="15 Atomic 原子类"></a>15 Atomic 原子类</h2><ol><li><p>Atomic 是指一个操作是不可中断的。即使是在多个线程一起执行的时候，一个操作一旦开始，就不会被其他线程干扰。</p></li><li><p>JUC 包中的原子类是哪4类?</p><ul><li>基本类型<ul><li>AtomicInteger：整形原子类</li><li>AtomicLong：长整型原子类</li><li>AtomicBoolean：布尔型原子类</li></ul></li><li>数组类型<ul><li>AtomicIntegerArray：整形数组原子类</li><li>AtomicLongArray：长整形数组原子类</li><li>AtomicReferenceArray：引用类型数组原子类</li></ul></li><li>引用类型<ul><li>AtomicReference：引用类型原子类</li><li>AtomicStampedReference：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于解决原子的更新数据和数据的版本号，可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。</li><li>AtomicMarkableReference ：原子更新带有标记位的引用类型</li></ul></li></ul></li><li><p>讲讲 AtomicInteger 的使用</p><pre><code> public final int get() //获取当前的值 public final int getAndSet(int newValue)//获取当前的值，并设置新的值 public final int getAndIncrement()//获取当前的值，并自增 public final int getAndDecrement() //获取当前的值，并自减 public final int getAndAdd(int delta) //获取当前的值，并加上预期的值 boolean compareAndSet(int expect, int update) //如果输入的数值等于预期值，则以原子方式将该值设置为输入值（update） public final void lazySet(int newValue)//最终设置为newValue,使用 lazySet 设置之后可能导致其他线程在之后的一小段时间内还是可以读到旧的值。</code></pre></li><li><p>能不能给我简单介绍一下 AtomicInteger 类的原理</p><ul><li>AtomicInteger 类主要利用 CAS (compare and swap) + volatile 和 native 方法来保证原子操作，从而避免 synchronized 的高开销，执行效率大为提升。</li><li>CAS的原理是拿期望的值和原本的一个值作比较，如果相同则更新成新的值。UnSafe 类的 objectFieldOffset() 方法是一个本地方法，这个方法是用来拿到“原来的值”的内存地址，返回值是 valueOffset。另外 value 是一个volatile变量，在内存中可见，因此 JVM 可以保证任何时刻任何线程总能拿到该变量的最新值。</li></ul></li></ol><h2 id="16-AQS"><a href="#16-AQS" class="headerlink" title="16 AQS"></a>16 AQS</h2><ol><li><p>AQS 介绍</p><p> AQS的全称为（AbstractQueuedSynchronizer），这个类在java.util.concurrent.locks包下面。</p><p> AQS是一个用来构建锁和同步器的框架，使用AQS能简单且高效地构造出应用广泛的大量的同步器，比如我们提到的ReentrantLock，Semaphore，其他的诸如ReentrantReadWriteLock，SynchronousQueue，FutureTask等等皆是基于AQS的。</p></li><li><p>AQS 原理分析</p><p> AQS核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制AQS是用CLH队列锁实现的，即将暂时获取不到锁的线程加入到队列中。</p><p> <img src="/2020/01/15/2020-01-15-java-duo-xian-cheng/lsr55cxg.bmp" alt></p><p> AQS使用一个int成员变量来表示同步状态，通过内置的FIFO队列来完成获取资源线程的排队工作。AQS使用CAS对该同步状态进行原子操作实现对其值的修改。</p><pre><code> private volatile int state;//共享变量，使用volatile修饰保证线程可见性</code></pre><ul><li><p>AQS定义两种资源共享方式</p><ul><li>Exclusive（独占）：只有一个线程能执行，如ReentrantLock。又可分为公平锁和非公平锁：<ul><li>公平锁：按照线程在队列中的排队顺序，先到者先拿到锁</li><li>非公平锁：当线程要获取锁时，无视队列顺序直接去抢锁，谁抢到就是谁的</li></ul></li><li>Share（共享）：多个线程可同时执行，如Semaphore/CountDownLatch。Semaphore、CountDownLatch、 CyclicBarrier、ReadWriteLock 我们都会在后面讲到。</li></ul><p>以ReentrantLock为例，state初始化为0，表示未锁定状态。A线程lock()时，会调用tryAcquire()独占该锁并将state+1。此后，其他线程再tryAcquire()时就会失败，直到A线程unlock()到state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A线程自己是可以重复获取此锁的（state会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证state是能回到零态的</p><p>再以CountDownLatch以例，任务分为N个子线程去执行，state也初始化为N（注意N要与线程个数一致）。这N个子线程是并行执行的，每个子线程执行完后countDown()一次，state会CAS(Compare and Swap)减1。等到所有子线程都执行完后(即state=0)，会unpark()主调用线程，然后主调用线程就会从await()函数返回，继续后余动作。</p></li></ul></li><li><p>AQS 组件总结</p><ul><li>Semaphore(信号量)-允许多个线程同时访问： synchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，Semaphore(信号量)可以指定多个线程同时访问某个资源。</li><li>CountDownLatch （倒计时器）： CountDownLatch是一个同步工具类，用来协调多个线程之间的同步。这个工具通常用来控制线程等待，它可以让某一个线程等待直到倒计时结束，再开始执行。</li><li>CyclicBarrier(循环栅栏)： CyclicBarrier 和 CountDownLatch 非常类似，它也可以实现线程间的技术等待，但是它的功能比 CountDownLatch 更加复杂和强大。主要应用场景和 CountDownLatch 类似。CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。CyclicBarrier默认的构造方法是 CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用await()方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞。</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> JAVA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java多线程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA集合</title>
      <link href="/2020/01/14/2020-01-14-java-ji-he/"/>
      <url>/2020/01/14/2020-01-14-java-ji-he/</url>
      
        <content type="html"><![CDATA[<h1 id="Java集合"><a href="#Java集合" class="headerlink" title="Java集合"></a>Java集合</h1><h2 id="1-说说List-Set-Map三者的区别？"><a href="#1-说说List-Set-Map三者的区别？" class="headerlink" title="1 说说List,Set,Map三者的区别？"></a>1 说说List,Set,Map三者的区别？</h2><ul><li>List(对付顺序的好帮手)： List接口存储一组不唯一（可以有多个元素引用相同的对象），有序的对象</li><li>Set(注重独一无二的性质): 不允许重复的集合。不会有多个元素引用相同的对象。</li><li>Map(用Key来搜索的专家): 使用键值对存储。Map会维护与Key有关联的值。两个Key可以引用相同的对象，但Key不能重复，典型的Key是String类型，但也可以是任何对象。</li></ul><h2 id="2-Arraylist-与-LinkedList-区别"><a href="#2-Arraylist-与-LinkedList-区别" class="headerlink" title="2 Arraylist 与 LinkedList 区别?"></a>2 Arraylist 与 LinkedList 区别?</h2><ul><li>是否保证线程安全： ArrayList 和 LinkedList 都是不同步的，也就是不保证线程安全；</li><li>底层数据结构： Arraylist 底层使用的是 Object 数组；LinkedList 底层使用的是 双向链表 数据结构（JDK1.6之前为循环链表，JDK1.7取消了循环。）</li><li>插入和删除是否受元素位置的影响： ① ArrayList 采用数组存储，所以插入和删除元素的时间复杂度受元素位置的影响。 比如：执行add(E e)方法的时候， ArrayList 会默认在将指定的元素追加到此列表的末尾，这种情况时间复杂度就是O(1)。但是如果要在指定位置 i 插入和删除元素的话（add(int index, E element)）时间复杂度就为 O(n-i)。因为在进行上述操作的时候集合中第 i 和第 i 个元素之后的(n-i)个元素都要执行向后位/向前移一位的操作。 ② LinkedList 采用链表存储，所以对于add(E e)方法的插入，删除元素时间复杂度不受元素位置的影响，近似 O（1），如果是要在指定位置i插入和删除元素的话（(add(int index, E element)） 时间复杂度近似为o(n))因为需要先移动到指定位置再插入。</li><li>是否支持快速随机访问： LinkedList 不支持高效的随机元素访问，而 ArrayList 支持。快速随机访问就是通过元素的序号快速获取元素对象(对应于get(int index)方法)。</li><li>内存空间占用： ArrayList的空 间浪费主要体现在在list列表的结尾会预留一定的容量空间，而LinkedList的空间花费则体现在它的每一个元素都需要消耗比ArrayList更多的空间（因为要存放直接后继和直接前驱以及数据）。</li></ul><p>双向链表和双向循环链表：</p><ul><li>双向链表： 包含两个指针，一个prev指向前一个节点，一个next指向后一个节点。</li></ul><p><img src="/2020/01/14/2020-01-14-java-ji-he/pvorp49b.bmp" alt></p><ul><li>双向循环链表： 最后一个节点的 next 指向head，而 head 的prev指向最后一个节点，构成一个环。</li></ul><p><img src="/2020/01/14/2020-01-14-java-ji-he/6vijr69x.bmp" alt></p><h2 id="3-ArrayList-与-Vector-区别呢-为什么要用Arraylist取代Vector呢？"><a href="#3-ArrayList-与-Vector-区别呢-为什么要用Arraylist取代Vector呢？" class="headerlink" title="3 ArrayList 与 Vector 区别呢?为什么要用Arraylist取代Vector呢？"></a>3 ArrayList 与 Vector 区别呢?为什么要用Arraylist取代Vector呢？</h2><ul><li>Vector类的所有方法都是同步的。可以由两个线程安全地访问一个Vector对象、但是一个线程访问Vector的话代码要在同步操作上耗费大量的时间。</li><li>Arraylist不是同步的，所以在不需要保证线程安全时建议使用Arraylist。</li></ul><h2 id="4-说一说-ArrayList-的扩容机制吧"><a href="#4-说一说-ArrayList-的扩容机制吧" class="headerlink" title="4 说一说 ArrayList 的扩容机制吧"></a>4 说一说 ArrayList 的扩容机制吧</h2><p><a href="https://github.com/Snailclimb/JavaGuide/blob/master/docs/java/collection/ArrayList-Grow.md" target="_blank" rel="noopener">https://github.com/Snailclimb/JavaGuide/blob/master/docs/java/collection/ArrayList-Grow.md</a></p><h2 id="5-HashMap-和-Hashtable-的区别"><a href="#5-HashMap-和-Hashtable-的区别" class="headerlink" title="5 HashMap 和 Hashtable 的区别"></a>5 HashMap 和 Hashtable 的区别</h2><ul><li>线程是否安全： HashMap 是非线程安全的，HashTable 是线程安全的；HashTable 内部的方法基本都经过synchronized 修饰。（如果你要保证线程安全的话就使用 ConcurrentHashMap 吧！）；</li><li>效率： 因为线程安全的问题，HashMap 要比 HashTable 效率高一点。另外，HashTable 基本被淘汰，不要在代码中使用它；</li><li>对Null key 和Null value的支持： HashMap 中，null 可以作为键，这样的键只有一个，可以有一个或多个键所对应的值为 null。。但是在 HashTable 中 put 进的键值只要有一个 null，直接抛出 NullPointerException。</li><li>初始容量大小和每次扩充容量大小的不同 ： ①创建时如果不指定容量初始值，Hashtable 默认的初始大小为11，之后每次扩充，容量变为原来的2n+1。HashMap 默认的初始化大小为16。之后每次扩充，容量变为原来的2倍。②创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为2的幂次方大小（HashMap 中的tableSizeFor()方法保证）。也就是说 HashMap 总是使用2的幂作为哈希表的大小。</li><li>底层数据结构： JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间。Hashtable 没有这样的机制。</li></ul><h2 id="6-HashMap-和-HashSet区别"><a href="#6-HashMap-和-HashSet区别" class="headerlink" title="6 HashMap 和 HashSet区别"></a>6 HashMap 和 HashSet区别</h2><p><img src="/2020/01/14/2020-01-14-java-ji-he/pxwusgh5.bmp" alt></p><h2 id="7-HashSet如何检查重复"><a href="#7-HashSet如何检查重复" class="headerlink" title="7 HashSet如何检查重复"></a>7 HashSet如何检查重复</h2><ul><li>当你把对象加入HashSet时，HashSet会先计算对象的hashcode值来判断对象加入的位置，同时也会与其他加入的对象的hashcode值作比较，如果没有相符的hashcode，HashSet会假设对象没有重复出现。但是如果发现有相同hashcode值的对象，这时会调用equals（）方法来检查hashcode相等的对象是否真的相同。如果两者相同，HashSet就不会让加入操作成功。</li></ul><h2 id="8-HashMap的底层实现"><a href="#8-HashMap的底层实现" class="headerlink" title="8 HashMap的底层实现"></a>8 HashMap的底层实现</h2><ol><li>JDK1.8之前：</li></ol><ul><li>JDK1.8 之前 HashMap 底层是 数组和链表 结合在一起使用也就是 链表散列。HashMap 通过 key 的 hashCode 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) &amp; hash 判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。</li></ul><ol start="2"><li>JDK1.8之后：</li></ol><ul><li>相比于之前的版本， JDK1.8之后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间。</li></ul><p><img src="/2020/01/14/2020-01-14-java-ji-he/9xadadqa.bmp" alt></p><h2 id="9-HashMap-的长度为什么是2的幂次方"><a href="#9-HashMap-的长度为什么是2的幂次方" class="headerlink" title="9 HashMap 的长度为什么是2的幂次方"></a>9 HashMap 的长度为什么是2的幂次方</h2><ul><li>表达不清</li></ul><h2 id="10-HashMap-多线程操作导致死循环问题"><a href="#10-HashMap-多线程操作导致死循环问题" class="headerlink" title="10 HashMap 多线程操作导致死循环问题"></a>10 HashMap 多线程操作导致死循环问题</h2><ul><li>主要原因在于 并发下的Rehash 会造成元素之间会形成一个循环链表。</li></ul><h2 id="11-ConcurrentHashMap-和-Hashtable-的区别"><a href="#11-ConcurrentHashMap-和-Hashtable-的区别" class="headerlink" title="11 ConcurrentHashMap 和 Hashtable 的区别"></a>11 ConcurrentHashMap 和 Hashtable 的区别</h2><ul><li><p>底层数据结构： JDK1.7的 ConcurrentHashMap 底层采用 分段的数组+链表 实现，JDK1.8 采用的数据结构跟HashMap1.8的结构一样，数组+链表/红黑二叉树。Hashtable 和 JDK1.8 之前的 HashMap 的底层数据结构类似都是采用 数组+链表 的形式，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的；</p></li><li><p>实现线程安全的方式（重要）： ① 在JDK1.7的时候，ConcurrentHashMap（分段锁） 对整个桶数组进行了分割分段(Segment)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。 到了 JDK1.8 的时候已经摒弃了Segment的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并发控制使用 synchronized 和 CAS 来操作。（JDK1.6以后 对 synchronized锁做了很多优化） 整个看起来就像是优化过且线程安全的 HashMap，虽然在JDK1.8中还能看到 Segment 的数据结构，但是已经简化了属性，只是为了兼容旧版本；② Hashtable(同一把锁) :使用 synchronized 来保证线程安全，效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用 put 添加元素，另一个线程不能使用 put 添加元素，也不能使用 get，竞争会越来越激烈效率越低。</p><p>  <img src="/2020/01/14/2020-01-14-java-ji-he/onv1se4a.bmp" alt><br>  JDK1.7的ConcurrentHashMap：<br>  <img src="/2020/01/14/2020-01-14-java-ji-he/51db1fha.bmp" alt><br>  JDK1.8的ConcurrentHashMap（TreeBin: 红黑二叉树节点 Node: 链表节点）：</p><p>  <img src="/2020/01/14/2020-01-14-java-ji-he/m4r4v5mg.bmp" alt></p></li></ul><h2 id="12-ConcurrentHashMap线程安全的具体实现方式-底层具体实现"><a href="#12-ConcurrentHashMap线程安全的具体实现方式-底层具体实现" class="headerlink" title="12 ConcurrentHashMap线程安全的具体实现方式/底层具体实现"></a>12 ConcurrentHashMap线程安全的具体实现方式/底层具体实现</h2><ul><li><p>JDK1.7：首先将数据分为一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据时，其他段的数据也能被其他线程访问。<br>ConcurrentHashMap 是由 Segment 数组结构和 HashEntry 数组结构组成。HashEntry 用于存储键值对数据。</p><p>  一个 ConcurrentHashMap 里包含一个 Segment 数组。Segment 的结构和HashMap类似，是一种数组和链表结构，一个 Segment 包含一个 HashEntry 数组，每个 HashEntry 是一个链表结构的元素，每个 Segment 守护着一个HashEntry数组里的元素，当对 HashEntry 数组的数据进行修改时，必须首先获得对应的 Segment的锁。</p></li><li><p>JDK1.8：ConcurrentHashMap取消了Segment分段锁，采用CAS和synchronized来保证并发安全。数据结构跟HashMap1.8的结构类似，数组+链表/红黑二叉树。Java 8在链表长度超过一定阈值（8）时将链表（寻址时间复杂度为O(N)）转换为红黑树（寻址时间复杂度为O(log(N))）</p><p>  synchronized只锁定当前链表或红黑二叉树的首节点，这样只要hash不冲突，就不会产生并发，效率又提升N倍。</p><p>  <img src="/2020/01/14/2020-01-14-java-ji-he/rnu6liek.bmp" alt></p></li></ul><h3 id="个人话术"><a href="#个人话术" class="headerlink" title="个人话术"></a>个人话术</h3><ol><li>ConcurrentHashMap在JDK1.7时是按照分段锁Segment和HashEntry实现线程安全的。每个ConcurrentHashMap里面包含有一个Segment数组，Segment数组里面包含有HashEntry数组，hashEntry是一个链表结构的元素。当对某个 HashEntry 数组的数据进行修改时，必须首先获得对应的 Segment的锁。</li><li>ConcurrentHashMap在JDK1.8时取消了Segment分段锁，采用Node数组和CAS和synchronized来保证线程安全。synchronized只锁定当前链表或红黑二叉树的首节点，这样只要hash不冲突，就不会产生并发，效率又提升N倍。</li></ol><h2 id="13-HashMap的put方法的具体流程"><a href="#13-HashMap的put方法的具体流程" class="headerlink" title="13 HashMap的put方法的具体流程"></a>13 HashMap的put方法的具体流程</h2><p><img src="/2020/01/14/2020-01-14-java-ji-he/0010.png" alt="putVal方法执行流程图"></p>]]></content>
      
      
      <categories>
          
          <category> JAVA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 集合 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA基础</title>
      <link href="/2020/01/13/2020-01-13-java-ji-chu/"/>
      <url>/2020/01/13/2020-01-13-java-ji-chu/</url>
      
        <content type="html"><![CDATA[<h1 id="Java基础"><a href="#Java基础" class="headerlink" title="Java基础"></a>Java基础</h1><h2 id="1-面向对象和面向过程的区别"><a href="#1-面向对象和面向过程的区别" class="headerlink" title="1. 面向对象和面向过程的区别"></a>1. 面向对象和面向过程的区别</h2><ul><li>面向过程 ：<strong>面向过程性能比面向对象高</strong>。 因为类调用时需要实例化，开销比较大，比较消耗资源，所以当性能是最重要的考量因素的时候，比如单片机、嵌入式开发、Linux/Unix 等一般采用面向过程开发。但是，面向过程没有面向对象易维护、易复用、易扩展。</li><li>面向对象 ：<strong>面向对象易维护、易复用、易扩展</strong>。 因为面向对象有封装、继承、多态性的特性，所以可以设计出低耦合的系统，使系统更加灵活、更加易于维护。但是，面向对象性能比面向过程低。</li></ul><h2 id="2-Java-语言有哪些特点"><a href="#2-Java-语言有哪些特点" class="headerlink" title="2. Java 语言有哪些特点?"></a>2. Java 语言有哪些特点?</h2><ol><li>简单易学；</li><li>面向对象（封装，继承，多态）；</li><li>平台无关性（ Java 虚拟机实现平台无关性）；</li><li>可靠性；</li><li>安全性；</li><li>支持多线程（ C++ 语言没有内置的多线程机制，因此必须调用操作系统的多线程功能来进行多线程程序设计，而 Java 语言却提供了多线程支持）；</li><li>支持网络编程并且很方便（ Java 语言诞生本身就是为简化网络编程设计的，因此 Java 语言不仅支持网络编程而且很方便）；</li><li>编译与解释并存；</li></ol><h2 id="3-关于-JVM-JDK-和-JRE-最详细通俗的解答"><a href="#3-关于-JVM-JDK-和-JRE-最详细通俗的解答" class="headerlink" title="3. 关于 JVM JDK 和 JRE 最详细通俗的解答"></a>3. 关于 JVM JDK 和 JRE 最详细通俗的解答</h2><ul><li><p>JVM：<br>Java 虚拟机（JVM）是运行 Java 字节码的虚拟机。JVM 有针对不同系统的特定实现（Windows，Linux，macOS），目的是使用相同的字节码，它们都会给出相同的结果。</p></li><li><p>什么是字节码?采用字节码的好处是什么?</p><p>  在 Java 中，JVM 可以理解的代码就叫做字节码（即扩展名为 .class 的文件），它不面向任何特定的处理器，只面向虚拟机。Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。所以 Java 程序运行时比较高效，而且，由于字节码并不针对一种特定的机器，因此，Java 程序无须重新编译便可在多种不同操作系统的计算机上运行。</p></li><li><p>Java 程序从源代码到运行一般有下面 3 步：</p><p>  <img src="/2020/01/13/2020-01-13-java-ji-chu/ckdcdas6.bmp" alt></p><p>  总结：Java 虚拟机（JVM）是运行 Java 字节码的虚拟机。JVM 有针对不同系统的特定实现（Windows，Linux，macOS），目的是使用相同的字节码，它们都会给出相同的结果。字节码和不同系统的 JVM 实现是 Java 语言“一次编译，随处可以运行”的关键所在。</p></li><li><p>JDK 和 JRE：</p><p>  JDK 是 Java Development Kit，它是功能齐全的 Java SDK。它拥有 JRE 所拥有的一切，还有编译器（javac）和工具（如 javadoc 和 jdb）。它能够创建和编译程序。</p><p>  JRE 是 Java 运行时环境。它是运行已编译 Java 程序所需的所有内容的集合，包括 Java 虚拟机（JVM），Java 类库，java 命令和其他的一些基础构件。但是，它不能用于创建新程序。</p></li></ul><h2 id="4-Java-和-C-的区别"><a href="#4-Java-和-C-的区别" class="headerlink" title="4. Java 和 C++的区别?"></a>4. Java 和 C++的区别?</h2><ul><li>都是面向对象的语言，都支持封装、继承和多态</li><li>Java 不提供指针来直接访问内存，程序内存更加安全</li><li>Java 的类是单继承的，C++ 支持多重继承；虽然 Java 的类不可以多继承，但是接口可以多继承。</li><li>Java 有自动内存管理机制，不需要程序员手动释放无用内存</li></ul><h2 id="5-字符型常量和字符串常量的区别"><a href="#5-字符型常量和字符串常量的区别" class="headerlink" title="5. 字符型常量和字符串常量的区别?"></a>5. 字符型常量和字符串常量的区别?</h2><ol><li>形式上: 字符常量是单引号引起的一个字符; 字符串常量是双引号引起的若干个字符</li><li>含义上: 字符常量相当于一个整型值( ASCII 值),可以参加表达式运算; 字符串常量代表一个地址值(该字符串在内存中存放位置)</li><li>占内存大小 字符常量只占 2 个字节; 字符串常量占若干个字节 (注意： char 在 Java 中占两个字节)</li></ol><h2 id="6-构造器-Constructor-是否可被-override"><a href="#6-构造器-Constructor-是否可被-override" class="headerlink" title="6. 构造器 Constructor 是否可被 override?"></a>6. 构造器 Constructor 是否可被 override?</h2><ul><li>Constructor 不能被 override（重写）,但是可以 overload（重载）,所以你可以看到一个类中有多个构造函数的情况。</li></ul><h2 id="7-重载和重写的区别"><a href="#7-重载和重写的区别" class="headerlink" title="7. 重载和重写的区别"></a>7. 重载和重写的区别</h2><ol><li><p>重载:</p><p> 发生在同一个类中，方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同。</p></li><li><p>重写:</p><p> 重写是子类对父类的允许访问的方法的实现过程进行重新编写,发生在子类中，方法名、参数列表必须相同，返回值范围小于等于父类，抛出的异常范围小于等于父类，访问修饰符范围大于等于父类。另外，如果父类方法访问修饰符为 private 则子类就不能重写该方法。也就是说方法提供的行为改变，而方法的外貌并没有改变。</p></li></ol><h2 id="8-Java-面向对象编程三大特性-封装-继承-多态"><a href="#8-Java-面向对象编程三大特性-封装-继承-多态" class="headerlink" title="8. Java 面向对象编程三大特性: 封装 继承 多态"></a>8. Java 面向对象编程三大特性: 封装 继承 多态</h2><ul><li><p>封装</p><p>  封装把一个对象的属性私有化，同时提供一些可以被外界访问的属性的方法，如果属性不想被外界访问，我们大可不必提供方法给外界访问。但是如果一个类没有提供给外界访问的方法，那么这个类也没有什么意义了。</p></li><li><p>继承</p><p> 继承是使用已存在的类的定义作为基础建立新类的技术，新类的定义可以增加新的数据或新的功能，也可以用父类的功能，但不能选择性地继承父类。通过使用继承我们能够非常方便地复用以前的代码。</p><ol><li><p>子类拥有父类对象所有的属性和方法（包括私有属性和私有方法），但是父类中的私有属性和方法子类是无法访问，只是拥有</p></li><li><p>子类可以拥有自己属性和方法，即子类可以对父类进行扩展。</p></li><li><p>子类可以用自己的方式实现父类的方法。</p></li></ol></li><li><p>多态</p><p>  所谓多态就是指程序中定义的引用变量所指向的具体类型和通过该引用变量发出的方法调用在编程时并不确定，而是在程序运行期间才确定，即一个引用变量到底会指向哪个类的实例对象，该引用变量发出的方法调用到底是哪个类中实现的方法，必须在由程序运行期间才能决定。</p><p>  在 Java 中有两种形式可以实现多态：继承（多个子类对同一方法的重写）（继承抽象）和接口（实现接口并覆盖接口中同一方法）。</p></li><li><p>抽象</p></li></ul><h2 id="9-String-StringBuffer-和-StringBuilder-的区别是什么-String-为什么是不可变的"><a href="#9-String-StringBuffer-和-StringBuilder-的区别是什么-String-为什么是不可变的" class="headerlink" title="9. String StringBuffer 和 StringBuilder 的区别是什么? String 为什么是不可变的?"></a>9. String StringBuffer 和 StringBuilder 的区别是什么? String 为什么是不可变的?</h2><ul><li><p>可变性</p><p>  String 类中使用 final 关键字修饰字符数组来保存字符串，private final char value[]，所以 String 对象是不可变的。</p><p>  而 StringBuilder 与 StringBuffer 都继承自 AbstractStringBuilder 类，在 AbstractStringBuilder 中也是使用字符数组保存字符串char[]value 但是没有用 final 关键字修饰，所以这两种对象都是可变的。</p></li><li><p>线程安全性</p><p>   String 中的对象是不可变的，也就可以理解为常量，线程安全。。StringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。StringBuilder 并没有对方法进行加同步锁，所以是非线程安全的。</p></li><li><p>性能</p><p>  每次对 String 类型进行改变的时候，都会生成一个新的 String 对象，然后将指针指向新的 String 对象。StringBuffer 每次都会对 StringBuffer 对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用 StringBuilder 相比使用 StringBuffer 仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险。</p></li><li><p>对于三者使用的总结：装箱：将基本类型用它们对应的引用类型包装起来；</p><ol><li>操作少量的数据: 适用 String</li><li>单线程操作字符串缓冲区下操作大量数据: 适用 StringBuilder</li><li>多线程操作字符串缓冲区下操作大量数据: 适用 StringBuffer</li></ol></li></ul><h2 id="10-自动装箱与拆箱"><a href="#10-自动装箱与拆箱" class="headerlink" title="10. 自动装箱与拆箱"></a>10. 自动装箱与拆箱</h2><ul><li>装箱：将基本类型用它们对应的引用类型包装起来；</li><li>拆箱：将包装类型转换为基本数据类型；</li></ul><h2 id="11-在一个静态方法内调用一个非静态成员为什么是非法的"><a href="#11-在一个静态方法内调用一个非静态成员为什么是非法的" class="headerlink" title="11. 在一个静态方法内调用一个非静态成员为什么是非法的?"></a>11. 在一个静态方法内调用一个非静态成员为什么是非法的?</h2><p>由于静态方法可以不通过对象进行调用，因此在静态方法里，不能调用其他非静态变量，也不可以访问非静态变量成员。</p><h2 id="12-在-Java-中定义一个不做事且没有参数的构造方法的作用"><a href="#12-在-Java-中定义一个不做事且没有参数的构造方法的作用" class="headerlink" title="12. 在 Java 中定义一个不做事且没有参数的构造方法的作用"></a>12. 在 Java 中定义一个不做事且没有参数的构造方法的作用</h2><p>ava 程序在执行子类的构造方法之前，如果没有用 super()来调用父类特定的构造方法，则会调用父类中“没有参数的构造方法”。因此，如果父类中只定义了有参数的构造方法，而在子类的构造方法中又没有用 super()来调用父类中特定的构造方法，则编译时将发生错误，因为 Java 程序在父类中找不到没有参数的构造方法可供执行。解决办法是在父类里加上一个不做事且没有参数的构造方法。</p><h2 id="13-接口和抽象类的区别是什么？"><a href="#13-接口和抽象类的区别是什么？" class="headerlink" title="13. 接口和抽象类的区别是什么？"></a>13. 接口和抽象类的区别是什么？</h2><ol><li>接口的方法默认是 public，所有方法在接口中不能有实现(Java 8 开始接口方法可以有默认实现），而抽象类可以有非抽象的方法。</li><li>接口中除了 static、final 变量，不能有其他变量，而抽象类中则不一定。</li><li>一个类可以实现多个接口，但只能实现一个抽象类。接口自己本身可以通过 extends 关键字扩展多个接口。</li><li>接口方法默认修饰符是 public，抽象方法可以有 public、protected 和 default 这些修饰符（抽象方法就是为了被重写所以不能使用 private 关键字修饰！）。</li><li>从设计层面来说，抽象是对类的抽象，是一种模板设计，而接口是对行为的抽象，是一种行为的规范。</li></ol><h2 id="14-成员变量与局部变量的区别有哪些？"><a href="#14-成员变量与局部变量的区别有哪些？" class="headerlink" title="14. 成员变量与局部变量的区别有哪些？"></a>14. 成员变量与局部变量的区别有哪些？</h2><ul><li>从语法形式上看:成员变量是属于类的，而局部变量是在方法中定义的变量或是方法的参数；成员变量可以被 public,private,static 等修饰符所修饰，而局部变量不能被访问控制修饰符及 static 所修饰；但是，成员变量和局部变量都能被 final 所修饰。</li><li>从变量在内存中的存储方式来看:如果成员变量是使用static修饰的，那么这个成员变量是属于类的，如果没有使用static修饰，这个成员变量是属于实例的。而对象存在于堆内存，局部变量则存在于栈内存。</li><li>从变量在内存中的生存时间上看:成员变量是对象的一部分，它随着对象的创建而存在，而局部变量随着方法的调用而自动消失。</li><li>成员变量如果没有被赋初值:则会自动以类型的默认值而赋值（一种情况例外:被 final 修饰的成员变量也必须显式地赋值），而局部变量则不会自动赋值。</li></ul><h2 id="15-一个类的构造方法的作用是什么-若一个类没有声明构造方法，该程序能正确执行吗-为什么"><a href="#15-一个类的构造方法的作用是什么-若一个类没有声明构造方法，该程序能正确执行吗-为什么" class="headerlink" title="15. 一个类的构造方法的作用是什么? 若一个类没有声明构造方法，该程序能正确执行吗? 为什么?"></a>15. 一个类的构造方法的作用是什么? 若一个类没有声明构造方法，该程序能正确执行吗? 为什么?</h2><ul><li>主要作用是完成对类对象的初始化工作。可以执行。因为一个类即使没有声明构造方法也会有默认的不带参数的构造方法。</li></ul><h2 id="16-构造方法有哪些特性？"><a href="#16-构造方法有哪些特性？" class="headerlink" title="16. 构造方法有哪些特性？"></a>16. 构造方法有哪些特性？</h2><ul><li>名字与类名相同。</li><li>没有返回值，但不能用 void 声明构造函数。</li><li>生成类的对象时自动执行，无需调用。</li></ul><h2 id="17-静态方法和实例方法有何不同"><a href="#17-静态方法和实例方法有何不同" class="headerlink" title="17. 静态方法和实例方法有何不同"></a>17. 静态方法和实例方法有何不同</h2><ul><li>在外部调用静态方法时，可以使用”类名.方法名”的方式，也可以使用”对象名.方法名”的方式。而实例方法只有后面这种方式。也就是说，调用静态方法可以无需创建对象。</li><li>静态方法在访问本类的成员时，只允许访问静态成员（即静态成员变量和静态方法），而不允许访问实例成员变量和实例方法；实例方法则无此限制。</li></ul><h2 id="18-对象的相等与指向他们的引用相等-两者有什么不同"><a href="#18-对象的相等与指向他们的引用相等-两者有什么不同" class="headerlink" title="18. 对象的相等与指向他们的引用相等,两者有什么不同?"></a>18. 对象的相等与指向他们的引用相等,两者有什么不同?</h2><ul><li>对象的相等，比的是内存中存放的内容是否相等。而引用相等，比较的是他们指向的内存地址是否相等。</li></ul><h2 id="19-在调用子类构造方法之前会先调用父类没有参数的构造方法-其目的是"><a href="#19-在调用子类构造方法之前会先调用父类没有参数的构造方法-其目的是" class="headerlink" title="19. 在调用子类构造方法之前会先调用父类没有参数的构造方法,其目的是?"></a>19. 在调用子类构造方法之前会先调用父类没有参数的构造方法,其目的是?</h2><ul><li>帮助子类做初始化工作。</li></ul><h2 id="20-与-equals-重要"><a href="#20-与-equals-重要" class="headerlink" title="20. == 与 equals(重要)"></a><strong><strong>20. == 与 equals(重要)</strong></strong></h2><ul><li>== : 它的作用是判断两个对象的地址是不是相等。即，判断两个对象是不是同一个对象(基本数据类型==比较的是值，引用数据类型==比较的是内存地址)。</li><li>equals() : 它的作用也是判断两个对象是否相等。但它一般有两种使用情况：</li></ul><ol><li>情况 1：类没有覆盖 equals() 方法。则通过 equals() 比较该类的两个对象时，等价于通过“==”比较这两个对象。</li><li>情况 2：类覆盖了 equals() 方法。一般，我们都覆盖 equals() 方法来比较两个对象的内容是否相等；若它们的内容相等，则返回 true (即，认为这两个对象相等)。</li></ol><pre><code>public class test1 {    public static void main(String[] args) {        String a = new String(&quot;ab&quot;); // a 为一个引用        String b = new String(&quot;ab&quot;); // b为另一个引用,对象的内容一样        String aa = &quot;ab&quot;; // 放在常量池中        String bb = &quot;ab&quot;; // 从常量池中查找        if (aa == bb) // true            System.out.println(&quot;aa==bb&quot;);        if (a == b) // false，非同一对象            System.out.println(&quot;a==b&quot;);        if (a.equals(b)) // true            System.out.println(&quot;aEQb&quot;);        if (42 == 42.0) { // true            System.out.println(&quot;true&quot;);        }    }}</code></pre><p>说明：</p><ul><li>String 中的 equals 方法是被重写过的，因为 object 的 equals 方法是比较的对象的内存地址，而 String 的 equals 方法比较的是对象的值。</li><li>当创建 String 类型的对象时，虚拟机会在常量池中查找有没有已经存在的值和要创建的值相同的对象，如果有就把它赋给当前引用。如果没有就在常量池中重新创建一个 String 对象。</li></ul><h2 id="21-hashCode-与-equals-重要"><a href="#21-hashCode-与-equals-重要" class="headerlink" title="21. hashCode 与 equals (重要)"></a>21. hashCode 与 equals (重要)</h2><ol><li><p>hashCode（）介绍</p><p>  hashCode() 的作用是获取哈希码，也称为散列码；它实际上是返回一个 int 整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode() 定义在 JDK 的 Object.java 中，这就意味着 Java 中的任何类都包含有 hashCode() 函数。</p><p>  散列表存储的是键值对(key-value)，它的特点是：能根据“键”快速的检索出对应的“值”。这其中就利用到了散列码！（可以快速找到所需要的对象）</p></li><li><p>为什么要有 hashCode</p><p> 我们先以“HashSet 如何检查重复”为例子来说明为什么要有 hashCode： 当你把对象加入 HashSet 时，HashSet 会先计算对象的 hashcode 值来判断对象加入的位置，同时也会与该位置其他已经加入的对象的 hashcode 值作比较，如果没有相符的 hashcode，HashSet 会假设对象没有重复出现。但是如果发现有相同 hashcode 值的对象，这时会调用 equals()方法来检查 hashcode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让其加入操作成功。如果不同的话，就会重新散列到其他位置。（摘自我的 Java 启蒙书《Head first java》第二版）。这样我们就大大减少了 equals 的次数，相应就大大提高了执行速度。</p><p> 通过我们可以看出：hashCode() 的作用就是获取哈希码，也称为散列码；它实际上是返回一个 int 整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode()在散列表中才有用，在其它情况下没用。在散列表中 hashCode() 的作用是获取对象的散列码，进而确定该对象在散列表中的位置。</p></li><li><p>hashCode（）与 equals（）的相关规定</p></li></ol><ul><li>如果两个对象相等，则 hashcode 一定也是相同的</li><li>两个对象相等,对两个对象分别调用 equals 方法都返回 true</li><li>两个对象有相同的 hashcode 值，它们不一定是相等的</li><li>因此，equals 方法被覆盖过，则 hashCode 方法也必须被覆盖</li><li>hashCode() 的默认行为是对堆上的对象产生独特值。如果没有重写 hashCode()，则该 class 的两个对象无论如何都不会相等（即使这两个对象指向相同的数据）</li></ul><h2 id="22-简述线程、程序、进程的基本概念。以及他们之间关系是什么"><a href="#22-简述线程、程序、进程的基本概念。以及他们之间关系是什么" class="headerlink" title="22. 简述线程、程序、进程的基本概念。以及他们之间关系是什么?"></a>22. 简述线程、程序、进程的基本概念。以及他们之间关系是什么?</h2><ul><li>线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享同一块内存空间和一组系统资源，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。</li><li>程序是含有指令和数据的文件，被存储在磁盘或其他的数据存储设备中，也就是说程序是静态的代码。</li><li>进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。线程是进程划分成的更小的运行单位。线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。从另一角度来说，进程属于操作系统的范畴，主要是同一段时间内，可以同时执行一个以上的程序，而线程则是在同一程序内几乎同时执行一个以上的程序段。</li></ul><h2 id="23-线程有哪些基本状态"><a href="#23-线程有哪些基本状态" class="headerlink" title="23. 线程有哪些基本状态?"></a>23. 线程有哪些基本状态?</h2><p><img src="/2020/01/13/2020-01-13-java-ji-chu/h38o9hmv.bmp" alt></p><p><img src="/2020/01/13/2020-01-13-java-ji-chu/bm3jpu2y.bmp" alt></p><ul><li>线程创建之后它将处于 NEW（新建） 状态，调用 start() 方法后开始运行，线程这时候处于 READY（可运行） 状态。可运行状态的线程获得了 cpu 时间片（timeslice）后就处于 RUNNING（运行） 状态。</li><li>当线程执行 wait()方法之后，线程进入 WAITING（等待）状态。进入等待状态的线程需要依靠其他线程的通知才能够返回到运行状态，而 TIME_WAITING(超时等待) 状态相当于在等待状态的基础上增加了超时限制，比如通过 sleep（long millis）方法或 wait（long millis）方法可以将 Java 线程置于 TIMED WAITING 状态。当超时时间到达后 Java 线程将会返回到 RUNNABLE 状态。当线程调用同步方法时，在没有获取到锁的情况下，线程将会进入到 BLOCKED（阻塞） 状态。线程在执行 Runnable 的run()方法之后将会进入到 TERMINATED（终止） 状态。</li></ul><h2 id="24-关于-final-关键字的一些总结"><a href="#24-关于-final-关键字的一些总结" class="headerlink" title="24 关于 final 关键字的一些总结"></a>24 关于 final 关键字的一些总结</h2><ul><li>final 关键字主要用在三个地方：变量、方法、类。</li></ul><ol><li>对于一个 final 变量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改；如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象。</li><li>当用 final 修饰一个类时，表明这个类不能被继承。final 类中的所有成员方法都会被隐式地指定为 final 方法。</li><li>使用 final 方法的原因有两个。第一个原因是把方法锁定，以防任何继承类修改它的含义；第二个原因是效率。在早期的 Java 实现版本中，会将 final 方法转为内嵌调用。但是如果方法过于庞大，可能看不到内嵌调用带来的任何性能提升（现在的 Java 版本已经不需要使用 final 方法进行这些优化了）。类中所有的 private 方法都隐式地指定为 final。</li></ol><h2 id="25-Java-中的异常处理"><a href="#25-Java-中的异常处理" class="headerlink" title="25 Java 中的异常处理"></a>25 Java 中的异常处理</h2><ul><li><p>Java 异常类层次结构图<br><img src="/2020/01/13/2020-01-13-java-ji-chu/7ze0qjp9.bmp" alt></p></li><li><p>在 Java 中，所有的异常都有一个共同的祖先 java.lang 包中的 Throwable 类。Throwable： 有两个重要的子类：Exception（异常） 和 Error（错误） ，二者都是 Java 异常处理的重要子类，各自都包含大量子类。</p></li><li><p>Error（错误）:是程序无法处理的错误，表示运行应用程序中较严重问题。大多数错误与代码编写者执行的操作无关，而表示代码运行时 JVM（Java 虚拟机）出现的问题。例如，Java 虚拟机运行错误（Virtual MachineError），当 JVM 不再有继续执行操作所需的内存资源时，将出现 OutOfMemoryError。这些异常发生时，Java 虚拟机（JVM）一般会选择线程终止。</p></li><li><p>Exception（异常）:是程序本身可以处理的异常。Exception 类有一个重要的子类 RuntimeException。RuntimeException 异常由 Java 虚拟机抛出。NullPointerException（要访问的变量没有引用任何对象时，抛出该异常）、ArithmeticException（算术运算异常，一个整数除以 0 时，抛出该异常）和 ArrayIndexOutOfBoundsException （下标越界异常）。</p></li><li><p><strong>注意：异常和错误的区别：异常能被程序本身处理，错误是无法处理。</strong></p></li><li><p>Throwable 类常用方法:</p><ol><li>public string getMessage():返回异常发生时的简要描述</li><li>public string toString():返回异常发生时的详细信息</li><li>public string getLocalizedMessage():返回异常对象的本地化信息。使用 Throwable 的子类覆盖这个方法，可以生成本地化信息。如果子类没有覆盖该方法，则该方法返回的信息与 getMessage（）返回的结果相同</li><li>public void printStackTrace():在控制台上打印 Throwable 对象封装的异常信息</li></ol></li><li><p>异常处理总结</p><ol><li>try 块： 用于捕获异常。其后可接零个或多个 catch 块，如果没有 catch 块，则必须跟一个 finally 块。</li><li>catch 块： 用于处理 try 捕获到的异常。</li><li><strong>finally 块： 无论是否捕获或处理异常，finally 块里的语句都会被执行。当在 try 块或 catch 块中遇到 return 语句时，finally 语句块将在方法返回之前被执行。</strong></li></ol></li><li><p>finally 块： 无论是否捕获或处理异常，finally 块里的语句都会被执行。当在 try 块或 catch 块中遇到 return 语句时，finally 语句块将在方法返回之前被执行。</p><ol><li>在 finally 语句块第一行发生了异常。 因为在其他行，finally 块还是会得到执行</li><li>在前面的代码中用了 System.exit(int)已退出程序。 exit 是带参函数 ；若该语句在异常语句之后，finally 会执行</li></ol></li></ul><pre><code>    public static int f(int value) {        try {            return value * value;        } finally {            if (value == 2) {                return 0;            }        }    }    // 如果调用 f(2)，返回值将是 0，因为 finally 语句的返回值覆盖了 try 语句块的返回值。</code></pre><h2 id="26-Java-序列化中如果有些字段不想进行序列化，怎么办？"><a href="#26-Java-序列化中如果有些字段不想进行序列化，怎么办？" class="headerlink" title="26 Java 序列化中如果有些字段不想进行序列化，怎么办？"></a>26 Java 序列化中如果有些字段不想进行序列化，怎么办？</h2><ul><li>对于不想进行序列化的变量，使用 transient 关键字修饰。</li><li>transient 关键字的作用是：阻止实例中那些用此关键字修饰的的变量序列化；当对象被反序列化时，被 transient 修饰的变量值不会被持久化和恢复。transient 只能修饰变量，不能修饰类和方法。</li></ul><h2 id="27-获取用键盘输入常用的两种方法"><a href="#27-获取用键盘输入常用的两种方法" class="headerlink" title="27 获取用键盘输入常用的两种方法"></a>27 获取用键盘输入常用的两种方法</h2><ol><li>方法 1：通过 Scanner</li></ol><pre><code>Scanner input = new Scanner(System.in);String s  = input.nextLine();input.close();</code></pre><ol start="2"><li>方法 2：通过 BufferedReader</li></ol><pre><code>BufferedReader input = new BufferedReader(new InputStreamReader(System.in));String s = input.readLine();</code></pre><h2 id="28-Java-中-IO-流"><a href="#28-Java-中-IO-流" class="headerlink" title="28 Java 中 IO 流"></a>28 Java 中 IO 流</h2><ol><li>Java 中 IO 流分为几种?</li></ol><ul><li>按照流的流向分，可以分为输入流和输出流；</li><li>按照操作单元划分，可以划分为字节流和字符流；</li><li>按照流的角色划分为节点流和处理流。</li></ul><ol start="2"><li>Java I0 流的 40 多个类都是从如下 4 个抽象类基类中派生出来的。</li></ol><ul><li><strong>InputStream/Reader: 所有的输入流的基类，前者是字节输入流，后者是字符输入流。</strong></li><li><strong>OutputStream/Writer: 所有输出流的基类，前者是字节输出流，后者是字符输出流。</strong></li></ul><p><img src="/2020/01/13/2020-01-13-java-ji-chu/ep4ijzng.bmp" alt></p><p><img src="/2020/01/13/2020-01-13-java-ji-chu/755xfuro.bmp" alt></p><ol start="3"><li>既然有了字节流,为什么还要有字符流?</li></ol><ul><li>字符流是由 Java 虚拟机将字节转换得到的，问题就出在这个过程还算是非常耗时，并且，如果我们不知道编码类型就很容易出现乱码问题。所以， I/O 流就干脆提供了一个直接操作字符的接口，方便我们平时对字符进行流操作。如果音频文件、图片等媒体文件用字节流比较好，如果涉及到字符的话使用字符流比较好。</li></ul><ol start="4"><li>BIO,NIO,AIO 有什么区别?</li></ol><ul><li>BIO (Blocking I/O): 同步阻塞 I/O 模式，数据的读取写入必须阻塞在一个线程内等待其完成。在活动连接数不是特别高（小于单机 1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的 I/O 并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。但是，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。因此，我们需要一种更高效的 I/O 处理模型来应对更高的并发量。</li><li>NIO (Non-blocking/New I/O): NIO 是一种同步非阻塞的 I/O 模型，在 Java 1.4 中引入了 NIO 框架，对应 java.nio 包，提供了 Channel , Selector，Buffer 等抽象。NIO 中的 N 可以理解为 Non-blocking，不单纯是 New。它支持面向缓冲的，基于通道的 I/O 操作方法。 NIO 提供了与传统 BIO 模型中的 Socket 和 ServerSocket 相对应的 SocketChannel 和 ServerSocketChannel 两种不同的套接字通道实现,两种通道都支持阻塞和非阻塞两种模式。阻塞模式使用就像传统中的支持一样，比较简单，但是性能和可靠性都不好；非阻塞模式正好与之相反。对于低负载、低并发的应用程序，可以使用同步阻塞 I/O 来提升开发速率和更好的维护性；对于高负载、高并发的（网络）应用，应使用 NIO 的非阻塞模式来开发</li><li>AIO (Asynchronous I/O): AIO 也就是 NIO 2。在 Java 7 中引入了 NIO 的改进版 NIO 2,它是异步非阻塞的 IO 模型。异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。AIO 是异步 IO 的缩写，虽然 NIO 在网络操作中，提供了非阻塞的方法，但是 NIO 的 IO 行为还是同步的。对于 NIO 来说，我们的业务线程是在 IO 操作准备好时，得到通知，接着就由这个线程自行进行 IO 操作，IO 操作本身是同步的。查阅网上相关资料，我发现就目前来说 AIO 的应用还不是很广泛，Netty 之前也尝试使用过 AIO，不过又放弃了。</li></ul><h2 id="29-常见关键字总结-static-final-this-super"><a href="#29-常见关键字总结-static-final-this-super" class="headerlink" title="29. 常见关键字总结:static,final,this,super"></a>29. 常见关键字总结:static,final,this,super</h2><ol><li>final 关键字</li><li>static 关键字</li></ol><ul><li>修饰成员变量和成员方法: 被 static 修饰的成员属于类，不属于单个这个类的某个对象，被类中所有对象共享，可以并且建议通过类名调用。被static 声明的成员变量属于静态成员变量，静态变量 存放在 Java 内存区域的方法区。调用格式：类名.静态变量名 类名.静态方法名() </li><li>静态代码块: 静态代码块定义在类中方法外, 静态代码块在非静态代码块之前执行(静态代码块—&gt;非静态代码块—&gt;构造方法)。 该类不管创建多少对象，静态代码块只执行一次.</li></ul><ol start="3"><li>this 关键字</li></ol><ul><li>this关键字用于引用类的当前实例。</li></ul><ol start="4"><li>super 关键字<ul><li>super关键字用于从子类访问父类的变量和方法。</li><li>在构造器中使用 super（） 调用父类中的其他构造方法时，该语句必须处于构造器的首行，否则编译器会报错。另外，this 调用本类中的其他构造方法时，也要放在首行。</li><li>this、super不能用在static方法中。</li></ul></li></ol><h2 id="30-深拷贝-vs-浅拷贝"><a href="#30-深拷贝-vs-浅拷贝" class="headerlink" title="30. 深拷贝 vs 浅拷贝"></a>30. 深拷贝 vs 浅拷贝</h2><ul><li>浅拷贝：对基本数据类型进行值传递，对引用数据类型进行引用传递般的拷贝，此为浅拷贝。</li><li>深拷贝：对基本数据类型进行值传递，对引用数据类型，创建一个新的对象，并复制其内容，此为深拷贝。</li></ul>]]></content>
      
      
      <categories>
          
          <category> JAVA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JAVA基础 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
